
  Agenda (PySpark)
  ----------------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variables
  -> Spark SQL
	-> DataFrames API
  -> Spark Streaming
	-> Spark Streaming (DStreams API)
	-> Structured Streaming
  -> Introduction to Spark MLlib


  Materials
  ---------
	PDF Presentations
	Code Modules 
	Class Notes
	=> GitHub: https://github.com/ykanakaraju/pyspark
 
  Spark
  -----    
      -> Spark is a unified in-memory distributed computing engine/framework.  

      -> Spark is a Big Data analytics tool.

      -> Spark is written in 'Scala' programming language

      -> Spark is a polyglot
	  -> Spark apps can be written in Scala, Java, Python, R

      Cluster =>  Is a unified entity that contains sevaral nodes whose cumulative resources
      can be used to distribute our processing.

      In-memory => Can persist the intermediate results of tasks in RAM and subsequent task can read
      the persisted data and advance the computations. This makes the processing very fast
	
	-> Spark is 100x faster than MR if 100% in-memory conmputation is used. 
	-> Spark is 6 to 7x faster than MR evenof disk-based computation is used. 
	

      Spark Unified Stack
      -------------------
	=> Spark provides a consistent set of APIs to process different analytics workloads using
	   the same execution engine using simple programming constructs.

		Batch Analytics of unstructured data  	: Spark Core API
		Batch Analytics of structured data	: Spark SQL
		Streaming analytics (real time)		: Spark Streaming, Structured Streaming
		Predictive analytics (ML)		: Spark MLlib
		Graph Parallel computations		: Spark GraphX


   Spark Architecture
   ------------------
    1. Cluster Manager

	-> Application are submitted to CM
	-> Schedules the job
	-> Allocates resources (RAM & CPU cores) to the application

	=> Supports Spark Standalone, YARN, Mesos, Kubernetes

    2.  Driver
	
	-> Master process
	-> Contains a "SparkContext" (which is an application context)
	-> Manages the user-code and sends the tasks to the executors

	Deploy-Modes
	  -> Client mode : default, Driver runs on the client machine.
	  -> Cluster mode : Driver runs on one of the node in the cluster.

    3. Executors
	
	-> Are allocated by Cluster Manager
	-> Tasks are executed in the executors
	-> Send the status to the driver.

    4. SparkContext

	-> Is an application context
	-> Is the starting point of executor
	-> Is the link between the driver and the various tasks on the cluster.


   Getting started with PySpark
   ----------------------------

   1. Working in your Lab

	-> Launch PySpark shell
	   -> Open a terminal window
	   -> type "pyspark" command at the prompt.

   	-> Launch 'Jupyter Notebook' environment.
	   -> Open a terminal window
	   -> type "jupyter notebook" or "jupyter notebook --allow-root" command at the prompt.

   2. Setup pyspark environment on your personal machine. 

	-> Install 'Anaconda' on your machine
		URL: https://www.anaconda.com/products/distribution

	-> Setup PySpark to run on Spyder or Jupyter Notebooks
		-> Follow the instructions given in the shared document.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Signup to Databricks Community Edition (free account) 
		
	-> Sign up : https://databricks.com/try-databricks
	-> Log-in: https://community.cloud.databricks.com/login.html


   Spark Core API 
   ---------------
     -> Is the low-level API
     -> RDD based API


   RDD (Resilient distributed dataset)
   -----------------------------------
	-> Is the fundamental data abstraction of Spark Core API

	-> RDD is a collection of distributed in-memory partitions
             -> Each partition is a collection of objects

	-> RDDs are lazily evaluated.

	-> RDDs are immutable

   How to create RDDs ?
   -------------------

    Three ways to create RDDs

	1. From some external data file

	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

        2. From some programmatic collection

	    rdd2 = sc.parallelize( [3,2,1,3,4,7,5,6,7,8,9,0], 2)

        3. By applying transformations on existing RDDs

	    rdd3 = rdd2.map(lambda x: x*2)
        
   What can you do with an RDD ?
   ----------------------------
	
	Only two operations:

	1. Transformations
		-> Transformations return RDD
		-> Transformations does not cause execution
		-> Transformations create RDD lineage DAGs

	2. Actions
		-> Cause the execution of the RDD and produces some output.
		-> Converts the logical plan (lineage graph) into a physical plan. 

   RDD Lineage DAG
   ----------------
   RDD Lineage DAG is a logical plan of dependencies (hierarchy) that caused the creation of this RDD
   all the way from the very first RDD

   RDD lineage DAGs are created when we perform 'transformations'

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	     	lineage DAG of rdd1 : (4) rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
		lineage DAG of rdd2 : (4) rdd2 -> rdd1.flatMap -> sc.textFile

         rdd3 = rdd2.map(lambda x: (x, 1))
		lineage DAG of rdd3 : (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x,y: x + y)
		lineage DAG of rdd4 : (4) rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile


   RDD Persistence
   ---------------
	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )	--> instruction to spark to not delete RDD partitions 
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile(...)
	   	=> sc.textFile, t3, t5, t6 -> rdd6

        rdd7.collect() 
	rdd7 -> rdd6.t7 
	      => t7 -> rdd7	

	rdd6.unpersist()

        Storage-Levels
        --------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	--------

	-> rdd1.persist() -> in-memory persistence
	-> rdd.cache()	  -> in-memory persistence
	-> rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	-> rdd1.unpersist()


   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------
    -> Transformations return an RDD
    -> Transformations create RDD lineage DAGs
    -> Transformations does not cause execution


   1. map		P: U -> V
			object to object transformation
			Transforms each object to another outout object by applying the function
			input RDD: N object, output RDD: N object 

		 rddFile.map( lambda x: x.split(" ") ).collect()


   2. filter		P: U -> Boolean
			Only those object of the input RDD for which the function returns True will be in the output.
			input RDD: N object, output RDD: <= N object 

		rddFile.filter(lambda x: len(x) > 51).collect()

   3. glom		P: None
			Returns a list object per partition with all the objects of the partition.


		rdd1		rdd2 = rdd1.glom()

		P0: 3,2,1,4,5,7 -> glom -> P0: [3,2,1,4,5,7]
		P1: 5,2,4,7,8,9 -> glom -> P1: [5,2,4,7,8,9]
		P2: 9,0,7,0,4,5 -> glom -> P2: [9,0,7,0,4,5]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function.
			input RDD: N object, output RDD: >= N object 	

	 rddWords = rddFile.flatMap(lambda x: x.split(" "))

	
  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )  

		P0: 3,2,1,4,5,7 -> mapPartitions -> P0: 22
		P1: 5,2,4,7,8,9 -> mapPartitions -> P1: 35
		P2: 9,0,7,0,4,5 -> mapPartitions -> P2: 25

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get the partition-index as additional parameter

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


  7. distinct		P: None, optional: numPartitions
			Returns distinct objects of the RDD
			input RDD: N object, output RDD: <= N object 

		rddWords.distinct().glom().collect()
		rddWords.distinct(4).glom().collect()


  Two types of RDDs
  -----------------
	-> Generic RDDs : RDD[U] 
	-> Pair RDDs	: RDD[(U, V)]


  8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the value part of the (K, V) pairs by applying the function. 

		rdd2.mapValues(lambda x: x*10).collect()

  9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the elements of the RDD based on the function output. 	

		rdd1.sortBy(lambda x: x > 5).glom().collect()
		rdd1.sortBy(lambda x: x > 5, False).glom().collect()
		rdd1.sortBy(lambda x: x > 5, True, 2).glom().collect()

  10. groupBy		P: U -> V, Optional: numPartitions
			Groups the objects based on the function output.
			Returns a Pair RDD, where :
			   key: each unique value of function output
			   value: ResultIterable of the objects of the RDD that produced the key.


		output = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
           			.flatMap(lambda x: x.split(" ")) \
           			.groupBy(lambda x: x) \
           			.mapValues(len) \
           			.sortBy(lambda x: x[1], False, 1)

  11. randomSplit	P: list of ratios (e.g. [0.6, 0.4])  Optional: seed
			Splits the RDD into multiple RDDs randomly in specified ratios

		rddList = rdd1.randomSplit([0.6, 0.4])
		rddList = rdd1.randomSplit([0.6, 0.4], 5757)  # here 5757 is a seed

  12. repartition	P: numPartitions
			Is used to increase or decrease the number of partitions
			Causes global shuffle.

  13. coalesce		P: numPartitions
			Is used to only decrease the number of partitions
			Causes partition merging.


	Recommandations
	---------------
	1. The size of each partition should be around 128 MB
	2. The number of partitions should be a multiple of number of cores.
	3. If the number of partitions is less than but close to 2000, bump it up to 2000
	4. The number of cores in each executor should be 5


  14. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				numPartitons
	--------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide 

  15. partitionBy	P: numPartitions, Optional: partioning-function (default: hash)
			Applied only on Pair RDDs
			Is used to control which keys go to which partitions.
			

transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]


def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;
    

rdd1 = sc.parallelize(transactions) \
        .map(lambda d: (d['city'], d)) \
        .partitionBy(4, custom_partitioner)


rdd1.glom().collect()


  ..ByKey Transformations 
  -----------------------
      	=> Are all wide transformations
	=> Are applied only to pair RDDs.

    16. sortByKey	P: None, Optional: ascending (True/False), numPartitions	
			Sort the RDD by the key

		rddPairs.sortByKey()
		rddPairs.sortByKey(False)
		rddPairs.sortByKey(True, 2)


   17. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where 
			   key: is unique keys of the RDDs
			   values: grouped values (ResultIterable) having the same key
			Cause global shuffle. Aviod if possible. 

		output = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
           		.flatMap(lambda x: x.split(" ")) \
           		.map(lambda x: (x,1)) \
           		.groupByKey() \
           		.mapValues(len)

   18. reduceByKey	P: (U, U) -> U,  Optional: numPartitions
			Reduces all the values of each unique key within every partition and then across partitions
			by iterativly applying the reduce function. 

		output = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
           		.flatMap(lambda x: x.split(" ")) \
           		.map(lambda x: (x, 1)) \
           		.reduceByKey(lambda x, y: x + y, 1)

  19. aggregateByKey   Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])


   20. joins		=> join (inner-join), leftOuterJoin, rightOuterJoin, fullOuterJoin
			   Performed on two pair RDDs

			   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup		=> Is used when you want to join RDDs with possibly duplicate keys and 
			   you want unique keys in the output

			    groupByKey (on each RDD) -> fullOuterJoin 


		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> groupByKey: [(key1, [10, 7]) (key2, [12,6]) (key3, [6])]

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> groupByKey: [(key1, [5, 17]) (key2, [4,7]) (key4, [17])]

		cogroup => (key1, ([10, 7],[5, 17])) (key2, ([12,6],[4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  ------------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P : (U, U) -> U
			Reduces the entire RDD into one final value of the same type by iterativly applying the
			function on each partition (narrow op) and then across partitions (wide-op)

		rdd1		rdd1.reduce(lambda x, y: x - y)

		P0: 9, 5, 3, 2, 1, 0  -> reduce -> -2 => 7
		P1: 9, 4, 3, 2, 1, 0  -> reduce -> -1
		P2: 8, 6, 4, 3, 2, 1  -> reduce -> -8

		rdd1.reduce(lambda x, y: x - y)

   5. aggregate   

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

   6. take
		rdd1.take(10)

   7. takeOrdered
		rdd1.takeOrdered(15)
		rddWords.takeOrdered(15, len)

   8. takeSample
		rdd1.takeSample(True, 15)
		rdd1.takeSample(True, 150, 98898)

		rdd1.takeSample(False, 50)
		rdd1.takeSample(False, 15, 98898)

   9.  countByValue

   10. countByKey

   11. first

   12. foreach		P: function that does not return anything but applied to all objects of the RDD.

   13. saveAsSequenceFile

	
   Use-Case
   ========

    dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

   From cars.tsv dataset, find the average weight of each make of American origin cars
   -> Arrange the data in the DESC order of average weight
   -> Save the output as a single text file.   
	
    => Try to solve it yourself..



  Closure
  --------
    
     A closure is all the code (variables and functions) that must be visible inside an executor
     for the tasks to perform their computation on the RDDs. 

     -> The closure is serialized and a separate copy is sent to every executor

	c = 0

        def isPrime( n ):
		returns True if n is Prime
		returns False if n is not Prime

	def f1( n ):
		global c
		if (isPrime(n)) c += 1
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)	# c = 0

	
	Limitation: You can not use local variables to implement global counters. 
		    Local varibles has only local scope within a task as they are local copies in the closure. 

	Solution: Use 'Accumulator' variable.


   Shared Variables
   ----------------

    1. Accumulator variable

	-> Is a shared variable shared by all the tasks
	-> Is not part of closure and hence not a local copy inside a task
	-> Is a single copy maintained by the driver
	-> All the tasks can add this single copy of the variable 
	-> This variable is maintained at the driver side
	-> Used to implement global counters. 


	c = sc.accumulator(0)

        def isPrime( n ):
		returns True if n is Prime
		returns False if n is not Prime

	def f1( n ):
		global c
		if (isPrime(n)) c.add(1)
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)	# c = 80


  2. Broadcast variable

	=> We can convert large immutable collection as broadcast variables
	-> These are not part of closure
	-> ONly one copy per executor is broadcasted by the driver to every executor
	-> All the tasks in that executor can refer to that single variable
        -> This saves a lot of memory. 

	d = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, 6: f, 7: g, ....})      #100 MB

	def f1(n):
		global d
		return d.value[n]

	rdd1 = sc.parallelize( [1,2,3,4,5,6,.....], 4 )
	rdd2 = rdd1.map( f1 )
	
	rdd2.collect()

  ========================================================================

   spark-submit 
   ------------
	-> Is a single command that is used to submit any spark program (written in python, scala, java, R)
	   to any cluster manager (standalone, yarn, mesos, kebernetes).

	
	spark-submit [options] <app jar | python file | R file> [app arguments]
  	
	=> bin/spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 4
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		E:\PySpark\wordcount.py [app args]				
	

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py	
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wordcount 2


  ====================================================================
     Spark SQL  (pyspark.sql) 
  ====================================================================
 
   -> High level API built on top of Spark Core API
    
   -> Spark's strcutured data processing API

	  Structured file formats:  Parquet (default), ORC, JSON, CSV (delimited text files)
	  JDBC format: RDBMS, NoSQL
	  Hive: Data warehousing platform for Hadoop

    -> SparkSession
	-> Represents user-session within an application 
	-> Can have multiple user-sessions in an application
	-> Starting point of execution for Spark SQL programing. 
	-> Introducing from SPark 2.0 onwards
    

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()  


   -> DataFrames (DF)
	-> Data abstraction of Spark SQL
	-> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	
	-> DF is a collection of Rows
		-> Row contains a group of Columns that are stored using Spark SQL internal types.

	-> Two components:
		1. data     	: collection of Rows
		2. schema	: StructType object

		StructType(
		   List(
		    	StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)
	
   Basic steps in a Spark SQL program
   ----------------------------------

	1. Read/load data from some data source into a DataFrame

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)
  
		df1.show()
		df1.printSchema()

	2. Apply transformations on the DF using DataFrame API methods or using SQL

		Using DataFrame Transformation methods:
		--------------------------------------

			df2 = df1.select("userid", "name", "gender", "age") \
        			.where("age is not null") \
        			.orderBy("gender", "age") \
        			.groupBy("age").count() \
        			.limit(4)

		Using SQL
		---------
			df1.createOrReplaceTempView("users")

			qry = """select age, count(*) as count
         			from users
         			where age is not null
         			group by age
         			order by age
         			limit 4"""
         
			 df3 = spark.sql(qry)
			 df3.show()


	3. Write/save the DF to a structured destination. 

		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)


  Save Modes
  ----------
     -> By default, writing to an existing directory resultsing Exception.

	-> ignore
	-> append
	-> overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


  DataFrame Transformations
  -------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                  column("DEST_COUNTRY_NAME").alias("destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

  2. where / filter
	
	df3 = df2.where("domestic = false and count > 500")
	df3 = df2.filter("domestic = false and count > 500")

	df3 = df2.filter( df2["count"] > 500 )

  3. orderBy  / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))
	df3 = df2.sort(desc("count"), asc("origin"))

  4. groupBy  (with aggregation methods) -> returns a 'pyspark.sql.group.GroupedData' object

	df3 = df2.groupBy("domestic", "highFrequency").count()
	df3 = df2.groupBy("domestic", "highFrequency").max("count")
	df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	df3 = df2.groupBy("domestic", "highFrequency").avg("count")

	df3 = df2.groupBy("domestic", "highFrequency") \
        	 .agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		avg("count").alias("avg"))

  5. limit

	  df2 = df1.limit(10)

  6. selectExpr

		df2 = df1.selectExpr( "ORIGIN_COUNTRY_NAME as origin", 
                  "DEST_COUNTRY_NAME as destination",
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

		IS SAME AS:

		df2 = df1.select( expr("ORIGIN_COUNTRY_NAME as origin"), 
                  expr("DEST_COUNTRY_NAME as destination"),
                  expr("count"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

   7. withColumn

		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", col("count") > 200) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        		.withColumn("count", col("count").cast("int"))

   8. withColumnRenamed
		
		df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", col("count") > 200) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        		.withColumn("count", col("count").cast("int")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")



  Working with different file formats
  -----------------------------------

      JSON	
	    read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	    write
		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)

      Parquet (default)
	    read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.parquet(inputPath)

	    write
		df2.write.format("parquet").save(outputPath)
		df2.write.parquet(outputPath)

      ORC
	    read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.orc(inputPath)

	    write
		df2.write.format("orc").save(outputPath)
		df2.write.orc(outputPath)

      CSV (delimited text file)
	    read
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputPath)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)

		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	    write
		df2.write.mode("overwrite").csv(outputPath, header=True)
		df2.write.mode("overwrite").csv(outputPath, header=True, sep="|")
		

    Creating an RDD from DataFrame
    ------------------------------
	
	rdd1 = df1.rdd
	rdd1.take(5)


    Creating a DataFrame from programmatic data
    -------------------------------------------     
	listUsers = [(1, "Raju", 5),
             	(2, "Ramesh", 15),
             	(3, "Rajesh", 18),
             	(4, "Raghu", 35),
             	(5, "Ramya", 25),
             	(6, "Radhika", 35),
             	(7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

 
    Creating a DataFrame from RDD
    ------------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])


    Creating a DataFrame with programmatic schema
    ---------------------------------------------   

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)
      
        ---------------------------------------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	inputPath = "E:\\PySpark\data\\flight-data\\json\\2015-summary.json"

	df1 = spark.read.json(inputPath, schema = mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)






