
  Agenda (PySpark)
  ----------------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variables
  -> Spark SQL
	-> DataFrames API
  -> Spark Streaming
	-> Spark Streaming (DStreams API)
	-> Structured Streaming
  -> Introduction to Spark MLlib


  Materials
  ---------
	PDF Presentations
	Code Modules 
	Class Notes
	=> GitHub: https://github.com/ykanakaraju/pyspark
 
  Spark
  -----    
      -> Spark is a unified in-memory distributed computing engine/framework.  

      -> Spark is a Big Data analytics tool.

      -> Spark is written in 'Scala' programming language

      -> Spark is a polyglot
	  -> Spark apps can be written in Scala, Java, Python, R

      Cluster =>  Is a unified entity that contains sevaral nodes whose cumulative resources
      can be used to distribute our processing.

      In-memory => Can persist the intermediate results of tasks in RAM and subsequent task can read
      the persisted data and advance the computations. This makes the processing very fast
	
	-> Spark is 100x faster than MR if 100% in-memory conmputation is used. 
	-> Spark is 6 to 7x faster than MR evenof disk-based computation is used. 
	

      Spark Unified Stack
      -------------------
	=> Spark provides a consistent set of APIs to process different analytics workloads using
	   the same execution engine using simple programming constructs.

		Batch Analytics of unstructured data  	: Spark Core API
		Batch Analytics of structured data	: Spark SQL
		Streaming analytics (real time)		: Spark Streaming, Structured Streaming
		Predictive analytics (ML)		: Spark MLlib
		Graph Parallel computations		: Spark GraphX


   Spark Architecture
   ------------------
    1. Cluster Manager

	-> Application are submitted to CM
	-> Schedules the job
	-> Allocates resources (RAM & CPU cores) to the application

	=> Supports Spark Standalone, YARN, Mesos, Kubernetes

    2.  Driver
	
	-> Master process
	-> Contains a "SparkContext" (which is an application context)
	-> Manages the user-code and sends the tasks to the executors

	Deploy-Modes
	  -> Client mode : default, Driver runs on the client machine.
	  -> Cluster mode : Driver runs on one of the node in the cluster.

    3. Executors
	
	-> Are allocated by Cluster Manager
	-> Tasks are executed in the executors
	-> Send the status to the driver.

    4. SparkContext

	-> Is an application context
	-> Is the starting point of executor
	-> Is the link between the driver and the various tasks on the cluster.


   Getting started with PySpark
   ----------------------------

   1. Working in your Lab

	-> Launch PySpark shell
	   -> Open a terminal window
	   -> type "pyspark" command at the prompt.

   	-> Launch 'Jupyter Notebook' environment.
	   -> Open a terminal window
	   -> type "jupyter notebook" or "jupyter notebook --allow-root" command at the prompt.

   2. Setup pyspark environment on your personal machine. 

	-> Install 'Anaconda' on your machine
		URL: https://www.anaconda.com/products/distribution

	-> Setup PySpark to run on Spyder or Jupyter Notebooks
		-> Follow the instructions given in the shared document.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Signup to Databricks Community Edition (free account) 
		
	-> Sign up : https://databricks.com/try-databricks
	-> Log-in: https://community.cloud.databricks.com/login.html


   Spark Core API 
   ---------------
     -> Is the low-level API
     -> RDD based API


   RDD (Resilient distributed dataset)
   -----------------------------------
	-> Is the fundamental data abstraction of Spark Core API

	-> RDD is a collection of distributed in-memory partitions
             -> Each partition is a collection of objects

	-> RDDs are lazily evaluated.

	-> RDDs are immutable

   How to create RDDs ?
   -------------------

    Three ways to create RDDs

	1. From some external data file

	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

        2. From some programmatic collection

	    rdd2 = sc.parallelize( [3,2,1,3,4,7,5,6,7,8,9,0], 2)

        3. By applying transformations on existing RDDs

	    rdd3 = rdd2.map(lambda x: x*2)
        
   What can you do with an RDD ?
   ----------------------------
	
	Only two operations:

	1. Transformations
		-> Transformations return RDD
		-> Transformations does not cause execution
		-> Transformations create RDD lineage DAGs

	2. Actions
		-> Cause the execution of the RDD and produces some output.
		-> Converts the logical plan (lineage graph) into a physical plan. 

   RDD Lineage DAG
   ----------------
   RDD Lineage DAG is a logical plan of dependencies (hierarchy) that caused the creation of this RDD
   all the way from the very first RDD

   RDD lineage DAGs are created when we perform 'transformations'

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	     	lineage DAG of rdd1 : (4) rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
		lineage DAG of rdd2 : (4) rdd2 -> rdd1.flatMap -> sc.textFile

         rdd3 = rdd2.map(lambda x: (x, 1))
		lineage DAG of rdd3 : (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x,y: x + y)
		lineage DAG of rdd4 : (4) rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )	--> instruction to spark to not delete RDD partitions 
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile(...)
	   	=> sc.textFile, t3, t5, t6 -> rdd6

        rdd7.collect() 
	rdd7 -> rdd6.t7 
	      => t7 -> rdd7	

	rdd6.unpersist()

        Storage-Levels
        --------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	--------

	-> rdd1.persist() -> in-memory persistence
	-> rdd.cache()	  -> in-memory persistence
	-> rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	-> rdd1.unpersist()


   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------
    -> Transformations return an RDD
    -> Transformations create RDD lineage DAGs
    -> Transformations does not cause execution


   1. map		P: U -> V
			object to object transformation
			Transforms each object to another outout object by applying the function
			input RDD: N object, output RDD: N object 

		 rddFile.map( lambda x: x.split(" ") ).collect()


   2. filter		P: U -> Boolean
			Only those object of the input RDD for which the function returns True will be in the output.
			input RDD: N object, output RDD: <= N object 

		rddFile.filter(lambda x: len(x) > 51).collect()

   3. glom		P: None
			Returns a list object per partition with all the objects of the partition.


		rdd1		rdd2 = rdd1.glom()

		P0: 3,2,1,4,5,7 -> glom -> P0: [3,2,1,4,5,7]
		P1: 5,2,4,7,8,9 -> glom -> P1: [5,2,4,7,8,9]
		P2: 9,0,7,0,4,5 -> glom -> P2: [9,0,7,0,4,5]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function.
			input RDD: N object, output RDD: >= N object 	

	 rddWords = rddFile.flatMap(lambda x: x.split(" "))

	
  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )  

		P0: 3,2,1,4,5,7 -> mapPartitions -> P0: 22
		P1: 5,2,4,7,8,9 -> mapPartitions -> P1: 35
		P2: 9,0,7,0,4,5 -> mapPartitions -> P2: 25

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get the partition-index as additional parameter

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


  7. distinct			P: None, optional: numPartitions
				Returns distinct objects of the RDD
				input RDD: N object, output RDD: <= N object 

		rddWords.distinct().glom().collect()
		rddWords.distinct(4).glom().collect()


  Two types of RDDs
  -----------------
	-> Generic RDDs : RDD[U] 
	-> Pair RDDs	: RDD[(U, V)]

  8. mapValues			P: U -> V
				Applied only on Pair RDDs
				Transforms the value part of the (K, V) pairs by applying the function. 

		rdd2.mapValues(lambda x: x*10).collect()

  9. sortBy


  10. groupBy











