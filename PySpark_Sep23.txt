
  Agenda
  ------

   Prerequisite: Python

   	-> Spark Basics, Architecture
	-> Spark Core API (Low Level API)
		-> RDDs - RDD Transformations & Actions
	-> Spark SQL 
	-> Spark MLlib (Machine Learning)
	-> Introduction to Spark Streaming (Real-time analytics)

  ----------------------------------------------------

     Materials
	-> PDF versions of the presentations
	-> Daily class notes 
	-> Code modules
	-> GitHub: https://github.com/ykanakaraju/pyspark

 ---------------------------------------------------

  Big Data
  --------   
     -> Large and Complex Data
     -> Data that is hard to store and process using traditional technologies that rely on single machines.

	How ig Big Data characterized:
	-> Volume
	-> Velocity
	-> Variety (unstructured & semi-structured data)
	-> Veracity
	-> Value
	
    -> A single machine become a limitation, the solution is to use the combined resources of many machines
       This is called a cluster.


    Computing Cluster	
    -----------------
	-> Is a unified entity consisting of many nodes (machines) whose cumulative resources 
	   (Disk, RAM, CPU Cores) to store and process big data.

    Hadoop
    ------
	-> Is a opensource framework for storing and processing bigdata using a computing 
	   cluster made of commodity hardware. 

	   -> HDFS (Hadoop Distributed File System) : Distributed Storage Framework. 
	   	-> Data is distributed among many machines as blocks of 128MB each
			-> 10 GB file is stored as 80 blocks spread on many nodes.
		-> For each block, 3 replicas are created (by default)
			-> Hence, to store a 10GB file on HDFS, we need 30GB disk space.

	  -> MapReduce -> Distributed computing Framework
		-> MR application contains two classes called Mapper & Reducer
		-> Many instances of the mapper are launched parallelly in the containers allocated 
		   on many nodes in the cluster.
		-> The intermediate outputs produced by the mappers are written to disk (local disk)
		-> This data is collected by the framework (shuffling) and then sent to the Reducer class
		-> The Reducer aggregates all these results and produce final output.

	  -> YARN -> Cluster Manager (or Resource Manager)
		-> Accepts the jobs and allocates resource containers to those jobs across lot of nodes
		-> The containers are allocated on the machine where the data blocks of the file are located.
	
   What is Spark?
   --------------
	-> Written in Scala language.
	-> Is a unified in-memory distributed/cluster computing framework.
	-> Spark supports multiple languages
		-> Scala, Java, Python, R 

	* in-memory distributed computing framework 
		-> The intermediate outputs of distributed tasks can be persisted in-memory (RAM)
		-> The subsequent tasks can operate on these in-memory persisted outputs. 
		
	
   Spark Unified Framework
   -----------------------
	-> Spark provides a consistent set of APIs (libraries) running on the same execution engine 
	   for processing different types of analytics workloads.	

	  	Batch Analytics on unstructred data	 -> Spark Core API (RDD)
		Batch Analytics on structured data       -> Spark SQL
		Streaming Analytics (real time process)	 -> Spark Streaming, Structred Streaming
		Predictive Analytics (Machine Learning)	 -> Spark MLlib
		Graph Parallel Computations		 -> Spark GraphX


   Spark Architecture & Building Blocks
   ------------------------------------        
	1. Cluster Manager

		-> CM receives the job sumissions
		-> Allocates executor containers to the applications
		-> Supports Spark Standalone Scheduler, YARN, Mesos, Kubernetes.

	2. Driver Process
		-> Whenever an application is launched, the driver process is created first
		-> master process which analysis the user-code and send tasks to be executed in the executors.
		-> Driver process contains a "SparkContext" object.

		Deploy-Modes
                ------------
		1. Client Mode (default)  -> The driver	runs on the client machine
		2. Cluster Mode		  -> The driver runs in one of the nodes inside the cluster	


	3. SparkContext
		-> Represents an application and a connection to the cluster.
		-> Link between the driver process and various tasks running on the cluster
		-> Starting point og any SparkCore application
		    NOTE: In case of Spark SQL, we use a "SparkSession" in place of SparkContext.

	4. Executors
		-> We have many executors allocated by Cluster manager and multiple processes
		   run with in each executor
		-> Driver sends tasks to be executed in the executors by analyzing the user-code
		-> All tasks does the same function, but on different partitions. 
		-> Executors report the status of the tasks to the driver.


    Getting started with PySpark
    ----------------------------

     1. Installing Spark locally and work with PySpark shell

	1.1 Download Spark from the URL: https://spark.apache.org/downloads.html
	    and extract it to a suitable folder.

	1.2 Setup the environment variables
		SPARK_HOME  	-> spark installation location
		HADOOP_HOME	-> spark installation location
		PATH		-> Add the <Spark>\bin 
	

    2. Using some IDE such as Spyder or Jupiter Notebook (or PyCharm)
	
	2.1 Install Anaconda Navigator
		https://docs.anaconda.com/anaconda/install/windows/

	2.2 To setup PySpark with Spyder or Jupyter Notebook, follow the instructions in the
	    document shared in the github.

    3. Using Databricks Community Edition Account (***)

	-> Signup to Databricks Community Edition Account
		=> https://databricks.com/try-databricks
		=> Read "QuickStart Tutorial"   		
	
	
    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> Fundamental data abstraction of Spark Core API

	-> RDD is a set of distributed in-memory partitions
	    -> Partition is a collection of objects
	    -> If you load a textfile into an RDD, you will have an RDD of Strings where each line
	       of the textfile is one object.

	-> RDD has two components:
		
		1. Meta Data : Lineage DAG (numPartitions, Persistence level..)
			       Maintained by the driver.

		2. Data : in-memory partitions

	-> RDD partitions are immutable.

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution.
		-> Execution is trigger by action commands only. 

	-> RDDs are resilient
		-> RDDs can automatically recreate missing partitions by reexecuting the tasks.


    Creating RDD
    ------------

	-> 3 ways

	1. Create an RDD from some external file.
		
		filePath = "E:\\Spark\\wordcount.txt"
		rddFile = sc.textFile( filePath  )
		rddFile = sc.textFile( filePath, 4 )

		-> the default number of partitions is given by the value of sc.defaultMinPartitions

	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize( range(1, 101) )
		rdd2 = sc.parallelize( range(1, 101), 3 )  // rdd2 will have 3 partitions

		-> the default number of partitions is given by the value of sc.defaultParallelism
		-> sc.defaultParallelism = total number of cores allocated to the application

	3. By applying transformations on existing RDDs, we can create new RDDs.

		rdd2 = rdd1.map(lambda x: x.upper())

	
   NOTE: rddFile.getNumPartitions()  -> get the partition count of an RDD



   What can we do with an RDD ?
   ----------------------------

	Only two things:

	1. Transformations
	   -> The output of a transformation is an RDD
	   -> Does not cause execution. 
	   -> They only cause the creation of Lineage DAG (at the driver)

	2. Actions
	   -> Produce some output by triggering excution on the RDDs
	   -> Cause execution.


   RDD Lineage DAG
   ---------------
	Lineage refers the a DAG of all dependencies all the way from the very first RDD 
	which caused the creation of this RDD.

	-> Lineage is a "Logical Plab"

	rddFile = sc.textFile( filePath, 4 )
	Lineage: rddFile -> sc.textFile

	rdd2 = rdd1.map(lambda x: x+10)
	Lineage: rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.filter(lambda x: x > 18)
	Lineage: rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

	rdd4 = rdd3.map(lambda x: [x, x+10])
	Lineage: rdd4 -> rdd3.map -> rdd2.filter -> rdd1.map -> sc.textFile

	rdd4.collect()
	Physical Plan => sc.textFile (rddFile) -> map (rdd2) -> filter (rdd3) -> map (rdd4) --> collect()





   Types of Transformations
   ------------------------

	1. Narrow Transformations
		-> Does not cause shuffling of the data from one partition to other partitions
		-> Partition to partition transformations
		-> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
		-> Causes shuffling of the data
		-> One output partition may need data from multiple input partitions
		-> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( .... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd1.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd3.t6(..)
	rdd7 = rdd6.t7(..)
	rdd7.persist(StorageLevel.MEMORY_ONLY)   -----> instruction to not GC the rdd7. save the partitions.
	rdd8 = rdd7.t8(..)

	rdd7.collect()

	lineage of rdd7:  rdd7 -> rdd6.t7 -> rdd3.t6 -> rdd1.t3 -> sc.textFile	
	tasks: sc.textFile -> t3 -> t6 -> t7 -> collect()

	rdd8.collect()
	
	lineage of rdd8: rdd8 -> rdd7.t8 -> rdd6.t7 -> rdd3.t6 -> rdd1.t3 -> sc.textFile
	tasks: t8 -> collect	



	Persistence types   ---> in-memory in deserialized format
			    ---> in-memory in serialized format
			    ---> disk

	Storage Levels
	--------------
	1. MEMORY_ONLY (default)	-> deserilized
	2. MEMORY_AND_DISK		-> deserilized	
	3. DISK_ONLY
	4. MEMORY_ONLY_SER		-> serialized
	5. MEMORY_AND_DISK_SER		-> serialized	
	6. MEMORY_ONLY_2	
	7. MEMORY_AND_DISK_2

	
	Persistence Commands:
	
		1. rdd.persist( <StorageLevel> )
		2. rdd.cache()   --> im-memory persistence
		3. rdd.unpersist()


   Executor Memory Structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------
	-> Output of a transformations is an RDD
	-> The number of partitions of output RDD = number of partitions of input RDD (by default)
		-> there are few exceptions..


    1. map		P: U -> V
			Element to element transformation
			input RDD: N objects, output RDD: N objects

    2. filter		P: U -> Boolean
			Only those elements for which the function returns True will be in the output.
			input RDD: N objects, output RDD: <= N objects

    3. glom		P: None
			Returns one array object will thte elements per partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 1,2,1,3,5,6 -> glom -> P0: [1,2,1,3,5,6]
		P1: 3,0,7,8,9,0 -> glom -> P1: [3,0,7,8,9,0]
		P2: 4,3,4,8,9,0 -> glom -> P2: [4,3,4,8,9,0]
	
		rdd1.count: 18		   rdd2.count: 3

    4. flatMap		P: U -> Iterable[V] 
			FlapMap flattens all the elements of the iterables returned by the function.

	
    5. mapPartitions	P: Iterator[U] -> Iterator[V]
			Applies the function to the entire partition

		rdd1		   rdd2 = rdd1.mapPartitions(lambda x : [sum(x)])

		P0: 1,2,1,3,5,6 -> glom -> P0: 18
		P1: 3,0,7,8,9,0 -> glom -> P1: 27
		P2: 4,3,4,8,9,0 -> glom -> P2: 28


    6. mapPartitionsWithIndex   P: Int, Iterator[U] -> Iterator[V]
				Applies the function to the entire partition. We get partition-id also as input.

	rdd1.mapPartitionsWithIndex(lambda i, data : [(i, sum(data))] ).collect()


    7. distinct			P: None, Optional: number of output partitions
				Returns distinct elements of the array.





    
  


























