
  Agenda (PySpark)
  -----------------   
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared variables
   -> Spark SQL
   -> Spark MLlib & Machine Learning
   -> Introduction to Spark Streaming

  Materials
  ---------	
    -> PDF Presentations
    -> Code Modules 
    -> Class Notes 
    -> Github: https://github.com/ykanakaraju/pyspark


   Spark
   ----------
    -> Framework written in 'Scala' programming language

    -> Spark is unified, in-memory distributed computing framework.
   
    -> Spark is a polyglot
	-> Scala, Java, Python, R

    -> Spark applications can be submitted to multiple cluster managers
	-> local, Spark Standalone Scheduler, YARN, Mesos, Kubernetes	

    Cluster => Is a unified entity comprising of many nodes whose combined resources can be used
	       to distribute your processing.

    Distributed Computing => Is a framework that allows your computation to be distributed in the memory
	       of many nodes and perform computations in parallel.

    In-memory distributed computing => The intermediate results of the computations can be persisted in RAM
		and subsequent tasks can read the data directly from RAM and advance the processing.

   Spark Unified Framework
   -----------------------
     => Spark Framework provides a consistent set of API running on the same execution engine
	to cater to different analytical workloads.

	Batch Analytics of Unstructured data	=> Spark Core (RDD)
	Batch Analytics of Structured data	=> Spark SQL
	Streaming Analytics (Real-time)		=> Spark Streaming, Structured Streaming
	Predictive Analytics (Machine Learning) => Spark MLlib
	Graph Parallel Computations		=> Spark GraphX
	
	
   Spark Architecture
   ------------------

   	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Creates a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.


    Getting Started with PySpark
    ----------------------------
	
     1. Working in your vLab

	 -> Click on the 'BigData Enbl' link shared your email
	 -> This connects to the Windows server.
	 -> Click on the CentOS7 icon and enter username and password (refer to README.txt file)
	 -> You are connected to the lab.

	  1. PySpark Shell

		-> Open a terminal and type 'pyspark'
		-> This launches 'PySpark' shell

	  2. Working with Jupyter Notebook

		-> Open a terminal
		-> Type "jupyter notebook --allow-root"
		-> This launches Jupyter Notebook, which uses a web interface.


    2. Working on your local machine

	-> Make sure you install Anaconda navigator : https://www.anaconda.com/products/individual

	-> The follow the instrauctions given in the shared document to setup PySpark with
	   Jupyter Notebooks or Spyder.

	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	  
    3. Databricks Community Edition Account

	-> URL: https://databricks.com/try-databricks

	
  
   RDD (Resilient Distributed Dataset)
   -----------------------------------
    => Fundamental data abstarction of Spark    

    => RDD is a collection of distributed in-memory partitions 
	   -> Partition is a collection of objects
  
    => RDD is 
	   -> created : When a tranformation command is executed
	   -> executed: When an Action command is launched.

	  RDD has two components:

		-> Meta Data : Lineage DAG (created when a transformation command is executed)
	
    -> RDDs are lazily evaluated
	  -> Transformations does not cause execution
	  -> Action commands triggers execution. 

    -> RDDs are immutable


   How to create RDD ?
   -------------------
	1. From external data files
	     	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		=> If you don't specificy the numPartitions, the default number of partitions
		   is determined by the value of 'sc.defaultMinPartitions' property
		
	2. From programmatic data
		rdd2 = sc.parallelize( [1,2,3,4,5,6,7,8,9], 2 )
		rdd2 = sc.parallelize( range(1, 401), 4 )

		=> If you don't specificy the numPartitions, the default number of partitions
		   is determined by the value of 'sc.defaultParallelism' property


	3. By applying transformations on existing RDDs
		rdd2 = rdd1.map( lambda x: x.upper() )
		

   What can you do with an RDD
   ---------------------------	
	1. Transformations		
             -> Transformations only cause Lineage DAGs of the RDDs to be created
	     -> Does not cause the actual execution.		
	
	2. Actions
	    -> Action commands trigger the execution on the RDD
	    -> Converts the logical plan into a phisical execution plan and causes a set of tasks
               to be sent to the executors.

   RDD Lineage DAG
   ---------------   
	=> Is a logical plan on how to compute the partitions of the RDD.

	=> This plan tracks all the dependencies all the way upto the very first RDD that caused the creation
	   of this RDD. 		
	
	rddFile = sc.textFile(file, 4)	
	   Lineage: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	   Lineage: (4) rdd2 -> rddFile.map -> sc.textFile 

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	    Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 
    
	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	    Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile 
		(sc.textFile -> map -> flatMap -> filter => rdd4 )    

	rdd4.collect()     # collect() is an action command


   RDD Persistence
   ---------------

	rdd1 = sc.textFile( ...., 10 )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_ONLY )   --> instruction to spark to not GC the rdd6 partitions
	rdd7 = rdd6.t7( .. )

	rdd6.collect()
	   rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   transformations: sc.textFile, t3, t5, t6 -> rdd6 -> collect()

	rdd7.collect()
	   rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   transformations: t7 -> rdd7 -> collect()

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		-> default, memory serialized 1x replicated
	2. MEMORY_AND_DISK	-> disk memory serialized 1x replicated
	3. DISK_ONLY		-> disk serialized 1x relicated
	4. MEMORY_ONLY_2	-> memory serialized 2x replicated
	5. MEMORY_AND_DISK_2    -> disk memory serialized 2x replicated

		
	Commands
        --------
	rdd1.persist()
	rdd1.persist( StorageLevel.DISK_ONLY )
	rdd1.cache()

	rdd1.unpersist()


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD



   RDD Transformations
   -------------------
    => A transformation always returns a RDD
    => Transformations does not cause execution of the tasks. 


   1. map		P: U -> V
			Object to Object transformation
			Transforms the objects by the applying the function on each object
			input RDD: N objects, output RDD: N objects

		rdd1.map(lambda x: x > 7).collect()


   2. filter		P: U -> Boolean
			The output RDD will have those objects for which the function returns True
   			input RDD: N objects, output RDD: <= N objects

		rdd2.filter(lambda x: x[1]%2 == 0).collect()

   3. glom		P: None
			Will returns a list object for each input partition.
			input RDD: N objects, output RDD: num objects = num partitions


		rdd1			rdd2 = rdd1.glom()
		P0: 2,3,1,2,4  -> glom -> P0: [2,3,1,2,4]
		P1: 5,2,3,4,6  -> glom -> P1: [5,2,3,4,6]
		P2: 7,3,5,4,6  -> glom -> P2: [7,3,5,4,6]

		rdd1.count() : 15 (int)   rdd2.count() : 3 (list)


   4. flatMap		P: U -> Iterable[V]
			flapMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: >= N objects

		 rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1	   rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )   

		P0: 2,3,1,2,4  -> mapPartitions -> P0: 12
		P1: 5,2,3,4,6  -> mapPartitions -> P1: 20
		P2: 7,3,5,4,6  -> mapPartitions -> P2: 25

		rdd1.mapPartitions(lambda p :  map(lambda x: x*10, p) ).glom().collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Partition to partition transformation
				Same as mapPartitions, but we get partition-index as an additional function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p :  map(lambda x: (i, x*10), p) ).glom().collect()


   7. distinct		P: None. Optional: numPartitions
			Returns distinct elements of the RDD
			input RDD: N objects, output RDD: <= N objects

   
   8. sortBy		F: U -> V, Optional: ascending (True/False), numPartitions
			Objects of the RDD are sorted based on the function output.
			input RDD: N objects, output RDD: N objects

		 rddWords.sortBy(lambda x: x[-1])
		 rddWords.sortBy(lambda x: x[-1], False)
		 rddWords.sortBy(lambda x: x[-1], True, 6)






 


 
















