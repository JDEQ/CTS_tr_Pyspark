
  Agenda (PySpark)
  -----------------   
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared variables
   -> Spark SQL
   -> Spark MLlib & Machine Learning
   -> Introduction to Spark Streaming

  Materials
  ---------	
    -> PDF Presentations
    -> Code Modules 
    -> Class Notes 
    -> Github: https://github.com/ykanakaraju/pyspark


   Spark
   ----------
    -> Framework written in 'Scala' programming language

    -> Spark is unified, in-memory distributed computing framework.
   
    -> Spark is a polyglot
	-> Scala, Java, Python, R

    -> Spark applications can be submitted to multiple cluster managers
	-> local, Spark Standalone Scheduler, YARN, Mesos, Kubernetes	

    Cluster => Is a unified entity comprising of many nodes whose combined resources can be used
	       to distribute your processing.

    Distributed Computing => Is a framework that allows your computation to be distributed in the memory
	       of many nodes and perform computations in parallel.

    In-memory distributed computing => The intermediate results of the computations can be persisted in RAM
		and subsequent tasks can read the data directly from RAM and advance the processing.

   Spark Unified Framework
   -----------------------
     => Spark Framework provides a consistent set of API running on the same execution engine
	to cater to different analytical workloads.

	Batch Analytics of Unstructured data	=> Spark Core (RDD)
	Batch Analytics of Structured data	=> Spark SQL
	Streaming Analytics (Real-time)		=> Spark Streaming, Structured Streaming
	Predictive Analytics (Machine Learning) => Spark MLlib
	Graph Parallel Computations		=> Spark GraphX
	
	
   Spark Architecture
   ------------------

   	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Creates a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.


    Getting Started with PySpark
    ----------------------------
	
     1. Working in your vLab

	 -> Click on the 'BigData Enbl' link shared your email
	 -> This connects to the Windows server.
	 -> Click on the CentOS7 icon and enter username and password (refer to README.txt file)
	 -> You are connected to the lab.

	  1. PySpark Shell

		-> Open a terminal and type 'pyspark'
		-> This launches 'PySpark' shell

	  2. Working with Jupyter Notebook

		-> Open a terminal
		-> Type "jupyter notebook --allow-root"
		-> This launches Jupyter Notebook, which uses a web interface.


    2. Working on your local machine

	-> Make sure you install Anaconda navigator : https://www.anaconda.com/products/individual

	-> The follow the instrauctions given in the shared document to setup PySpark with
	   Jupyter Notebooks or Spyder.

	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	  
    3. Databricks Community Edition Account

	-> URL: https://databricks.com/try-databricks

	
  
   RDD (Resilient Distributed Dataset)
   -----------------------------------
    => Fundamental data abstarction of Spark    

    => RDD is a collection of distributed in-memory partitions 
	   -> Partition is a collection of objects
  
    => RDD is 
	   -> created : When a tranformation command is executed
	   -> executed: When an Action command is launched.

	  RDD has two components:

		-> Meta Data : Lineage DAG (created when a transformation command is executed)
	
    -> RDDs are lazily evaluated
	  -> Transformations does not cause execution
	  -> Action commands triggers execution. 

    -> RDDs are immutable


   How to create RDD ?
   -------------------
	1. From external data files
	     	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		=> If you don't specificy the numPartitions, the default number of partitions
		   is determined by the value of 'sc.defaultMinPartitions' property
		
	2. From programmatic data
		rdd2 = sc.parallelize( [1,2,3,4,5,6,7,8,9], 2 )
		rdd2 = sc.parallelize( range(1, 401), 4 )

		=> If you don't specificy the numPartitions, the default number of partitions
		   is determined by the value of 'sc.defaultParallelism' property


	3. By applying transformations on existing RDDs
		rdd2 = rdd1.map( lambda x: x.upper() )
		

   What can you do with an RDD
   ---------------------------	
	1. Transformations		
             -> Transformations only cause Lineage DAGs of the RDDs to be created
	     -> Does not cause the actual execution.		
	
	2. Actions
	    -> Action commands trigger the execution on the RDD
	    -> Converts the logical plan into a phisical execution plan and causes a set of tasks
               to be sent to the executors.

   RDD Lineage DAG
   ---------------   
	=> Is a logical plan on how to compute the partitions of the RDD.

	=> This plan tracks all the dependencies all the way upto the very first RDD that caused the creation
	   of this RDD. 		
	
	rddFile = sc.textFile(file, 4)	
	   Lineage: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	   Lineage: (4) rdd2 -> rddFile.map -> sc.textFile 

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	    Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 
    
	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	    Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile 
		(sc.textFile -> map -> flatMap -> filter => rdd4 )    

	rdd4.collect()     # collect() is an action command


   RDD Persistence
   ---------------

	rdd1 = sc.textFile( ...., 10 )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_ONLY )   --> instruction to spark to not GC the rdd6 partitions
	rdd7 = rdd6.t7( .. )

	rdd6.collect()
	   rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   transformations: sc.textFile, t3, t5, t6 -> rdd6 -> collect()

	rdd7.collect()
	   rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   transformations: t7 -> rdd7 -> collect()

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		-> default, memory serialized 1x replicated
	2. MEMORY_AND_DISK	-> disk memory serialized 1x replicated
	3. DISK_ONLY		-> disk serialized 1x relicated
	4. MEMORY_ONLY_2	-> memory serialized 2x replicated
	5. MEMORY_AND_DISK_2    -> disk memory serialized 2x replicated

		
	Commands
        --------
	rdd1.persist()
	rdd1.persist( StorageLevel.DISK_ONLY )
	rdd1.cache()

	rdd1.unpersist()


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Transformations
   -------------------
    => A transformation always returns a RDD
    => Transformations does not cause execution of the tasks. 


   1. map		P: U -> V
			Object to Object transformation
			Transforms the objects by the applying the function on each object
			input RDD: N objects, output RDD: N objects

		rdd1.map(lambda x: x > 7).collect()


   2. filter		P: U -> Boolean
			The output RDD will have those objects for which the function returns True
   			input RDD: N objects, output RDD: <= N objects

		rdd2.filter(lambda x: x[1]%2 == 0).collect()

   3. glom		P: None
			Will returns a list object for each input partition.
			input RDD: N objects, output RDD: num objects = num partitions


		rdd1			rdd2 = rdd1.glom()
		P0: 2,3,1,2,4  -> glom -> P0: [2,3,1,2,4]
		P1: 5,2,3,4,6  -> glom -> P1: [5,2,3,4,6]
		P2: 7,3,5,4,6  -> glom -> P2: [7,3,5,4,6]

		rdd1.count() : 15 (int)   rdd2.count() : 3 (list)


   4. flatMap		P: U -> Iterable[V]
			flapMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: >= N objects

		 rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1	   rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )   

		P0: 2,3,1,2,4  -> mapPartitions -> P0: 12
		P1: 5,2,3,4,6  -> mapPartitions -> P1: 20
		P2: 7,3,5,4,6  -> mapPartitions -> P2: 25

		rdd1.mapPartitions(lambda p :  map(lambda x: x*10, p) ).glom().collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Partition to partition transformation
				Same as mapPartitions, but we get partition-index as an additional function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p :  map(lambda x: (i, x*10), p) ).glom().collect()


   7. distinct		P: None. Optional: numPartitions
			Returns distinct elements of the RDD
			input RDD: N objects, output RDD: <= N objects

   
   8. sortBy		P U -> V, Optional: ascending (True/False), numPartitions
			Objects of the RDD are sorted based on the function output.
			input RDD: N objects, output RDD: N objects

		 rddWords.sortBy(lambda x: x[-1])
		 rddWords.sortBy(lambda x: x[-1], False)
		 rddWords.sortBy(lambda x: x[-1], True, 6)

   Types of RDDs
   -------------
	-> Generic RDDs :  RDD[U]	
	-> Pair RDDs	:  RDD[(U, V)]


   9. mapValues		P: U -> V
			Applied only to pair RDDs
			Applies the function only to the value part of the key-value pairs.
		
		rdd3.mapValues(list).collect()


   10. groupBy		P: U -> V, Optional: numPartitions
			Returns a pair RDD where the 
			  key: is the unique output of the function
			  value: is a ResultIterable object of all all the objects that produced the key

			RDD[U].groupBy(U -> V) => RDD[(V, ResultIterable[U])]

			rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            				.flatMap(lambda x: x.split(" ")) \
            				.groupBy(lambda x: x) \
            				.mapValues(len) \
            				.sortBy(lambda x: x[1], False, 1)

   11. randomSplit	P: a list of ratios (ex: [0.6, 0.4])
			Returns a list of rdds randomly split in the given ratios.


   12. repartition	P: numPartitions
			Used to increase or decrease the number of output partitions
			causes global shuffle	


   13. coalesce		P: numPartitions
			Used to decrease the number of output partitions
			causes partition-merging 

     Recommendations:
	-> The optimal size of the partitions : ~ 128 MB
	-> The number of partitions should be a multiple of number of cores alloted. 
	-> If the number of partitions is close to 2000, bump it up to more than 2000.
	-> The number of cores in each executor should be 5


   14. partitionBy	P: numPartitions, Optional: partitioning function (default: hash)
			Applied only on Pair RDDs
			Used to control which keys go to which partition based on the partitioning function. 

		rdd3 = rdd2.partitionBy(3)   			# default function is 'hash'
		rdd3 = rdd2.partitionBy(3, lambda x: x + 1)

   
   15. union, intersection, subtract, cartesian


   ..ByKey transformations
   ------------------------ 
        -> Applied on only Pair RDDs
	-> Are all wide transformation


   16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the objects of the RDD based on key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 6).collect()


   17. groupByKey		P: None, Optional: numPartitions
				Returns a Pair RDD where the key is the unique key and value is the ResultIterable
				object with all the values that has the same key

				NOTE: Avoid groupByKey if possible. 
			
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.map(lambda s: (s, 1)) \
            		.groupByKey() \
            		.mapValues(sum) \
            		.sortBy(lambda x: x[1], False, 1)

   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key first with the partitions and then
				across partitions by iterativly applying the reduce function. 

		    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda s: (s, 1)) \
            			.reduceByKey(lambda x, y: x + y, 1)

   19. aggregateByKeyIs used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 


	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
	student_rdd.collect()

	output = student_rdd.map(lambda t: (t[0], t[2])) \
            	.aggregateByKey( (0,0),
                            lambda z, v: (z[0] + v, z[1] + 1),
                            lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            	.mapValues(lambda x: x[0]/x[1]) \
            	.sortBy(lambda x: x[1], False)


   20. joins	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			
			RDD[(U, V)].join( RDD[(U,W)] ) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)		

   21. cogroup		Is used to join RDDs with duplicate keys and when the want unique keys in the output
			-> groupByKey of each RDD -> fullOuterJoin

	 [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		-> (key1, [10, 7]) (key2, [12,6]) (key3, [6])

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		-> (key1, [5, 17]) (key2, [4, 7]) (key4, [17])

	=> (key1, ([10, 7], [5, 17])) (key2, ([12,6], [4, 7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------
    1. collect
    2. count
    3. saveAsTextFile
  
    4. reduce	=>  	   P: U, U -> U
			   reduces the entire RDD into one final value of the same type by iterativly
			   applying the reduce function on each partition in the first stage and 
			   across partitions in the second stage.

  		P0: 6, 5, 4, 3, 6, 8, 7  -> reduce -> -27 => -1
		P1: 9, 0, 2, 3, 1, 5, 4  -> reduce -> -6
		P2: 6, 7, 9, 5, 4, 0, 1  -> reduce -> -20

		rdd1.reduce(lambda x, y : x - y)

  
    5. aggregate	-> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.

		
			      rdd1.aggregate( (0,0), 
			        lambda z,v : (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1])
			      )	

    6. take

    7. takeOrdered

		rddWords.takeOrdered(100)
		rddWords.takeOrdered(100, len)

    8. takeSample
		 rdd1.takeSample(True, 15)
		 rdd1.takeSample(True, 15, 345)
		 rdd1.takeSample(False, 15)

    9. countByValue

    10. countByKey

    11. first

    12. foreach   -> applies a function on all objects of the RDD
		     does not return any value.

    13. saveAsSequenceFile


   Use-Case
   --------   
   
	Find the average weight of all 'make's of 'American' origin cars from cars.tsv dataset.
	Arrange the data in the DESC order of average weight.
	Save the output as a single text file

	https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	=> Try that yourself

  
   Shared Variables
   ================
	
     -> Accumulator
     -> Broadcast
	
      Closure: A closure contains all the code (variables and methods) that must be visible inside an 
	       executor for the task to perform its computations on the RDD. 

	-> The closure is serialized and copy is sent to every executors task.  

	-> LOcal Variables can not be used to implement a global counter. 

	c = 0
	
	def isPrime(n):
		returns 1 if n is prime
		returns 0 if n is not prime

	def f1(n) :
		global c
		if ( isPrime(n) == 1 ) c = c + 1
		return n*2

	rdd1 = sc.paralleize( range(1, 4001), 4 )
	
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)      # 0
 

  
    Accumulator : 
    ------------
		-> Is a shared variable that can be added to by multiple distributed tasks
		-> Mainitained by the driver.
		-> Not part of function closure and hence is not a local copy. 

	c = sc.accumulator(0)
	
	def isPrime(n):
		returns 1 if n is prime
		returns 0 if n is not prime

	def f1(n) :
		global c		
		if ( isPrime(n) == 1 ) c.add(1)
		return n*2

	rdd1 = sc.paralleize( range(1, 4001), 4 )
	
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c.value )      # 75


   Broadcast Variable
   ------------------
	-> You can convert large immutable collections into broadcast variables
	-> Is not part of the function closure and hence is not a local variable
	-> One copy of the variable is sent to every executor
	-> All tasks within that executor can read from that single copy (per executor)	


	d = {1: a, 2: b, 3: c, 4: d, 5: e, 6: f, ....}       // 100 MB
	bc = sc.broadcast( d )

	def f1(n) :
		global bc
		return bc.value[n]

	rdd1 = sc.parallelize( [1,2,3,4,5,6, ...], 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()       


  ---------------------- 
   spark-submit command
  ----------------------  

    Is single command to submit any spark application (scala, java, python, R) to any 
    cluster manager (local, spark standalone, yarn, mesos, kubernetes). 

	spark-submit [options] <app jar | python file | R file> [app arguments]

        spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		E:\PySpark\spark_core\examples\wordcount.py [application arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput 1

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py



=============================================
       Spark SQL (pyspark.sql)   
=============================================
   
     => Spark Structured data processing API

		Structured data files:  Parquet (default), ORC, JSON, CSV (delimited text)
		JDBC format : RDBMS database, NoSQL databases.
		Hive (Hadoop's Data Warehousing Platform)

     => Spark Session
	 -> Starting point of execution in Spark SQL
 	 -> Represents a user-session (with its own configuration) within an application

		SparkContext => Spark Application
		SparkSession => Spark user session within an application

	   spark = SparkSession \
        	  .builder \
        	  .appName("Dataframe Operations") \
        	  .config("spark.master", "local[*]") \
        	  .getOrCreate() 

	   spark.conf.set("spark.sql.shuffle.partitions", "10") 

	
     => DataFrame (DF)	

	-> Data abstraction of Spark SQL
	-> DataFrame is a collection of distributed, in-memory partitions that are immutable and lazily evaluated.
	-> DataFrame is a collection of "Row" objects. 	  (pyspark.sql.Row)

	-> DataFrame has two components:
		-> Data   : Row objects
		-> Schema : Structure of the DF   (StructType object)

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)


    Working with Spark SQL
    ----------------------

      1. Read/load the data from some data source (structured file, rdbms, rdd, python collection etc.)
	 into a Dataframe

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)



      2. Apply transformations on the DF using DF API methods or using SQL

		Using DF API methods
		--------------------
			df2 = df1.select("userid", "name", "gender", "age") \
         			.where("age is not null") \
         			.orderBy("age", "name") \
         			.groupBy("age").count() \
         			.limit(4)
			
		Using SQL
		---------

			spark.catalog.listTables()
			df1.createOrReplaceTempView("users")
			#spark.catalog.dropTempView("users")

			qry = """select age, count(*) as count
        			from users
        			where age is not null
        			group by age
        			order by age
        			limit 5"""

			df3 = spark.sql(qry)
			

      3. Write/save the DFs data into some destination (structured file, rdbms, hive etc.)

		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)

		df2.write.json(outputPath, mode="overwrite")

		
   LocalTempViews & GlobalTempViews
   ---------------------------------

	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.

  Save Modes
  ----------
	-> ignore
	-> append
	-> overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  DF Transformations
  ------------------

   1. select

		df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")


		df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 		column("DEST_COUNTRY_NAME").alias("destination"), 
                 		expr("count"),
                 		expr("count + 10 as newCount"),
                	 	expr("count > 200 as highFrequency"),
                	 	expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                 	)		

   2. where / filter
		df3 = df2.where("domestic = False and count > 100")
		df3 = df2.where( col("count") > 100 )
		df3 = df2.filter( col("count") > 100 )

   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.sort("count", "origin")

		df3 = df2.orderBy(desc("count"), asc("origin"))

   4. groupBy (with aggregation methods)

		df4 = df2.groupBy("domestic", "highFrequency").count()
		df4 = df2.groupBy("domestic", "highFrequency").sum("count")
		df4 = df2.groupBy("domestic", "highFrequency").avg("count")

		df4 = df2.groupBy("domestic", "highFrequency") \
        			.agg( count("count").alias("count"), 
             				sum("count").alias("sum"), 
             				avg("count").alias("avg"), 
             				max("count").alias("max")
				    )
		

   5. limit

		df2 = df1.limit(10)

   6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin", 
                 "DEST_COUNTRY_NAME as destination", 
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"
                 )


   7. withColumn &  withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
        	.withColumn("count", col("count").cast("int")) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination")

	df3.show()
	df3.printSchema()

	users2 = userDf.withColumn("ageGroup", 
                           when(userDf["age"] < 12, "child")
                           .when(userDf["age"] < 20, "teenager")
                           .when(userDf["age"] < 60, "adult")
                           .otherwise("senior"))


   8. drop
		df3 = df2.drop("newCount", "highFrequency")


   9. distinct
		df1.select("DEST_COUNTRY_NAME").distinct().count()

   10. randomSplit
		
		dfList = df1.randomSplit([0.5, 0.5], 456)
		df10, df11 = df1.randomSplit([0.5, 0.5], 456)

   11. sample
	
		df3 = df1.sample(True, 0.5, 5755)
		df3 = df1.sample(True, 1.5, 5755)

		df3 = df1.sample(False, 0.5, 5755)

   12. union, intersect, subtract

		df7 = df5.union(df6)
		df8 = df7.intersect(df5) 
		df9 = df7.subtract(df6)

   13. repartition
		
		df3 = df1.repartition(4)
		df4 = df3.repartition(2)
		df5 = df3.repartition(3, col("DEST_COUNTRY_NAME"))
		df5 = df3.repartition(col("DEST_COUNTRY_NAME"))

   14. coalesce

		df7 = df6.coalesce(5)

   15. join => discussed separatly.
   


   Working with different structured file formats
   ---------------------------------------------- 

   1. JSON
	read
		df1 = spark.read.format("json").load(inputFile)  
		df1 = spark.read.json(inputFile)  
	write
		df3.write.format("json").save(outputPath)
		df3.write.json(outputPath)

   2. Parquet

	read
		df1 = spark.read.format("parquet").load(inputFile)  
		df1 = spark.read.parquet(inputFile)  
	write
		df3.write.format("parquet").save(outputPath)
		df3.write.parquet(outputPath)

   3. ORC

	read
		df1 = spark.read.format("orc").load(inputFile)  
		df1 = spark.read.orc(inputFile)  
	write
		df3.write.format("orc").save(outputPath)
		df3.write.orc(outputPath)

 
   4. CSV (delimited text file)

	read
		df1 = spark.read.format("csv").load(inputFile, header=True, inferSchema=True)  
		df1 = spark.read.csv(inputFile, header=True, inferSchema=True)  
		df1 = spark.read.csv(inputFile, header=True, inferSchema=True, sep="|")
	write
		df3.write.format("csv").save(outputPath, header=True)
		df3.write.csv(outputPath, header=True)
		df2.write.mode("overwrite").csv(outputPath, header=True, sep="|")

   
   Creating an RDD from DataFrame
   ------------------------------
	rdd1 = df1.rdd

  
   Creating a DataFrame from programmatic data
   -------------------------------------------
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Rajesh", 40),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 40)]

	df1 = spark.createDataFrame(listUsers ).toDF("id", "name", "age")
	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])


   Creating a DataFrame from RDD
   ------------------------------

	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")
	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])


   Creating a DataFrame with programmatic Schema
   ---------------------------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)   

       ----------------------------------------------------

  	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)])

	inputFile = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"
	df1 = spark.read.json(inputFile, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputFile)


   Joins
   ======

	Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti

   	left-semi join
	--------------
	Is similar to inner join but the data comes only from the left-side table. 
	Is equivalent to:
		select * from emp where deptid in (select id from deptid)

	left-anti join
	--------------
        Is equivalent to:
		select * from emp where deptid not in (select id from deptid)

employee = spark.createDataFrame([
    (1, "Raju", 25, 101),
    (2, "Ramesh", 26, 101),
    (3, "Amrita", 30, 102),
    (4, "Madhu", 32, 102),
    (5, "Aditya", 28, 102),
    (6, "Pranav", 28, 100)])\
  .toDF("id", "name", "age", "deptid")
  
department = spark.createDataFrame([
    (101, "IT", 1),
    (102, "ITES", 1),
    (103, "Opearation", 1),
    (104, "HRD", 2)])\
  .toDF("id", "deptname", "locationid") 
  
employee.show()  
department.show()

spark.catalog.listTables()

employee.createOrReplaceTempView("emp")
department.createOrReplaceTempView("dept")

qry = """select *
        from emp left anti join dept
        on emp.deptid = dept.id"""
        
joinedDf = spark.sql(qry)

joinedDf.show()

 -----------------------------------------
	Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti

	joinedCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinedCol, "left_anti")

 ======================================================

   Use-Case: 

   From movies.csv and ratings.csv datasets. fetch the top 10 movies with highest average rating
   -> Consider only those that are rated by atleast 30 users
   -> Data: movieId, title, totalRatings, averageRating
   -> Arrange the data in the DESC order of average rating
   -> Save the output as a single pipe-separate CSV file with header.

	=> Please try to solve.

 ========================================================

   JDBC format: Working with MySQL
   -------------------------------
   
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'  

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()

   ====================================================

    Working with Hive
    -----------------

     -> Hive is a Data Warehousing platform built on top of Hadoop

	Warehouse: A directory where Hive stores all its data files. 
	Metastore: A service (such as an RDBMS) where Hive stores all its metadata. 
   
	
# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()



 =============================================================
    Spark MLlib & Machine Learning
 =============================================================  

   ML Model =>  Learned Entity
		Learns from some historic data.
		The data is given to an algorithm, the algortihm analyzes the data and establishes
		a relation between the output and inputs. This rlation is returned as an ML Model

         -------------------------------------
	   model = algorithm( traning-data )
	 -------------------------------------

   Terminology
   -----------

    1. Training Data

    2. Features		-> Inputs, Dimensions

    3. Label		-> Output

    4. Algorithm	-> Is a mathematical computation that analyzes the training data and creates a model
			   with a goal to minimize a loss function. 

    5. ML Model		-> Is a trained entity (tarianed by an algorithm based on training data)
			   Based on ite learning, a model can:
				-> Predcit the output on new input
				-> Can give Forecasts, Projections, Recommendations etc.

    6. Error		-> Difference between the actual and predicted values of a given data point

    7. Loss		-> Combined error of the entore dataset computed with a loss function.



	X	Y	Z (label)  pred.  Error
	--------------------------------------
	1000	1000	3100	3000	100
	2000	1000	4950	5000	-50
	1000	500	2550	2500	 50
	1200	600	3010	3000	 10
	1100	500	2690	2700	-10
	1500	500	?  	
	------------------------------------- 
			  Loss: 220/5 =  44
         
	Model 1:   z = 2x + y			Loss: 44
	Model 2:   z = 1.9x + 1.1y + 10		Loss: 35
	Model 3:
	Model 4: 

         






   Mini Project : Titanic - Machine Learning from Disaster
   --------------------------------------------------------
    URL: https://www.kaggle.com/c/titanic








