
   Agenda (PySpark)
   -----------------
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD Transfomations & Actions
	-> Shared variables
   -> Spark SQL (DataFrames)
   -> Spark MLlib & Machine Learning
   -> Introduction Spark Streaming

   Materials
   ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark

     
   Spark
   ------
  	
	-> Spark is a unified in-memory distributed computing framework.

    	-> Spark is written in Scala.

	-> Spark is a polyglot
	   -> Supports Scala, Java, Python and R

	Cluster : Is a unified entity containing many nodes whose combined resources can be used to
	distribute you storage as well as processing. 

	In-memory distributed computed : The intermediate results of the compuattions can be persisted
	in-memory and subsequent tasks can directly operate these persisted data. 


       Spark Unified Framework
       -----------------------

	Spark provides a consistent set of APIs for processing different types of analytical workloads
	using the same execution engine.

	Batch Analytics of Unstructured Data	=> Spark Core API (Low Level API)
	Batch Analytics of Structured Data	=> Spark SQL
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics (ML)		=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX
	
	
    Spark Architecture
    ------------------

     	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


   Getting starting with PySpark   
   ----------------------------

    1. Using your vLab (toolwire lab)		
	  -> Connect to the lab as per the instructions given in the email. (Clicking on 'BigData Enbl' icon)
	  -> You are connected to Windows Server. 
	  -> Click on the "CentOS7" icon and login using username and password.
	    (Refer to README.txt file on the desktop for username and password)
	  
	  1. Launch PySpark Shell
		
		-> Open a Terminal
		-> Type "pyspark"
		-> This will launch the PySpark shell.
		-> Connect to the WebUI
			-> Open FireFox Browser  (Application -> FireFox Menu at top-left corner)
			-> Type "localhost:4040" in the address bar

	 2. Connect to Jupyter Notebook.
		-> Open a Terminal
		-> Type "jupyter notebook --allow-root"
		-> This will lauch the Jupyter Notebook web interface. 

    2. Installing and Setting up PySpark on your local machine.

	-> Make sure you download and install "Anaconda navigator"
		URL: https://www.anaconda.com/products/individual

	-> You can use pip install to setup pyspark.

	-> You can also setup pyspark to work with Jupyter Notebook & Spyder using the instruction given
	   in the shared document.
		NOTE: Make sure the python & pyspark versions are compatible (Python 3.7)

    3. Signup to Databrick Community Edition

	    URL: https://databricks.com/try-databricks	
	    Spend some time exploring the "The Quickstart Tutorial"  


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    => Is the fundamental data adstraction of Spark

    => RDD is a collection distributed in-memory partitions
	-> A 'partition' is a collection of objects.

    => RDDs are immutable

    => RDDs are resilient (fault-tolerant)

    => RDDs are lazily evaluated
	-> Transformations does not cause execution.
	-> Only action commands cause execution.

    => RDD has two components
	-> Meta Data : RDD Lineage DAG
	-> Data      : A set of partitions


   How to create RDDs ?
   --------------------	
	3 Ways:

	1. From external files

		rdd1 = sc.textFile( <filePath>, [ numPartitions ] )
		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

		=> default number of partitions: sc.defaultMinPartitions

	2. From programmatic data

		rdd1 = sc.parallelize( range(1, 101), 3 )
		rdd1 = sc.parallelize( [2,3,1,4,3,5,6,7,8,9,8,7,6,4,5,6,3,6,5,7,8,9], 3 )

		=> default number of partitions: sc.defaultParallelism

	3. By applying transformations on existing RDDs. 

		rdd2 = rddFile.map(lambda x: x.upper())


   What can you do with an RDD ?
   -----------------------------

      Two things:

	1. Transformations		
             -> Transformations only cause Lineage DAGs of the RDDs to be created
	     -> Does not cause the actual execution.		
	
	2. Actions
	    -> Action commands trigger the execution on the RDD
	    -> Converts the logical plan into a phisical execution plan and causes a set of tasks
               to be sent to the executors.


  RDD Lineage DAG
  ----------------
  
    	-> Lineage DAG is a logical plan on how to create the partitions of the RDD when asked for.
    	-> Lineage DAG contains a graph of all dependencies on other RDDs all the way from the very first RDD
    	-> Lineage DAGs are created when you perform transformations
    	-> Loagical Plan is converted to physical plan and executes the tasks when you run an 'action' command. 


	rddFile = sc.textFile( file, 4 )
	Lineage DAG : (4) rddFile -> sc.textFile
		
	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG : (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG : (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG : (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

        rdd4.collect()   => returns a list object


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( .., 4 )
	rdd2 = rdd1.t2( ... )
	rdd3 = rdd1.t3( ... )
	rdd4 = rdd3.t4( ... )
	rdd5 = rdd3.t5( ... )
	rdd6 = rdd5.t6( ... )
	rdd6.persist(StorageLevel.MEMORY_AND_DISK)   ---> instruction to persist the partitions of rdd6
	rdd7 = rdd6.t7( ... )

	rdd6.collect()	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile -> t3 -> t5 -> t6 => rdd6 ==> collected

	rdd7.collect()
	lineage of rdd6: rdd7 -> rdd6.t7
	rdd6 -> t7 => rdd7  ==> collected

	rdd6.unpersist()


	Storage Levels
        --------------	
	1. MEMORY_ONLY		-> default, memory serialized 1x replicated
				   partitions are persisted only in RAM (if RAM is available)
	
	2. MEMORY_AND_DISK	-> disk memory serialized 1x replicated
				   partitions are persisted in RAM if RAM is available, else on disk

	3. DISK_ONLY		-> disk serialized 1x replicated

	4. MEMORY_ONLY_2	-> memory serialized 2x replicated	

	5. MEMORY_AND_DISK_2	-> disk memory serialized 2x replicated

	Commands
        --------	
	rdd1.persist()  
	rdd1.persist(StorageLevel.MEMORY_AND_DISK) 
	rdd1.cache()

	rdd1.unpersist()
       	 

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD
	
   
   RDD Transformations
   -------------------

    1. map		P: U -> V
			Object to Object Transformation
			Transforms each object of the input RDD to the corresponding object of the
			output RD by applying the function
			Input RDD: N objects, Output RDD: N objects. 

		rddFile.map(lambda x: len(x.split(" ")) ).collect()

   2. filter		P: U -> Boolean 
			The output RDD will have only those objects for which the function return True.  
			Input RDD: N objects, Output RDD: <= N objects. 

		rddFile.filter(lambda st : len(st.split(" ")) > 8).collect()

   3. glom		P: None
		   	Returns one list object per partition with all the objects of the partition	

		rdd1			rdd2 = rdd1.glom()
		
		P0: 1,2,3,4,5,6 -> glom -> P0: [1,2,3,4,5,6]
		P1: 2,3,5,6,7,8 -> glom -> P1: [2,3,5,6,7,8]
		P2: 9,0,3,6,5,7 -> glom -> P2: [9,0,3,6,5,7]

		rdd1.count() : 18 (int)	  rdd2.count(): 3 (list)

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.
			Input RDD: N objects, Output RDD: > N objects. 
	
		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partion transformation
			Transforms each input partition to an output partition by applying a function
			on the entire partition. 

		rdd1	  rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] ) 
		
		P0: 1,2,3,4,5,6 -> mapPartitions -> P0: 21
		P1: 2,3,5,6,7,8 -> mapPartitions -> P1: 31
		P2: 9,0,3,6,5,7 -> mapPartitions -> P2: 30

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p) ).collect()	


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get the partition-index as additional function parameter.

		 rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x*10), p) ).collect()


   7. distinct		P: None, Optional: numPartitions
			Returns an RDD with distinct elements of the input RDD.	
			Input RDD: N objects, Output RDD: <= N objects. 

  
   8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			sorts the objects of the RDD based on the output of the functions.
			Input RDD: N objects, Output RDD: N objects. 

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], True, 5).collect()

	
   Types of RDDs
   -------------

	Two types of RDDs

	-> Generic RDDs : RDD[U]
	-> Pair RDDs: RDD[(K, V)]	


   9. mapValues		P: U -> V
			Applied only on Pair RDDs
			Applies the function on only the value part of the (K,V) pairs.
			Input RDD: N objects, Output RDD: N objects. 

		rdd3 = rdd2.mapValues(lambda x: (x, x))


   10. groupBy		P: U -> V,  Optional: numPartitions
			Groups the objects based on the function output
			Returns a Pair RDD where the 
				key: unique function output
				value: (ResultIterable) grouped values that produced the key

			RDD[U].groupBy(U -> V) => RDD[(V, ResultIterable[U])]

		rdd1.groupBy(lambda x: x%2).mapValues(list).collect()
		rdd1.groupBy(lambda x: x%2, 1).mapValues(list).collect()

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.groupBy(lambda x: x, 1) \
            		.mapValues(len)


   11. randomSplit	P: list of ratios
			Returns a list of RDDs split rabdomly in the specified ratios. 	

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 5343)  # here, 5343 is a seed.


   12. repartition	P: numPartitions
			Used to increase or decrease the number of partitions of the output RDD
			Causes global shuffle.

		rddWords2 = rddWords.repartition(10)


   13. coalesce		P: numPartitions
			Used to decrease the number of partitions of the output RDD
			Causes partition merging

		rddWords2 = rddWords.coalesce(2)
	
   Recommendations
   ---------------
	-> The size of the Partition should be approx 128 MB  
	-> The number of partitions should be a multiple of number of CPU cores.
	-> The number of partitions is close to 2000, bump up more then 2000
	-> The number of cores per executor = 5


   14. partitionBy	P: numPartitions, Optional: partitioning function
			Applied only on Pair RDD
			Based on the function output of the keys of the RDD, the elements of the RDD
			are organized in partition. 

		rdd3 = rdd2.partitionBy(4, lambda x: x+10)


    15. union, intersection, subtract, cartesian


  ..ByKey Transformations
  -----------------------
     -> Applied only on pair RDD
     -> Wide transformations (optional parameter: numPartitions)
	 

    16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Applied only on Pair RDDs
			Sorts the elements of the RDD based on the key

		rdd3.sortByKey().glom().collect()
		rdd3.sortByKey(False).glom().collect()
		rdd3.sortByKey(False, 3).glom().collect()

    17. groupByKey	P: None, Optional: numPartitions
			Applied only on Pair RDDs
			Groups the elements of the RDD by Key
			We get a pair RDD with unique keys and grouped values (ResultIterable)
			NOTE: Avoid 'groupByKey' if possible. 	

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		   .flatMap(lambda x: x.split(" ")) \
            		   .map(lambda x: (x, 1)) \
            		   .groupByKey() \
            		   .mapValues(sum)

    18. reduceByKey	P: U, U -> U, Optional: numPartitions
			Reduces all the values of each unique-key first within each partitions and the across
			different partitions.
			Applied only on Pair RDDs            

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		    .flatMap(lambda x: x.split(" ")) \
            		    .map(lambda x: (x, 1)) \
            		    .reduceByKey(lambda x, y: x + y, 1)

    19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                        		lambda z, v: (z[0] + v, z[1] + 1),
                        		lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                			.mapValues(lambda x: x[0]/x[1])

    20. joins	-> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		  RDD[(K, V)].join(RDD[(K, W)]) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

    21. cogroup	    =>  Join RDDs with possibly deplicate key and you want unique keys in the output
			groupByKey on each RDD -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12,6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

        cogroup => (key1, ([10, 7], [5, 17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile	P: non-existing directory path

  4. reduce		P: U, U -> U
			Reduces the entire RDD into one final value of the same type.
			Applies the reduce function iterativly first on each partition and then across partitions. 

		rdd1  	
		P0: 9, 4, 5, 6, 0, 6	 -> reduce -> -12 -> 16
		P1: 7, 3, 4, 2, 1, 5	 -> reduce -> -8
		P2: 7, 8, 9, 0, 3, 2, 5  -> reduce -> -20

		rdd1.reduce( lambda x, y: x - y )

  5. aggregate

	Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )

	rdd2.aggregate( ("", 0, 0), 
			lambda z, v: (z[0] + "," + str(v[0]), z[1]+v[1], z[2] if (z[2] > v[1]) else v[1]), 
			lambda a, b: (a[0] + "," + b[0], a[1] + b[1], a[2] if (a[2] > b[2]) else b[2]) 
		      )

    6. first
		rdd1.first()

    7. take
		rdd1.take(10)

    8. takeOrdered

		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: x%3)

    9. takeSample

		rdd1.takeSample(True, 15)	=> withRelacement sampling
		rdd1.takeSample(True, 15, 575)

		rdd1.takeSample(False, 15)	=> withOutRelacement sampling
		rdd1.takeSample(False, 15, 575)

   10. countByValue

   11. countByKey  -> applied to pair RDDs

   12. foreach	-> P: function as a parameter
		      function is applied on all the values of the RDD, but does not returned.

   13. saveAsSequenceFile


  ========================================================

   Use-Case
   --------
    => From cars.tsv file, fetch the average weight of each make of American origin cars. 
       Sort the output in the desc order of average weight       
       Save the output in a single text file.

	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	=> Please try it yourrself.

  ========================================================
  
  Shared Variables
  ----------------


     Closure:  A closure is all the code (all the variable and methods) that must be visible inside
	       an task for an executor to perform its computation on the RDDs.

      -> A closure is serialized and copy is sent to every executor.


	  c = 0		# num of primes

          def isPrime( n ) :
		return 1 if n is prime
		return 0 if n is not prime

	  def f1 (n) :
		global c
		if ( isPrime(n) == 1 ) c = c + 1
		return n * 2	   

	  rdd1 = sc.parallelize( range(1, 4001), 4 )

	  rdd2 = rdd1.map( f1 )

	  rdd2.collect()
	 
	  print(c)     # 0
   

     Limitation of using local variables inside a closure :
	-> Because the variables inside a closure are local they can not global state (across executors)
	   Hence local variables can not be used to implement global counter.
 		

    1. Accumulator variable

	=> Shared Variable (shared by multiple tasks)
	=> Maintained by the driver
	=> Not part of the closure (hense not a local copy)
	=> All tasks can add to this variable. 

	  c = sc.accumulator(0)		# num of primes

          def isPrime( n ) :
		return 1 if n is prime
		return 0 if n is not prime

	  def f1 (n) :
		global c
		if ( isPrime(n) == 1 ) c.add(1)
		return n * 2	   

	  rdd1 = sc.parallelize( range(1, 4001), 4 )

	  rdd2 = rdd1.map( f1 )

	  rdd2.collect()
	 
	  print( c.value )  


   2. Broadcast variable

	=> broadcast variable is shared variable
	=> Not part of the closure (hense not a local copy)
	=> A copy of the broadcast variable is sent to each executor node (not to each task)
	   All the tasks in that node, shared the same copy.
		
	  bc = sc.broadcast( {1: a,  2: b, 3: c, 4: d, 5: e, ........ }  )

	  def f1(n):
		global bc
		return bc.value[n]
		
	
	 rdd1 = sc.parallelize([1,2,3,4,5,...], 4)

	 rdd2 = rdd1.map( f1 )     

	rdd2.collect()    


  ====================================================================

    spark-submit command
    --------------------

	Spark-submit command is a single command to submit any spark application (pyspark, scala, java, R)
	to any cluster manager (local, spark standalone, yarn, mesos, kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory	2G
		--executor-memory 10G
		--driver-cores 2
		--executor-cores 5
		--num-executors 20
		E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 1
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


   ============================================================
      Spark SQL  (pyspark.sql)
   ============================================================	

      => Spark's Structured Data Processsing API

	   Structured Data File Formats :  Parquet (default), ORC, JSON, CSV (delimited text file)
	   JDBC Format : RDBMS, NoSQL Databases 
	   Hive (is a data warehousing platform on Hadoop)

      
     => SparkSession

	 -> Is the starting point of execution in Spark SQL applications
	 -> represents a user session inside an application
		-> We can have multiple sessions (SparkSession) inside an application (SparkContext)
	-> Introduced in Spark 2.0 (before that we has sqlContext)	

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()   

     => DataFrame
	
	-> Is the data abstract of SparkSQL	
	-> Is a collection of in-memory distributed partitions that are immutable and lazily executed.
	-> DataFrames is a collection of "Row" objects
	-> DataFrame:		
		Two Components:
		-> Meta Data: schema (StructType object)
		-> Data: collection of "Rows" (in-memory partitions)


        


 
  







