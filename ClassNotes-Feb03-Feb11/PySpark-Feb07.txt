
   Agenda (PySpark)
   -----------------
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD Transfomations & Actions
	-> Shared variables
   -> Spark SQL (DataFrames)
   -> Spark MLlib & Machine Learning
   -> Introduction Spark Streaming

   Materials
   ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark

     
   Spark
   ------
  	
	-> Spark is a unified in-memory distributed computing framework.

    	-> Spark is written in Scala.

	-> Spark is a polyglot
	   -> Supports Scala, Java, Python and R

	Cluster : Is a unified entity containing many nodes whose combined resources can be used to
	distribute you storage as well as processing. 

	In-memory distributed computed : The intermediate results of the compuattions can be persisted
	in-memory and subsequent tasks can directly operate these persisted data. 


       Spark Unified Framework
       -----------------------

	Spark provides a consistent set of APIs for processing different types of analytical workloads
	using the same execution engine.

	Batch Analytics of Unstructured Data	=> Spark Core API (Low Level API)
	Batch Analytics of Structured Data	=> Spark SQL
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics (ML)		=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX
	
	
    Spark Architecture
    ------------------

     	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


   Getting starting with PySpark   
   ----------------------------

    1. Using your vLab (toolwire lab)		
	  -> Connect to the lab as per the instructions given in the email. (Clicking on 'BigData Enbl' icon)
	  -> You are connected to Windows Server. 
	  -> Click on the "CentOS7" icon and login using username and password.
	    (Refer to README.txt file on the desktop for username and password)
	  
	  1. Launch PySpark Shell
		
		-> Open a Terminal
		-> Type "pyspark"
		-> This will launch the PySpark shell.
		-> Connect to the WebUI
			-> Open FireFox Browser  (Application -> FireFox Menu at top-left corner)
			-> Type "localhost:4040" in the address bar

	 2. Connect to Jupyter Notebook.
		-> Open a Terminal
		-> Type "jupyter notebook --allow-root"
		-> This will lauch the Jupyter Notebook web interface. 

    2. Installing and Setting up PySpark on your local machine.

	-> Make sure you download and install "Anaconda navigator"
		URL: https://www.anaconda.com/products/individual

	-> You can use pip install to setup pyspark.

	-> You can also setup pyspark to work with Jupyter Notebook & Spyder using the instruction given
	   in the shared document.
		NOTE: Make sure the python & pyspark versions are compatible (Python 3.7)

    3. Signup to Databrick Community Edition

	    URL: https://databricks.com/try-databricks	
	    Spend some time exploring the "The Quickstart Tutorial"  


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    => Is the fundamental data adstraction of Spark

    => RDD is a collection distributed in-memory partitions
	-> A 'partition' is a collection of objects.

    => RDDs are immutable

    => RDDs are resilient (fault-tolerant)

    => RDDs are lazily evaluated
	-> Transformations does not cause execution.
	-> Only action commands cause execution.

    => RDD has two components
	-> Meta Data : RDD Lineage DAG
	-> Data      : A set of partitions


   How to create RDDs ?
   --------------------	
	3 Ways:

	1. From external files

		rdd1 = sc.textFile( <filePath>, [ numPartitions ] )
		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

		=> default number of partitions: sc.defaultMinPartitions

	2. From programmatic data

		rdd1 = sc.parallelize( range(1, 101), 3 )
		rdd1 = sc.parallelize( [2,3,1,4,3,5,6,7,8,9,8,7,6,4,5,6,3,6,5,7,8,9], 3 )

		=> default number of partitions: sc.defaultParallelism

	3. By applying transformations on existing RDDs. 

		rdd2 = rddFile.map(lambda x: x.upper())


   What can you do with an RDD ?
   -----------------------------

      Two things:

	1. Transformations		
             -> Transformations only cause Lineage DAGs of the RDDs to be created
	     -> Does not cause the actual execution.		
	
	2. Actions
	    -> Action commands trigger the execution on the RDD
	    -> Converts the logical plan into a phisical execution plan and causes a set of tasks
               to be sent to the executors.


  RDD Lineage DAG
  ----------------
  
    	-> Lineage DAG is a logical plan on how to create the partitions of the RDD when asked for.
    	-> Lineage DAG contains a graph of all dependencies on other RDDs all the way from the very first RDD
    	-> Lineage DAGs are created when you perform transformations
    	-> Loagical Plan is converted to physical plan and executes the tasks when you run an 'action' command. 


	rddFile = sc.textFile( file, 4 )
	Lineage DAG : (4) rddFile -> sc.textFile
		
	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG : (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG : (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG : (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

        rdd4.collect()   => returns a list object


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( .., 4 )
	rdd2 = rdd1.t2( ... )
	rdd3 = rdd1.t3( ... )
	rdd4 = rdd3.t4( ... )
	rdd5 = rdd3.t5( ... )
	rdd6 = rdd5.t6( ... )
	rdd6.persist(StorageLevel.MEMORY_AND_DISK)   ---> instruction to persist the partitions of rdd6
	rdd7 = rdd6.t7( ... )

	rdd6.collect()	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile -> t3 -> t5 -> t6 => rdd6 ==> collected

	rdd7.collect()
	lineage of rdd6: rdd7 -> rdd6.t7
	rdd6 -> t7 => rdd7  ==> collected

	rdd6.unpersist()


	Storage Levels
        --------------	
	1. MEMORY_ONLY		-> default, memory serialized 1x replicated
				   partitions are persisted only in RAM (if RAM is available)
	
	2. MEMORY_AND_DISK	-> disk memory serialized 1x replicated
				   partitions are persisted in RAM if RAM is available, else on disk

	3. DISK_ONLY		-> disk serialized 1x replicated

	4. MEMORY_ONLY_2	-> memory serialized 2x replicated	

	5. MEMORY_AND_DISK_2	-> disk memory serialized 2x replicated

	Commands
        --------	
	rdd1.persist()  
	rdd1.persist(StorageLevel.MEMORY_AND_DISK) 
	rdd1.cache()

	rdd1.unpersist()
       	 

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD
	
   
   RDD Transformations
   -------------------

    1. map		P: U -> V
			Object to Object Transformation
			Transforms each object of the input RDD to the corresponding object of the
			output RD by applying the function
			Input RDD: N objects, Output RDD: N objects. 

		rddFile.map(lambda x: len(x.split(" ")) ).collect()

   2. filter		P: U -> Boolean 
			The output RDD will have only those objects for which the function return True.  
			Input RDD: N objects, Output RDD: <= N objects. 

		rddFile.filter(lambda st : len(st.split(" ")) > 8).collect()

   3. glom		P: None
		   	Returns one list object per partition with all the objects of the partition	

		rdd1			rdd2 = rdd1.glom()
		
		P0: 1,2,3,4,5,6 -> glom -> P0: [1,2,3,4,5,6]
		P1: 2,3,5,6,7,8 -> glom -> P1: [2,3,5,6,7,8]
		P2: 9,0,3,6,5,7 -> glom -> P2: [9,0,3,6,5,7]

		rdd1.count() : 18 (int)	  rdd2.count(): 3 (list)

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.
			Input RDD: N objects, Output RDD: > N objects. 
	
		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partion transformation
			Transforms each input partition to an output partition by applying a function
			on the entire partition. 

		rdd1	  rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] ) 
		
		P0: 1,2,3,4,5,6 -> mapPartitions -> P0: 21
		P1: 2,3,5,6,7,8 -> mapPartitions -> P1: 31
		P2: 9,0,3,6,5,7 -> mapPartitions -> P2: 30

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p) ).collect()	


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get the partition-index as additional function parameter.

		 rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x*10), p) ).collect()


   7. distinct		P: None, Optional: numPartitions
			Returns an RDD with distinct elements of the input RDD.	
			Input RDD: N objects, Output RDD: <= N objects. 

  
   8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			sorts the objects of the RDD based on the output of the functions.
			Input RDD: N objects, Output RDD: N objects. 

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], True, 5).collect()

	
   Types of RDDs
   -------------

	Two types of RDDs

	-> Generic RDDs : RDD[U]
	-> Pair RDDs: RDD[(K, V)]	


   9. mapValues		P: U -> V
			Applied only on Pair RDDs
			Applies the function on only the value part of the (K,V) pairs.
			Input RDD: N objects, Output RDD: N objects. 

		rdd3 = rdd2.mapValues(lambda x: (x, x))


   10. groupBy		P: U -> V,  Optional: numPartitions
			Groups the objects based on the function output
			Returns a Pair RDD where the 
				key: unique function output
				value: (ResultIterable) grouped values that produced the key

			RDD[U].groupBy(U -> V) => RDD[(V, ResultIterable[U])]

		rdd1.groupBy(lambda x: x%2).mapValues(list).collect()
		rdd1.groupBy(lambda x: x%2, 1).mapValues(list).collect()

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.groupBy(lambda x: x, 1) \
            		.mapValues(len)


   11. randomSplit	P: list of ratios
			Returns a list of RDDs split rabdomly in the specified ratios. 	

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 5343)  # here, 5343 is a seed.


   12. repartition	P: numPartitions
			Used to increase or decrease the number of partitions of the output RDD
			Causes global shuffle.

		rddWords2 = rddWords.repartition(10)


   13. coalesce		P: numPartitions
			Used to decrease the number of partitions of the output RDD
			Causes partition merging

		rddWords2 = rddWords.coalesce(2)
	
   Recommendations
   ---------------
	-> The size of the Partition should be approx 128 MB  
	-> The number of partitions should be a multiple of number of CPU cores.
	-> The number of partitions is close to 2000, bump up more then 2000
	-> The number of cores per executor = 5


   14. partitionBy	P: numPartitions, Optional: partitioning function
			Applied only on Pair RDD
			Based on the function output of the keys of the RDD, the elements of the RDD
			are organized in partition. 

		rdd3 = rdd2.partitionBy(4, lambda x: x+10)


    15. union, intersection, subtract, cartesian


  ..ByKey Transformations
  -----------------------
     -> Applied only on pair RDD
     -> Wide transformations (optional parameter: numPartitions)
	 

    16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Applied only on Pair RDDs
			Sorts the elements of the RDD based on the key

		rdd3.sortByKey().glom().collect()
		rdd3.sortByKey(False).glom().collect()
		rdd3.sortByKey(False, 3).glom().collect()

    17. groupByKey	P: None, Optional: numPartitions
			Applied only on Pair RDDs
			Groups the elements of the RDD by Key
			We get a pair RDD with unique keys and grouped values (ResultIterable)
			NOTE: Avoid 'groupByKey' if possible. 	

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		   .flatMap(lambda x: x.split(" ")) \
            		   .map(lambda x: (x, 1)) \
            		   .groupByKey() \
            		   .mapValues(sum)

    18. reduceByKey	P: U, U -> U, Optional: numPartitions
			Reduces all the values of each unique-key first within each partitions and the across
			different partitions.
			Applied only on Pair RDDs            

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		    .flatMap(lambda x: x.split(" ")) \
            		    .map(lambda x: (x, 1)) \
            		    .reduceByKey(lambda x, y: x + y, 1)

    19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

    20. joins

    21. cogroup



  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile	P: non-existing directory path

  4. reduce		P: U, U -> U
			Reduces the entire RDD into one final value of the same type.
			Applies the reduce function iterativly first on each partition and then across partitions. 

		rdd1  	
		P0: 9, 4, 5, 6, 0, 6	 -> reduce -> -12 -> 16
		P1: 7, 3, 4, 2, 1, 5	 -> reduce -> -8
		P2: 7, 8, 9, 0, 3, 2, 5  -> reduce -> -20

		rdd1.reduce( lambda x, y: x - y )

  5. aggregate

	Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )

	rdd2.aggregate( ("", 0, 0), 
			lambda z, v: (z[0] + "," + str(v[0]), z[1]+v[1], z[2] if (z[2] > v[1]) else v[1]), 
			lambda a, b: (a[0] + "," + b[0], a[1] + b[1], a[2] if (a[2] > b[2]) else b[2]) 
		      )




