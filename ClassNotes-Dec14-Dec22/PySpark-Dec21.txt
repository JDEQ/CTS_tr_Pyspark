
  Agenda
  --------
   -> Spark - basics and architecture
   -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
   -> Spark SQL
   -> Machine Learning & Spark MLlib
   -> Introduction Spark Streaming


  Materials
  ---------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   
  
  Spark
  -----
     => Written in Scala programming language

     => Is a unified in-memory distributed computing framework.

     => Spark is polyglot
	 -> Scala, Java, Python, R	

     => Spark supports multiple cluster manager
	-> local, Spark Standalone, YARN, Mesos, Kubernetes.	

  Cluster
  -------
	=> Is a group of nodes whose cumulative resources can be used to ditribute the storage of very large
	   datasets and also to distribute processing on very large amount of data. 
 
   In-memory Computing Framework
   -----------------------------
	Spark allows to save the intermediate results of computations in memory so that the
        subsequent tasks can directly read this persisted data from memory and act upon it.
     
   Unified Framework
   -----------------
	=> Spark provides a consistent set of APIs for different analytical workloads based on the
	   same execution engine.	

          -> Batch Processsing of unstructured data	=> Spark Core API
	  -> Batch Processsing of structured data	=> Spark SQL
          -> Stream Processing 				=> Spark Streaming, Structured Streaming
          -> Predictive Analytics (ML)			=> Spark MLlib
          -> Graph Parallel Computations    		=> Spark GraphX


   Spark Architecture
   ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.


    Getting started with Spark
    --------------------------
		
	1. In your vLab :
		-> Launch the vLab by following the instructions in the email	
		-> Click on "CentOS 7" icon to launch the VM using userid & password.

		Launching a  PySpark Shell
		-> Lauch a Terminal
		-> Type 'pyspark' at the prompt

		Working with Jupyter Notebooks
                -> Open a Terminal
		-> Type the following:
			$jupyter notebook
			$jupyter notebook --allow-root

		-> There is a sample notebook available "PySparkTest.ipynb"

	2. Install Spyder/Jupyter Notebook on your local machine

		-> Install the "Anaconda Navigator"
			https://docs.anaconda.com/anaconda/navigator/index.html
			https://docs.anaconda.com/anaconda/install/windows/

		-> Setting up PySpark wityh Spyder or Jupyter Notebook
			=> Try pip install
			   => Open Anaconda Prompt (terminal Window)
			   => Type  C:> pip install pyspark

			=> if pip install is not working:
			   -> Try instruction given in the document to manually setup.
			      https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

	3. Signup for Databrick Community Edition (Free)
		=> URL: https://databricks.com/try-databricks
		
		=> Login with your credentials
		=> Read "Quickstart Tutorial" and "data import"


   RDD (Resilient Distributed Dataset)
   -----------------------------------

      => Fundamental data abstraction of Spark
     
      => Collection of distributed in-memory partitions. 
	   -> Each partition is a collection of objects.
       
      => RDDs are immutable

      => RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   What can you do on an RDD ?
   ---------------------------	
	Two things:

	1. Transformations
		-> Returns an RDD. No output is generated
		-> Causes Lineage DAG of the RDD to be generated at the driver.
		-> Execution is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan (lineage) into a physical execution plan
		-> Launches a job on the cluster.	
	
   How to create RDDs ?
   --------------------
	
	3 ways:

	1. Creating an RDD from some external data file such as a textfile

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Createing an RDD from programmatic data such as a collection

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3.  By applying transformations on existing RDDs

		rdd1 = rddFile.map(lambda x: x.upper())

   RDD Lineage
   ------------	

    An RDD lineage is created at the driver when an RDD object is created
    
    An RDD lineage is a logical plan (set of instructions) on how to create this RDD and tracks
    the hierarchy of the dependencies all the way from the very first RDD. 		


     -> rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG: (4) rddFile -> sc.textFile
		
     -> rdd2 = rddFile.map(lambda x: x.upper())
  	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile

     -> rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 
	
     -> rdd4 = rdd3.filter(lambda x: len(x) > 1)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

	sc.textFile -> map -> flatMap -> filter -> rdd4 ==> collected to the driver

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   RDD persistence
   ---------------

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )     -> instruction to spark to persist the rdd6 partitions. 
	rdd7 = rdd6.t7( .. )

	rdd6.collect() 
	
	rdd6 lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	execution: sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> list

	rdd7.collect()

	rdd7 lineage: rdd7 -> rdd6.t7
	execution: (rdd6) --> t7 (rdd7) --> list

	
	Storage Levels
        ---------------
	
	1. MEMORY_ONLY 		-> serialized, in-memory persistence

	2. MEMORY_AND_DISK	-> serialized, disk-memory persistence
	
	3. DISK_ONLY

        4. MEMORY_ONLY_SER	->  (not applicable to pyspark)

	5. MEMORY_AND_DISK_SER  ->  (not applicable to pyspark)

	6. MEMORY_ONLY_2        -> 2 replicas on on two different executors.

	7. MEMORY_AND_DISK_2


	Commands:
        ---------	
	  rdd1.persist()     
	  rdd1.persist( StorageLevel.DISK_ONLY )
	  rdd1.cache()  -> in-memory persistence

	  rdd1.unpersist()
	   

  RDD Transformations
  -------------------

    1. map			P: U -> V
				Element to Element transformtion
				input RDD: N elements, Output RDD: N elements

		rdd1.map(lambda x: x > 8).collect()	
		rddFile.map(lambda x: len(x.split(" ")) ).collect()


    2. filter			P: U -> Boolean	
				Only those elements that return Trye for the function will be in the output.
	  			input RDD: N elements, Output RDD: <= N elements

		rddCarsAmerican = rddCars1.filter(lambda x: x[-1] == "American")


   3. glom			P: None
				Creates one list object per partition with all its elements.
	
	rdd1				rdd2 = rdd1.glom()

	P0:  2,3,1,4,5,6,7  -> glom ->  P0: [2,3,1,4,5,6,7]
	P1:  3,4,2,6,7,8,0  -> glom ->  P1: [3,4,2,6,7,8,0]
	P2:  4,2,6,8,9,1,8  -> glom ->  P2: [4,2,6,8,9,1,8]

	rdd1.count() = 21 (int)		rdd2.count() = 3 (lists)

	rddCarsAmerican.glom().map(len).collect()


   4. flatMap			P: U -> Iterable[V]
				flatMap flattens all the objects of the iterables produced by the function.
    				input RDD: N elements, Output RDD: >= N elements

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				mapPartitions applies a function on the entire partition.
                                (i.e the input to the function is the entire partition)
		
	rdd1				rdd2 = rdd1.mapPartitions ( lambda x : y )

	P0:  2,3,1,4,5,6,7  -> mapPartitions  -> P0: 
	P1:  3,4,2,6,7,8,0  -> mapPartitions  -> P1:
	P2:  4,2,6,8,9,1,8  -> mapPartitions  -> P2:

	rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).glom().collect()
	rdd1.mapPartitions(lambda x: [sum(x)] ).collect()


   6. mapPartitionsWithIndex	P: (int, Iterable[U]) -> Iterable[V]	
				mapPartitionsWithIndex is asme as mapPartitions but it get partition-index
				as an additional function parameter.

	rdd1.mapPartitionsWithIndex(lambda index, data: map(lambda a: (index, a*10), data)).glom().collect()
	rdd1.mapPartitionsWithIndex(lambda i, x: [(i, sum(x))] ).collect()

   
   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct elements of the input RDD


         rdd2 = rdd1.distinct()       # number of partitions of rdd2 =  number of partitions of rdd1
	 rdd2 = rdd1.distinct(4)      # number of partitions of rdd2 = 4  

   
   8. sortBy			P:  U -> V, Optional: ascending (True/False), numPartitions
				Elements of the RDD are sorted based on function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, False, 10).glom().collect()

   Types of RDDs
   -------------
	=> Generic RDDs : RDD[U]
	=> Pair RDDs    : RDD[(U, V)]

   9. mapValues			P: U -> V
				Applied only on Pair RDDs
				Transforms only the 'value' part of the (k, v) pairs by applying the function.	

		rddPairs2.mapValues(lambda x: 'A' + str(x)).collect()	

   10. groupBy			P: U -> V, Optional: numPartitions
				Groups the objects the RDD based on the function output.
				Returns a pair RDD where:
					key: Is the unique-value of the function output
					value: ResultIterable object with all the objects of the RDD that produced the key.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)	


		rdd1.groupBy(lambda x: x > 4, 2).mapValues(list).glom().collect()   // output has 2 partitions

   11. randomSplit		P: A list of ratios
				Splits the rdd randomly into multiple rdds in the approx. specified ratios

	  rddList = rdd1.randomSplit([0.5, 0.5], 657)


   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of the output RDD. 
				Performs global shuffle

		rdd2 = rdd1.repartition(5)   // rdd2 will have 5 partitions

   13. coalesce			P: numPartitions
				Used to decrease the number of partitions of the output RDD. 

		rdd2 = rdd1.coalesce(2)   // rdd2 will have 2 partitions


   15. partitionBy		P: numPartitions, Optional: partitioning-function (U -> Int)	
				Applied only to pair RDDs.

				Is used to control which objects should go to which partitions based on
				the key of the (K, V) pairs.

	  rddPairs1.partitionBy(3, lambda key: key%4).glom().collect()


   16. union, intersection, subtract, cartesian

	 Assume rdd1 has M partitions, rdd2 has N partitions
		
	 command			# of output partitions
         ----------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd10.cartesian(rdd11)		M * N, wide


   ..ByKey Transformations
   -----------------------
	-> Are wide transformations
	-> Are applied to only pair RDDs

    17. sortByKey			P: None, Optional: ascending, numPartitions
					Sorts the RDD by key 	

		rdd5.sortByKey().glom().collect()
		rdd5.sortByKey(False).glom().collect()
		rdd5.sortByKey(True, 8).glom().collect()

   18. groupByKey		P: None, Optional: numPartitions
				Groups the elements with unique keys and grouped values.
				GroupByKey cause global shuffle. Hence inefficient. 

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey(1) \
        		.mapValues(sum)

   19. reduceByKey		P: (U, U) -> U
				Reduces all the values of each unique key by applying the reduce
				function on the partitions of the RDD.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y)

   20. aggregateByKey	Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.  	

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)


	student_rdd.glom().collect()

	output_rdd = student_rdd.map(lambda x: (x[0], x[2])) \
               .aggregateByKey( 
                   (0,0),
                   lambda z,v: (z[0]+v, z[1]+1),
                   lambda a,b:(a[0]+b[0],a[1]+b[1]),
                   1
                ) \
                .mapValues(lambda x: x[0]/x[1])


   21. Joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin
		  
		RDD[(U, V)].join(RDD[(U, W)]) => RDD[(U, (V,W))]

		names1 = sc.parallelize(["vijay", "aditya", "raju", "amrita"]).map(lambda a: (a, 1))
		names2 = sc.parallelize(["amrita", "raju", "pavan", "pranav"]).map(lambda a: (a, 2))

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   22. cogroup => groupByKey + fullOuterJoin


	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	   => [(key1, [10, 7]), (key2, [12,6]), (key3, [6])]

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
	    => [(key1, [5, 17]), (key2, [4,7]), (key4, [17])]	

	  [(key1, ([10, 7], [5, 17])), (key2, ([12,6], [4,7])), (key3, ([6], [])), (key4, ([], [17]))]
 

  RDD Actions
  -----------

    1. collect

    2. count

    3. saveAsTextFile

    4. reduce			F: (U, U) -> U
				Reduces the entire RDD into one final value of the same type  y iterativly
				applying the function
		rdd1 

		P0: 6, 5, 3, 2, 1  => -4  => 18
		P1: 9, 7, 5, 2, 3  => -8
		P2: 8, 7, 6, 5, 4  => -14

		rdd1.reduce(lambda x, y : x - y)

   5. aggregate	        -> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final values of the type
                  of the zero-value.


   6. first	=> rdd1.first()

   7. take	=> rdd1.take(10)

   8. takeOrdered
	
	rdd1.takeOrdered(10)
	rdd1.takeOrdered(10, lambda x: x%3)

   9. takeSample

	rdd1.takeSample(True, 10)   	 // withReplacement
	rdd1.takeSample(True, 10, 5675)  // withReplacement with seed
	rdd1.takeSample(False, 10)	 // withReplacement : false (without replacement sampling)

  10. countByValue

  11. countByKey

  12. foreach	=> Take a function that does not return anything as a parameter
		   Applies the function on all the objects of the rdd.

  13. saveAsSequenceFile		

		
   Use-Case
   --------
   From cars.tsv dataset, find out the average weight of each make of 'American' cars	
   Arrange the data in the desc order of average weight
   Save the output as a single textfile.

     --> Try to solve this. 
     --> Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv


  Closure
  -------
     -> Closure consists of those variables and methods that must be visible to an executor
        to perform its comoutations on the RDDs.

     -> This closure (set of code) is serialized and a copy of this is sent to every executor  

	counter = 0

	def isPrime(a) :
		returns 1 if a is Prime
		else returns 0

	def f1(n) :
		global counter
		if (isPrime(n) == 1) counter = counter + 1
		return n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
        
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	println( counter )    // 0


      => Limitation: Local Variable can not used to capture a global state of something across all
                     partitions. Because these variables become 'local copies' and hence they capture
		     only state, and that can not be proparated back to the driver.  


  Shared Variables
  ----------------

   1. Accumulator
	=> Used to implement global counters.
  	
	counter = sc.accumulator(0)

	def isPrime(a) :
		returns 1 if a is Prime
		else returns 0

	def f1(n) :
		global counter
		if (isPrime(n) == 1) counter.add(1)
		return n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
        
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	println( counter.value )   // 120
 

    2. Broadcast
	-> A broadcast variable is not part of closure. Driver sends one copy per executor node and tasks
	   running in that executor can access (read) that variable. 
 
	-> Broadcasting large immutable variables will save a lot of memory


	d1 = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, ..... })   // 100 MB
	
        def f1(n) :
		global d1
		return d1.value[n]
	
        rdd1 = sc.parallelize([1,2,3,4,5,......], 4)
	
	rdd2 = rdd1.map( f1 )

	rdd2.collect()    


   Spark-submit
   ------------
	=> Is a single command to submit any spark application (Scala, Java, Python, R) to any cluster manager
    
	=> spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--driver-cores 3
		--num-executors 20
                wordcount.py <command-line-args>

  spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2
  spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount.py


 ======================================    
    Spark SQL  (pyspark.sql)
 ======================================
   
   -> Spark's structured data processing API

   -> Using Spark SQL, you can process:

	-> Structured data formats: Parquet (default), ORC, JSON, CSV (delimited text file)
	-> Hive 
	-> JDBC sources : RDBMS, NoSQL databases

   -> SparkSession
	-> Is the starting point of execution in Spark SQL
	-> Represents a user-session inside an application (sparkcontext)
	-> Introduced from Spark 2.0 onwards

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate() 

   -> DataFrame (DF)
	-> It is a collection of distributed in-memory partitions that are immutable and lazily evaluated.
	-> Is a collection of "Row" objects  (pyspark.sql.Row)
		-> Row is a collection of Columns
        -> DF has two components:
		-> data    : in-memory partitions of Row objects
		-> schema  : represents the structure of the DF
			     is an object of type "StructType"

		      StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
		      )


   How to work with DataFrames ?
   -----------------------------
   
    1. Read/load the data from some structured data source into a DataFrame.

	filePath = "E:\\PySpark\\data\\users.json"
	df1 = spark.read.format("json").load(filePath)
	df1 = spark.read.json(filePath)   # another syntax for reading from json


    2. Apply the transformations on the DFs using DF API methods or using SQL.

	Using DF API methods
        ---------------------
		df2 = df1.select("userid", "name", "age", "phone") \
        		.where("age is not null") \
        		.groupBy("age").count() \
        		.orderBy("count") \
        		.limit(4)
	Using SQL
        ---------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count 
         		from users
         		where age is not null
         		group by age
         		order by count
         		limit 4"""
         
		df3 = spark.sql(qry)


    3. Write/save the content of a DF in a stuctured file or a database/hive etc.    

	outputDir = "E:\\PySpark\\output\\json"
	df3.write.format("json").save(outputDir)
	df3.write.json(outputDir) # another syntax for writing to json


  SaveModes
  ---------
     -> Specity what to happen when writing to an existing directory.

	-> errorIfExists
	-> ignore
	-> append
	-> overwrite

	df3.write.json(outputDir, mode="overwrite")
	df3.write.mode("overwrite").json(outputDir)	
  

  LocalTempView & GlobalTempView
  -------------------------------

	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.
		-> Attached to a database called "global_temp"


  DataFrame Transformations
  -------------------------

   1. select

	df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

	df2 = df1.select(col("DEST_COUNTRY_NAME").alias("destination"), 
                 column("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 365 as highFrequency"),
                 expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

   2. where / filter

	df3 = df2.where("domestic = False and count > 100")
	df3 = df2.where(col("count") > 100)

   3. orderBy / sort

	df3 = df2.orderBy("count", "destination")
	df3 = df2.orderBy(desc("count"), asc("destination"))

   4. groupBy   -> returns a "GroupedData" object
		   apply some aggregation methods on the "GroupedData" object to return a DF

	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
         	.agg(   count("count").alias("count"), 
               		sum("count").alias("sum"), 
               		avg("count").alias("avg"), 
               		max("count").alias("max") )
        

   5. limit

	df1.limit(10)

   6. selectExpr

		df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination", 
                 		"ORIGIN_COUNTRY_NAME as origin", 
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 365 as highFrequency",
                 		"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
		df2.show(20)
		df2.printSchema()

   7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 365")) \
        	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))

	df3.show(5)


   8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	 .withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

   9. drop

	df3 = df2.drop("newCount", "highFrequency")

   10. union

	df3 = df2.where("count > 1000")
	df4 = df2.where("destination = 'India'")
	df5 = df4.union(df3)

   11. sample

	df3 = df2.sample(True, 0.6)  	    	# sample 60% withReplacement
	df3 = df2.sample(True, 1.6)		# sample 160% withReplacement - VALID
	df3 = df2.sample(True, 0.6, 464)	# sample 60% withReplacement with seed

	df3 = df2.sample(False, 0.6, 464)	# sample 60% without Replacement with seed
	df3 = df2.sample(False, 1.6)		# ERROR - sample 160% without Replacement - INVALID

   12. randomSplit

	df10, df11 = df2.randomSplit([0.6, 0.4], 3543)
	df10.count()
	df11.count()

   13. distinct
	df2.select("origin").distinct().count()

   14. repartition

	df3 = df2.repartition(4)
	df3.rdd.getNumPartitions()

	df4 = df3.repartition(2)
	df4.rdd.getNumPartitions()

	df5 = df2.repartition( col("destination") )
	df5.rdd.getNumPartitions()
		
	     => You can repartition based on the (hash of a) column. 
	     => The default number fo shuffle partitions is determined by the value of 
		"spark.sql.shuffle.partitions" (default: 200)
	     => You can set the value as shown:
		spark.conf.set("spark.sql.shuffle.partitions", 10)
		

	df6 = df2.repartition( 8, col("destination") )
	df6.rdd.getNumPartitions()

   15. coalesce 

	df7 = df6.coalesce(4)
	df7.rdd.getNumPartitions()


   16. join   ==> discussed seperatly ..


   Working with Strcutured file formats
   ------------------------------------	
	JSON
		read:
			 
			df1 = spark.read.format("json").load(filePath)
			df1 = spark.read.json(filePath)
		write:
			df3.write.format("json").save(outputDir)
			df3.write.json(outputDir)

	Parquet 

		read:
			 
			df1 = spark.read.format("parquet").load(filePath)
			df1 = spark.read.parquet(filePath)
		write:
			df3.write.format("parquet").save(outputDir)
			df3.write.parquet(outputDir)

	ORC
		read:
			 
			df1 = spark.read.format("orc").load(filePath)
			df1 = spark.read.orc(filePath)
		write:
			df3.write.format("orc").save(outputDir)
			df3.write.orc(outputDir)

	CSV
		read:
			df1 = spark.read.format("csv").load(filePath, header=True, inferSchema=True)
			df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(filePath)
			df1 = spark.read.csv(filePath, header=True, inferSchema=True)
			df1 = spark.read.csv(filePath, header=True, inferSchema=True, sep="|")
		write:
			df3.write.format("csv").save(outputDir, header=True)
			df2.write.csv(outputDir, header=True)
			df2.write.csv(outputDir, header=True, sep="|")



   Creating an RDD from a DataFrame
   ---------------------------------
	rdd1 = df1.rdd


   Creating a DataFrame from programmatic data
   ---------------------------------------------
   	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Raghu", 25),
             (4, "Rajeev", 45),
             (5, "Ramya", 35),
             (6, "Radhika", 25),
             (7, "Raghava", 45)]

	df1 = spark.createDataFrame(listUsers).toDF("userid", "name", "age")	


   Creating a DataFrame from an RDD
   --------------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	df1 = spark.createDataFrame(rdd1).toDF("userid", "name", "age")


   Creating a DataFrame from an RDD with custom schema
   ---------------------------------------------------
	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)


   Creating a DataFrame from an external file with custom schema
   -------------------------------------------------------------

	filePath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.json(filePath, schema=mySchema)

   Joins
   ------

    Supported:  inner, left_outer, right_outer, full_outer, left_semi, left_anti

    Left-Semi join
    --------------
        -> Is like inner join, but the data comes only from the left-side table.
	-> Equivalent to the following sub-query:
		select * from emp where deptid IN (select id from dept)

    Left-Anti join
    --------------
        -> Equivalent to the following sub-query:
		select * from emp where deptid NOT IN (select id from dept)

	employee = spark.createDataFrame([
    		(1, "Raju", 25, 101),
    		(2, "Ramesh", 26, 101),
    		(3, "Amrita", 30, 102),
    		(4, "Madhu", 32, 102),
    		(5, "Aditya", 28, 102),
    		(6, "Pranav", 28, 100)])\
  		.toDF("id", "name", "age", "deptid")     

	department = spark.createDataFrame([
    		(101, "IT", 1),
    		(102, "ITES", 1),
    		(103, "Opearation", 1),
    		(104, "HRD", 2)])\
  		.toDF("id", "deptname", "locationid")
  
	employee.show()
	department.show()


   SQL Approach
   ------------
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")
  
	qry = """select emp.*, dept.* 
        	from emp full outer join dept
       	 	on emp.deptid = dept.id"""
        
	joinedDf = spark.sql(qry)
	joinedDf.show()


   DF API approach
   ---------------
	Join Types:  inner, left_outer, right_outer, full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti" )
	joinedDf.show()

	// You can enforce a broadcast join by using "broadcast" method.
	joinedDf = employee.join(broadcast(department), joinCol, "inner" )


  Use-Case  
  --------

   From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average
   user ratings.
   -> Consider only those movies with atleast 30 ratings (numberOfRatings >= 30)
   -> Data required: movieId, title, totalRatings, avgRating
   -> Arrange the data in the DESC order of avgRating
   -> Save the data as a single CSV file with pipe ("|") as delimter with header. 

      ==> Try this yourself
      ==> Datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens


  JDBC Format:  Working with MySQL
  --------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()

  
  Working with Hive
  -----------------
  
   Hive => Hive is a data warehousing platform on Hadoop	
       1. warehouse :  directory where Hive stores all the data files of its managed tables. 
       2. metastore :  external RDBMS service where Hive stores all its metadata. 



 # -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


 ===========================================
     Machine Learning & Spark MLlib
 ===========================================

   ML Model : Learned Entity
	      Learn from historic data
	      Based on that learning, an ML model can:
		-> predict the output when new inputs are given
		-> Identify the patterns in the data
	        -> Recommendations, Forcasting, Projections

   => A ML model is created by an algorithm. 

	 
  Terminology
  ------------

   1. Features       : inputs, dimensions

   2. Label	     : output  

   3. Training Data  : Data that is used to train a model.         
   
   4. ML Algorithm   : Is an iterative mathematical computation that establishes a relation
		       between features and label with a goal to minimize the loss. 

   5. ML Model       : The final output of an algorithmic process. 

   6. Error	     : difference between the actual and predicted values of some data point

   7. Loss	     : Effective error of the entire data

   8. Loss function  : Is the fucntion based on which the loss is computed.

	X	Y	Z(lbl)   pred.  error
	----------------------------------------
	2000	500	4400	4500	-100
	1000	1000	3050	3000	  50
	1500	400	3500	3400	 100
	1200	600	2920	3000	 -80
	1100	300	?
	-------------------------------------
			  Loss: 330/4 = 82.5

        model 1 :  z = 2x + y  	    	   ==> loss: 82.5
	model 2 :  z = 1.9x + 0.9y  	   ==> loss: 80
	model 3 :  z = 1.95x + 0.95y - 2   ==> loss: 78   (final model with minimal loss)

    
   Steps in a machine learning project
   -----------------------------------

   1. Data Collection
	
   2. Data Preparation 

	Two Steps:
	
	2.1  EDA (Exploratory data analysis)
	2.2  FE  (Feature engineering)

	-> All the data must be numeric
        -> There should be no nulls, empty data etc. 

	Output: Is some prepared data that can given to an algorithm for training.
		Creating a "Feature Vector"

   3. Train the model using one or more algorithms.

	Output: trained (but not evaluated) model

	=> Split the prepared data into train (70%) and validation (30%) sets
	=> Train the model using train dataset (70%)
	=> Get the prediction using the model on the validation (30%) set
	=> By comparing the predictions and actuals, you can measure the accuracy.

   4. Evaluate the model

	Output: evaluated model

   5. Deploy the model


   Types of Machine Learning
   --------------------------

      1. Supervised Learning
	  
           -> Training data contains both label and features

	   1.1  Classification
		-> Label is one of two/few fixed values.
                -> Ex: 1/0, [1,2,3,4,5]	
		-> Email Spam prediction, Survival prediction etc.
	    
	   1.2  Regression
		-> Label is a continuous value
		-> House Price Prediction.

      2. Unsupervised Learning
	 
      	-> Training data contains only features
	-> here model identifies the patterns in the data to group the data into clusters

	  2.1  Clustering (& Collaborative Filtering)

	  2.2  Dimesionality Reduction	


      3. Reinforcement Learning












