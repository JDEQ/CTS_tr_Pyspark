
  Agenda
  --------
   -> Spark - basics and architecture
   -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
   -> Spark SQL
   -> Machine Learning & Spark MLlib
   -> Introduction Spark Streaming


  Materials
  ---------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   
  
  Spark
  -----
     => Written in Scala programming language

     => Is a unified in-memory distributed computing framework.

     => Spark is polyglot
	 -> Scala, Java, Python, R	

     => Spark supports multiple cluster manager
	-> local, Spark Standalone, YARN, Mesos, Kubernetes.	

  Cluster
  -------
	=> Is a group of nodes whose cumulative resources can be used to ditribute the storage of very large
	   datasets and also to distribute processing on very large amount of data. 
 
   In-memory Computing Framework
   -----------------------------
	Spark allows to save the intermediate results of computations in memory so that the
        subsequent tasks can directly read this persisted data from memory and act upon it.
     
   Unified Framework
   -----------------
	=> Spark provides a consistent set of APIs for different analytical workloads based on the
	   same execution engine.	

          -> Batch Processsing of unstructured data	=> Spark Core API
	  -> Batch Processsing of structured data	=> Spark SQL
          -> Stream Processing 				=> Spark Streaming, Structured Streaming
          -> Predictive Analytics (ML)			=> Spark MLlib
          -> Graph Parallel Computations    		=> Spark GraphX


   Spark Architecture
   ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.


    Getting started with Spark
    --------------------------
		
	1. In your vLab :
		-> Launch the vLab by following the instructions in the email	
		-> Click on "CentOS 7" icon to launch the VM using userid & password.

		Launching a  PySpark Shell
		-> Lauch a Terminal
		-> Type 'pyspark' at the prompt

		Working with Jupyter Notebooks
                -> Open a Terminal
		-> Type the following:
			$jupyter notebook
			$jupyter notebook --allow-root

		-> There is a sample notebook available "PySparkTest.ipynb"

	2. Install Spyder/Jupyter Notebook on your local machine

		-> Install the "Anaconda Navigator"
			https://docs.anaconda.com/anaconda/navigator/index.html
			https://docs.anaconda.com/anaconda/install/windows/

		-> Setting up PySpark wityh Spyder or Jupyter Notebook
			=> Try pip install
			   => Open Anaconda Prompt (terminal Window)
			   => Type  C:> pip install pyspark

			=> if pip install is not working:
			   -> Try instruction given in the document to manually setup.
			      https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

	3. Signup for Databrick Community Edition (Free)
		=> URL: https://databricks.com/try-databricks
		
		=> Login with your credentials
		=> Read "Quickstart Tutorial" and "data import"


   RDD (Resilient Distributed Dataset)
   -----------------------------------

      => Fundamental data abstraction of Spark
     
      => Collection of distributed in-memory partitions. 
	   -> Each partition is a collection of objects.
       
      => RDDs are immutable

      => RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   What can you do on an RDD ?
   ---------------------------	
	Two things:

	1. Transformations
		-> Returns an RDD. No output is generated
		-> Causes Lineage DAG of the RDD to be generated at the driver.
		-> Execution is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan (lineage) into a physical execution plan
		-> Launches a job on the cluster.	
	
   How to create RDDs ?
   --------------------
	
	3 ways:

	1. Creating an RDD from some external data file such as a textfile

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Createing an RDD from programmatic data such as a collection

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3.  By applying transformations on existing RDDs

		rdd1 = rddFile.map(lambda x: x.upper())

   RDD Lineage
   ------------	

    An RDD lineage is created at the driver when an RDD object is created
    
    An RDD lineage is a logical plan (set of instructions) on how to create this RDD and tracks
    the hierarchy of the dependencies all the way from the very first RDD. 		


     -> rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG: (4) rddFile -> sc.textFile
		
     -> rdd2 = rddFile.map(lambda x: x.upper())
  	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile

     -> rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 
	
     -> rdd4 = rdd3.filter(lambda x: len(x) > 1)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

	sc.textFile -> map -> flatMap -> filter -> rdd4 ==> collected to the driver

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   RDD persistence
   ---------------

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )     -> instruction to spark to persist the rdd6 partitions. 
	rdd7 = rdd6.t7( .. )

	rdd6.collect() 
	
	rdd6 lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	execution: sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> list

	rdd7.collect()

	rdd7 lineage: rdd7 -> rdd6.t7
	execution: (rdd6) --> t7 (rdd7) --> list

	
	Storage Levels
        ---------------
	
	1. MEMORY_ONLY 		-> (not applicable to pyspark) 
				   deserialized, in-memory persistence

	2. MEMORY_AND_DISK	-> (not applicable to pyspark) 
	
	3. DISK_ONLY

        4. MEMORY_ONLY_SER	-> (default in pyspark) serialized, in-memory persistence

	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2        -> 2 replicas on on two different executors.

	7. MEMORY_AND_DISK_2


	Commands:
        ---------	
	  rdd1.persist()     
	  rdd1.persist( StorageLevel.DISK_ONLY )
	  rdd1.cache()  -> in-memory persistence

	  rdd1.unpersist()
	   

  RDD Transformations
  -------------------

    1. map			P: U -> V
				Element to Element transformtion
				input RDD: N elements, Output RDD: N elements

		rdd1.map(lambda x: x > 8).collect()	
		rddFile.map(lambda x: len(x.split(" ")) ).collect()


    2. filter			P: U -> Boolean	
				Only those elements that return Trye for the function will be in the output.
	  			input RDD: N elements, Output RDD: <= N elements

		rddCarsAmerican = rddCars1.filter(lambda x: x[-1] == "American")


   3. glom			P: None
				Creates one list object per partition with all its elements.
	
	rdd1				rdd2 = rdd1.glom()

	P0:  2,3,1,4,5,6,7  -> glom ->  P0: [2,3,1,4,5,6,7]
	P1:  3,4,2,6,7,8,0  -> glom ->  P1: [3,4,2,6,7,8,0]
	P2:  4,2,6,8,9,1,8  -> glom ->  P2: [4,2,6,8,9,1,8]

	rdd1.count() = 21 (int)		rdd2.count() = 3 (lists)

	rddCarsAmerican.glom().map(len).collect()


   4. flatMap			P: U -> Iterable[V]
				flatMap flattens all the objects of the iterables produced by the function.
    				input RDD: N elements, Output RDD: >= N elements

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				mapPartitions applies a function on the entire partition.
                                (i.e the input to the function is the entire partition)
		
	rdd1				rdd2 = rdd1.mapPartitions ( lambda x : y )

	P0:  2,3,1,4,5,6,7  -> mapPartitions  -> P0: 
	P1:  3,4,2,6,7,8,0  -> mapPartitions  -> P1:
	P2:  4,2,6,8,9,1,8  -> mapPartitions  -> P2:

	rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).glom().collect()
	rdd1.mapPartitions(lambda x: [sum(x)] ).collect()


   6. mapPartitionsWithIndex	P: (int, Iterable[U]) -> Iterable[V]	
				mapPartitionsWithIndex is asme as mapPartitions but it get partition-index
				as an additional function parameter.

	rdd1.mapPartitionsWithIndex(lambda index, data: map(lambda a: (index, a*10), data)).glom().collect()
	rdd1.mapPartitionsWithIndex(lambda i, x: [(i, sum(x))] ).collect()

   
   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct elements of the input RDD


         rdd2 = rdd1.distinct()       # number of partitions of rdd2 =  number of partitions of rdd1
	 rdd2 = rdd1.distinct(4)      # number of partitions of rdd2 = 4  

   
   8. sortBy			P:  U -> V, Optional: ascending (True/False), numPartitions
				Elements of the RDD are sorted based on function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, False, 10).glom().collect()

   Types of RDDs
   -------------
	=> Generic RDDs : RDD[U]
	=> Pair RDDs    : RDD[(U, V)]

   9. mapValues			P: U -> V
				Applied only on Pair RDDs
				Transforms only the 'value' part of the (k, v) pairs by applying the function.	

		rddPairs2.mapValues(lambda x: 'A' + str(x)).collect()	

   10. groupBy			P: U -> V, Optional: numPartitions
				Groups the objects the RDD based on the function output.
				Returns a pair RDD where:
					key: Is the unique-value of the function output
					value: ResultIterable object with all the objects of the RDD that produced the key.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)	


		rdd1.groupBy(lambda x: x > 4, 2).mapValues(list).glom().collect()   // output has 2 partitions

   11. randomSplit		P: A list of ratios
				Splits the rdd randomly into multiple rdds in the approx. specified ratios

	  rddList = rdd1.randomSplit([0.5, 0.5], 657)


   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of the output RDD. 
				Performs global shuffle

		rdd2 = rdd1.repartition(5)   // rdd2 will have 5 partitions

   13. coalesce			P: numPartitions
				Used to decrease the number of partitions of the output RDD. 

		rdd2 = rdd1.coalesce(2)   // rdd2 will have 2 partitions


   15. partitionBy		P: numPartitions, Optional: partitioning-function (U -> Int)	
				Applied only to pair RDDs.

				Is used to control which objects should go to which partitions based on
				the key of the (K, V) pairs.

	  rddPairs1.partitionBy(3, lambda key: key%4).glom().collect()


   16. union, intersection, subtract, cartesian

	 Assume rdd1 has M partitions, rdd2 has N partitions
		
	 command			# of output partitions
         ----------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd10.cartesian(rdd11)		M * N, wide


   ..ByKey Transformations
   -----------------------
	-> Are wide transformations
	-> Are applied to only pair RDDs

    17. sortByKey			P: None, Optional: ascending, numPartitions
					Sorts the RDD by key 	

		rdd5.sortByKey().glom().collect()
		rdd5.sortByKey(False).glom().collect()
		rdd5.sortByKey(True, 8).glom().collect()

   18. groupByKey			P: None, Optional: numPartitions
					Groups the elements with unique keys and grouped values.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey(1) \
        		.mapValues(sum)

   19. reduceByKey	==> to be continued
 

  RDD Actions
  -----------

    1. collect

    2. count

    3. saveAsTextFile

    4. reduce			F: (U, U) -> U
				Reduces the entire RDD into one final value of the same type  y iterativly
				applying the function
		rdd1 

		P0: 6, 5, 3, 2, 1  => -4  => 18
		P1: 9, 7, 5, 2, 3  => -8
		P2: 8, 7, 6, 5, 4  => -14

		rdd1.reduce(lambda x, y : x - y)	

		
		








