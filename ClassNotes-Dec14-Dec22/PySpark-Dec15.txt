
  Agenda
  --------
   -> Spark - basics and architecture
   -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
   -> Spark SQL
   -> Machine Learning & Spark MLlib
   -> Introduction Spark Streaming


  Materials
  ---------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   
  
  Spark
  -----
     => Written in Scala programming language

     => Is a unified in-memory distributed computing framework.

     => Spark is polyglot
	 -> Scala, Java, Python, R	

     => Spark supports multiple cluster manager
	-> local, Spark Standalone, YARN, Mesos, Kubernetes.	

  Cluster
  -------
	=> Is a group of nodes whose cumulative resources can be used to ditribute the storage of very large
	   datasets and also to distribute processing on very large amount of data. 
 
   In-memory Computing Framework
   -----------------------------
	Spark allows to save the intermediate results of computations in memory so that the
        subsequent tasks can directly read this persisted data from memory and act upon it.
     
   Unified Framework
   -----------------
	=> Spark provides a consistent set of APIs for different analytical workloads based on the
	   same execution engine.	

          -> Batch Processsing of unstructured data	=> Spark Core API
	  -> Batch Processsing of structured data	=> Spark SQL
          -> Stream Processing 				=> Spark Streaming, Structured Streaming
          -> Predictive Analytics (ML)			=> Spark MLlib
          -> Graph Parallel Computations    		=> Spark GraphX


   Spark Architecture
   ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.


    Getting started with Spark
    --------------------------
		
	1. In your vLab :
		-> Launch the vLab by following the instructions in the email	
		-> Click on "CentOS 7" icon to launch the VM using userid & password.

		Launching a  PySpark Shell
		-> Lauch a Terminal
		-> Type 'pyspark' at the prompt

		Working with Jupyter Notebooks
                -> Open a Terminal
		-> Type the following:
			$jupyter notebook
			$jupyter notebook --allow-root

		-> There is a sample notebook available "PySparkTest.ipynb"

	2. Install Spyder/Jupyter Notebook on your local machine

		-> Install the "Anaconda Navigator"
			https://docs.anaconda.com/anaconda/navigator/index.html
			https://docs.anaconda.com/anaconda/install/windows/

		-> Setting up PySpark wityh Spyder or Jupyter Notebook
			=> Try pip install
			   => Open Anaconda Prompt (terminal Window)
			   => Type  C:> pip install pyspark

			=> if pip install is not working:
			   -> Try instruction given in the document to manually setup.
			      https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

	3. Signup for Databrick Community Edition (Free)
		=> URL: https://databricks.com/try-databricks
		
		=> Login with your credentials
		=> Read "Quickstart Tutorial" and "data import"


   RDD (Resilient Distributed Dataset)
   -----------------------------------

      => Fundamental data abstraction of Spark
     
      => Collection of distributed in-memory partitions. 
	   -> Each partition is a collection of objects.
       
      => RDDs are immutable

      => RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   What can you do on an RDD ?
   ---------------------------	
	Two things:

	1. Transformations
		-> Returns an RDD. No output is generated
		-> Causes Lineage DAG of the RDD to be generated at the driver.
		-> Execution is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan (lineage) into a physical execution plan
		-> Launches a job on the cluster.	
	
   How to create RDDs ?
   --------------------
	
	3 ways:

	1. Creating an RDD from some external data file such as a textfile

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Createing an RDD from programmatic data such as a collection

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3.  By applying transformations on existing RDDs

		rdd1 = rddFile.map(lambda x: x.upper())

   RDD Lineage
   ------------	

    An RDD lineage is created at the driver when an RDD object is created
    
    An RDD lineage is a logical plan (set of instructions) on how to create this RDD and tracks
    the hierarchy of the dependencies all the way from the very first RDD. 		


     -> rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG: (4) rddFile -> sc.textFile
		
     -> rdd2 = rddFile.map(lambda x: x.upper())
  	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile

     -> rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 
	
     -> rdd4 = rdd3.filter(lambda x: len(x) > 1)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

	sc.textFile -> map -> flatMap -> filter -> rdd4 ==> collected to the driver

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   RDD persistence
   ---------------

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_ONLY )          --> instruction to spark to persist the rdd6 partitions. 
	rdd7 = rdd6.t7( .. )

	rdd6.collect() 
	
	rdd6 lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	execution: sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> list

	rdd7.collect()

	rdd7 lineage: rdd7 -> rdd6.t7
	execution: (rdd6) --> t7 (rdd7) --> list

	
	Storage Levels
        ---------------
	
	1. MEMORY_ONLY 		-> (not applicable to pyspark) 
				   deserialized, in-memory persistence

	2. MEMORY_AND_DISK	-> (not applicable to pyspark) 
	
	3. DISK_ONLY

        4. MEMORY_ONLY_SER	-> (default in pyspark) serialized, in-memory persistence

	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2        -> 2 replicas on on two different executors.

	7. MEMORY_AND_DISK_2


	Commands:
        ---------	
	  rdd1.persist()     
	  rdd1.persist( StorageLevel.DISK_ONLY )
	  rdd1.cache()  -> in-memory persistence

	  rdd1.unpersist()


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( file, 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist(StorageLevel.????)       --> instruction to spark to persist rdd6 partitions.
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	Lineage of rdd6 : rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile 
		sc.textFile -> t3 -> t5 -> t6 -> rdd6 => collected

        rdd7.collect()

	Lineage of rdd6 : rdd7 -> rdd6.t7
		   t7 -> rdd7 => collected

        rdd6.unpersist()


	Storage Levels
        --------------
	-> to be discussed...
   

  RDD Transformations
  -------------------

    1. map			P: U -> V
				Element to Element transformtion
				input RDD: N elements, Output RDD: N elements

		rdd1.map(lambda x: x > 8).collect()	
		rddFile.map(lambda x: len(x.split(" ")) ).collect()


    2. filter			P: U -> Boolean	
				Only those elements that return Trye for the function will be in the output.
	  			input RDD: N elements, Output RDD: <= N elements

		rddCarsAmerican = rddCars1.filter(lambda x: x[-1] == "American")


   3. glom			P: None
				Creates one list object per partition with all its elements.
	
	rdd1				rdd2 = rdd1.glom()

	P0:  2,3,1,4,5,6,7  -> glom ->  P0: [2,3,1,4,5,6,7]
	P1:  3,4,2,6,7,8,0  -> glom ->  P1: [3,4,2,6,7,8,0]
	P2:  4,2,6,8,9,1,8  -> glom ->  P2: [4,2,6,8,9,1,8]

	rdd1.count() = 21 (int)		rdd2.count() = 3 (lists)

	rddCarsAmerican.glom().map(len).collect()


   4. flatMap			P: U -> Iterable[V]
				flatMap flattens all the objects of the iterables produced by the function.
    				input RDD: N elements, Output RDD: >= N elements

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				mapPartitions applies a function on the entire partition.
                                (i.e the input to the function is the entire partition)
		
	rdd1				rdd2 = rdd1.mapPartitions ( lambda x : y )

	P0:  2,3,1,4,5,6,7  -> mapPartitions  -> P0: 
	P1:  3,4,2,6,7,8,0  -> mapPartitions  -> P1:
	P2:  4,2,6,8,9,1,8  -> mapPartitions  -> P2:

	rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).glom().collect()
	rdd1.mapPartitions(lambda x: [sum(x)] ).collect()


   6. mapPartitionsWithIndex	P: (int, Iterable[U]) -> Iterable[V]	
				mapPartitionsWithIndex is asme as mapPartitions but it get partition-index
				as an additional function parameter.

	rdd1.mapPartitionsWithIndex(lambda index, data: map(lambda a: (index, a*10), data)).glom().collect()
	rdd1.mapPartitionsWithIndex(lambda i, x: [(i, sum(x))] ).collect()

   
   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct elements of the input RDD






	












 

  RDD Actions
  -----------

    1. collect

    2. count








