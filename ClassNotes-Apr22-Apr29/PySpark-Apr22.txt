
 Agenda (PySpark)
 -----------------
  Spark - Basic & Architecture
  Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> DataFrame Operations
  Spark Streaming
	-> Structured Streaming
  Introduction to Spark MLLib


  Materials
  ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes   
	-> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  ------

    => Apache Spark is a open source unified distributed computing engine to process data using 
       in-memory computations using simple programming constructs. 

	=> Spark is 100x faster than MapReduce if 100% in-mmeory computations are used
	=> Spark is 6 to 7 times faster than MapReduce

    => Spark is written in 'SCALA' programming language 

    => Spark is polyglot
	-> Scala, Java, Python, R, SQL

    => Spark Unified Stack:

	 Spark provides a consistent set of API built on the same execution engine to process various
	 types of analytical workloads.

	    Batch Analytics of Unstructured Data    : Spark Core API
	    Batch Analytics of Structured Data	    : Spark SQL		
	    Streaming Analytics			    : Spark Streaming, Strcutured Streaming
	    Predictive Analytics		    : Spark MLlib
	    Graph Parallel Computations		    : Spark GraphX
  

   Spark Architecture
   ------------------
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.

   
   Getting started with Spark
   ---------------------------

    1. Working in the vLab
	
	-> PySpark Shell
	-> Jupyter Notebook

    2. Setting up PySpark on your personal machine. 
	
	-> Download and install 'Anaconda Navigator' 
	   URL: https://www.anaconda.com/products/distribution#windows

	-> Follow the instruction in the document to setup PySpark with Jupyter Notebook/Spyder
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Sign-up to Databricks Community Edition

	   Sign-up: https://databricks.com/try-databricks
	   Login: https://community.cloud.databricks.com/login.html

	   Read: Guide: Quickstart tutorial


   Spark Core API - RDD (Resilient Distributed Dataset)
   ----------------------------------------------------
   
    => RDD is the fundamental data abstraction of Spark Core API (Low Level API)

    => RDD is a collection of distributed in-memory partitions
	-> A partition is a collection of objects of any type. 

    => RDDs are lazily evaluated

    => RDDs are immutable


   Creating RDDs
   -------------

	Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		=> default number of partitions (if partition count is not mentioned): sc.defaultMinPartitions

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([9,6,4,3,2,1,8,6,4,2,1,0,9,6,4,3,1,0], 3)
		=> default number of partitions (if partition count is not mentioned): sc.defaultParallelism

	3. By applying transformations on existing RDDs
	
		rdd2 = rdd1.map(lambda x: (x,x))
		=> number of output partitions is equal to number of input partitions


   Operations on RDDs
   ------------------

	1. Transformations
		-> Create RDD Lineage DAGs
		-> Does not cause execution.

	2. Actions
		-> Trigger execution of the RDD by converting the logical plan to physical plan
		-> Produces some output.


   RDD Lineage DAG
   ----------------
     -> RDD Lineage DAG is a logican plan on how to create the partitions of the RDD
     -> RDD Lineage tracks the heirarchy of dependencies all the way from the very first RDD. 
	

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	    rddFile Lineage: (4) rddFile -> textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	    rddWords Lineage: (4) rddWords -> rddFile.flatMap -> textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	    rddPairs Lineage: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile     

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    rddWc Lineage: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile  


  RDD Transformations
  --------------------
     -> Transformations does not cause execution
     -> Output of a transformations is an RDD
     -> The number of tasks lauched per stage  = the number of partitions of the RDD.
     

   1. map		P: U -> V
			Object to object transformation
			input RDD: N object, output RDD: N objects






                















 



