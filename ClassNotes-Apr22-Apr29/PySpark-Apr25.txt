
 Agenda (PySpark)
 -----------------
  Spark - Basic & Architecture
  Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> DataFrame Operations
  Spark Streaming
	-> Structured Streaming
  Introduction to Spark MLLib


  Materials
  ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes   
	-> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  ------

    => Apache Spark is a open source unified distributed computing engine to process data using 
       in-memory computations using simple programming constructs. 

	=> Spark is 100x faster than MapReduce if 100% in-mmeory computations are used
	=> Spark is 6 to 7 times faster than MapReduce

    => Spark is written in 'SCALA' programming language 

    => Spark is polyglot
	-> Scala, Java, Python, R, SQL

    => Spark Unified Stack:

	 Spark provides a consistent set of API built on the same execution engine to process various
	 types of analytical workloads.

	    Batch Analytics of Unstructured Data    : Spark Core API
	    Batch Analytics of Structured Data	    : Spark SQL		
	    Streaming Analytics			    : Spark Streaming, Strcutured Streaming
	    Predictive Analytics		    : Spark MLlib
	    Graph Parallel Computations		    : Spark GraphX
  

   Spark Architecture
   ------------------
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.

   
   Getting started with Spark
   ---------------------------

    1. Working in the vLab
	
	-> PySpark Shell
	-> Jupyter Notebook

    2. Setting up PySpark on your personal machine. 
	
	-> Download and install 'Anaconda Navigator' 
	   URL: https://www.anaconda.com/products/distribution#windows

	-> Follow the instruction in the document to setup PySpark with Jupyter Notebook/Spyder
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Sign-up to Databricks Community Edition

	   Sign-up: https://databricks.com/try-databricks
	   Login: https://community.cloud.databricks.com/login.html

	   Read: Guide: Quickstart tutorial


   Spark Core API - RDD (Resilient Distributed Dataset)
   ----------------------------------------------------
   
    => RDD is the fundamental data abstraction of Spark Core API (Low Level API)

    => RDD is a collection of distributed in-memory partitions
	-> A partition is a collection of objects of any type. 

    => RDDs are lazily evaluated

    => RDDs are immutable


   Creating RDDs
   -------------

	Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		=> default number of partitions (if partition count is not mentioned): sc.defaultMinPartitions

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([9,6,4,3,2,1,8,6,4,2,1,0,9,6,4,3,1,0], 3)
		=> default number of partitions (if partition count is not mentioned): sc.defaultParallelism

	3. By applying transformations on existing RDDs
	
		rdd2 = rdd1.map(lambda x: (x,x))
		=> number of output partitions is equal to number of input partitions


   Operations on RDDs
   ------------------

	1. Transformations
		-> Create RDD Lineage DAGs
		-> Does not cause execution.

	2. Actions
		-> Trigger execution of the RDD by converting the logical plan to physical plan
		-> Produces some output.


   RDD Lineage DAG
   ----------------
     -> RDD Lineage DAG is a logican plan on how to create the partitions of the RDD
     -> RDD Lineage tracks the heirarchy of dependencies all the way from the very first RDD. 
	

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	    rddFile Lineage: (4) rddFile -> textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	    rddWords Lineage: (4) rddWords -> rddFile.flatMap -> textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	    rddPairs Lineage: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile     

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    rddWc Lineage: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile  


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  ---> instruction to Spark to persist the rdd partition (not subject to GC)
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	
		lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks: [sc.textFile -> t3 -> t5 -> t6] => rdd6 -> collect()

	rdd7.collect()

		lineage of rdd7: rdd7 -> rdd6.t7 
		tasks: [t7] => rdd7 -> collect()


	Storage Levels
        --------------
	1. MEMORY_ONLY		=> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	=> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		=> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	=> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2    => Disk Memory Serialized 2x Replicated

	Commands
	---------

	rdd1.persist()  => in-memory persistence
	rdd1.cache()	=> in-memory persistence

	rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	rdd1.unpersist()


   	Executor's memory structure
   	---------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


    Types of Transformations
    ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  --------------------
     -> Transformations does not cause execution
     -> Output of a transformations is an RDD
     -> The number of tasks lauched per stage  = the number of partitions of the RDD.
     -> The number of output partitions is by default equal to number of input partitions.
     

   1. map		P: U -> V
			Object to object transformation
			input RDD: N object, output RDD: N objects

		rdd1.map(lambda x: [x, x+1, x-1]).collect()


   2. filter		P: U -> Boolean
			Only those objects for which the function retuns True will be in the output RDD
			input RDD: N object, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) > 51).collect()

   3. glom		P: None
			Returns one list object per partition with all the elements of the partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 2,3,8,7,0,9 -> glom -> P0: [2,3,8,7,0,9]
		P1: 6,4,5,7,5,8 -> glom -> P1: [6,4,5,7,5,8]
		P2: 6,5,0,8,5,5 -> glom -> P2: [6,5,0,8,5,5]
			
		rdd1.count() = 18 (int)   rdd2.count() = 3 (list)

		rdd1.glom().map(sum).collect()
		
   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the elements of the iterables generated by the function
			input RDD: N object, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation


		rdd1        rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )

		P0: 2,3,8,7,0,9 -> mapPartitions -> P0: 29 
		P1: 6,4,5,7,5,8 -> mapPartitions -> P1: 35
		P2: 6,5,0,8,5,5 -> mapPartitions -> P2: 29

		rdd1.mapPartitions(lambda p : [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions, but we get partition-index as additional 
				function input.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD	
                

		rddWords.distinct().collect()
		rddWords.distinct(4).collect()


   Types of RDDs
   -------------
	1. Generic RDDs		: RDD[U]
	2. Pair RDD		: RDD[(K, V)]

   
   8. mapValues			P: U -> V
				Applied to Pair RDDs only
				Transform the value part by applying the function.
				input RDD: N object, output RDD: N objects

		rdd5 = rdd3.mapValues(lambda x: (x, x+1))
		rdd5.mapValues(sum).collect()


    9. sortBy			P: U -> V    Optional: ascending (True/False), numPartitions
				Sorts the elements of the RDD based on the output of the function

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 3).collect()








 




 



