
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================


   Spark
   -----
       	-> Spark is a unified in-memory distributed computing framework.
	-> Used for big data analytics

	-> Spark is written in "Scala" language.

	-> Spark is a polyglot
		Supports  Scala, Java, Python, R


   Spark Unified Framework
   -----------------------

	Spark provided a set of consistent APIs based on the same execution engine to process different
	analytics workloads. 

		Batch Processing of unstructured data	=> Spark Core API (RDDs)
		Batch Processing of structured data	=> Spark SQL
		Stream processing (real time)		=> Structured Streaming, DStreams API
		Predictive analytics (ML)		=> Spark MLLib
		Graph parallel computations		=> Spark GraphX



    Getting started with Spark
    --------------------------
	
     1. Working in your vLab

	    -> Follow the instructions document and go to Windows server.
	    -> Windows server desktop has:
			1. A word document with user credentials and other instructions
			2. An icon of "Oracle VM virtualbox" to launch the lab VM

	    -> Click on the "Oracle VM virtualbox" to launch the lab 

		  -> Open a terminal and type "pyspark"
		  -> Launch Spyder IDE to write programs.


     2. Setting up your own PySpark environment on a local machine.

	-> Install "Anaconda Navigator" 
	-> Open an  Ananconda Terminal and pip install pyspark.

	-> If pip install is not working for you, you can use the instructions mentioned in the shared document.
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


     3. Signup to free account - Databricks community edition

		Signup URL: https://www.databricks.com/try-databricks
		=> Make sure you clikc on "Databrick Comminity Edition" option

		Signin: https://community.cloud.databricks.com/login.html


	Downloading files from Databricks
	---------------------------------
	/FileStore/<FILEPATH>
	https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

	Example:
		/FileStore/tables/cars/part-00000
		https://community.cloud.databricks.com/files/tables/cars/part-00000?o=1072576993312365


	DBFC copy command:

		dbfs cp "dbfs:/FileStore/tables/cars/part-00000" "E:/cars" --overwrite


    Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


    RDD (Resilient Distributed Datasets)
    ------------------------------------
  
    -> RDD is the fundamental data abstraction of Spark.

    -> RDD is a collection of distributed in-memory partitions.
	-> A partition is a collection of objects. 

    -> RDDs are lazily evaluated
	-> Transformations only cause the lineage DAGs to be created. Does not cause executition	
	-> Action commands causes execution.

    -> RDDs are immmutable


   Creating RDDs
   -------------
	
    Three ways:

	1. Create an RDD from external data file

		rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   RDD Operations
   --------------

    Two operations:

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.

   RDD Lineage DAG
   ---------------

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )
	Lineage of rddFile :  (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage of rddWords :  (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage of rddPairs :  (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)		
	Lineage of rddWc :  (4) rddWc ->  rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	

   Types of Tansformations
   -----------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------
	rdd1 = sc.textFile(<filePath>, 10)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd4.persist()
	rdd5 = rdd4.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist()   -----> instruction of Spark to save rdd6 partitions. 
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collect

        rdd7.collect()

	lineage of rdd7: rdd7 -> rdd6.t7
		[t7] -> collect

	rdd6.unpersist()

	Storage Levels
        ---------------	
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()    				-> in-memory persistence
	rdd1.persist()	 				-> in-memory persistence
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


   Executors Memory Structure
   --------------------------
        Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------

   1. map		P: U -> V
			object to object transformation
			input RDD: N objects, output RDD: N objects

		rdd2 = rdd1.map(lambda x: x > 7)

   2. filter		P: U -> Boolean
			only those objects of the input RDD for which the function returns True will be
			there in the output RDD.
			input RDD: N objects, output RDD: <= N objects

   3. glom		P: None
			returns one list object per input partition with all the objects of the partition'
			input RDD: N objects, output RDD: = # of partitions

		rdd1		 rdd2 = rdd1.glom()
		P0: 3,2,5,6,4,6,7 -> glom -> P0: [3,2,5,6,4,6,7]
		P1: 5,7,5,6,8,9,0 -> glom -> P1: [5,7,5,6,8,9,0]
		P2: 7,0,3,6,1,1,3 -> glom -> P2: [7,0,3,6,1,1,3]

		rdd1.count() = 21 (int)	     rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.
			input RDD: N objects, output RDD: = > N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				partition to partition transformation

		rdd1	      rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )
		P0: 3,2,5,6,4,6,7 -> mapPartitions -> P0: 33
		P1: 5,7,5,6,8,9,0 -> mapPartitions -> P1: 40
		P2: 7,0,3,6,1,1,3 -> mapPartitions -> P2: 21

		rdd1.mapPartitions(lambda p: map(lambda x: x*2, p)).glom().collect()
		

   6. mapPartitionsWithIndex	p: Int, Iterable[U] -> Iterable[V]
				partition to partition transformation
				We get the partition-index as additional function parameter.   

		rdd1.mapPartitionsWithIndex(lambda i,p: [(i, len(list(p)))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))]).filter(lambda p: p[0] == 1).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns unique objects of the RDD

		rddWords.distinct().collect()


    Spark Application Execution Flow
    ---------------------------------
	Spark Application (SparkContext)  (-> PySpark, Spark-Submit)	
	    -> Job  (Each action command)
		-> Stages (1 or more stages)
		    -> Tasks (every wide transformation causes a stage transition)
			-> Transformations

	
    8. sortBy			P: U -> V, Optional: ascending (True/False),  numPartitions
				Sorts the objects of the RDD based on the function ouput.

		rddWords.sortBy(len).glom().collect()
		rddWords.sortBy(len, False).glom().collect()
		rddWords.sortBy(len, False, 3).glom().collect()



   Types of RDDs
   -------------
	1. Generic RDDs : RDD[U]
	2. Pair RDDs:  RDD[(K, V)]


    9. mapValues		P: U -> V
				Applied only to Pair RDDs
				Transforms only the value part of the (K, V) pairs
	
		rdd3.mapValues(list).collect()


    10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD where 
				   key: each unique value of the function output
				   value: ResultIterable containing all the object that produced the key.

		rdd1 = sc.textFile(filePath, 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)   

   11. randomSplit		P: List of weights  (ex: [0.6, 0.4]), optional: seed
				Returns a list of RDDs split in the specified weights

		 rddList = rdd1.randomSplit( [0.6, 0.4], )
		 rddList = rdd1.randomSplit( [0.6, 0.4], 4644 )


   12. repartition		p: numPartitions
				Created an output RDD with specified number of partitions
				You can increase or decrease the number of partitions
				Performs global shuffle


   13. coalesce			p: numPartitions
				Created an output RDD with specified number of partitions
				You can only decrease the number of partitions
				Performs partition merging	

	  Recommendations
	  ---------------
	  1. Size of each partition should be between 100 MB to 1 GB. Preferably 128 MB
	  2. The number of partitions should be a multiple of number of CPU cores allocated
	  3. If the number of partitions is less than but close to 2000, bump it up to 2000
	  4. The number of CPU cores in each executor should be 5. 

  
   14. partitionBy   		P: numPartitions, Optional: partition-function (U -> Int, default: hash) 
				Applied only on Pair RDD
				Partitioning of the data happens based on the key


   15. union, intersection, subtract, cartesian

	   Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   	command				partitions & type
           	------------------------------------------------------
	   	rdd1.union(rdd2)			M + N, narrow
	   	rdd1.intersection(rdd2)			M + N, wide
	   	rdd1.subtract(rdd2)			M + N, wide
	   	rdd1.cartesian(rdd2)			M * N, wide
   

   ..ByKey Transformations
   -----------------------
	-> Applied only on Pair RDDs
	-> Are wide transformation


   16. sortByKey		P: None, Optional: ascending(True/False), numPartitions
				Sorts the objects of the RDD by key	

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()
		rdd2.sortByKey(False, 2).glom().collect()


   17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD where
					key: each unique key
					value: Results iterable with grouped values.

				Warning: Avoid groupByKey
   
    		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False) \
        		.coalesce(1)

    18. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Reduce values of each unique first for each partition and then across the
			outputs of partitions.

		wordcount = text_file.flatMap(lambda line: line.split(" "))  \
                		.map(lambda word: (word, 1)) \
                		.reduceByKey(lambda a, b: a + b, 1)

    19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))



  Action Commands
  ----------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one value of the same type by iterativly applying the
			function first at each partition and then on the outputs of each partition.

		rdd1	
		P0: 3, 2, 1, 4, 3, 5, 7, 8, 6	  -> reduce -> -33   -> reduce -> 28
		P1: 9, 0, 7, 0, 7, 8, 9, 4, 6	  -> reduce -> -32
		P2: 3, 7, 6, 8, 0, 2, 3, 1, 2, 3  -> reduce -> -29	

		rdd1.reduce(lambda x, y: x - y)

  5. aggregate

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))


   6. first

   7. take(n)

   8. takeOrdered(n)

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)


   9. takeSample	=> rdd1.takeSample(withReplacement(True/False), n)
			   rdd1.takeSample(True, 10)

		  rdd1.takeSample(True, 10)		-> withReplacement sampling
		  rdd1.takeSample(True, 10, 345)	-> withReplacement sampling with seed
		  rdd1.takeSample(False, 100, 645)	-> withOutReplacement sampling with seed

  10. countByValue

  11. countByKey

  12. foreach	=> does not return anything
		   runs a function on all the objects of the RDD.

		rddWc.foreach(lambda p: print("Key: {0}, Value:{1}".format(p[0], p[1])))


   Use Case
   --------
    
    From cars.csv dataset, find the average weight of each make of American origin cars
	-> Arrange the data in the desc order of average weight
	-> Save the output as a single text file.

        Please try yourself..

 
  Spark-Submit
  ------------
   
    Is a single command to submit any spark application (Scala, Java, python or R) to any cluster manager

    
	spark-submit [options] <application file> [app-args]

	spark-submit --master yarn   \
		--deploy-mode cluster \
		--driver-memory 2G \
		--driver-cores 2 \
		--executor-memory 10G \
		--executor-cores 5 \
		--num-executors 10 \		
		E:\PySpark\spark_core\examples\wordcount.py  [app args]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py		
		

  Closure
  --------
	In Spark, a closure constitutes all the variables and methods which must be visible 
	for the executor to perform its computations on the RDD. 

	This closure is serialized and sent to each executor.


	c = 0

	def isPrime(n):
	    return True if n is Prime
	    else
	    return False

	def f1(n):
	   global c
	   if (isPrime(n)) c += 1
	   return n*10

	rdd1 = sc.prallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)          // c = 0


     Limitation:  Local variables can not be used to implement global counters. 
     Solutions: Use "Accumulator" variables.


  Shared Variables
  ----------------

   1. Accumulator

	-> Is a shared variable that is accessible from all the executors.
	-> Not part of the closure
	-> Is maintained by the driver
	-> All tasks can write to this variable. 	
	
	c = sc.accumulator(0)

	def isPrime(n):
	    return True if n is Prime
	    else
	    return False

	def f1(n):
	   global c
	   if (isPrime(n)) c.add(1)
	   return n*10

	rdd1 = sc.prallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c) 
	

  2. Broadcast Variables
	-> A broadcast variable is used to save memory
	-> Driver sends one copy of the broadcast variable to each executor
	-> All tasks in that executor-node can access that single copy.
	-> Broadcast variable is not part of the closure.
	
	d = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, 6: f, ..............})   # 100 MB 

	def f1(n):
	  global d
	  return d.value[n]
	
	rdd1 = sc.parallelize([1,2,3,4,5,6, ....], 4)
	rdd2 = rdd1.map( f1 )

	rdd2.collect()


  ---------------------------------------------------
      Spark SQL (pyspark.sql)
  ---------------------------------------------------

   => Main API of Spark

   -> Spark's structured data processing API

             Structured data formats : Parquet (default), ORC, JSON, CSV (delimited text), text
			 JDBC format : RDBMS, NoSQL
			 Hive format : Hive warehouse 

   -> SparkSession
          -> Represents a user-session, with its own configuration, inside an application	
		    
		spark = SparkSession \
    			.builder \
    			.appName("Basic Dataframe Operations") \
    			.config("spark.master", "local") \
    			.getOrCreate()


   -> DataFrame	(DF)
	-> Collection of distributed in-memory partitions
	-> immutable
	-> Lazily evaluated.

	DataFrame components:
		-> Data: Partitions containing "Row" objects
		-> Metadata: Schema (is a StructType object)

			StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			)
  

   Basic Steps in a Spark SQL program
   ----------------------------------

	1.  Read/Load data from some data source (external / internal) into a DF

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)


	2. Apply transformations on the DD using transformation methods or using SQL


		Using transformation methods:
		-----------------------------
			df2 = df1.select("userid", "name", "gender", "age", "phone") \
        			.where("age is not null") \
        			.orderBy("age", "gender") \
        			.groupBy("age").count() \
        			.limit(4)

		Using SQL
		---------			
			df1.createOrReplaceTempView("users")

			# drop temp-view
			spark.catalog.dropTempView("users")

			qry = """select age, count(*) as count 
         			 from users
         			 where age is not null
         			 group by age
         			 order by age
         			 limit 4"""         
         
			df3 = spark.sql(qry)
			df3.show()


	3. Write/save the DF to some structured destination.

		outputPath = "E:\\PySpark\\output\\json"
		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)

		df2.write.mode("overwrite").json(outputPath)
		df2.write.json(outputPath, mode="overwrite")


  SaveModes
  ----------
	errorifexsists  (default)
	ignore	
	append
	overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  DataFrame Transformations
  -------------------------

   1. select

	df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

	df2 = df1.select( col("DEST_COUNTRY_NAME").alias("destination"),
                  column("ORIGIN_COUNTRY_NAME").alias("origin"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequecy"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )      

   2. where / filter

		df3 = df2.where("domestic = false and count > 200")
		df3 = df2.filters("domestic = false and count > 200")

		df3 = df2.where( col("count") > 200 )

   3. orderBy / sort

		df3 = df2.orderBy("count", "destination")
		df3 = df2.sort("count", "destination")
		df3 = df2.sort(col("count").desc(), col("destination").asc())
		df3 = df2.sort(desc("count"), asc("destination"))

   4. groupBy -> does not return a DataFrame; returns a pyspark.sql.group.GroupedData object
		 apply some aggrgation function to return a Df

		
		df3 = df2.groupBy("highFrequecy", "domestic").count()
		df3 = df2.groupBy("highFrequecy", "domestic").avg("count")
		df3 = df2.groupBy("highFrequecy", "domestic").max("count")
		df3 = df2.groupBy("highFrequecy", "domestic").sum("count")

		df3 = df2.groupBy("highFrequecy", "domestic") \
        		.agg(  sum("count").alias("sum"),
               			max("count").alias("max"),
               			count("count").alias("count"),
               			round(avg("count"), 2).alias("avg")
            		)


   5. limit

		df2 = df1.limit(40)

   6. selectExpr


	df2 = df1.selectExpr( "DEST_COUNTRY_NAME as destination",
                  "ORIGIN_COUNTRY_NAME as origin",
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequecy",
                  "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"
                )


   7 & 8. withColumn & withColumnRenamed

		df3 = df1.withColumn("newCount",  col("count") + 10) \
        		.withColumn("highFrequecy", expr("count > 200")) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        		.withColumn("count", col("count").cast("int")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

		df3 = userDf.withColumn("ageGroup", when(col("age") < 13, "child")
                                    			.when(col("age") < 20, "teenager")
                                    			.when(col("age") < 60, "adult")
                                    			.otherwise("senior") )


    9. udf   (user defined function)

		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		get_age_group = udf(getAgeGroup, StringType())

		df3 = userDf.withColumn("ageGroup", get_age_group(col("age")) )
		-------------------------------------------------------------
                # annotating an existig function as udf

		@udf(returnType = StringType()) 
		def getAgeGroup( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"

		df3 = userDf.withColumn("ageGroup", getAgeGroup(col("age")) )
		---------------------------------------------------------------
		spark.udf.register("get_age_group", getAgeGroup, StringType())


		spark.catalog.listFunctions()

		userDf.createOrReplaceTempView("user")
		qry = "select id, name, age, get_age_group(age) as ageGroup from user"    
		df4 = spark.sql(qry)		

   10. drop

		df3 = df2.drop("newCount", "highFrequency") 
		df3.show(10)

   11. dropna

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()
	
		df3 = usersDf.dropna()
		df3 = usersDf.dropna(subset=['phone', 'age'])

   12. dropDuplicates

		   listUsers = [(1, "Raju", 5),
             			(1, "Raju", 5),
             			(3, "Raju", 5),
             			(4, "Raghu", 35),
             			(4, "Raghu", 35),
             			(6, "Raghu", 35),
             			(7, "Ravi", 70)]

		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
		userDf.show()

		df3 = userDf.dropDuplicates()
		df3 = userDf.dropDuplicates(["name", "age"])

		df3.show()

    13. distinct

		df3 = userDf.distinct()
		df3.show()

   14. repartition

		df4 = df1.repartition(4)
		df4.rdd.getNumPartitions()

		df3 = df4.repartition(3)
		df3.rdd.getNumPartitions()

		df4 = df1.repartition(4, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df1.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()

		spark.conf.set("spark.sql.shuffle.partitions", "5")
		spark.conf.get("spark.sql.shuffle.partitions")


    15. coalesce 

		df3 = df5.coalesce(3)
		df3.rdd.getNumPartitions()

    16. join  => taken as a separate topic	
 
	
   Working with different file formats
   -----------------------------------

   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)
		df2.write.mode("overwrite").json(outputPath)
		df2.write.json(outputPath, mode="overwrite")

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath)
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")



  Create an RDD from a DafaFrame
  -------------------------------
	rdd1 = df1.rdd
	rdd1.take(5)


  Create a DF from programmatic data
  ----------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])

	df2.show()
	df2.printSchema()


  Create a DF from an RDD
  -----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df2 = rdd1.toDF()
	df2 = rdd1.toDF(["id","name", "age"])

	df2.show()
	df2.printSchema()


  Create a DF with Programmatic Schema
  ------------------------------------ 

	mySchema = StructType([
            StructField("id", IntegerType(), True), 
            StructField("name", StringType(), True), 
            StructField("age", IntegerType(), True), 
        ])

	df2 = rdd1.toDF(schema = mySchema)

	df2.show()
	df2.printSchema()
   
        ----------------------------------------------
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True), 
            StructField("DEST_COUNTRY_NAME", StringType(), True), 
            StructField("count", IntegerType(), True), 
        ])


	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)

	df1.show()
	df1.printSchema()


   Joins
   -----

     Supported Joins: inner, left (left_outer), right (right_outer), full (full_outer), left_semi, left_anti.
 
     left_semi join
     --------------
	-> Similar to inner join, but data comes only from left side table.
	-> Equivalent to the folowing subquery:
		select * from emp where deptid in (select id from dept)

     left_anti join
     --------------
	-> Equivalent to the folowing subquery:
		select * from emp where deptid not in (select id from dept)


    Use-Case: Movie Data Analysis
    -----------------------------
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv files, find the top 10 movies with highest average user rating.
    -> Consider only those movies with rating-count > 30
    -> Data required: movieId, title, totalRatings, averageRating
    -> Arrange the data in the DESC order of averageRating
    -> Save the output as a single pipe-separated CSV file with header.



Working with Hive
  -----------------
# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Hive Datasource") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listDatabases()
spark.catalog.currentDatabase()
spark.catalog.listTables()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL


moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()

joinCol = summaryDf.movieId == moviesDF.movieId
   
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()



  Working with JDBC - Integrating with MySQL
  ------------------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()



 ===================================
    Spark Streaming
 ===================================
   
     Spark's streaming data (real-time) analytics API

     Two libraries
	1. Spark Streaming (DStreams API)
	2. Structured Streaming (this is preferred)

  
    Spark Structured Streaming
    -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	-> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 

	=> Sources: File (text, csv, json, parquet, orc), Socket, Rate, Kafka
	=> Sinks: File (text, csv, json, parquet, orc), Console, Kafka, ForEachBatch, ForEach, Memory




   


























 =================================================================
     Spark Streaming   (Real time data analytics)
 =================================================================







