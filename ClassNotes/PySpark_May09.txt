
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark

	-> Spark is an open source unified in-memory distributed computing framework for big data analytics.
	-> Spark is written in SCALA programming language

	in-memory computing -> the intermediate results can be persisted in RAN and subsequent tasks
			       can directly run on those persisted data. 

				If there is not enough RAM for persisting your partitions, then Spark 
				resorts to disk-based computation.

   Spark Unified Framework
	
	-> Spark comes with a consistent set of libraries for performing analytics on different types of
	   analytical workloads running on the same execution engine.
		
		-> Batch Processing of unstructured data	=> Spark Core (low level)
		-> Batch Processing of structured data		=> Spark SQL
		-> Stream processing (real time)		=> Spark Streaming, Structured Streaming
		-> Predictive analytics (machine learning)	=> Spark MLLib
		-> Graph parallel computations			=> Spark GraphX

   Spark is a polyglot	
	-> Spark supports Scala, Java, Python, R 

   Spark applications can run on multiple cluster managers
	-> local, standalone scheduler, YARN, Mesos, Kubernetes


   Getting started with Spark
   --------------------------	
    1. Using your Big Data Lab (that is assigned to you)

	-> You will land up on a Windows server. 
	-> On the desktop of the Window server you find "Oracle VM Virtualbox"
	-> On the desktop of the Window server you find a "Word" document with userids and passwords.

	-> Connect to Ubuntu VM using Oracle VM Virtualbox

		-> Start the PySpark shell by opening a terminal and enter the command "pyspark"
		-> Start the Spyder IDE by clicking on the icon.

   2. Installing your own dev environment.

	-> Make sure you have Anaconda Navigator Installed. 

	-> Try installing pyspark using pip install
		-> Open an anaconda terminal and use the following command

		pip install pyspark
		
	-> Follow the instruction provided in the document to sepup PySpark to work with Spyder and 
	   Jupyter Notebook, if pip install option is not working for you.

  
    3. Databricks Community Edition  (Free Signup)

	     https://docs.databricks.com/getting-started/community-edition.html
	     
	     Signup link: https://www.databricks.com/try-databricks
		=> Make sure you click on "Community Edition" option 
	     Login: https://community.cloud.databricks.com/login.html

	To Download a file from databricks:
	-----------------------------------
	/FileStore/<FILEPATH>
	https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

	Example:
	/FileStore/tables/wordcount-5.txt
	https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


   Spark Architecture (Building Blocks)
   ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


    RDD (Resiliant Distributed Dataset)
    -----------------------------------

	=> Fundamental data abstraction for Spark Core API

	=> Represents a collection of distributed in-memory partitions

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data 	=> A collection of distributed in-memory partitions.
		       	   -> Each partition is a collection of objects.

	=> RDDs are lazily evaluated.

	=> RDDs are immutable


   Creating RDDs
   -------------

     1. create an RDD from external data files

		rdd1 = sc.textFile( <filePath>, n )
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

    2. create an rdd from programmatic data

	        rdd1 = sc.parallelize( [2,1,3,2,4,3,5,6,2,7,5,8,9,0], 3)

    3. 	apply transformations on existing rdds

		rddWords = rddFile.flatMap(lambda x: x.split(" "))
				


   RDD Operations
   --------------
	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


  RDD Linage DAG
  --------------

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	  -> Lineage: (4) rddFile -> sc.textFile 

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	  -> Lineage: (4) rddWords -> rddFile.flatMap -> sc.textFile 

	rddPairs = rddWords.map(lambda x: (x,1))
	  -> Lineage: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile 

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	   -> Lineage: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile 


  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Persistence
  ---------------

	rdd1 = sc.textFile( <filePath>, 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist()          -> is an instruction to Spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()	
	Lineage DAG of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks -> (sc.textFile, t3, t5, t6) 

	rdd7.collect()
	Lineage DAG of rdd6: (4) rdd7 -> rdd6.t7
		tasks -> (t7) 

	rdd6.unpersist()


	Storage Levels
        ---------------	
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_MEMORY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()    				-> in-memory persistence
	rdd1.persist()	 				-> in-memory persistence
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )

	rdd1.unpersist()

 Spark execution flow
 --------------------
	
     application (identified by SparkContext object)
	|
	=> Job (an app can have mnay jobs, each action command launches a job)
		|
		=> Stages (a job can have 1 or more stages, Each wide transformation causes a new stage to be launched)
			|
			=> Tasks (each stage has one more tasks, # tasks = # partitions)
				|
				=> Transformations (each task has some transformations that can run in parallel)


  RDD Transformations
  -------------------
   => Transformations return an RDD.
   => Does not cause execution.


   1. map		P: U => V
			object to object transforrmation
			input RDD: N objects, output RDD: N objects

	   	rddLists = rddFile.map(lambda x: x.split(" "))


  2. filter		P: U -> Boolean
			Filters the objects to the output based on the function
			input RDD: N objects, output RDD: < N objects

		rddFile.filter(lambda s: len(s.split(" ")) > 8).collect()

  3. glom		P: None
			Returns one list object per partition with all the elements of that partition.
			input RDD: N objects, output RDD: number of partitions

		rdd1		         rdd2 = rdd1.glom()
		P0: 2,1,2,5,6   -> glom  -> P0: [2,1,2,5,6]
		P1: 4,2,1,3,2   -> glom  -> P1: [4,2,1,3,2]
		P2: 8,6,7,9,0   -> glom  -> P2: [8,6,7,9,0] 
		
		rdd1.count() = 15 (int)	    rdd2.count() = 3 (list)

		rdd1.glom().map(len).collect()

  4. flatMap		P: U -> Iterable[V]
			flapMap flattens the elements of the iterables produced by the function
			input RDD: N objects, output RDD: > N objects

		rdd1.flatMap(lambda x: range(0,x)).collect()

  5. mapPartitions		P: Iterable[U] -> Iterable[V]

		rdd1	   rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )
		P0: 2,1,2,5,6   -> mapPartitions  -> P0:  16
		P1: 4,2,1,3,2   -> mapPartitions  -> P1:  12
		P2: 8,6,7,9,0   -> mapPartitions  -> P2:  30

		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			      Similar to mapPartitions, but will get the partition-index as an additional parameter.
		
		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()

  7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects.
				
		rdd2 = rdd1.distinct(4)
		rdd2 = rdd1.distinct()

  

   Types of RDDs
   --------------
	1. Generic RDDs:   RDD[U]
	2. Pair RDD:       RDD[(K, V)]


  8. mapValues			P: U -> V
				Applied only on Pair RDD
				Transforms only the value part by applying the transformation.

		rdd3.mapValues(list).collect()

  9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Returns an RDD sorted by the output of the function.

		rdd3.sortBy(lambda x: sum(x[1]))
		rdd3.sortBy(lambda x: sum(x[1]), False)
		rdd3.sortBy(lambda x: sum(x[1]), False, 4)

  10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD where
					key: is each unique value of the function output
					value: ResultIterable containing all objects that produced the key
			
		outputRdd = sc.textFile("E:/Spark/wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x, 1) \
              			.mapValues(len)


  11. randomSplit		P: List of weights
				Returns a list of RDDs split in the specified weights.

			rddList = rdd1.randomSplit([0.6, 0.4], 3435)
			rddList[0].collect()
			rddList[1].collect()

  12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitons
				Causes global shuffle

  13. coalesce			P: numPartitions
				Is used only to decrease the number of partitons
				Causes partition-merging (hence preferred)
		

    Some recommendations for better performance
    -------------------------------------------
      	1. Size of each partition should be 100 MB to 1000 MB 
	   Ideally the partition size should be 128 MB (if you are running on HDFS)

	2. The nmber of partitions should be a multiple of number of CPU cores

	3. The number of CPU cores per executor should be 5


  14. partitionBy		P: numPartitions, Optional: partition-function (default: hash)
				Applied only on Pair-RDDs
				Partitioning logic is applied based on the key


	 transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


 15. union, intersection, subtract, cartesian
   
	Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   command				partitions & type
           ------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide


  ..ByKey transformations
  -----------------------
	-> Are wide transformation
	-> Are applied only on PairRDDs


  16. sortByKey		 	P: None, Optional: ascending (True/False), numPartitions
				Sorts the RDD by key

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(True, 3).glom().collect()

  17. groupByKey		P: None, Optional: numPartitions
				Returns a Pair RDD where 
					Key: each unique key
					Value: grouped values of the same key

				CAUTION: Do not use 'groupByKey' as much as possible.


		wordcountRdd = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
                		.flatMap(lambda x: x.split(" ")) \
                		.map(lambda x: (x, 1)) \
                		.groupByKey() \
                		.mapValues(sum) \
                		.sortBy(lambda x: x[1], False, 1)

    18. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Reduce values of each unique first for each partition and then across the
			outputs of partitions.

		wordcount = text_file.flatMap(lambda line: line.split(" "))  \
                		.map(lambda word: (word, 1)) \
                		.reduceByKey(lambda a, b: a + b, 1)

    19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))



 Actions
 -------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one value of the same type by iterativly applying the
			function first at each partition and then on the outputs of each partition.
		rdd1			

		P0: 1,2,1,3,5,6,1 -> reduce ->  -17 => 4
		P1: 2,3,1,4,5,7,1 -> reduce ->  -19
		P2: 9,5,3,2,1,0,0 -> reduce ->  -2 
	
		rdd1.reduce( lambda x, y : x - y ) = 4
   
		rddWc.reduce(lambda a, b: (a[0] + "," + b[0], a[1] + b[1]) )

   5. aggregate		
	
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. first

   7. take(n)

   8. takeOrdered(n)

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)


   9. takeSample	=> rdd1.takeSample(withReplacement(True/False), n)
			   rdd1.takeSample(True, 10)

		  rdd1.takeSample(True, 10)		-> withReplacement sampling
		  rdd1.takeSample(True, 10, 345)	-> withReplacement sampling with seed
		  rdd1.takeSample(False, 100, 645)	-> withOutReplacement sampling with seed

  10. countByValue

  11. countByKey

  12. foreach	=> does not return anything
		   runs a function on all the objects of the RDD.

		rddWc.foreach(lambda p: print("Key: {0}, Value:{1}".format(p[0], p[1])))

  13. saveAsSequenceFile


   Use Case
   --------

	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, find the average weights of all models in each make of American origin cars
	-> Arrange the data in the descending order of average weight
	-> Save the output as a sinlge text file

	=> Please try to solve this. 


   Spark-Submit command
   --------------------

   Is a single command to submit any Spark application (Java, Scala, Python or R) to any cluster manager (local, 
   spark standalone, YARN, Mesos, Kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt wordcount_output 2

	spark-submit --master yarn   \
		--deploy-mode cluster \
		--driver-memory 2G \
		--driver-cores 2 \
		--executor-memory 5G \
		--executor-cores 5 \
		--num-executors 10 \
		E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt wordcount_output 2


  Closures
  --------
    A closure constitute all variables and methods that must be visible inside an executor for the tasks to
    perform their computations.

    The closure is serialized and separate copy of the closure is sent to each executor.


	c = 0

	def isPrime(n):
	   return True if n is prime
	   else retrun False

	def f1(n):
	   global c
	   if ( isPrime(n) ) c += 1
	   return n*10

	rdd1 = sc.parallelize( range(1, 4000), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c )     // 0 

    
    Limitation: We can not use local variables (that are part of closure) to implement global counters. 
    Solution: Use "Accumulator" variables

    Shared Variables
    -----------------

    1. accumulator

	-> accumulator is a shared variable
	-> is not part of the closure (hence not a local copy)
	-> all functions can add to this accumulator variable.

	c = sc.accumulator(0)

	def isPrime(n):
	   return True if n is prime
	   else retrun False

	def f1(n):
	   global c
	   if ( isPrime(n) ) c.add(1)
	   return n*10

	rdd1 = sc.parallelize( range(1, 4000), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c.value )     // 0 


  2. broadcast variable
	-> A broadcast variable is used to save memory
	-> Driver sends one copy of the broadcast variable to each executor
	-> All tasks in that executor-node can access that single copy.
	-> Broadcast variable is not part of the closure.

	bc = sc.broadcast( {1: 100, 2: 200, 3: 300, 4: 400, 5: 500, .... } )	# 100 MB

	def f1(n):
	   global bc
	   return bc.value[n]

	rdd1 = sc.parllelize([1,2,3,4,5, ...], 4)

	rdd2 = rdd1.map( f1 )

	rdd2.collect()
	

 ------------------------------------------------
   Spark SQL       (pyspark.sql)
 ------------------------------------------------
  
    -> Spark's structured data processing API
    -> Built on top of spark core API

	 Structured Data Formats:  Parquet (default), ORC, JSON, CSV (delimited text), Text
		     JDBC format:  RDBMS, NoSQL
		     Hive format:  Hive  (Hadoop's data warehouse)

   SparkSession
	-> Starting point of execution
	-> Represents a user session inside an application

		spark  = SparkSession \
    			.builder \
    			.appName("Basic Dataframe Operations") \
    			.config("spark.master", "local[*]") \
    			.getOrCreate()   


   DataFrame (DF)
	-> Main data abstraction of Spark SQL
	-> DF is a collection of in-memory partitions that are immutable and lazily evaluated. 
	-> DF contains "Row" objects
		
		DF Data: Collection of Rows
		DF Metadata : Schema (StructType)

			StructType(
			   List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			   )
			)


   Basic steps in Spark SQL
   ------------------------

	1. Read/Load the data from a data-source into a DataFrame.

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	2. Transform the DF using Transformation methods or using SQL

		Using Transformation methods
		----------------------------
			df2 = df1.select("userid", "name", "age", "gender", "phone") \
        			.where("age is not null") \
        			.orderBy("gender", "age") \
        			.groupBy("age").count() \
        			.limit(4)

		Using SQL
		---------
			df1.createOrReplaceTempView("users")

			qry = """select age, count(*) as count
        			from users 
       				where age is not null
        			group by age
        			order by age
        			limit 4"""
        
			df3 = spark.sql(qry)
			df3.show()
	

	3. Write/save to some external destination

		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

		df3.write.mode("overwrite").json(outputPath)


   LocalTempView & GlobalTempView
   ------------------------------


   Save Modes
   ----------
     -> define the behaviour when writing to existing directory.

	1. errorIfExists   (default mode)
	2. ignore
	3. append
	4. overwrite

		df3.write.mode("overwrite").json(outputPath)
		df3.write.json(outputPath, mode="overwrite")

	
   DF Transformations
   ------------------

    1. select

		df2 = df1.select("ORIGIN_COUNTRY_NAME",
                 		"DEST_COUNTRY_NAME",
                 		"count")

		df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 		col("DEST_COUNTRY_NAME").alias("destination"),
                 		expr("count").cast("int"),
                 		lit("India").alias("country"),
                 		expr("count+10 as newCount"),
                 		expr("count>200 as highFrequency"),
                 		expr("ORIGIN_COUNTRY_NAME=DEST_COUNTRY_NAME as domestic"))


    2. where / filter

		df3 = df2.where("count > 1000 and origin = 'United States'")
		df3 = df2.filter("count > 1000 and origin = 'United States'")

    3. orderBy / sort

		df3 = df2.sort("count", "origin")
		df3 = df2.orderBy("count", "origin")		
		df3 = df2.orderBy(col("count").desc(), col("origin").asc())
		df3 = df2.orderBy(desc("count"), asc("origin"))
		df3.show()

    4. groupBy	=> returns pyspark.sql.group.GroupedData object
		   apply some aggregation meethod to return a DataFrame

		df3 = df2.groupBy("highFrequency", "domestic").count()
		df3 = df2.groupBy("highFrequency", "domestic").max("count")
		df3 = df2.groupBy("highFrequency", "domestic").sum("count")
		df3 = df2.groupBy("highFrequency", "domestic").avg("count")


		df3 = df2.groupBy("highFrequency", "domestic") \
         		.agg(   expr("count(count) as total"),
               			sum("count").alias("sum"),
               			max("count").alias("max"),
               			round(avg("count"), 2).alias("avg")
			     )

    5. limit
		df2 = df1.limit(10)

    6. selectExpr
		df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 		"DEST_COUNTRY_NAME as destination",
                 		"count",
                 		"count+10 as newCount",
                 		"count>200 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME=DEST_COUNTRY_NAME as domestic")

    7. withColumn

		df3 = df1.withColumn( "newCount", col("count") + 10) \
        		.withColumn("highFrequecy", expr("count > 200") ) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME=DEST_COUNTRY_NAME") ) \
        		.withColumn("count", col("count").cast("int") )

		df2 = df1.withColumn("ageGroup", when(col("age") < 13, "child")
                                .when(col("age") < 20, "teenager")
                                .when(col("age") < 60, "adult")
                                .otherwise("senior"))

    8. withColumnRenamed 

		df3 = df1.withColumn( "newCount", col("count") + 10) \
        		.withColumn("highFrequecy", expr("count > 200") ) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME=DEST_COUNTRY_NAME") ) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination" )  \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin" ) 
        

    9. udf	

	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"
    
	get_age_group_udf = udf(getAgeGroup, StringType())    
	
	df2 = df1.withColumn("ageGroup", get_age_group_udf( col("age") ))
	-------------------------------------------------------------
	@udf(returnType=StringType()) 
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"
	
	df2 = df1.withColumn("ageGroup", getAgeGroup( col("age") ))
	---------------------------------------------------------------

	spark.udf.register("get_age_group", getAgeGroup, StringType())

	df2 = spark.sql("select id, name, age, get_age_group(age) as ageGroup from users")

	df2.show()


  10. drop   -> used to exclude columns in the output dataframe.

	df3 = df2.drop("newCount", "highFrequency")
	df3.printSchema()

  11. dropna  => drops rows containing nulls 

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	df4 = userDf.dropna()
	df4 = usersDf.dropna( subset=["age", "phone"] )


  12. dropDuplicates

	listUsers = [(1, "Raju", 5),
             (1, "Raju", 5),
             (3, "Raju", 5),
             (4, "Raghu", 35),
             (4, "Raghu", 35),
             (6, "Raghu", 35),
             (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	df3 = userDf.dropDuplicates()
	df3 = userDf.dropDuplicates(["name", "age"])


   13. distinct

		df1.select("DEST_COUNTRY_NAME").distinct().count()

   15. randomSplit

		dfList = df1.randomSplit([0.4, 0.4, 0.2], 46)
		print(dfList[0].count(), dfList[1].count(), dfList[2].count())

   16. sample 

		df2 = df1.sample(True, 0.5)
		df2 = df1.sample(True, 0.5, 345)
		df2 = df1.sample(True, 1.5, 345)

		df2 = df1.sample(False, 0.5)
		df2 = df1.sample(False, 0.5, 345)		
		df2 = df1.sample(False, 1.5, 345)    # Error : sample fraction should be [0,1] range


   17. union, intersect, subtract

		df4 = df2.union(df3)

		df4.show()
		df4.count() 
		df4.rdd.getNumPartitions()

		df5 = df4.intersect(df2)
		df5.show()
		df5.count() 
		df5.rdd.getNumPartitions()

		df6 = df4.subtract(df2)
		df6.show()
		df6.rdd.getNumPartitions()


   18. repartition

		df7 = df6.repartition(3)
		df7.rdd.getNumPartitions()

		df8 = df6.repartition(10)
		df8.rdd.getNumPartitions()

		df8 = df6.repartition(2, col("DEST_COUNTRY_NAME"))
		df8.rdd.getNumPartitions()

		df8 = df6.repartition(col("DEST_COUNTRY_NAME"))
		df8.rdd.getNumPartitions()

   19. coalesce

		df8 = df6.coalesce(2)
		df8.rdd.getNumPartitions()

   20. join    => doscussed as a separate topic


  show() command
  ---------------
	df1.show()
	df1.show(50)
	df1.show(50, False)
	df1.show(3, False, True)


  Working with different file formats
  -----------------------------------

   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)
		df2.write.mode("overwrite").json(outputPath)
		df2.write.json(outputPath, mode="overwrite")

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath)
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")


     Text
		df1 = spark.read.text(<textFilePath>)

   
  Create an RDD from a DafaFrame
  -------------------------------

	rdd1 = df1.rdd


  Create a DF from programmatic data
  ----------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Create a DF from an RDD
  -----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	df1 = rdd1.toDF(["id", "name", "age"])
	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1.show()


  Create a DF with Programmatic Schema
  ------------------------------------ 

	mySchema = StructType([
               StructField("id", IntegerType(), True),
               StructField("name", StringType(), True),
               StructField("age", IntegerType(), True)
            ])

	df1 = spark.createDataFrame(listUsers, schema=mySchema)

       ------------------------------------------------

	mySchema = StructType([
               StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
               StructField("DEST_COUNTRY_NAME", StringType(), True),
               StructField("count", IntegerType(), True)
            ])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)


  Joins
  ------
     Supported joins => inner (default), left, right, full, left_semi, left_anti 

	
    left_semi join
    ---------------
	=> Similar to inner join, but gets data only from left table. 
	=> Equivalent to the following subquery:
		select * from emp where deptid IN (select deptid from dept)

    left_anti join
    --------------
	=> Fetches rows from left table for which there is no match in the right table
	=> Equivalent to the following subquery:
		select * from emp where deptid NOT IN (select deptid from dept)


    'join' transformation method
    ---------------------------
		joinCol = employee.deptid == department.id
		joinedDf = employee.join(department, joinCol, "left")     $ left outer join
		joinedDf.show()


    Use-Case: Movie Data Analysis
    -----------------------------
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv files, find the top 10 movies with highest average user rating.
    -> Consider only those movies with rating count > 30
    -> Data required: movieId, title, totalRatings, averageRating
    -> Arrange the data in the DESC order of averageRating
    -> Save the output as a single pipe-separated CSV file with header.

    => Try to solve it yourself ...


  Working with Hive
  -----------------
# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Hive Datasource") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listDatabases()
spark.catalog.currentDatabase()
spark.catalog.listTables()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL


moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()

joinCol = summaryDf.movieId == moviesDF.movieId
   
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()



  Working with JDBC - Integrating with MySQL
  ------------------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()



 ===================================
    Spark Streaming
 ===================================
   
     Spark's streaming data (real-time) analytics API

     Two libraries
	1. Spark Streaming (DStreams API)
	2. Structured Streaming (this is preferred)

  
    Spark Structured Streaming
    -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	-> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 

	=> Sources: File (text, csv, json, parquet, orc), Socket, Rate, Kafka
	=> Sinks: File (text, csv, json, parquet, orc), Console, Kafka, ForEachBatch, ForEach, Memory












   
































   	






























		

			      




























