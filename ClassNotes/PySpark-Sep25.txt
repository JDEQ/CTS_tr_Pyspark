
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   
  Spark Basics
  -------------
     
    -> Spark is an in-memory distributed computing framework (cluster computing framework)
       for BigData analytics.   

    -> Spark is written in 'Scala'
    
	in-memory => ability to persist intermediate results in memory and allow subsequent tasks/stages
		     to read the data directly from persisted data. 

	Spark resorts to disk-based computation (disk-spills) if there is not enough RAM for persisting 
	intermediate results. 


   -> Spark is a unified framework

	  Spark provides a consistent set of APIs based on well-defined data structures and a common execution
	  engine to perform processing of different analytical workloads.  

	  	Batch Analytics			=> Spark SQL (and 'Spark Core')
	  	Streaming Analytics		=> Spark Streaming, Structured Streaming
	  	Predictive Analytics		=> Spark MLlib
		Graph Parallel Computations	=> Spark GraphX


   -> Spark is a polyglot	
	 Supports multiple programming language: Scala, Java, Python, R

   
   -> Spark applications can be submitted to multiple cluster managers.
		local, Spark Standalone, YARN, Mesos, Kubernetes


   Getting started with Spark
   --------------------------

    -> Make sure you have Anaconda installed.


    1. Downloading spark and working locally using shell.

	 Download URL: https://spark.apache.org/downloads.html
	 Download the xxxxx.tgz file and extract it to a suitable location.

	 Add <spark-dir>\bin folder to your PATH environment variable.

	-> Open a terminal (Anaconda Terminal) and type 'pyspark'
	-> This will launch the 'PySpark' shell.

	default Web UI location: http://localhost:4040/


   2. Making PySpark accessible from Spyder and Jupyter Notebooks

	-> Follow the instructions given in the shared document
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


   3. Using Databricks Community Edition (free version)

	Signup URL: https://www.databricks.com/try-databricks#account
	Provide a valid email.
	
	-> In the next page, click on "Get started with Community Edition" link (not 'Continue' button)
    
	Login URL: https://community.cloud.databricks.com/login.html



   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
    

   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> Fundamental data abstraction of Spark.

	-> Represents a collection of in-memory partitions
		-> Partition is a collection of objects

	-> RDDs follow lazy evaluation
		-> Transformations does not cause execution
		-> Action command execute the RDDs

	-> RDDs partitions are immutable
    

   Creating RDDs
   -------------
    Three ways:

	1. Create an RDD from some external datasets.

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)	

		
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")

		Default number of partitions is determined by "sc.defaultMinPartitions" config parameter
		The default value of "sc.defaultMinPartitions" = 2 if number of cores >= 2
								 1 if number of cores == 1	


	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,6,7,6,8,7,9,0,7,9,0,0,6,6,4,3,3,2,1,5,6,7,8,4,6], 3)

		Default number of partitions is determined by "sc.defaultParallelism" config parameter
		The default value of "sc.defaultParallelism" = number of cores allocated.


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))
	

     Note: Use rdd1.getNumPartitions() to get the partition-count of rdd1


   RDD Operations
   --------------
	
     Two types of operations


	1. Transformations
		-> Create RDDs
		-> Cause lineage DAGs to be created
		-> Does not cause execution
		
	2. Actions
		-> Execute RDDs
		-> Convert the logical plan to the physical plan



   RDD Lineage DAG
   ---------------
   RDD Lineage DAG is a logical plan on how to create the RDD
   Driver maintains Lineage DAGs for all RDDs
   A DAG maintains heirarchy of dependencies all the way from the very first RDD.
   Transformations & data loading commnads create DAG.


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage of rddFile : (4) rddFile -> sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage of rddPairs : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddWc = rddPairs.reduceByKey(lambda x, y : x + y)
	Lineage of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile on wordcount.txt

	[ sc.textFile -> flatMap -> map -> reduceByKey => rddWc ===> sent to driver ]

	rddWc.collect()


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<filePath>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  --> instruction to save rdd6 partitions
	rdd7 = rdd6.t7(...)
	
	rdd6.collect()
	rdd6 DAG : (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile -> t3 -> t5 -> t6] ==> collected

	rdd7.collect()
	rdd7 DAG : (4) rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[t7] ==> collected


	StorageLevels
	-------------
	MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK		-> Disk Memory Serialized 1x Replicated
	DISK_ONLY		-> Disk Serialized 1x Replicated
	MEMORY_ONLY_2		-> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated


	Commands
	--------		
	rdd1.cache()     			-> in-memory persistence
	rdd1.persist()				-> in-memory persistence
	rdd1.persist(StorageLevel.DISK_ONLY)

	rdd1.unpersist()


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.


  Types of Transformations
  ------------------------
	Two Types

	1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive



  Spark Application Execution Heirarchy
  --------------------------------------

	Spark Application (Each applicatio has a SparkContext)
	|
	|=> Jobs (Each action command spans a Job)
	    |
	    |=> Stages (one or more stages, Each wide transformation will cause stage transition)
		|
		|=> Tasks (one or more tasks, # tasks = #partitions in that stage)
		    |
		    |=> Transformations (that can run in parallel)
		

  RDD Transformations
  -------------------
  
  1. map		P: U -> V
			Transforms each object of the input RDD by applying the function
			object to object transformation
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: len(x.split())).collect()


  2. filter		P: U -> Boolean
			Only those objects for which the function return True will be in the output RDD.
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) > 51).collect()


  3. glom		P: None
			Return one list object per partition with all the objects of the partition

		
		rdd1		rdd2 = rdd1.glom()

		P0: 1,5,4,7,8 -> glom -> P0: [1,5,4,7,8]
		P1: 4,5,6,7,8 -> glom -> P1: [4,5,6,7,8]
		P2: 7,0,9,1,3 -> glom -> P2: [7,0,9,1,3]

		rdd1.count() = 15 (int)  rd2.count() = 3 (list)


  4. flatMap		P: U -> Iterable[V]
			flatMap flatten the iterables produced by the function.

		rddFile.flatMap(lambda x: x.split()).collect()



  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation


		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 1,5,4,7,8 -> mapPartitions -> P0: 25
		P1: 4,5,6,7,8 -> mapPartitions -> P1: 30
		P2: 7,0,9,1,3 -> mapPartitions -> P2: 20


		rdd1.mapPartitions(lambda p: [sum(p)] ).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()	



  6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
			Similar to mapPartitions, but you get the partition-index as additional function parameter.
				
		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).glom().collect()


  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.
		 
		rddWords.distinct().collect()
		rddWords.distinct(4).collect()


  Types of RDDs
  -------------

	1. Generic RDD => RDD[U]                
	2. Pair RDD    => RDD[(K, V)]		


  8. mapValues		P: U -> V
			Applied to only Pair RDDs
			Transforms the 'value' part of the (K,V) by applying the function.	

		rdd3.mapValues(len).collect()


  9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the value of function output

		rddWords.sortBy(lambda x: x[0]).collect()
		rddWords.sortBy(lambda x: x[0], False).collect()
		rddWords.sortBy(lambda x: x[0], True, 3).collect()

 
  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where
			   key: Each unique value of the function output
			   value: ResultIterable. objects of the RDD that produced the key

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split()) \
            		.groupBy(lambda x: x) \
            		.mapValues(len) \
            		.sortBy(lambda x: x[1], False, 1)


  11. randomSplit	P: A list of weights (ex: [0.5, 0.5]), Optional: seed
			Returns a list of RDDs split in the specified weights	

		rddList = rdd1.randomSplit([3, 2])
		rddList = rdd1.randomSplit([3, 2], 3345)	


  12. repartition	P: numPartition
			Is used to increase or decrease the number of output partitions
			Perform global shuffle

		rdd2 = rdd1.repartition(10)


  13. coalesce		P: numPartition
			Is used to only decrease the number of output partitions
			Perform partition merging

		rdd2 = rdd1.coalesce(5)


	Recommendations
	---------------
	-> The size of each partition should be between 100MB to 1GB
	-> If you are running on Hadoop data (HDFS), 128MB is ideal size
	-> If the number of partition is less than but close to 2000, bump it up to 2000
        -> The number of cores in each executor should be 5 

  14. union, intersection, subtract

		-> Operate on two generic RDDs.

		Assume rdd1 has M partitions and rdd2 has N partitions

		command				output
		----------------------------------------------
		rdd1.union(rdd2)		M+N, narrow
		rdd1.intersection(rdd2)		M+N, wide
		rdd1.subtract(rdd2)		M+N, wide


  15. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           

  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])

	
  ..ByKey Transformations
  -----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only
	

  16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(False, 3).glom().collect()


  17. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: do not use groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)


  18. reduceByKey	P: (U, U) -> U
			Reduce all the values of each key by iterativly applying the reduce function.
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)

	  

  RDD Actions
  -----------

  1. collect   	    => Returns a list with all the objects of all the partitions of the RDD.

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one object by iterativly applying the function first on
			each partition and then across the reduced values of all partitions. 	
		
		rdd1									
	     	P0: 1, 2, 4, 3, 5, 7, 6, 8, 7, 0    -> reduce -> 43 -> reduce -> 153
		P1: 9, 8, 7, 9, 7, 9, 6, 8, 4, 2    -> reduce -> 69
		P2: 1, 3, 2, 1, 3, 5, 9, 0, 9, 0, 8 -> reduce -> 41

		rdd1.reduce(lambda x, y: x + y)

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )


  5. aggregate		Three parameters
			
			1. zero-value : starting value of the type of final output you want to produce
			2. sequence fn : Operate on each partition
					Iterativly merges all the values of the partition with the zero-value
			3. combine fn : Reduces all the per-partition outputs generated by seq-fn.

	rdd1.aggregate( (0,0), lambda z,v:(z[0]+v, z[1]+1), lambda x,y: (x[0]+y[0], x[1]+y[1]) )
	
  
  6. take(n)

		rdd1.take(10)  -> returns a list of first 10 objects


  7. takeOrdered(n, [fn])

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, len)


  8. takeSample(withReplacement, n, [seed])

		with-replacement sampling
			rdd1.takeSample(True, 10)
			rdd1.takeSample(True, 10, 45645)    # 45645 is a seed

		without-replacement sampling
			rdd1.takeSample(False, 10)
			rdd1.takeSample(False, 10, 45645)   # 45645 is a seed


  9. countByValue


  10. countByKey

  11. foreach  => P: function;
		  Returns nothing. 
		  Executes the function on all objects of the RDD.


  Use Case
  ---------
		
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, get the average weight of all the models of each make 
	of American origin cars. 
	-> Arrange in the DESC order of average weight. 
        -> Save the output as a single text file. 

        => Try it yourself



  Spark Closures
  --------------

	In spark, a closure constitute all the code (variables and methods) that must be visible inside an
        executor to the task to perform their computations on RDD partitions. 

	Spark serializes the closure and separate copy of it is sent to every executor. 


	c = 0

	def isPrime(n):
	  return True if n is Prime
	  else return False

	def f1(n):
	  global c
	  if (isPrime(n)) c += 1
	  return n*2     	

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     


        Limitation of closure: The local variables can not be used to implement globl counters, as these varibale
	that are part od the closure will separate copies at every executors. 
	
	Solution: Use "Accumulator" varibale.


  Distributed Shared Variables
  ----------------------------

    1. Accumulator
	-> Is a shared variable, not part of a closure
	-> Not a local copy
	-> One variable maintained by the driver
	-> All tasks can add to this variable (the driver copy is updated)
	-> Is used to implement "global counter"


	c = sc.accumulator(0)

	def isPrime(n):
	  return True if n is Prime
	  else return False

	def f1(n):
	  global c
	  if (isPrime(n)) c.add(1)
	  return n*2     	

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c.value )    # 80 


  2. Broadcast variable

	d = {1: a, 2: b, 3: c, 4: d, 5: e, 6: f, ....}   # 100 MB
	bc = sc.broadcast(d)

	def f1(n):
	   global bc
	   return bc.value[n]

	rdd1 = sc.parallelize( [1,2,3,4,5,......], 4)

	rdd2 = rdd1.map( f1 )

	rdd2.collect()   => a,b,c,d,e,...



  Spark-Submit Command
  ====================

     -> Is a single command to submit any spark application (Python, Scala, Java, R) to any cluster
        manager (local, spark standalone, YARN, mesos, kubernetes)

 	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
		--executor-memory 10G \
		--driver-cores 2 \
		--executor-cores 5 \
		--num-executors 10 \
		E:\PySpark\wordcount.py [app args]


  =======================================
     Spark SQL (pyspark.sql)
  =======================================

   Spark's Structured data processing API

	File Formats: Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format: RDBMS & NoSQL databases
	Hive Format: Hive Warehouse directories. 


  SparkSession
  ------------
	Starting point of execution.
	Represents a user-session within an application (SparkContext)
	Every SparkSession can have its own configuration.

   	spark = SparkSession \
		.builder \
		.appName("Basic Dataframe Operations") \
		.config("spark.master", "local[*]") \
		.getOrCreate()   


  DataFrame
  ---------
	-> Represents a collection of distributed in-memory partitions
	-> immutable and lazily evaluated. 
	-> DataFrame is a collection of "Row" objects.

	DataFrame has two components:
		Data   : Collection of Rows
		Schema : StructType object

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)


  Basic Steps in a Spark SQL program
  ----------------------------------

  1. Create a SparkSession.

	spark = SparkSession \
		.builder \
		.appName("Basic Dataframe Operations") \
		.config("spark.master", "local[*]") \
		.getOrCreate()  


	spark.conf.set("spark.sql.shuffle.partitions", "10")
	spark.conf.set("spark.sql.adaptive.enable", "true") 
	spark.conf.get("spark.sql.shuffle.partitions")
 

  2. Create a DataFrame from some data source.

	filePath = "E:\\PySpark\\data\\users.json"

	df1 = spark.read.format("json").load(filePath)
	df1 = spark.read.load(filePath, format="json")
	df1 = spark.read.json(filePath)

	df1.show()
	df1.printSchema()


  3. Apply transformations on the DataFrame using DataFrame transformation methods or using SQL

	Using DataFrame transformation methods
        ---------------------------------------
	       df2 = df1.where("age is not null") \
			.select("userid", "name", "age", "gender", "phone") \			
			.orderBy("gender", "age") \
			.groupBy("age", "gender").count() \
			.limit(4)

	Using SQL
	---------
		spark.catalog.listTables()

		df1.createOrReplaceTempView("users")
		#spark.catalog.dropTempView("users")   => to drop temp-view

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""

		df3 = spark.sql(qry)

		df3.show()


  4. Save the DataFrame to some structured destination. 

	outputPath = "E:\\PySpark\\output\\df2"

	df2.write.format("json").save(outputPath)
	df2.write.save(outputPath, format="json")
	df2.write.json(outputPath)


  SaveModes
  ---------
     -> Modes that you can apply while writing to existing directory

	errorifexists (default)
	ignore
	append
	overwrite

	df2.write.mode("overwrite").json(outputPath)
	df2.write.json(outputPath, mode="overwrite")


  LocalTempView & GlobalTempView
  ------------------------------

	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")

	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView ("gusers")

   
  DataFrame Transformations
  -------------------------

  1. select
	
		df2 = df1.select("ORIGIN_COUNTRY_NAME",
				"DEST_COUNTRY_NAME",
				"count")
		----------------------------------------
		df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
			expr("DEST_COUNTRY_NAME as destination"),
			expr("count").cast("int"),
			expr("count+10 as newCount"),
			expr("count > 100 as highFrequency"),
			expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME").alias("domestic"))


		df2.show()
		df2.printSchema()


  2. where / filter

  3. orderBy / sort

  4. groupBy   => returns a 'pyspark.sql.group.GroupedData' object (not a DataFrame)
		  Apply aggregation methods to return a DataFrame

  5. limit



  Getting Column objects
  ----------------------
	col("ORIGIN_COUNTRY_NAME")
	Out[282]: Column<b'ORIGIN_COUNTRY_NAME'>

	column("ORIGIN_COUNTRY_NAME")
	Out[283]: Column<b'ORIGIN_COUNTRY_NAME'>

	expr("ORIGIN_COUNTRY_NAME")
	Out[284]: Column<b'ORIGIN_COUNTRY_NAME'>

	df1["ORIGIN_COUNTRY_NAME"]
	Out[285]: Column<b'ORIGIN_COUNTRY_NAME'>

	df1.ORIGIN_COUNTRY_NAME
	Out[286]: Column<b'ORIGIN_COUNTRY_NAME'>


  Working with different file formats
  -----------------------------------
  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)

  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)	


  CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")


  Text
	read
		df1 = spark.read.text(inputPath)

	write
		df1.write.text(outputPath)



  Creating an RDD from DataFrame
  ------------------------------
   
        rdd1 = df1.rdd


  Creating a DataFrame from programmatic data
  -------------------------------------------
   
     	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])


  Creating a DataFrame from RDD
  -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame with custom/programmatic schema
  ----------------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)  
	-----------------------------------------------------
        filePath = "E:\\PySpark\data\\flight-data\\2015-summary-nh.csv"

	mySchema = "origin STRING, destination STRING, count INT"
	df1 = spark.read.csv(filePath, schema=mySchema) 
	-----------------------------------------------------
	mySchema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

	df1 = spark.read.json(filePath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()
	-----------------------------------------------------
	mySchema = StructType([
				 StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
				 StructField("DEST_COUNTRY_NAME", StringType(), True),
				 StructField("count", IntegerType(), True)
			   ])

	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()
















