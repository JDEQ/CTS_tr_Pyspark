
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> RDD Shared Variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	-> Dstreams API (introduction)


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

  Spark
  -----
     -> Used for big-data analytics.

     -> Spark is a in-memory distributed computing framework.

	 -> in-memory means intermediate results of transformations can be saved in memory (RAM)
	    and subsequent transformations can process those savs results. 

     -> Spark is a unified framework. 
	   
           -> Provides a set of consistent APIs for processing different analytical workloads using
	      the same execution engine and well-defines data abstractions.
	
		Batch Analytics	of unstructured data 	=> Spark Core API (RDD)
		Batch Analytics	of structured data 	=> Spark SQL (DataFrames)
          	Stream Analytics (real-time)		=> Structured Streaming, DStreams
		Predictive Analytics (ML)		=> Spak MLlib
		Graph Parallel Computations		=> Spark GraphX

      -> Spark is written in Scala language.

      -> Spark is a poliglot
	   => Supports scala, java, python & R

      -> Spark is very fast
	   -> Spark is 100 times faster than MapReduce if you use 100% in-memory
	   -> Spark is 6 to 7 times faster than MapReduce if you use disk based computation
  
  
  Getting started with Spark
  --------------------------

    1. Working in your vLab	
	  1.1 PySpark Shell
		=> pyspark   (type on a terminal)

	  1.2 Spyder IDE   
		=> Search for spyder and launch
		=> If spyder is not found, you can install it
			pip install spyder
			sudo apt install spyder

	  1.3 Jupyter notebooks
		=> Open a terminal
		=> Type :
			jupyter notebook
			jupyter notebook --allow-root
	
    2. Setup PySpark environment on your local machine.
	2.1 Make sure you have anaconda distribution installed.
		https://docs.anaconda.com/anaconda/install/

	2.2 Follow the step from this document:
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition
	     https://docs.databricks.com/getting-started/community-edition.html
	     
	     Signup link: https://www.databricks.com/try-databricks
		=> Make sure you click on "Community Edition" option 
	     Login: https://community.cloud.databricks.com/login.html

	     To Download a file from databricks:
	     -----------------------------------

	    /FileStore/<FILEPATH>
            https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

	    Example:
	    /FileStore/tables/wordcount-5.txt
	    https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


	
   Spark Architecture (Building Blocks)
   ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


  Spark Core API
  --------------
    => Is the low-level API
    => Takes care of all low-level operations
	-> memory-managenement
	-> monitoring
	-> fault-recovery.
    => RDD is the main data abstraction.
	
   
    RDD (Resilient distributed dataset)
    -----------------------------------

	=> Is the fundamental data abstraction of Spark

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data => A colletion of distributed in-memory partitions.
		       -> Each partition is a collection of objects.

	=> RDDs are lazily evaluated
		-> RDD transformations does not cause execution
		-> RDD Action commands trigger execution.

	=> RDDs are immutable


   Creating RDDs
   -------------
    Three ways:

	1. Create an RDD from some external data file.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The number of partitions is based on 'sc.defaultMinPartitions'

	2.  Create an RDD from programmatic data

		rdd1 = sc.parallelize( range(1, 100), 4 )
		rdd2 = sc.parallelize( [1,1,2,3,2,4,5,6,4,6,7,7,8,8,0], 2 )

		rdd1 = sc.parallelize( range(1, 100))
		-> The number of partitions is based on 'sc.defaultParallelism'

	3. By applying transformations on existsing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))

	Note: To get the number of partitions of an RDD use:

		rddFile.getNumPartitions()


   RDD Operations
   --------------
     2 operations

	1. Transformations
		-> Create the RDD lineage DAGs 
		-> Does not trigger the actual execution
		-> Every transformation returns an other RDD.

	2. Actions
		-> Trigges execution.
		-> Converts the logical plan to physical plan and sends the tasks to the executors.

   RDD Lineage DAG
   ---------------
	=> Is the meta data of an RDD maiantained by the driver	

        rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
	     	Lineage DAG: (4) rdd1 -> sc.textFile on "E:\\Spark\\wordcount.txt"

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
		Lineage DAG: (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))	
		Lineage DAG: (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
		Lineage DAG: (4) rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile

		Tasks: (sc.textFile, flatMap, map, reduceByKey)

	rdd4.collect()


   RDD Persistence
   ===============

	rdd1 = sc.textFile( <File>, 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )        => instruction to the Spark to save rdd6 partition.
	rdd7 = rdd6.t7(...)

	rdd6.collect()

		Lineage DAG of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile -> t3 -> t5 -> t6] -> [rdd6]

	rdd7.collect()
	
		Lineage DAG of rdd7: rdd7 -> rdd6.t7
		[t7] -> [rdd7]	

	rdd6.unpersist()

	StorageLevel
	------------
	1. MEMORY_ONLY	      	=> default. Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	=> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		=> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	=> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2    => Disk Memory Serialized 2x Replicated		

	Commands
	--------
	rdd1.cache()     			=> in-memory persistence
	rdd1.persist()   			=> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()
	

   Executor's memory structure
   ===========================
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   =========================

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 

   
   RDD Transformations
   ===================

    Note: A transformation returns an RDD.

    1. map 		P: U -> V
			Transforms each input object to output object by applying the function.
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: x.split(" ") ).collect()

    2. filter		P: U -> Boolean 
			Only those input objects for which the function returns True will be in the output
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) > 51).collect()

    3. glom		P: None
			Returns one list object per partition with all the objects of the partition. 


		rdd1		rdd2 = rdd1.glom()

		P0: 1,4,2,5,7,8 -> glom -> P0: [1,4,2,5,7,8]
		P1: 5,7,2,1,4,3 -> glom -> P1: [5,7,2,1,4,3]
		P2: 9,0,5,7,1,4 -> glom -> P2: [9,0,5,7,1,4]

		rdd1.count() = 18 (int)	  rdd2.count() = 3 (list)
	
  		rdd1.glom().map(len).collect()

    4. flatMap		P: U -> Iterable[V]     (iterable -> sth that you can loop through)
			flatMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: >= N objects

		 rddFile.flatMap(lambda x: x.split(" ")).collect()


   5. mapPartitions	P: Iterable[U] : Iterable[V]
			Take an entire input partition as function input and returns the output partition
			partition to partition transformation.

		rdd1	 rdd2 = rdd1.mapPartitions( lambda p: [sum(p)] ) 						

		P0: 1,4,2,5,7,8 -> mapPartitions -> P0: 27
		P1: 5,7,2,1,4,3 -> mapPartitions -> P1: 22
		P2: 9,0,5,7,1,4 -> mapPartitions -> P2: 26

		rdd1.mapPartitions(lambda p: [sum(p)]).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()

   6. mapPartitionsWithIndex	p: Int, Iterable[U] : Iterable[V]
				Similar to mapPartitions, but we get the partition-index as an additional function
				parameter.

		rdd1.mapPartitionsWithIndex( lambda i, p : [(i, sum(p))] ).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x), p)) \
		    .filter(lambda t: (t[0] == 1)) \
                    .map(lambda x: x[1]) \
                    .collect()

   7. distinct		p: None, Optional: numPartitions
			Returns an RDD with distinct objects of the RDD.
			input RDD: N objects, output RDD: <= N objects

		rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(4)
		
   Types of RDDs
   -------------
	Generic RDDs:  RDD[U]
	Pair RDDs:     RDD[(K, V)] 	

	 
   8. mapValues		p: U -> V
			Applied only on pair RDDs
			Transforms only the 'Value' part by applying the function.
			input RDD: N objects, output RDD: N objects

		rdd3 = rdd2.mapValues(lambda x: (x, x+10))
		-> where rdd2 MUST be a pair RDD

   9. groupBy		p: U -> V, Optional: numPartitions
			Returns a pair RDD where:
				key: each unique function output 
				value: ResultIterable containing all the objects of the RDD that produced the key.
		
		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           		   .flatMap(lambda x: x.split(" ")) \
           		   .groupBy(lambda x: x, 1) \
           		   .mapValues(len)

   10. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			The objects of the RDD are sorted based on the value of the funcion output
			All the objects with the same function output will always be there in the same partition.

		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           			.flatMap(lambda x: x.split(" ")) \
           			.groupBy(lambda x: x) \
           			.mapValues(len) \
           			.sortBy(lambda x: x[1], False, 1)

    11. randomsplit	P: List of weights (ex: [0.5, 0.3, 0.2])
			Returns a list of RDDs split randomly in the specified weights 

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 464)


    12. repartition	P: numPartitions
			Is used to increase or decrease in number of partitions of the output RDD
			Causes global shuffle

		rddWords6 = rddWords.repartition(6)
		rddWords3 = rddWords6.repartition(3)

    13. coalesce	P: numPartitions
			Is used to decrease in number of partitions of the output RDD
			Cause partition-merging
	
		rddWords6 = rddWords.coalesce(6)

	Recommendations
        ---------------
	-> The size of each partition should be between 100 MB to 1 GB (ideally 128 MB)
	-> If the number of partitions is close to but less than 2000, bump it up to 2000
	-> The number of CPU cores per executor should be 5


    14. partitionBy	P: numPartitions, Optional: partitioning-function  (default: hash)
			Applied only on pair RDDs
			Partitioning is performed based on the 'key'

	 transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


    15. union, intersection, subtract, cartesian

	   Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   command				partitions & type
           ------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide


    ..ByKey Transformations
    -----------------------
  	-> Are wide transformations
	-> Are applied only on Pair RDDs


    16. sortByKey	P; None, Optional; ascending (True/False), numPartitions
			The objects of the RDD are sorted based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 4).collect()

    17. groupByKey	P: None, Optional: numPartitions
			Returns a pair RDD where:
				key: Unique keys of the input RDD
				value: ResultIterable - grouped values for each unique key.	

			CAUTION: AVOID groupByKey as much as possible.   

		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           		.flatMap(lambda x: x.split(" ")) \
           		.map(lambda x: (x, 1)) \
           		.groupByKey() \
           		.mapValues(sum) \
           		.sortBy(lambda x: x[0][-1], True, 1)	

    18. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Reduce values of each unique first for each partition and then across the
			outputs of partitions.

		wordcount = text_file.flatMap(lambda line: line.split(" "))  \
                		.map(lambda word: (word, 1)) \
                		.reduceByKey(lambda a, b: a + b, 1)

    19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))

		
   
  RDD Actions
  ===========

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one value of the same type by iterativly applying the
			function first at each partition and then on the outputs of each partition.
		rdd1			

		P0: 1,2,1,3,5,6,1 -> reduce ->  -17 => 4
		P1: 2,3,1,4,5,7,1 -> reduce ->  -19
		P2: 9,5,3,2,1,0,0 -> reduce ->  -2 
	
		rdd1.reduce( lambda x, y : x - y ) = 4
   
		rddWc.reduce(lambda a, b: (a[0] + "," + b[0], a[1] + b[1]) )

   5. aggregate		
	
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. first

   7. take(n)

   8. takeOrdered(n)

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)


   9. takeSample	=> rdd1.takeSample(withReplacement(True/False), n)
			   rdd1.takeSample(True, 10)

		  rdd1.takeSample(True, 10)		-> withReplacement sampling
		  rdd1.takeSample(True, 10, 345)	-> withReplacement sampling with seed
		  rdd1.takeSample(False, 100, 645)	-> withOutReplacement sampling with seed

  10. countByValue

  11. countByKey

  12. foreach	=> does not return anything
		   runs a function on all the objects of the RDD.

		rddWc.foreach(lambda p: print("Key: {0}, Value:{1}".format(p[0], p[1])))

  13. saveAsSequenceFile


  Use-Case
  --------
    dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, find the average-weight of each make from among 'American' origin cars.
	-> Arrange the data in the DESC order of average-weight
	-> Save the output as a single text file.

	=> Try it yourself..


  Spark Closures
  ==============

	In spark, a 'closure' represents all the variables and methods that must be visible inside
        an executor for the tasks to complete their computations on RDD partitions.

	=> Driver serializes the closure and a separate copy is sent to each executor. 

	c = 0

	def isPrime(n):
	   return True or False

	def f1(n):
	   global c
	   if (isPrime(n)) c += 1
	   return n + 10
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     // c = 0

	Limitation: We CAN NOT use local variables to implement global counter.
	Solution: Use "Accumulator" variable.


  Shared Variables
  ================
	
    1. Accumulator

	-> Is a shared variable that is accessible from all the executors.
	-> Not part of the closure
	-> Is maintained by the driver
	-> All tasks can write to this variable. 

	c = sc.accumulator(0)

	def isPrime(n):
	   return True or False

	def f1(n):
	   global c
	   if (isPrime(n)) c.add(1)
	   return n + 10
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     // c = 0


    2. Broadcast Variable

	-> A broadcast variable is used to save memory
	-> Driver sends one copy of the broadcast variable to each executor
	-> All tasks in that executor-node can access that single copy.
	-> Broadcast variable is not part of the closure.

	bc = sc.broadcast({ 1: a, 2: b, 3: c, 4: d, 5: e, 6: f, .... })      # 100 MB
	
	def f1(n):		
	   global bc
	   return bc.value[n]	  
	
	rdd1 = sc.parallelize( [1,2,3,4,5,6, .... ], 4 )	
	rdd2 = rdd1.map(f1)
	rdd2.collect()         # => a, b, c, d, ...

  =======================
   spark-submit command
  =======================
   -> Is a single command to submit any spark application (python, java, scala, R) to any cluster
      manager (local, spark standalone, YARN, Mesos, Kubernetes)
	

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G
		--driver-cores 2
		--executor-memory 5G
		--executor-cores 5
		--num-executors 10
		E:\\Spark\\wordcount.py [application args]
	
 ============================	
   Spark SQL (pyspark.sql)
 ============================

   => High level API built on top of Spark Core

   => Spark's structured data processing API
	
	  Structured file formats : parquet (default), ORC, JSON, CSV (delimited text file)
	  		     JDBC : RDBMS, NoSQL
			     Hive : Hive Warehouse

   => SparkSession
	-> Starting point of execution for Spark SQL
	-> Represents a user-session (with its own configuration) running inside an application

		spark = SparkSession \
    			.builder \
    			.appName("Basic Dataframe Operations") \
    			.config("spark.master", "local[*]") \
    			.getOrCreate() 

  => DataFrame  (DF)
	-> Spark SQL's data abstraction
	-> Is a collection distributed in-memory partitions
	-> Are immutable

	-> DataFrame :
		-> Schema: Is a "StructType" object

			StructType(
                           List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			 )

		-> Data: Collection of "Row" objects


  Basic steps of a Spark SQL program
  ===================================

	1. Create a DataFrame from some data source.

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	2. Transform the DataFrame using DF Transformations API methods or using SQL

		Using DF API methods:
		---------------------
		     df2 = df1.where("age is not null") \
         		      .select("userid", "name", "gender", "age") \
         		      .orderBy("gender", "age") \
         		      .groupBy("age").count() \
         		      .limit(4)

		Using SQL:
		---------
		     spark.catalog.listTables()
		     df1.createOrReplaceTempView("users")
		     df3 = spark.sql("""select age, count(*) as count
                			from users
                			where age is not null
                			group by age
                			order by count
                			limit 4""")

                     # drop temp table
		     spark.catalog.dropTempView("users")

			
	3. Save the results to some destination
		
		outputPath = "E:\\PySpark\\output\\json"
		df3.write.format("json").save(outputPath)
		df3.write.json(outputPath)


  SaveModes
  ---------
	What to do when you are writing to an existing directory?

	errorIfExists (default)
	ignore
	append
	overwrite

	df3.write.json(outputPath, mode="overwrite")
	df3.write.mode("overwrite").json(outputPath)


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

  DataFrame Transformations
  -------------------------

   1. select

	 df2 = df1.select( ... strings ... )
	 df2 = df1.select( ... columns ... )

	df2 = df1.select(col("DEST_COUNTRY_NAME"),
                 	column("ORIGIN_COUNTRY_NAME"),
                 	expr("count"),
                 	df1["DEST_COUNTRY_NAME"],
                 	df1.DEST_COUNTRY_NAME)

	df2 = df1.select(col("DEST_COUNTRY_NAME").alias("destination"),
                 col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

   2. where / filter

		df3 = df2.where("domestic = false and count > 1000")
		df3 = df2.filter("domestic = false and count > 1000")
		df3 = df2.where( col("count") > 100 )

   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.sort("count", "origin")
		df3 = df2.sort(desc("count"), asc("origin"))

   4. groupBy   => Returns GroupedData object
		     -> Run aggregation methods to return a DataFrame

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").max("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
        		.agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 3).alias("avg"))

   5. limit  
		df2 = df1.limit(10)

   6. selectExpr

		df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                 	"ORIGIN_COUNTRY_NAME as origin",
                 	"count",
                 	"count + 10 as newCount",
                 	"count > 200 as highFrequency",
                 	"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")



   7. withColumn & withColumnRenamed

		df3 = df1.withColumn("count", col("count").cast("int")) \
         		.withColumn("newCount", expr("count + 10")) \
         		.withColumn("highFrequency", expr("count > 200")) \
         		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
         		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

		df2 = userDf.withColumn("ageGroup", when((col("age") < 13), "Child")
                                    .when((col("age") < 20), "Teenager")
                                    .when((col("age") < 60), "Adult")
                                    .otherwise("Senior"))

   8. udf  (user defined function)

	
		def getAgeGroup( age ):
    			if (age <= 12):
        			return "child"
    			elif (age >= 13 and age <= 19):
        			return "teenager"
    			elif (age >= 20 and age < 60):
        			return "adult"
    			else:
        			return "senior"
    
		getAgeGroupUdf = udf(getAgeGroup, StringType())    
   
		df2 = userDf.withColumn("ageGroup", getAgeGroupUdf(col("age")))

               ---------------------------------------------------

		@udf(returnType = StringType())
                def getAgeGroup( age ):
    			if (age <= 12):
        			return "child"
    			elif (age >= 13 and age <= 19):
        			return "teenager"
    			elif (age >= 20 and age < 60):
        			return "adult"
    			else:
        			return "senior"   
		 
   
		df2 = userDf.withColumn("ageGroup", getAgeGroup(col("age")))

		----------------------------------------------------

		spark.udf.register("get_age_group_udf", getAgeGroup, StringType())

		qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"

		df3 = spark.sql(qry)
		df3.show()

   9. drop

		df3 = df2.drop("newCount", "highFrequency")

   10. dropna  => used to drop 'Rows' that have nulls. 

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df3 = usersDf.dropna()
		df3 = usersDf.dropna(subset = ["phone", "age"])

   11. dropDuplicates => drops the duplicate rows.

		listUsers = [(1, "Raju", 5),
             		(1, "Raju", 5),
             		(3, "Raju", 5),
             		(4, "Raghu", 35),
             		(4, "Raghu", 35),
             		(6, "Raghu", 35),
             		(7, "Ravi", 70)]

		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
		userDf.show()

		df3 = userDf.dropDuplicates()
		df3.show()

		df3 = userDf.dropDuplicates(["name", "age"])
		df3.show()


   12. distinct

		df3 = userDf.distinct()
		df3.show()

		Q:  how many unique DEST_COUNTRY_NAMEs are there?
		df1.select("DEST_COUNTRY_NAME").distinct().count()
		df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()

   13. sample
		df2 = df1.sample(True, 0.5)
		df2 = df1.sample(True, 0.5, 4646)
		df2 = df1.sample(True, 1.5, 4646)     
		# fraction > 1 allowed in withreplacement sampling

		df2 = df1.sample(False, 0.6)
		df2 = df1.sample(False, 0.6, 7676)
		df2 = df1.sample(False, 1.5, 4646)     
		# ERROR: fraction > 1 not allowed in without-replacement sampling
   14. randomSplit

		dfList = df1.randomSplit( [0.4, 0.3, 0.3], 4543534 )

		dfList[0].count()
		dfList[1].count()
		dfList[2].count()

   15. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(2, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()


		Note: The number of shuffle partitions of an output dataframe (created as a result of shuffle
		      operation) is determined by "spark.sql.shuffle.partitions" configuration property 
		     (of the sparkSession)

 		 -> spark.conf.set("spark.sql.shuffle.partitions", "10")
	         -> spark.conf.get("spark.sql.shuffle.partitions")

   16. coalesce
		df6 = df2.coalesce(3)
		df6.rdd.getNumPartitions()

   17. join -> discussed separatly



   Working with different file formats
   -----------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath)
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")
   

   Creating an RDD from DF
   -----------------------
	rdd1 = df1.rdd

   
   Creating a DF from programmatic data
   ------------------------------------
   	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

   	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
   	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Creating a DF from RDD
   ----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")
	df1 = rdd1.toDF(["id", "name", "age"])
   
   Creating a DF with programmatic schema
   -------------------------------------- 
	mySchema = StructType([
              StructField("id", IntegerType(), True),
              StructField("name", StringType(), True),
              StructField("age", IntegerType(), True) 
            ])

	df1 = spark.createDataFrame(rdd1, schema = mySchema)
     
       ------------------------------------------------------

	mySchema = StructType([
              StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
              StructField("DEST_COUNTRY_NAME", StringType(), True),
              StructField("count", IntegerType(), True) 
            ])

	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	df2 = spark.read.json(inputPath, schema=mySchema)
	df2 = spark.read.schema(mySchema).json(inputPath)


   Joins
   ------
	Supported join types => inner, left, right, full, left_semi, left_anti

	left_semi join
        --------------
	=> Is like inner join but the data comes from only the left-side table.
	=> Equivalent to the following sub-query:
		select * from emp where deptid in (select id from dept)

	left_anti join
	--------------
	=> Equivalent to the following sub-query:
		select * from emp where deptid not in (select id from dept)


  Use-Case
  --------
    
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv datasets fetch the top 10 movies with highest average-user rating
      -> Consider only those movies that are rated by atleast 30 users
      -> data: movieId, title, numberOfRatings, averageRating
      -> Arrange the data in the DESC order of averageRating
      -> Save the output as a single pipe-separated CSV file with header.

	=> Try it yourself.


  JDBC Format - MySQL integration
  -------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


  Hive Format - Integrating with Hive
  -----------------------------------

    Hive is a data warehousing platform for Hadoop


	
    # -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.catalog.currentDatabase()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL


moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


 ===================================
    Spark Streaming
 ===================================

  Spark's streaming data (real-time) analytics API

     Two libraries
	1. Spark Streaming
	2. Structured Streaming (this is preferred)

  
    Spark Structured Streaming
    -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 

	=> Sources: File, Socket, Rate, Kafka
	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch 













