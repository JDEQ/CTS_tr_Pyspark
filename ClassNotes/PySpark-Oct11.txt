
  Agenda (8 sessions)
  ------------------- 
    Spark - basics & architecture
    Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variables
    Spark-Submit
    Spark-SQL
	-> Working with DataFrames
	-> Integrations - MySQL & Hive
    Spark Streaming
	-> DStreams (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF presentations
	=> Code Modules
        => Class Notes
	=> Github: https://github.com/ykanakaraju/pyspark
  
  Getting started with Spark
  --------------------------
	1. Using vLab
	      -> Follow the instruction given in the attached document.
              -> Three tools:
		   -> PySpark shell
		   -> Spyder IDE
		   -> Jupyter Notebooks

	2. Installing development environment in your personal machine.
		-> First make sure you have "Anaconda distribution"
		    => URL: https://docs.anaconda.com/anaconda/install/
                -> Follow the instruction given in the shared document to setup "PySpark" with Spyder & Jupyter

        3. Signup for Databricks community edition (free signup)
		Signup link: https://www.databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html
   

  Spark
  ------
        Cluster: Is a unified entity comprising of many nodes whose combined resources can be used to
                 distribute your storage or processing.

	=> Spark is written in Scala programming language

	=> Spark is a in-memory distributed computing framework.

	=> Spark is a 'unified' analytics framework

	    Spark provides a consistent set of APIs for processing different analytics workloads
	    using the same execution engine.

		Batch Analytics	of unstructured data    => Spark Core API
		Batch Analytics	of structured data	=> Spark SQL
		Streaming Analytics			=> Spark Streaming, Structured Streaming 
		Predictive Analytics			=> Spark MLLib
		Graph Parallel Computations		=> Spark GraphX

	=> Spark is a polyglot
		-> Supports Scala, Java, Python and R

	=> Spark jobs can be submitted to mutiple cluster managers
		=> local, spark standalone, YARN, Mesos, Kubernetes


  Spark Architecture (& Building Blocks)
  --------------------------------------

       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
	

   RDD (Resilient Distributed Dataset)
   ------------------------------------
    -> Fundamental in-memory data abstraction of Spark Core API

    -> RDD is a collection of distributed in-memory partitions.
	-> Partition is a collection of objects (of any type)

	RDD =>   Meta Data (Lineage DAG)
		 Data 	

    -> RDDs are immutable
   
    -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Actions commands 

  Creating RDDs
  -------------

    Three ways:

	1. Creating RDDs from external data files
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		=> default number fo partitions is given by sc.defaultMinPartitions

	2. Creating RDDs from programmatic data
		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0], 3)

		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0])
		=> default number fo partitions is given by sc.defaultParallelism

        3. By applying transformations on existing RDDs
		
		rdd2 = rdd1.map(lambda x: x*100)

  RDD Operations
  --------------
    Two things:

	1. Transformations
		-> Create the RDD lineage DAGs 
		-> Does not trigger the actual execution
		-> Every transformation returns an other RDD.

	2. Actions
		-> Trigges execution.
		-> Converts the logical plan to physical plan and sends the tasks to the executors.


  RDD Lineage DAG
  ---------------
   -> Are created at the driver when data-loading or transformation commands  
   -> Represents a 'logical plan' on how to create the RDD partitions
   -> Maintains the hierarchy of dependencies all the way from the very first RDD. 

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
	   Lineage DAG =>  rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   Lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    Lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

      
   RDD Persistence
   ---------------
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   --> instruction to spark to save the rdd6 partition
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> rdd6

	rdd7.collect()

	lineage of rdd7 => rdd7 -> rdd6.t7
		[t7] -> rdd7

	StorageLevels
        -------------
	1. MEMORY_ONLY	      -> default, Memory Serialized 1x replicated
	2. MEMORY_AND_DISK    -> Disk Memory Serialized 1x replicated
	3. DISK_ONLY	      -> Disk Serialized 1x replicated
	4. MEMORY_ONLY_2      -> Memory Serialized 2x replicated
	5. MEMORY_AND_DISK_2  -> Disk Memory Serialized 2x replicated
 

        Commands
        --------
		rdd1.cache()      -> memory only
		rdd1.persist()    -> memory only
		rdd1.persist( StorageLevel.DISK_ONLY )

		rdd1.unpersist()


   Executor Memory Structure
   -------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of transformations
   ------------------------

	Two types:

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


   RDD Transformations
   -------------------
  
    1. map		P: U => V
			Object to object transformation
			input RDD: N objects, output RDD: N objects
 	
		rddFile.map(lambda s: s.upper()).collect()

    2. filter		P: U -> Boolean
			Filters the objects of the input RDD based on the function output.
			input RDD: N objects, output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

    3. glom		P: None
			Returns one list object per partition with all the objects of the partition

		rdd1			rdd2 = rdd1.glom()
		P0: 1,2,1,3,4,5 -> glom -> P0: [1,2,1,3,4,5]
		P1: 4,3,5,4,6,7 -> glom -> P1: [4,3,5,4,6,7]
		P2: 3,1,3,5,6,7 -> glom -> P2: [3,1,3,5,6,7]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flattens the iterables produced by the function 
			input RDD: N objects, output RDD: >= N objects	

		rddWords.flatMap(lambda w: w.upper()).collect()

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 1,2,1,3,4,5 -> mapPartitions -> P0: 
		P1: 4,3,5,4,6,7 -> mapPartitions -> P1: 
		P2: 3,1,3,5,6,7 -> mapPartitions -> P2: 
		
		rdd1.mapPartitions(lambda p: [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()

   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Same as mapPartitions, but you get partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))] ) \
		    .filter(lambda t: t[0] == 0)\
		    .flatMap(lambda x: x[1])\
                    .map(lambda x: x*10)\
                    .collect()

   7. distinct		P: None, Optional: numPartitions
			returns an RDD with distinct objects (removes the duplicates in the output RDD)			 
		
			rddWords.distinct().collect()
			rddWords.distinct(4).collect()

   Types of RDDs	
   --------------
	1. Generic RDDs : RDD[U]
	2. Pair RDDS :	RDD[(K, V)]

   8. mapValues		P: U => V
			Applied only on Pair RDDs
			The 'value' part of the (K,V) pairs is transformed using the function.

		rdd4 = rdd2.mapValues(lambda v: (v, v+4) )

   9. sortBy		P: U => V, Optional: ascending (True/False), numPartition
			The elements of the RDD are sorted based on the function output

		rddWords.sortBy(lambda x: x[-1], True).collect()
		rdd1.sortBy(lambda x: x%5, False, 2).glom().collect()

   
















	







