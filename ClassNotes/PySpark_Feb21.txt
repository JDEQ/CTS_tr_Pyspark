
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> RDD shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	-> Dstreams API (introduction)


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 
  Spark
  ----- 

      Spark is wriiten in SCALA programming language

      Spark is a unified in-memory distributed computing framework.  
   
      Computing Cluster : A unified entity that consitutes many nodes whose cumulative resources 
      (RAM, CPU Cores, Disk) can be use to distribute your storage or processing.

      In-memory => The intermediate results (partitions) can be persisted in RAM and subsequent
      transformations that read those in-memory partitions (assuming you have enough RAM)
      

      Spark Unified Stack
	Spark comes with a unified set of APIs for performing computations on different Analytics
        Workloads.
	
		Batch Analytics of Unstructured data	-> Spark Core API
		Batch Analytics of Structured data	-> Spark SQL
		Stream Analytics			-> Spark Streaming
        	Predictive Analytics		        -> Spark MLlib
		Graph Parallel Computation		-> Spark GraphX
       

     Spark supports multiple cluster managers
        -> Local
	-> Spark Standalone
        -> YARN
	-> Mesos
	-> Kubernetes
     
     Spark is a polyglot
	-> Scala  (native language)
	-> Java
	-> Python 
	-> R


   Getting started with PySpark
   ----------------------------

    1. Working in your vLab

        -> Login in to Ubuntu VM
	
	  1.1 PySpark Shell
		=> pyspark   (type on a terminal)

	  1.2 Spyder IDE   
		=> Search for spyder and launch
		=> If spyder is not found, you can install it
			pip install spyder
			sudo apt install spyder

	  1.3 Jupyter notebooks
		=> Open a terminal
		=> Type :
			jupyter notebook
			jupyter notebook --allow-root

    2. Setup PySpark environment on your local machine	 

	 -> Make sure "Anaconda" is installed and configured. 
	 -> Try: "pip install pyspark"
	    OR
	 -> Follow the instructions given in the shared document
	   
  
    3. Databricks Community Edition  (Free Signup)

	     https://docs.databricks.com/getting-started/community-edition.html
	     
	     Signup link: https://www.databricks.com/try-databricks
		=> Make sure you click on "Community Edition" option 

	     Login: https://community.cloud.databricks.com/login.html


   Spark Architecture (Building Blocks)
   ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks is reported to the driver.


    RDD (Resiliant Distributed Dataset)
    -----------------------------------

	=> Fundamental data abstraction for Spark Core API

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data     => A colletion of distributed in-memory partitions.
		           -> Each partition is a collection of objects.

        => RDD partitions are immutable

        => RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Action commands triggers exection


    Creating RDDs
    --------------

       Three ways

	1. Create an RDD from some external data source (such as a text file)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		=> default partitions: sc.defaultMinPartitions
				       [Equal to 2 if cores allocated >= 2, else 1]

	2. Create an RDD from programmatic data (such as a collection)

		rdd1 = sc.parallelize( [1,2,1,3,3,5,3,5,7,8,4,7,8], 2)

		=> default partitions: sc.defaultParallelism
				       [Equal to the number of cores allocated to your app]

	3. By applying transformations on existsing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))

    RDD Operations
    --------------

	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


    RDD Lineage DAG
    ---------------

	=> Lineage DAG is a logical plan of an RDD which maintains the hierarchy of dependencies all the 
           way upto very first RDD

        => Lineage DAGs are created when you perform a transformation

	=> Lineage DAGs are maintained by the Driver         

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage: (4) rdd1 -> sc.textFile  (E:\\Spark\\wordcount.txt)

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		Lineage: (4) rddWords -> rddFile.flatMap -> sc.textFile

        rddPairs = rddWords.map(lambda x: (x, 1))
		Lineage: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile
    
	rddWordCount = rddPairs.reduceByKey(lambda x, y: x + y)
		Lineage: (4) rddWordCount -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

   
  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  -------------------
   => Transformations return a RDD.
   => Does not cause execution.


  1. map		P: U -> V
			Transforms each input object to corresponding output object	
			input RDD: N objects, output RDD: N objects		

		rdd2 = rddFile.map( lambda x: x.split(" ") )


  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be there in the output
			input RDD: N objects, output RDD: <= N objects	

		rdd2 = rdd1.filter(lambda x: x > 10)

  
  3. glom		P: None
			Returns a list object for every partition with all the elements of the partition
			input RDD: N objects, output RDD: equal to number of partitions

	rdd1			rdd2 = rdd1.glom()

	P0: 3,2,1,4,7  -> glom -> P0: [3,2,1,4,7]
	P1: 6,0,7,1,1  -> glom -> P1: [6,0,7,1,1]
	P2: 5,1,3,2,4  -> glom -> P2: [5,1,3,2,4]

	rdd1.count() = 15 (int)   rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			Flattens all the elements of the Iterables produced by the function
			input RDD: N objects, output RDD: >= N object

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Apply the transformation on the entire partition to produce the corresponding
			output partition.

	rdd1		rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )

	P0: 3,2,1,4,7  -> mapPartitions -> P0: 17
	P1: 6,0,7,1,1  -> mapPartitions -> P1: 15
	P2: 5,1,3,2,4  -> mapPartitions -> P2: 15

	rdd1.mapPartitions(lambda a: map(lambda x: x*10, a) ).glom().collect()


  6. mapPartitionsWithIndex  	P: Int, Iterable[U] -> Iterable[V]
			  Similar to mapPartitions, but you get partition index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x), p)).filter(lambda x: x[0] == 1).map(lambda x: x[1]).collect()

   
  7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD

		rddWords.distinct().collect()

   Types of RDDs
   -------------

	=> Generic RDDs: RDD[U]
              Pair RDDs: RDD[(K, V)]

  8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the valur part (only) of the Pairs by applying the function
			input RDD: N objects, output RDD: N object

		rdd4.mapValues(list).collect() 

			

  

		

 	


   
 










          
    







