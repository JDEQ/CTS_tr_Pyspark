
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - MySQL & Hive 
   Spark Streaming
	-> Structured Streaming	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ==============================================================

   Spark
   ------

	-> Spark is a unified in-memory distributed computing framework

		in-memory computation -> Spark's allows you to persist intermediate results of tasks in memory
		so that subsequent tasks ca directly process these saved partitions. 

        -> Spark is written in SCALA language
	
	-> Spark is a polyglot
		-> Scala, Java, Python, R

	-> Spark appilication can run on multiple clusters managed by several cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes
  
  
  Spark unified stack
  -------------------

	-> Spark provides a consistent set of APIs for analyzing different analytics work loads on the same
	   execution engine using well defined dat abstractions.

		Batch Analytics			=> Spark SQL
		Streaming Analytics		=> Spark Streaming, Structured Streaming
		Predictive Analytics		=> Spark MLLib
		Graph Parallel Computations	=> Spark GraphX


  Getting started with Spark
  --------------------------
	
   1. Working on your Lab

	-> Try opening the email link in Edge browser (if Chorme is not working for you)
	-> Follow instaructions given in the email.
	-> You will land up on Windows server (click 'Remind me later' link if asks for Updates)
	-> Check your user-id in the "HadoopBig...." word document
	-> Click on "Oracle VM Virtualbox" icon and double click on the VM to lauch the Ubuntu lab

	-> Inside your Ubuntu lab:
		-> Open PySpark shell
			-> Open a terminal and type "pyspark"

		-> Open "Spyder" ide.	
			-> A spark app is already setup as reference.

   2. Setting up Spark dev environment on a personal machine. 
	
	-> Make sure you have Anaconda distribution installed. 
	-> Try 'pip install' in Anaconda command shell
		pip install pyspark
	-> If that is not working, try the instra=uction given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Sign-up to Databricks Community Edition 
	
	Signup: https://www.databricks.com/try-databricks
		-> Make sure you select "Databrick Community Edition" link (in the second screen)

	Login: https://community.cloud.databricks.com/login.html


  Spark Architecture
  ------------------
   
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
  
	
  RDD (Resilient Distributed Dataset)
  -----------------------------------

     => RDD is the fundamental data abstraction of Spark core.

     => RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects. 

     => RDDs are lazily evaluation

     => RDDs are immutable


  Creating RDDs
  -------------
	Three ways:

	1. Creating an RDD from some external data source  (such as a file)

		rdd1 = sc.textFile(<FilePath>, numPartitions)
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating an RDD from progrmmatic data

		rdd1 = sc.parallelize(<collection>, numPartitions)
		rdd1 = sc.parallelize([2,1,3,2,4,3,5,6,5,6,7,8,6], 3)


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------
	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


  RDD Lineage DAG
  ----------------

    RDD Lineage is a logical plan maintained by Driver
    RDD Lineage DAG is created when an RDD is created.
    RDD Lineage DAg tracks the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		rddFile Lineage : rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		rddWords Lineage : rddWords -> rddFile.flatMap -> sc.textFile
   	
	rddPairs = rddWords.map(lambda x: (x, 1))
  		rddPairs Lineage : rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
   		rddWc Lineage : rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  RDD Persistence
  ---------------     
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist(StorageLevel.DISK_ONLY)  --> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	rdd6 DAG: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collect

	rdd7.collect()
	rdd7 DAG: (4) rdd7 -> rdd6.t7
		[t7] -> collect


	Storage Levels
        ---------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated		
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	---------
	1. cache    -> in-memory (Memory Serialized 1x Replicated)
	2. persist  -> can take storage-level as an argument.
	3. unpersist	


   Executors Memory Structure
   --------------------------
        Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  Types of Transformations
  ------------------------

     Types of transformtions

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformations
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

     
  RDD Transformations
  -------------------

    1. map		Parameter: U -> V
			Object to object transformation
			Applies the function on each input object to transform it to the output object
			input RDD: N object, output RDD: N objects

		rddFile.map(lambda s: s.split(" ")).collect()

    2. filter		P: U -> Boolean
			Only those those objects for which the function returns True, will be there in the
			output.
			input RDD: N object, output RDD: <= N objects

		rddFile.filter(lambda s: len(s.split(" ")) > 8).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition


		rdd1		      rdd2 = rdd1.glom()
		P0: 2,1,3,2,4 -> glom -> P0: [2,1,3,2,4]
		P1: 5,6,7,5,7 -> glom -> P1: [5,6,7,5,7]
		P2: 0,8,5,2,3 -> glom -> P2: [0,8,5,2,3]		

	       rdd1.count() = 15 (int)	rdd2.count() = 3 (list)

   		rdd1.glom().map(sum).collect()

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function. 
			input RDD: N object, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation.

		rdd1.mapPartitions(lambda p: [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but given partition-index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect() 
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct		P: None, Optional: numPartitions
			Returns the distinct objects of the RDD.
	
		rddFile.flatMap(lambda x: x).distinct().collect()
  
   Types of RDDs
   --------------
	1. Generic RDD:   RDD[U]		
	2. Pair RDD:	  RDD[(K, V)]		

   
   8. mapValues		P: U -> V
			Applied only to Pair RDDs
			Transforma only the value part of (K,V) pairs by applying the function

		rddWc.mapValues(lambda x: x*10).collect()


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the function output.
		
		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rdd1.sortBy(lambda x: x > 3, True, 2).glom().collect()


   10. groupBy		P: U -> V, Optional: numPartitions
			Returns a pair RDD where
				key: Each unique value of the function output
				value: A ResultIterable containing objects of the RDD grouped for the key.

		rddWords.groupBy(lambda x: x).mapValues(len).collect()

   11. randomSplit	P: list of weights (ex: [0.6, 0.4])
			Returns a list of RDDs split randomlt in the specified weights.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 64657)

   12. repartition	P: numPartitions
			Can be used to increase or decrease the number of output partititons
			Performs global shuffle

   13. coalesce		P: numPartitions
			Can be used to only decrease the number of output partititons
			Performs partition-merging
	
	Recommendations
  	---------------
	-> The size of the RDD partitions should be between 100 MB and 1 GB. 
	   Ideally, the size should be 128 MB (if you are using Hadoop)

        -> The number of partitions should be a multiple of number of Cores

	-> If the number of partitions is close to but less than 2000, bump it up to 2000.

	-> The number of CPU cores per executor should be 5


   14. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on PairRDDs
			The partitioning happens based on the key of the (K,V) pairs


   	transactions = [
    	 	{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 	{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 	{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 	{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 	{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 	{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 	{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 	{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 	{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 	{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


  15. union, intersection, subtract, cartesian
   
	Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   command				partitions & type
           ------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide


  ..ByKey transformations
  -----------------------
    -> Are wide transformations
    -> Are applied only to PairRDDs

  16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
		   	   Sorts the objects of the RDD by key.

		rdd4.sortByKey().glom().collect()
		rdd4.sortByKey(False).glom().collect()
		rdd4.sortByKey(False,2).glom().collect()


  17. groupByKey	P: None, Optional: numPartitions
			Groups the objects of the RDD by key
			Returns a Pair RDD
				key: Each unique of of input RDD
				value: ResultIterable with grouped values.

			WARNING: Avoid groupByKey if possible

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
				.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
				.groupByKey() \
				.mapValues(sum) \
				.coalesce(1)

   18. reduceByKey	P: (U, U) -> U,   Optional: numPartitions
			Reduces all the values of each unique key within each partition in the first stage
			and across partition in the second stage.

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y, 1)

   19. aggregateByKey	P: zero-value, seq-function, comb-function;  Optional: numPartitions

			Is used to aggregate all the values of each unique key to a type
			different that the values of (k,v) pairs.		
			-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])
   

   20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


   RDD Actions
   ------------


  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one value (of the same type) by iterativly applying the
			function on each partition in the first stage and across partitions in the next stage.

			rdd1
			P0: 2,1,4,5,8 -> reduce -> 20 -> reduce -> 57
			P1: 5,7,3,2,1 -> reduce -> 18
			P2: 9,1,0,2,7 -> reduce -> 19

			n = rdd1.reduce(lambda x,y: x + y)

  5. aggregate
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))
			
   6. take : returns first n objects
		rdd1.take(25)

   7. takeOrdered : returns a list with first n objects in sorting order.

		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: -x)

   8. takeSample

   9. countByValue

   10. countByKey

   11. foreach
		rdd2.foreach(lambda x: print("k: {}, v: {}".format(x[0], x[1])))
   
   12. saveAsSequenceFile


  Use-Case
  --------
    dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
	
    From cars.tsv, find out the average weights of all models of each make from American Origin models.
    -> Sort the data in the DESC order of average-weight
    -> Save the data as a single text file. 

	=> Try to solve this yourself. 


  Spark Closures
  --------------
	A closure in Spark, constitute all the variabls ans methods that must be visible inside an executor for the 
	tasks to perform their computations on the RDD.

	=> Closure is serialized, and driver sends a separate copy is sent to every executor 


	c = 0

	def isPrime(n):
		return True if n is prime
		return False if n is not prime

	def f1(n):
		global c
		if (isPrime(n)) c += 1
		return n*10

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)       # c = 0

	
	Limitation: Local variable can not be used to impliment global counter.
	Solution: Use Accumulator variables



  Spark Shared Variables
  ---------------------- 

  1. Accumulator
  ---------------
	-> accumulator is a shared variable
	-> is not part of the closure (hence not a local copy)
	-> all functions can add to this accumulator variable.

	c = sc.accumulator(0)

	def isPrime(n):
		return True if n is prime
		return False if n is not prime

	def f1(n):
		global c
		if (isPrime(n)) c.add(1)
		return n*10

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)       # c = 80


  2. Broadcast Variable
  ---------------------

	bc = sc.broadcast( {1: a, 2: b, 3: c, 4: d, 5: e, 6: f, 7: g, ....} )    

	def f1(n):
		global bc
		return bc.value[n]	

	rdd1 = sc.parallelize([1,2,3,4,5,...], 4)

	rdd2 = rdd1.map( f1 )
	
	rdd2.collect()   => // a, b, c, d, e ....


 ===================================================
     Spark Submit command
 ===================================================

   Spark-submit is a single command that is used to submit any spark application (Python, Scala, Java or R) to any cluster manager (local, standalone, YARN, Mesos, Kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
		--executory-memory 5G \
		--driver-cores 2 \
		--executor-cores 5 \
		--num-executors 10 \
		E:\\Spark\\wordcount.py [app args]


  =============================================
      Spark SQL
  =============================================

   Spark's structured data processing API

		File Formats : Parquet (default), ORC, JSON, CSV (delimited text file), Text
		JDBC Format  : RDBMS, NoSQL
		Hive Format  : Hive Warehouse

  SparkSession
  ------------
	-> Starting point of execution for Spark SQL application
	-> SparkSession represents a user session inside an application 

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate() 


  DataFrame (DF)
  --------------
    -> Main data abstraction in Spark SQL
    -> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
    -> DataFrames have two components:
		Data: A collection of "Row" objects
		Schema: StructType object

			StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			)
	
  
  Basic Steps in a Spark SQL applications
  ---------------------------------------

    1. Create a Spark Session

	    spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate() 

	    spark.config.set(<name-of-the-config-variable>, <value>)


    2. Read the data from a data source (external or internal) into a DataFrame

		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

    3. Transform the dataframe using DF transformation methods or using SQL

		using DF transformation methods
		-------------------------------
		df2 = df1.select("userid", "name", "gender", "age", "phone") \
         		.where("age is not null") \
         		.orderBy("gender", "age") \
         		.groupBy("age").count() \
         		.limit(4)

		using SQL
		---------
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		df2 = spark.sql("""select age, count(*) as count
                		from users 
                		where age is not null
                		group by age
                		order by age
                		limit 4""")		

    4. Write the dataframe into some external file or database.  

	 	outputPath = "E:\\PySpark\\output\\json"

		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)


  SaveModes
  ---------
	default: errorIfExists

	ignore
	append
	overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  LocalTempViews & GlobalTempViews
  --------------------------------

     LocalTempView 
	-> created at Session scope
	-> created using df1.createOrReplaceTempView("users")
	-> accessble only from its own SparkSession.

     GlobalTempView
	-> created at Application scope
	-> Accessible from all SparkSessions
	-> created using df1.createOrReplaceGlobalTempView("gusers")
	-> Attached to a temp database called "global_temp"

  DataFrame Transformations
  -------------------------

  1. select

	df2 = df1.select("DEST_COUNTRY_NAME",
                 	"ORIGIN_COUNTRY_NAME",
                 	"count")

	df2 = df1.select(
			col("DEST_COUNTRY_NAME").alias("destination"),
                 	col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 	expr("count").cast("int"),
                 	expr("count + 10 as newCount"),
                 	expr("count > 200 as highFrequency"),
                 	expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
		   )


  2. where / filter

	df3 = df2.filter("domestic = false and highFrequency = true")
	df3 = df2.where("domestic = false and highFrequency = true")
	df3 = df2.where( col("count") > 1000 )


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")
	df3 = df2.sort(desc("count"), asc("origin"))


  4. groupBy	=> Returns a pyspark.sql.group.GroupedData object
		   Use an aggregation methos with groupBy to return a DataFrame.


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	count("count").alias("count"),
              		sum("count").alias("sum"),
              		round(avg("count"), 2).alias("avg"),
              		max("count").alias("max")
		    )

	df3.show(100)


  5. limit


  6. selectExpr

	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                 "ORIGIN_COUNTRY_NAME as origin",
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")


  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", expr("count > 200")) \
         	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
         	.withColumn("count", col("count").cast("int")) \
         	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	df3.show(10)
	df3.printSchema()

	-------------------------------
	usersDf2 = usersDf.withColumn("ageGroup", when( expr("age < 13"), "child" )
                                         .when( expr("age < 20"), "teenager" )
                                         .when( expr("age < 60"), "adult" )
                                         .otherwise("senior"))


  8. udf  (user-defined-function)


		def get_age_group( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"
			
		get_age_group_udf = udf(get_age_group, StringType())

		usersDf2 = usersDf.withColumn("ageGroup", get_age_group_udf(col("age")))

		---------------------------------------------------------------

		@udf (returnType = StringType())
		def get_age_group( age ):
			if (age <= 12):
				return "child"
			elif (age >= 13 and age <= 19):
				return "teenager"
			elif (age >= 20 and age < 60):
				return "adult"
			else:
				return "senior"    

		usersDf2 = usersDf.withColumn("ageGroup", get_age_group(col("age")))

		----------------------------------------------------------------

		spark.udf.register("get_age_group_udf", get_age_group, returnType=StringType())

		qry = "select id, name, age, get_age_group_udf(age) as age_group from users"
		spark.sql(qry).show()


  9. drop	=> exclude somecolumns in the output df
	
		df3 = df2.drop("newCount", "highFrequency", "one")
		df3.show(5)

  10. dropna		=> Drops rows with null values

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df3 = usersDf.dropna()
		df3 = usersDf.dropna(subset=["phone", "age"])
		df3.show()


  11. dropDuplicates	=> Drops rows with duplicate values

		listUsers = [   (1, "Raju", 5),
			        (1, "Raju", 5),
				(3, "Raju", 5),
				(4, "Raghu", 35),
				(4, "Raghu", 35),
				(6, "Raghu", 35),
				(7, "Ravi", 70)]

		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
		userDf.show()

		df3 = userDf.dropDuplicates()
		df3 = userDf.dropDuplicates(["name", "age"])

		df3.show()

  12. distinct -> returns distinct rows of input df

	df1.distinct().count()

	# how many unique DEST_COUNTRY_NAME are there in df1

	df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()
	df1.select("DEST_COUNTRY_NAME").distinct().count()


  13. randomSplit

	dfList = df1.randomSplit([0.6, 0.4], 465)
	print( dfList[0].count(), dfList[1].count() )

  14. sample

	dfSample = df1.sample(True, 1.6, 3534)
	dfSample.count()
	dfSample.show()

	dfSample = df1.sample(False, 0.6, 3534)
	dfSample.count()
	dfSample.show()


  15. union, intersect, subtract


		df2 = df1.where("count > 1000")
		df2.show()
		df2.count() # 14 rows
		df2.rdd.getNumPartitions()

		df3 = df1.where("ORIGIN_COUNTRY_NAME = 'India'")
		df3.show() # 1 row
		df3.rdd.getNumPartitions()

		df4 = df2.union(df3)
		df4.rdd.getNumPartitions()  # 2 partitions
		df4.show()

		df5 = df4.intersect(df3)
		df5.show()
		df5.rdd.getNumPartitions() 

		df6 = df4.subtract(df3)
		df6.show()
		df6.rdd.getNumPartitions() 


   spark.sql.shuffle.partitions
   ----------------------------
	-> Above config parameter decides the default number of suffle partitions for a dataframe created
           as a result of shuffle operation. 

        -> default value is 200.

	spark.conf.get("spark.sql.shuffle.partitions")
	spark.conf.set("spark.sql.shuffle.partitions", "10")


  16. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(2, col("ORIGIN_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df4 = df2.repartition(col("ORIGIN_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

  17. coalesce

		df3 = df2.coalesce(1)
		df3.rdd.getNumPartitions()


  18. join => discussed separatly


  Working with file formats
  -------------------------
      
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)
		df2.write.mode("overwrite").json(outputPath)
		df2.write.json(outputPath, mode="overwrite")

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath)
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")

	

   Creating a RDD from DF
   -----------------------

	rdd1 = df1.rdd


   Creating a DF from Programmatic data
   -------------------------------------
	listUsers = [(1, "Raju", 5),
				 (2, "Ramesh", 15),
				 (3, "Rajesh", 18),
				 (4, "Raghu", 35),
				 (5, "Ramya", 25),
				 (6, "Radhika", 35),
				 (7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])


   Creating a DF from RDD
   ----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = rdd1.toDF(["id", "name", "age"])


   Creating a DF with custom/programmatic schema
   ----------------------------------------------

	mySchema2 = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

	df1 = spark.read.json(inputPath, schema=mySchema2)
	df1.printSchema()

	-----------------------------

	mySchema2 = StructType([
			StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
			StructField("DEST_COUNTRY_NAME", StringType(), True),
			StructField("count", IntegerType(), True) ])

	df1 = spark.read.json(inputPath, schema=mySchema2)
	df1.printSchema()
	df1.show(5)

	------------------------------

	mySchema = "id INT, name STRING, age INT"
	df2 = spark.createDataFrame(listUsers, schema=mySchema)
	df2.show()
	df2.printSchema()

	---------------------------------

	mySchema = "id INT, name STRING, age INT"
	df3 = rdd1.toDF(schema=mySchema)
	df3.show()
	df3.printSchema()


   Joins
   -----

     Supported Joins : inner, left, right, full, left_semi, left_anti, cross


     left_semi join
     ---------------
	-> Similar to inner join, but data comes ony from the left side table
	-> Equivalent to the following sub-query:
		
		select * from emp where deptid in (select id from dept)


     left_anti join
     ---------------
	-> Equivalent to the following sub-query:
		
		select * from emp where deptid not in (select id from dept)

      Example
      --------

		employee = spark.createDataFrame([
			(1, "Raju", 25, 101),
			(2, "Ramesh", 26, 101),
			(3, "Amrita", 30, 102),
			(4, "Madhu", 32, 102),
			(5, "Aditya", 28, 102),
			(6, "Pranav", 28, 100)])\
		  .toDF("id", "name", "age", "deptid")
		  
		  
		department = spark.createDataFrame([
			(101, "IT", 1),
			(102, "ITES", 1),
			(103, "Opearation", 1),
			(104, "HRD", 2)])\
		  .toDF("id", "deptname", "locationid")

		  
		employee.show()
		department.show()

		joinCol = employee.deptid == department.id
		joinedDf = employee.join(department, joinCol, "left_semi")
		joinedDf.show()


   Window Functions
   ----------------

    emp
    ----
	empid	salary	 dept	dense_rank
	2	45000	 IT	1
	10	45000	 IT	1	
	12	65000	 IT	2	
	5	65000	 IT	2
	11	80000	 IT	3
	3	90000	 IT	4	
	9	60000	 Sales 	1
	8	60000	 Sales	1
	4	70000	 Sales	2
	1	50000	 HR	1
	6	50000	 HR     1   
	7	80000	 HR	2


        window = Window.partitionBy(dept) \
		   .orderBy("salary") \
                   .rowsBetween(Window.unboundedPreceding, Window.currentRow)

	df2 = df1.withColumn("dense_rank", dense_rank().over(window))


  Use-Case
  --------
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv, find the top 10 movies with highest average user rating.
    -> Consider only those movies with atleast 30 ratings
    -> Data to be fetched: movieId, title, totalRatings, avgRating
    -> Arrange the data in the DESC order of avgRating
    -> Save the output as a single pipe-separated CSV file with header
    -> Use DF transformation methods only. 

    => Try it yourself


  Working with Hive - Hive Format
  -------------------------------

import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Hive Datasource") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listDatabases()
spark.catalog.currentDatabase()
spark.catalog.listTables()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()

joinCol = summaryDf.movieId == moviesDF.movieId
   
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
#topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies = spark.table("topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()



   Working with JDBC - Integrating with MySQL
   ------------------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()

 ==============================================
     Spark Streaming - Structured Streaming
 ==============================================

     
     Spark's streaming data (real-time) analytics API

     Two libraries
	1. Spark Streaming (DStreams API)
	2. Structured Streaming (this is preferred)

  
    Spark Structured Streaming
    -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	-> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 

	=> Sources: File (text, csv, json, parquet, orc), Socket, Rate, Kafka
	=> Sinks: File (text, csv, json, parquet, orc), Console, Kafka, ForEachBatch, ForEach, Memory 
   



















 
		

















































    




 









			
	
	






























   






 































  
