
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - MySQL & Hive 
   Spark Streaming
	-> Structured Streaming	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ==============================================================

   Spark
   ------

	-> Spark is a unified in-memory distributed computing framework

		in-memory computation -> Spark's allows you to persist intermediate results of tasks in memory
		so that subsequent tasks ca directly process these saved partitions. 

        -> Spark is written in SCALA language
	
	-> Spark is a polyglot
		-> Scala, Java, Python, R

	-> Spark appilication can run on multiple clusters managed by several cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes
  
  
  Spark unified stack
  -------------------

	-> Spark provides a consistent set of APIs for analyzing different analytics work loads on the same
	   execution engine using well defined dat abstractions.

		Batch Analytics			=> Spark SQL
		Streaming Analytics		=> Spark Streaming, Structured Streaming
		Predictive Analytics		=> Spark MLLib
		Graph Parallel Computations	=> Spark GraphX


  Getting started with Spark
  --------------------------
	
   1. Working on your Lab

   2. Setting up Spark dev environment on a personal machine. 
	
	-> Make sure you have Anaconda distribution installed. 
	-> Try 'pip install' in Anaconda command shell
		pip install pyspark
	-> If that is not working, try the instra=uction given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Sign-up to Databricks Community Edition 
	
	Signup: https://www.databricks.com/try-databricks
		-> Make sure you select "Databrick Community Edition" link (in the second screen)

	Login: https://community.cloud.databricks.com/login.html


  Spark Architecture
  ------------------
   
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
  
	
  RDD (Resilient Distributed Dataset)
  -----------------------------------

     => RDD is the fundamental data abstraction of Spark core.

     => RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects. 

     => RDDs are lazily evaluation

     => RDDs are immutable


  Creating RDDs
  -------------
	Three ways:

	1. Creating an RDD from some external data source  (such as a file)

		rdd1 = sc.textFile(<FilePath>, numPartitions)
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating an RDD from progrmmatic data

		rdd1 = sc.parallelize(<collection>, numPartitions)
		rdd1 = sc.parallelize([2,1,3,2,4,3,5,6,5,6,7,8,6], 3)


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------
	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


  RDD Lineage DAG
  ----------------

    RDD Lineage is a logical plan maintained by Driver
    RDD Lineage DAG is created when an RDD is created.
    RDD Lineage DAg tracks the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		rddFile Lineage : rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		rddWords Lineage : rddWords -> rddFile.flatMap -> sc.textFile
   	
	rddPairs = rddWords.map(lambda x: (x, 1))
  		rddPairs Lineage : rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
   		rddWc Lineage : rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  RDD Persistence
  ---------------
     
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist(StorageLevel.DISK_ONLY)  --> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	rdd6 DAG: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collect

	rdd7.collect()
	rdd7 DAG: (4) rdd7 -> rdd6.t7
		[t7] -> collect


	Storage Levels
        ---------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated		
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	---------
	1. cache    -> in-memory (Memory Serialized 1x Replicated)
	2. persist  -> can take storage-level as an argument.
	3. unpersist	


   Executors Memory Structure
   --------------------------
        Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)




  Types of Transformations
  ------------------------

     Types of transformtions

	1. Narrow Transformations

	2. Wide Transformations

     
  RDD Transformations
  -------------------

    1. map		Parameter: U -> V
			Object to object transformation
			Applies the function on each input object to transform it to the output object
			input RDD: N object, output RDD: N objects

		rddFile.map(lambda s: s.split(" ")).collect()

    2. filter		P: U -> Boolean
			Only those those objects for which the function returns True, will be there in the
			output.
			input RDD: N object, output RDD: <= N objects

		rddFile.filter(lambda s: len(s.split(" ")) > 8).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition


		rdd1		      rdd2 = rdd1.glom()
		P0: 2,1,3,2,4 -> glom -> P0: [2,1,3,2,4]
		P1: 5,6,7,5,7 -> glom -> P1: [5,6,7,5,7]
		P2: 0,8,5,2,3 -> glom -> P2: [0,8,5,2,3]		

	       rdd1.count() = 15 (int)	rdd2.count() = 3 (list)

   		rdd1.glom().map(sum).collect()

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function. 
			input RDD: N object, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation.

		rdd1.mapPartitions(lambda p: [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but given partition-index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect() 
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct		P: None, Optional: numPartitions
			Returns the distinct objects of the RDD.
	
		rddFile.flatMap(lambda x: x).distinct().collect()

  
   Types of RDDs
   --------------
	1. Generic RDD:   RDD[U]		
	2. Pair RDD:	  RDD[(K, V)]		

   
   8. mapValues		P: U -> V
			Applied only to Pair RDDs
			Transforma only the value part of (K,V) pairs by applying the function

		rddWc.mapValues(lambda x: x*10).collect()


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the function output.

		
		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rdd1.sortBy(lambda x: x > 3, True, 2).glom().collect()



   10. groupBy



  
   






 































  
