
  Agenda (PySpark)
  --------------------
  1. Python Crash Course - Basics of Python
  2. Spark - Basics & Architecture
  3. Spark APIs
  4. Spark Core - RDD API - Introduction & Basics
  5. Spark SQL in details
	Spark SQL Transformations
	Spark SQL Integrations with RDBMS & Hive

 ------------------------------------------------------
 
  Course Materials
  ----------------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> https://github.com/ykanakaraju/pyspark
	
   
  Installing Python Anaconda Distribution
  ---------------------------------------
	URL: https://www.anaconda.com/download
	https://www.anaconda.com/download#downloads

	Click on Download link
	Run the msi installer.

 

  Spark - Basics & Architecture
  -----------------------------
   
   Spark is a distributed computing framework for big data analytics.

     	-> Spark is written using Scala programming language

	-> Spark is an in-memory distributed computing framework

		Spark can persist intermediate partitions in-memory and subsequent tasks
		can directly process these stored partitions to advance the computations. 

        -> Spark is a unified framework.
		
		Spark provides a consistant set of APIs for processing different analytical
		workloads based on the same execution engine.

		Spark Core API (RDD)	-> Low level API for unstrauctured data processing
		Spark SQL (DataFrames)  -> Used for structured batch analytics
		Spark Streaming		-> Used for real-time data analytics
		Spark MLLib		-> Predictive analytics using Machine Learning
		Spark GraphX		-> Graph parallel computations.

	-> Spark is a polyglot		
		-> Supports Scala, Java, Python and R programming language.

	-> Spark can run on multiple clusters
		-> Local, Standalone Scheduler, YARN, Mesos, Kubernetes
           

   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client  : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


  RDD (Resilient Distributed Dataset)
  -----------------------------------

	-> RDD is the fundamental data abstraction of Spark.
	-> RDDs are Collection of distributed in-memory partitions.
	-> RDDs have to components
		RDD Loagical plan -> RDD Lineage DAG
		RDD runtime data  -> Collection of in-memory partitions

	-> RDDs are lazily evaluated.

	-> RDDs are immutable


  Creating RDDs
  -------------

    Three ways:

	1. Create an RDD from external dataset

		rddFile = sc.textFile(filePath, 4)

	2. Create an RDD from programmatics data

	        rdd1 = sc.parallelize( [3,2,1,4,2,4,6,5,3,5,4,5,7,8,5,7,8,9,6,7,1,2,1,2,3,2,5,0,2,3], 3 )

	3. By transforming existing RDDs.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------

    Two operations : 

	1. Transformations
		-> Returns an RDD
		-> Does not cause execution
		-> Causes creation of lineage DAGs by the driver

	2. Actions
		-> Triggers execution and launches Job on the cluster
		-> Returns some output
	

  RDD Lineage DAG
  ----------------
	
     RDD Lineage DAG is a logical plan on how to create the RDD
     RDD Lineage DAG maintains a hierarchy of depencies to create the RDD partitions (when asked for) all the way from the very first RDD.
	

	rddFile = sc.textFile(filePath, 4)
	Lineage DAG of rddFile : (4) rddFile -> sc.textFile on filePath

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage DAG of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile on filePath

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	


  Types of Transformations
  ------------------------
	-> narrow transformations


	-> wide transformations


  RDD Transformations
  -------------------

   1. map		P: U -> V
			object to object transformation

		rddFile.map(len).collect()


   2. filter		P: U -> Boolean
			filter the objects into output based on the function

		rddFile.filter(lambda x: len(x) > 51).collect()

  3. glom		P: None
			Returns one list object per partition.
		
		rdd1		rdd2 = rdd1.glom()

		P0: 1,2,3,6,4 -> glom -> P0: [1,2,3,6,4]
		P1: 5,6,4,7,2 -> glom -> P1: [5,6,4,7,2]
		P2: 9,0,5,8,1 -> glom -> P2: [9,0,5,8,1]

		 rdd1.glom().map(lambda p: len(p)).collect()


  4. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1	    rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )

		P0: 1,2,3,6,4 -> mapPartitions -> P0: 16
		P1: 5,6,4,7,2 -> mapPartitions -> P1: 24
		P2: 9,0,5,8,1 -> mapPartitions -> P2: 23

		rdd1.mapPartitions(lambda p: [sum(p)]).collect()


  5. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions but gives the partition-index as an additional
			function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()


   6. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.

		 rddWords = rddFile.flatMap(lambda x: x.split(" "))

   Types of RDDs:
   --------------
	1. Generic RDDs: RDD[U]
	2. Pair RDD: RDD[(U, V)]

   7. mapValues		P: U -> V
			Applied only to Pair RDD
			Transforms only the value part of (K,V) pairs by applying the function

   8. distinct()	P: None, Optional: numPartitions
			Returns only distinct objects of the RDD.

   9. sortBy 		P: U -> V, Optional: ascending(True/False), numPartitions
 			Sorts the RDD based on function output

		rdd1.sortBy(lambda x: x%3, True, 5).glom().collect()

  NOTE:
  -----
	All ...ByKey transformations are wide transformations and applied only to Pair RDDs

  10: sortByKey	       P: None, Optional: ascending(True/False), numPartitions
		       Sorts the RDD by key	

			rdd2.sortByKey(False, 2).glom().collect()

  11. groupBy

  12. groupByKey

  13. reduceByKey

  14. repartition

  15. coalesce

  16. partitionBy


  RDD Persistence
  ---------------
	-> To be discussed ... 

			



			












	

	












  








































  



   







