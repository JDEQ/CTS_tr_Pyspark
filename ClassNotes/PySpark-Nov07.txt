
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> RDD shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	-> Dstreams API (introduction)


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

  Spark
  -----
     -> Used for big-data analytics.

     -> Spark is a in-memory distributed computing framework.

	 -> in-memory means intermediate results of transformations can be saved in memory (RAM)
	    and subsequent transformations can process those savs results. 

     -> Spark is a unified framework. 
	   
           -> Provides a set of consistent APIs for processing different analytical workloads using
	      the same execution engine and well-defines data abstractions.
	
		Batch Analytics	of unstructured data 	=> Spark Core API (RDD)
		Batch Analytics	of structured data 	=> Spark SQL (DataFrames)
          	Stream Analytics (real-time)		=> Structured Streaming, DStreams
		Predictive Analytics (ML)		=> Spak MLlib
		Graph Parallel Computations		=> Spark GraphX

      -> Spark is written in Scala language.

      -> Spark is a poliglot
	   => Supports scala, java, python & R

      -> Spark is very fast
	   -> Spark is 100 times faster than MapReduce if you use 100% in-memory
	   -> Spark is 6 to 7 times faster than MapReduce if you use disk based computation
  
  
  Getting started with Spark
  --------------------------

    1. Working in your vLab	
	  1.1 PySpark Shell
		=> pyspark   (type on a terminal)

	  1.2 Spyder IDE   
		=> Search for spyder and launch
		=> If spyder is not found, you can install it
			pip install spyder
			sudo apt install spyder

	  1.3 Jupyter notebooks
		=> Open a terminal
		=> Type :
			jupyter notebook
			jupyter notebook --allow-root
	
    2. Setup PySpark environment on your local machine.
	2.1 Make sure you have anaconda distribution installed.
		https://docs.anaconda.com/anaconda/install/

	2.2 Follow the step from this document:
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition
	     https://docs.databricks.com/getting-started/community-edition.html
	     https://www.databricks.com/try-databricks

	
   Spark Architecture (Building Blocks)
   ------------------------------------

    1. Cluster Manager

	-> Supports local, spark standalone, YARN, Mesos, Kubernetes

    2. Driver

    3. SparkContext

    4. Executors


  Spark Core API
  --------------
    => Is the low-level API
    => Takes care of all low-level operations
	-> memory-managenement
	-> monitoring
	-> fault-recovery.
    => RDD is the main data abstraction.
	
   
    RDD (Resilient distributed dataset)
    -----------------------------------

	=> Is the fundamental data abstraction od Spark

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data => A colletion of distributed in-memory partitions.
		       -> Each partition is a collection of objects.


   Creating RDD
   ------------
    Three ways:

	1. Create an RDD from some external data file.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2.  ????? 

	3. By applying transformations on existsing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   RDD Operations
   --------------
     2 operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.

   RDD Lineage DAG
   ---------------
	=> Is the meta data of an RDD maiantained by the driver	

        rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
	     	Lineage DAG: (4) rdd1 -> sc.textFile on "E:\\Spark\\wordcount.txt"

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
		Lineage DAG: (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))	
		Lineage DAG: (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
		Lineage DAG: (4) rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile

		Tasks: (sc.textFile, flatMap, map, reduceByKey)

	rdd4.collect()

   

	

   



     




















