
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark

	-> Spark is an open source unified in-memory distributed computing framework for big data analytics.
	-> Spark is written in SCALA programming language

	in-memory computing -> the intermediate results can be persisted in RAN and subsequent tasks
			       can directly run on those persisted data. 

				If there is not enough RAM for persisting your partitions, then Spark 
				resorts to disk-based computation.

   Spark Unified Framework
	
	-> Spark comes with a consistent set of libraries for performing analytics on different types of
	   analytical workloads running on the same execution engine.
		
		-> Batch Processing of unstructured data	=> Spark Core (low level)
		-> Batch Processing of structured data		=> Spark SQL
		-> Stream processing (real time)		=> Spark Streaming, Structured Streaming
		-> Predictive analytics (machine learning)	=> Spark MLLib
		-> Graph parallel computations			=> Spark GraphX

   Spark is a polyglot	
	-> Spark supports Scala, Java, Python, R 

   Spark applications can run on multiple cluster managers
	-> local, standalone scheduler, YARN, Mesos, Kubernetes


   Getting started with Spark
   --------------------------	
    1. Using your Big Data Lab (that is assigned to you)

	-> You will land up on a Windows server. 
	-> On the desktop of the Window server you find "Oracle VM Virtualbox"
	-> On the desktop of the Window server you find a "Word" document with userids and passwords.

	-> Connect to Ubuntu VM using Oracle VM Virtualbox

		-> Start the PySpark shell by opening a terminal and enter the command "pyspark"
		-> Start the Spyder IDE by clicking on the icon.

   2. Installing your own dev environment.

	-> Make sure you have Anaconda Navigator Installed. 

	-> Try installing pyspark using pip install
		-> Open an anaconda terminal and use the following command

		pip install pyspark
		
	-> Follow the instruction provided in the document to sepup PySpark to work with Spyder and 
	   Jupyter Notebook, if pip install option is not working for you.

  
    3. Databricks Community Edition  (Free Signup)

	     https://docs.databricks.com/getting-started/community-edition.html
	     
	     Signup link: https://www.databricks.com/try-databricks
		=> Make sure you click on "Community Edition" option 
	     Login: https://community.cloud.databricks.com/login.html


   Spark Architecture (Building Blocks)
   ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


    RDD (Resiliant Distributed Dataset)
    -----------------------------------

	=> Fundamental data abstraction for Spark Core API

	=> Represents a collection of distributed in-memory partitions

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data 	=> A collection of distributed in-memory partitions.
		       	   -> Each partition is a collection of objects.

	=> RDDs are lazily evaluated.

	=> RDDs are immutable


   Creating RDDs
   -------------

     1. create an RDD from external data files

		rdd1 = sc.textFile( <filePath>, n )
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

    2. create an rdd from programmatic data

	        rdd1 = sc.parallelize( [2,1,3,2,4,3,5,6,2,7,5,8,9,0], 3)

    3. 	apply transformations on existing rdds

		rddWords = rddFile.flatMap(lambda x: x.split(" "))
				


   RDD Operations
   --------------
	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


  RDD Linage DAG
  --------------

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	  -> Lineage: (4) rddFile -> sc.textFile 

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	  -> Lineage: (4) rddWords -> rddFile.flatMap -> sc.textFile 

	rddPairs = rddWords.map(lambda x: (x,1))
	  -> Lineage: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile 

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	   -> Lineage: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile 


  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Persistence
  ---------------

	rdd1 = sc.textFile( <filePath>, 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist()          -> is an instruction to Spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()	
	Lineage DAG of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks -> (sc.textFile, t3, t5, t6) 

	rdd7.collect()
	Lineage DAG of rdd6: (4) rdd7 -> rdd6.t7
		tasks -> (t7) 

	rdd6.unpersist()


	Storage Levels
        ---------------	
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_MEMORY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()    				-> in-memory persistence
	rdd1.persist()	 				-> in-memory persistence
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


  RDD Transformations
  -------------------
   => Transformations return an RDD.
   => Does not cause execution.


   1. map		P: U => V
			object to object transforrmation
			input RDD: N objects, output RDD: N objects

	   	rddLists = rddFile.map(lambda x: x.split(" "))


  2. filter		P: U -> Boolean
			Filters the objects to the output based on the function
			input RDD: N objects, output RDD: < N objects

		rddFile.filter(lambda s: len(s.split(" ")) > 8).collect()

  3. glom		P: None
			Returns one list object per partition with all the elements of that partition.
			input RDD: N objects, output RDD: number of partitions

		rdd1		         rdd2 = rdd1.glom()
		P0: 2,1,2,5,6   -> glom  -> P0: [2,1,2,5,6]
		P1: 4,2,1,3,2   -> glom  -> P1: [4,2,1,3,2]
		P2: 8,6,7,9,0   -> glom  -> P2: [8,6,7,9,0] 
		
		rdd1.count() = 15 (int)	    rdd2.count() = 3 (list)

		rdd1.glom().map(len).collect()

  4. flatMap		P: U -> Iterable[V]
			flapMap flattens the elements of the iterables produced by the function
			input RDD: N objects, output RDD: > N objects

		rdd1.flatMap(lambda x: range(0,x)).collect()

  5. mapPartitions		P: Iterable[U] -> Iterable[V]

		rdd1	   rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )
		P0: 2,1,2,5,6   -> mapPartitions  -> P0:  16
		P1: 4,2,1,3,2   -> mapPartitions  -> P1:  12
		P2: 8,6,7,9,0   -> mapPartitions  -> P2:  30

		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			      Similar to mapPartitions, but will get the partition-index as an additional parameter.
		
		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()

  7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects.
				
		rdd2 = rdd1.distinct(4)
		rdd2 = rdd1.distinct()

  

   Types of RDDs
   --------------
	1. Generic RDDs:   RDD[U]
	2. Pair RDD:       RDD[(K, V)]


  8. mapValues			P: U -> V
				Applied only on Pair RDD
				Transforms only the value part by applying the transformation.

		rdd3.mapValues(list).collect()

  9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Returns an RDD sorted by the output of the function.

		rdd3.sortBy(lambda x: sum(x[1]))
		rdd3.sortBy(lambda x: sum(x[1]), False)
		rdd3.sortBy(lambda x: sum(x[1]), False, 4)

  10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD where
					key: is each unique value of the function output
					value: ResultIterable containing all objects that produced the key
			
		outputRdd = sc.textFile("E:/Spark/wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x, 1) \
              			.mapValues(len)


  11. randomSplit		P: List of weights
				Returns a list of RDDs split in the specified weights.

			rddList = rdd1.randomSplit([0.6, 0.4], 3435)
			rddList[0].collect()
			rddList[1].collect()

  12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitons
				Causes global shuffle

  13. coalesce			P: numPartitions
				Is used only to decrease the number of partitons
				Causes partition-merging (hence preferred)
		

    Some recommendations for better performance
    -------------------------------------------
      	1. Size of each partition should be 100 MB to 1000 MB 
	   Ideally the partition size should be 128 MB (if you are running on HDFS)

	2. The nmber of partitions should be a multiple of number of CPU cores

	3. The number of CPU cores per executor should be 5


  14. partitionBy		P: numPartitions, Optional: partition-function (default: hash)
				Applied only on Pair-RDDs
				Partitioning logic is applied based on the key




















 Spark execution flow
 --------------------
	
     application (identified by SparkContext object)
	|
	=> Job (an app can have mnay jobs, each action command launches a job)
		|
		=> Stages (a job can have 1 or more stages, Each wide transformation causes a new stage to be launched)
			|
			=> Tasks (each stage has one more tasks, # tasks = # partitions)
				|
				=> Transformations (each task has some transformations that can run in parallel)




 Actions
 -------

  1. collect

  2. count

  3. saveAsTextFile






	

		
			      




























