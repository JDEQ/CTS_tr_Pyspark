
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark


  Prerequisites
  -------------
	-> Python
        -> SQL
	 
  Spark
  -----

	Spark is written is Scala programming language

  	Spark is an open-source distributed computing framework for Big-data Analytics

	Spark is an in-memory framework

		in-memory => Spark can persist intermediate results of the tasks in memory and subsequent
                             tasks can directly work on in-memory results. 


	Spark is a unified framework
	
		-> Spark provides a consistent set of APIs for processing different Analytics workloads
		   using the same execution engine and some well define data abstractions. 

		Batch Processing 		=> Spark Core API (RDD API)
		Structured Data Processing	=> Spark SQL
		Streaming data processing	=> Spark Streaming (Structured Streaming)
		Predictive analytics		=> Spark MLLib
		Graph Parallel computations	=> Spark GraphX


	Spark is a polyglot
		-> Spark supports Scala, Java, Python and R


	Spark supports multiple cluster managers
		-> Local, Spark Standalone, YARN, Mesos, Kubernetes 

       

   Getting Started with Spark
   --------------------------

     1. Working in your vLab

     2. Setup PySpark dev environement on your personal machine.

	-> Install and setup Anaconda distribution for Python
		download: https://www.anaconda.com/download
		
	-> Install and Setup PySpark ro work with 'Spyder' and 'Jupyter Notebooks'
		
		1. pip install pyspark

		OR

		2. Follow the instructions shared in the attached document.
		<Github>/Pyspark-JupyterNotebooks-Windows-Setup.pdf

      3. Using Databricks community edition (free edition)

		Sign-up: https://www.databricks.com/try-databricks

			=> Fill the details with valid email-address
			=> Next page, click on "Get started with Community Edition" link 
			   (Don't click on 'Continue' button)

		Login: https://community.cloud.databricks.com/login.html


		To Download a file from databricks:
		-----------------------------------

		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439

		Example:
		/FileStore/cars_output/part-00000
		https://community.cloud.databricks.com/files/cars_output/part-00000?o=4949609693130439



   Spark Architecture
   ------------------
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


   RDD (Resilient Distributed Dataset)
   -----------------------------------

   -> RDD is the fundamental data abstraction of Spark core API

   -> RDD is a collection of distributed in-memory partitions
	-> Each partition is a collection of objects (of some type)

   -> RDD has two components:
	  Data: Is created when RDD is executed. Is a collection of in-memory partitions	
	  Meta-Data: Lineage DAG

   -> RDDs are immutable

   -> RDD are lazily evaluated
	-> Transformations does not cause execution
	-> Only action commands trigger execution	


   Creating an RDD
   ---------------

     Three ways to create an RDD:

	1. Create an RDD from some external data source (like text files)

		rddFile = sc.textFile(<dataPath>, n)    n=numPartitions
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		NOTE: To get the partition-count of an RDD:
			 rddFile.getNumPartitions()

		-> The default number of partitions is determined by the value of 'sc.defaultMinPartitions'
		   'sc.defaultMinPartitions' = 2 if number-of-cores >= 2, else 1

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,5,6,6,5,7,8,9,9], 3)

		-> The default number of partitions is determined by the value of 'sc.defaultParallelism'
		   'sc.defaultParallelism'= number of cores allocated


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split())
		

   RDD Operations
   --------------

    Two types of operations

	1. Transformation
		-> Creates an RDD
		-> Does not cause execution
		-> Returns an RDD

	2. Action
		-> Executes an RDD DAG
		-> Launches Jobs to the cluster
		-> Generates output


   RDD Lineage DAG
   ---------------
   Transformations (data loading commands) create RDD Lineage DAGs
   Maintained by driver
   Represents a logical-plan on how to create RDD partition
   Contains the hierarchy of dependencies all the way from very first RDD.   
	

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG: (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG: (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

        
  Types of Transformations
  ------------------------

	1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Persistence
  ---------------
	rdd1 = sc.textFile(<data-path>, 4)
	rdd2 = rdd1.t2(...) 
	rdd3 = rdd1.t3(...) 
	rdd4 = rdd3.t4(...) 
	rdd5 = rdd3.t5(...) 
	rdd6 = rdd5.t6(...) 
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  --> instruction to Spark to save rdd6 partitions
	rdd7 = rdd6.t7(...) 

	rdd6.collect()
	Lineage of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile -> t3 -> t5 -> t6] ==> collect

	rdd7.collect()
	Lineage of rdd6: (4) rdd7 -> rdd6.t7
		[t7] ==> collect

	rdd6.unpersist()


	Storage Levels
        --------------
	MEMORY_ONLY		=> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x Replicated
	DISK_ONLY		=> Disk Serialized 1x Replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x Replicated	
	
	Commands
	---------
	rdd1.cache()    	=> in-memory persistence
	rdd1.persist()		=> in-memory persistence
	rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	rdd1.unpersist()


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.



 RDD Transformations & Actions
 -----------------------------

 1. map			P: U -> V
			Object to object transnformation
			Transforms each input object by applying the function
			input RDD: N objects, output RDD: N object

	rdd1.map(lambda x: x > 8).collect()


 2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output
			input RDD: N objects, output RDD: < N object

	rddFile.filter(lambda x: len(x.split()) > 8).collect()


 3. glom		P: None
			Returns one list object per partition with all the objects of the partition

		rdd1		rdd2 = rdd1.glom()

		P0: 2,3,5,1,4 -> glom -> P0: [2,3,5,1,4]
		P1: 3,6,8,7,9 -> glom -> P1: [3,6,8,7,9]
		P2: 5,0,7,9,3 -> glom -> P2: [5,0,7,9,3]

		rdd1.count() = 15 (int)   rdd2.count() = 3 (list)

		rdd1.glom().map(sum).collect()

  4. flatMap		P: U -> Iterable[V]
			flapMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: > N object
	
		rddWords = rddFile.flatMap(lambda x: x.split())


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1.mapPartitions(lambda p: [sum(p)]).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x+10, p)).glom().collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartition but you get partition-index as an additional function parameter	

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).glom().collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x+10), p)).collect()


  7. distinct		P: None, Optional: numPartitons
			Returns the distinct objects of the RDD.

		rddWords.distinct().collect()
		rddWords.distinct(3).collect()


  8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Returns an RDD with objects sorted based on function output

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 4).collect()

  Types of RDDs

	Generic RDD : RDD[U]
	Pair RDD : RDD[(K, V)]


  9. mapValues		P: U -> V
			Applied only to Pair RDDs
			Transforms the 'value part' only of the pair RDD by applying the function.
	
		rdd3.mapValues(list).collect()


  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where:
				key: Each unique value of the function output
				value: ResultIterable with grouped objects.
					

  		wordcountRdd = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
					 .flatMap(lambda x: x.split()) \
					 .groupBy(lambda x: x) \
					 .mapValues(len) \
					 .sortBy(lambda x: x[1], False, 1)

		wordcountRdd.saveAsTextFile("E:\\PySpark\\output\\wordcount")


  11. repartition	P: numPartition
			Is used to increase or decrease the number of partitions
			Performs global shuffle

		rddWords.repartition(3).glom().collect()
		

  12. coalesce		P: numPartition
			Is used only to decrease the number of partitions
			Performs partition merging

		 rddWords.coalesce(2).glom().collect()


     	Recommendations
	----------------
	1. The size of each partition should be between 100MB to 1GB
	   Ideal size is ~128MB if you are using Hadoop HDFS

        2. The number of partitions should be a multiple of number of cores

	3. The number of cores per executor (in case of YARN & K8S) is 5


  13. union, intersection, subtract, cartesian

		-> Operate on two generic RDDs.

		Assume rdd1 has M partitions and rdd2 has N partitions

		command				output
		----------------------------------------------
		rdd1.union(rdd2)		M+N, narrow
		rdd1.intersection(rdd2)		M+N, wide
		rdd1.subtract(rdd2)		M+N, wide
		rdd1.cartesian(rdd2)		M*N, wide


  14. partitionBy	P: numPartition, Optional: partition-function (default: hash)
			Applied only on pair RDDs
			Partitioning happends based on the keys

		transactions = [
			{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
			{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},
			{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
			{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]


		def custom_partitioner(city):
			if (city == "Hyderabad" or city == "Vijayawada"):
				return 0
			elif (city == "Bangalore"):
				return 1
			elif (city == "Chennai"):
				return 2
			else:
				return 3
			
			
		rdd1 = sc.parallelize(transactions, 3) \
				.map(lambda d: (d['city'], d)) \
				.partitionBy(4, custom_partitioner)


  ..ByKey Transformations
  -----------------------
	=> Are wide transformation
	=> Applied to only Pair RDD


  15. sortByKey		P: None, Optional: ascending (True/False), numPartitions
			Sorts the RDD based on the key

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 3).collect()


  16. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where
				key: Each unique key of the RDD
				value: grouped values (ResultIterable)

			CAUTION: Avoid 'groupByKey' if possible. 

		wordcountRdd = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
                 	.flatMap(lambda x: x.split()) \
                 	.map(lambda x: (x, 1)) \
                 	.groupByKey() \
                 	.mapValues(sum) \
                 	.sortBy(lambda x: x[1], False, 1)


  17. reduceByKey	P: (U, U) => U, Optional: numPartitions


		wordcountRdd = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
                 	.flatMap(lambda x: x.split()) \
                 	.map(lambda x: (x, 1)) \
                 	.reduceByKey(lambda x, y: x + y) \
                 	.sortBy(lambda x: x[1], False, 1)


  Use-Case
  --------
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.csv dataset, find out the average-weight of all the models of each make 
	of Amerigin origin cars. 
	-> Arrange the data in the DESCENDING order of average weight
	-> Save the output as a single text file.

	=> Try it yourself


  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) => U
			Reduces an entire RDD to one value of the same type by iterativly applying the function
			on each partition in the first stage and then across partitions in the second stage.

		rdd1		

		P0: 2,3,5,1,4 -> reduce -> -11 -> reduce => 30
		P1: 3,6,8,7,9 -> reduce -> -27
		P2: 5,0,7,9,3 -> reduce -> -14

		rdd1.reduce(lambda x, y: x - y)

		rddWc.collect()
		[('hadoop', 25), ('flatmap', 12), ('hdfs', 6), ('sadas', 1), ('das', 6), ('spark', 40), ('asd', 5), ('scala', 28), ('hive', 19), ('transformations', 10), ('d', 1), ('map', 6), ('groupby', 6), ('flume', 6), ('oozie', 6), ('sqoop', 6), ('mapreduce', 6), ('rdd', 43), ('actions', 10)]

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )
		('hadoop,flatmap,hdfs,sadas,das,spark,asd,scala,hive,transformations,d,map,groupby,flume,oozie,sqoop,mapreduce,rdd,actions', 242)



  5. take(n)

		rdd1.take(5)  => returns a list with first 5 objects.

  6. takeOrdered

		rddWords.takeOrdered(20)   => returns a list with first 5 ordered objects.
		rddWords.takeOrdered(20, lambda x: len(x))

  7. takeSample	

	
		rddWords.takeSample(True, 20)   	=> withReplacement = True	
		rddWords.takeSample(True, 20, 53)	=> 53 is a seed here

		rddWords.takeSample(False, 20)   	=> withReplacement = False	
		rddWords.takeSample(False, 20, 53)	=> 53 is a seed here

  8. countByValue

		rdd1.countByValue()

  9. countByKey

  10. foreach  	 => Runs a function on all objects of the RDD, but does not return any value.

  11. saveAsSequenceFile



  Closure
  =======

	In Spark, a closure constitutes all the variables and methods which must be visible for the 
	executor to perform its computations on the RDD. 

	This closure is serialized and a seaparte copy is sent to each executor.

		c = 0   

		def isPrime(n):
			if n is Prime return True
			if n is not Prime return False

		def f1(n):
			global c
			if (isPrime(n)) c += 1
			return n*2

		rdd1 = sc.parallelize( range(1, 4001), 4 )
		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(c)	// c = 0 


	Limitation: You can not use local variables (that are part of closure) to impliment global counters.
	Solution: Use "Accumulator" variables to impliment global counters.


  Shared Variables
  ================     

   1. Accumulator Variable
	
	-> Is a shared variable that is not part of the closure
	-> Maintained by driver
	-> All tasks can add to it using 'add' method.	
	-> Only driver can read the value of accumulator. Tasks can only write to it.
	-> Use accumulators to implement global counters. 


		c = sc.accumulator(0)  

		def isPrime(n):
			if n is Prime return True
			if n is not Prime return False

		def f1(n):
			global c
			if (isPrime(n)) c.add(1)
			return n*2

		rdd1 = sc.parallelize( range(1, 4001), 4 )
		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(c)	// c = 0 



   2. Broadcast Variable

	-> broadcast variable is a shared variable, hence, not a part of closure
	-> Driver sends a copy of the broadcast variable to every executor
	-> All the task within that executor can read from the one copy (of the executor)
	-> You can convert large immutable collections into broadcast variables. 
	

		d = { 1:a, 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, ..... }   # 100 MB
		bc = sc.broadcast(d)
		
		def f1(n):
			global bc
			return bc.value[n]

		rdd1 =  sc.parallelize( [1,2,3,4,,5,6 ..], 4 )
		rdd2 = rdd1.map( f1 )
		rdd2.collect()           # a,b,c,d,...
		
	

   Spark Submit command
   =====================

    'Spark-submit' command is a single command that is used to submit any spark application (Scala, Java, Python, R)
    to any cluster manager (local, spark syandalone, YARN, Mesos or Kubernetes)
			

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt output 1
  


  ====================================
      Spark SQL (pyspark.sql)
  ====================================
   
   -> High level API built on top of Spark Core

   -> Used for structured data processing

	File formats: Parquet (default), ORC, JSON, CSV (delimited text), Text
	 JDBC format: RDBMS, NoSQL
	 Hive format: Hive warehouse

   -> SparkSession

	-> Starting point of execution 
	-> Represents a user-session inside an application

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate()   

   -> DataFrame (DF)

	-> Is the data abstraction used in Spark SQL
	
	-> DF is a collection of distributed in-memory partitions
	-> DF is immutable and lazily evaluated

	-> DF is a collection of 'Row' objects

	-> DF has two components

		Data: Collection of 'Row' objects
		      Row is a collection od 'Column' objects

		Schema: StructType object

			StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			 )



   Basic steps is a Spark SQL application
   --------------------------------------

   1. Create a DataFrame from some a data source (external or internal)

 		filePath = "E:\\PySpark\\data\\users.json"

		df1 = spark.read.format("json").load(filePath)
		df1 = spark.read.load(filePath, format="json")
		df1 = spark.read.json(filePath)


   2. Perform transformation on the DF using transformation methods or using SQL


	Using transformation methods
        ----------------------------

		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)

	Using SQL
	---------

		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
         		from users
         		where age is not null
         		group by age
         		order by age
         		limit 4"""
         
		df3 = spark.sql(qry)
		df3.show()


   3. Save the DF into some structured destination	
	
		outputPath = "E:\\PySpark\\output\\json"

		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

 Save Modes
 ----------
    -> Defines what happens when a DF is being written to an existing directory.

	ErrorIfExists (default)
	Ignore
	Append
	Overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


 LocalTempView & GloblTempView
 -----------------------------

	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")


 DataFrame Transformations
 -------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


  2. where / filter

	df3 = df2.where("highFrequency = true and destination = 'United States'")
	df3 = df2.filter("highFrequency = true and destination = 'United States'")

	df3 = df2.where( col("count") > 100 )

	df3.show(5)


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))


  4. groupBy  => returns a "GroupedData" object
		 apply some aggregation method to return a DataFrame


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	  .agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 2).alias("avg")
		   )


  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	   df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 		"DEST_COUNTRY_NAME as destination",
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 200 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")


  7. withColumn 

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) \
        	.withColumn("country", lit("India") )

	df3 = df2.withColumn("ageGroup", when(col("age") < 13, "child")
                                 	.when(col("age") < 20, "teenager")
                                 	.when(col("age") < 60, "adult")
                                 	.otherwise("senior"))

  8. withColumnRenamed

	df3 = df1.withColumn("newCount", col("count") + 10) \
		.withColumn("highFrequency", expr("count > 200")) \
		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


  9. udf (user-defined-function)

	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"		
	   
	get_age_group_udf = udf(getAgeGroup, StringType())    
		
	df3 = df2.withColumn("ageGroup", get_age_group_udf(col("age")) )

	-------------------------------------------

	@udf(returnType = StringType())
	def get_age_group( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"
		  
	df3 = df2.withColumn("ageGroup", get_age_group(col("age")) )
	
	-------------------------------------------

	spark.udf.register("get_age_group_udf", get_age_group, StringType())

	qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"

	spark.sql(qry).show()


  10. drop	=> excludes the specified columns of the input DF in the output DF.

	df3 = df2.drop("newCount", "highFrequency")
	df3.show()


  11. dropna	=> drops the rows with NULL values in any or specified columns

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	usersDf.dropna().show()
	usersDf.dropna(subset=["name", "phone"]).show()


  12. dropDuplicates   => drops the duplicate rows

	listUsers = [(1, "Raju", 5),
				 (1, "Raju", 5),
				 (3, "Raju", 5),
				 (4, "Raghu", 35),
				 (4, "Raghu", 35),
				 (6, "Raghu", 35),
				 (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	userDf.dropDuplicates().show()
	userDf.dropDuplicates(["name", "age"]).show()


  13. distinct	 => returns distinct rows of the DF

	userDf.distinct().show()


	Q: How many UNIQUE values are there in DEST_COUNTRY_NAME column?
	-----------------------------------------------------------------
	df1.select("DEST_COUNTRY_NAME").distinct().count()
	df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()


 14. randomSplit => Spalit the DF into a list of DFs in the specified weights. 

	dfList = df1.randomSplit([0.6, 0.4])
	dfList = df1.randomSplit([0.6, 0.4], 345)   => 345 is a seed

	print( dfList[0].count(),  dfList[1].count() )


 15. union, intersect, subtract


	df2 = df1.where("count > 1000")
	df2.show()
	df2.count()   # 14 rows
	df2.rdd.getNumPartitions()

	df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
	df3.show()
	df3.count()   # 1 rows
	df3.rdd.getNumPartitions()

	df4 = df2.union(df3)
	df4.rdd.getNumPartitions()
	df4.show()

	df5 = df4.subtract(df2)   # df4: 2, df2: 1
	df5.show()
	df5.rdd.getNumPartitions()

	df6 = df4.intersect(df2)
	df6.show()
	df6.rdd.getNumPartitions()


  spark.sql.shuffile.partitions
  -----------------------------
	=> This config parameter determines the number of shuffle partitions created
	=> default value is 200

	spark.conf.get("spark.sql.shuffle.partitions")
	spark.conf.set("spark.sql.shuffle.partitions", "3")


 16. sample

	df2 = df1.sample(True, 0.65)		# with-replacement sampling
	df2 = df1.sample(True, 1.65)		# fraction > 1 is allowed
	df2 = df1.sample(True, 0.65, 42342)	# 42342 is a seed

	df2 = df1.sample(False, 0.65)		# without-replacement sampling
	df2 = df1.sample(False, 1.65)		# ERROR: fraction > 1 is NOT allowed
	df2 = df1.sample(False, 0.65, 42342)    # 42342 is a seed


 17. repartition

	df2 = df1.repartition(6)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(3)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()

	df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
	df5.rdd.getNumPartitions()


 18. coalesce

	df6 = df5.coalesce(3)
	df6.rdd.getNumPartitions()


 19. join    => discussed as a separate topic.
  
  

 Working with different file formats
 -----------------------------------

   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)


   CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")

   Text
	read
		df1 = spark.read.text(inputPath)
		=> df1 will have one columns called 'value' of 'string' type

	write
		df1.write.text(outputPath)
		=> You can only save a DF with a single text column in 'text' format.



  Create an RDD from DataFrame
  ----------------------------
	rdd1 = df1.rdd
	rdd1.take(5)


  Create a DataFrame from programmatic data
  -----------------------------------------

	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Create a DataFrame from an RDD
  ------------------------------

	rdd1 = spark.sparkContext.parallelize(listUsers, 1)
	df1 = rdd1.toDF(["id", "name", "age"])


  Create a DataFrame with Programmatic Schema
  -------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = rdd1.toDF(schema = mySchema)

	------------------------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = rdd1.toDF(schema = mySchema)

       -------------------------------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	filePath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"
	df1 = spark.read.json(filePath, schema=mySchema)

	

  Joins
  -----
	Supported Joins: inner, left, right, full, left_semi, left_anti, cross


	left_semi Join
	--------------
	Is similar to inner join but the data is fetched only from the left table.

	Equivalent to the following sub-query:
	
	     select * from emp where deptid in (select id from dept)


	left_anti Join
	--------------
	Equivalent to the following sub-query:
	
	     select * from emp where deptid not in (select id from dept)


	SQL Way
	-------
	employee = spark.createDataFrame([
		(1, "Raju", 25, 101),
		(2, "Ramesh", 26, 101),
		(3, "Amrita", 30, 102),
		(4, "Madhu", 32, 102),
		(5, "Aditya", 28, 102),
		(6, "Pranav", 28, 100)])\
	  .toDF("id", "name", "age", "deptid")
	  
	department = spark.createDataFrame([
		(101, "IT", 1),
		(102, "ITES", 1),
		(103, "Opearation", 1),
		(104, "HRD", 2)])\
	  .toDF("id", "deptname", "locationid")

	employee.show()
	department.show()

	spark.catalog.listTables()

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*, dept.*
		 from emp join dept
		 on emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)

	joinedDf.show()


	DF Transformation method
        ------------------------
	joinCol = employee.deptid == department.id
	joinedDf = employee.join(department, joinCol, "full")
	joinedDf.show()


  Use-Case
  --------

   datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

   From movies.csv and ratings.csv files, fetch the top 10 movies with heighest average user rating.
	-> Consider only those movies with atleast 30 ratings
	-> Data: movieId, title, totalRatings, averageRating
	-> Arrange the data in the DESC order of averageRating
	-> Save the output as a single pipe-separated CSV file with header. 
	-> Use DF transformation methods only.

        => Try it yourself..


  JDBC Format - Working with MySQL
  --------------------------------
	import os
	import sys

	# setup the environment for the IDE
	os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
	os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

	sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
	sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

	from pyspark.sql import SparkSession

	spark = SparkSession.builder \
				.appName("JDBC_MySQL") \
				.config('spark.master', 'local') \
				.getOrCreate()
	  
	qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
					  
	df_mysql = spark.read \
				.format("jdbc") \
				.option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
				.option("driver", "com.mysql.jdbc.Driver") \
				.option("dbtable", qry)  \
				.option("user", "root") \
				.option("password", "kanakaraju") \
				.load()
					
	df_mysql.show()  

	spark.catalog.listTables()

	df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

	df2.show()   

	df2.printSchema()    

	df2.write.format("jdbc") \
		.option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
		.option("driver", "com.mysql.jdbc.Driver") \
		.option("dbtable", "emp2")  \
		.option("user", "root") \
		.option("password", "kanakaraju") \
		.mode("overwrite") \
		.save()      
										 
	spark.stop()


  Hive Format - Working with Hive warehouse
  -----------------------------------------

 	# -*- coding: utf-8 -*-
	import findspark
	findspark.init()

	from os.path import abspath
	from pyspark.sql import SparkSession
	from pyspark.sql.functions import desc, avg, count

	warehouse_location = abspath('spark-warehouse')

	spark = SparkSession \
		.builder \
		.appName("Datasorces") \
		.config("spark.master", "local") \
		.config("spark.sql.warehouse.dir", warehouse_location) \
		.enableHiveSupport() \
		.getOrCreate()
		
	spark.catalog.listTables()  

	spark.catalog.currentDatabase()
	spark.catalog.listDatabases()

	spark.sql("show databases").show()

	spark.sql("drop database sparkdemo cascade")
	spark.sql("create database if not exists sparkdemo")
	spark.sql("use sparkdemo")

	spark.catalog.listTables()
	spark.catalog.currentDatabase()

	spark.sql("DROP TABLE IF EXISTS movies")
	spark.sql("DROP TABLE IF EXISTS ratings")
	spark.sql("DROP TABLE IF EXISTS topRatedMovies")
		
	createMovies = """CREATE TABLE IF NOT EXISTS 
			 movies (movieId INT, title STRING, genres STRING) 
			 ROW FORMAT DELIMITED 
			 FIELDS TERMINATED BY ','"""
		
	loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
			 OVERWRITE INTO TABLE movies"""
		
	createRatings = """CREATE TABLE IF NOT EXISTS 
			 ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
			 ROW FORMAT DELIMITED 
			 FIELDS TERMINATED BY ','"""
		
	loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
			 OVERWRITE INTO TABLE ratings"""
			 
	spark.sql(createMovies)
	spark.sql(loadMovies)
	spark.sql(createRatings)
	spark.sql(loadRatings)
		
	spark.catalog.listTables()
		 
	#Queries are expressed in HiveQL

	moviesDF = spark.sql("SELECT * FROM movies")
	ratingsDF = spark.sql("SELECT * FROM ratings")

	moviesDF.show()
	ratingsDF.show()
			   
	summaryDf = ratingsDF \
				.groupBy("movieId") \
				.agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
				.filter("ratingCount > 25") \
				.orderBy(desc("ratingAvg")) \
				.limit(10)
				  
	summaryDf.show()
		
	joinStr = summaryDf.movieId == moviesDF.movieId
		
	summaryDf2 = summaryDf.join(moviesDF, joinStr) \
					.drop(summaryDf["movieId"]) \
					.select("movieId", "title", "ratingCount", "ratingAvg") \
					.orderBy(desc("ratingAvg")) \
					.coalesce(1)
		
	summaryDf2.show()
	 
	summaryDf2.write \
	   .mode("overwrite") \
	   .format("hive") \
	   .saveAsTable("topRatedMovies")
	   
	spark.catalog.listTables()
			
	topRatedMovies = spark.table("topRatedMovies")
	topRatedMovies.show() 
		
	spark.catalog.listFunctions()  
	  
	spark.stop()


 =============================================
    Spark Streaming (Structured Streaming) 
 =============================================

  Spark Streaming APIs:

	-> Spark Streaming (DStreams API) built on top of Spark Core
		-> pyspark.streaming
		-> Old & legacy API

	-> Structured Streaming built on top of Spark SQL 
		-> pyspark.sql.streaming
		-> Current and preferred API

  Notes:

  The key idea in Structured Streaming is to treat a live data stream as a table that is being 
  continuously appended.

  You express your streaming computation as standard batch-like query as on a static table, 
  and Spark runs it as an incremental query on the unbounded input table.

  Programming Model
  -----------------
	A query on the input will generate the “Result Table”.

	Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	which eventually updates the Result Table.

	Whenever the result table gets updated, we would want to write the changed result rows 
	to an external sink.

	Structured Streaming does not materialize the entire table.

	It reads the latest available data, processes it incrementally to update the result, and 
	then discards the source data.

	Spark is responsible for updating the Result Table when there is new data, thus relieving 
	the users from reasoning about fault-tolerance and data consistency.


  Sources
  -------

	Socket Stream (host & port)	
	File Stream (directory) => text, csv, json, parquet, orc
	Rate stream
	Kafka Stream


  Sinks
  -----

	Console Sink
	File Sinks  (directory) => text, csv, json, parquet, orc
	Kafka sink
	ForEachBatch sink
	ForEach sink 
	Memory sink

   












  


