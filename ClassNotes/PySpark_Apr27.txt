
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark

	-> Spark is an open source unified in-memory distributed computing framework for big data analytics.
	-> Spark is written in SCALA programming language

	in-memory computing -> the intermediate results can be persisted in RAN and subsequent tasks
			       can directly run on those persisted data. 

				If there is not enough RAM for persisting your partitions, then Spark 
				resorts to disk-based computation.

   Spark Unified Framework
	
	-> Spark comes with a consistent set of libraries for performing analytics on different types of
	   analytical workloads running on the same execution engine.
		
		-> Batch Processing of unstructured data	=> Spark Core (low level)
		-> Batch Processing of structured data		=> Spark SQL
		-> Stream processing (real time)		=> Spark Streaming, Structured Streaming
		-> Predictive analytics (machine learning)	=> Spark MLLib
		-> Graph parallel computations			=> Spark GraphX

   Spark is a polyglot	
	-> Spark supports Scala, Java, Python, R 

   Spark applications can run on multiple cluster managers
	-> local, standalone scheduler, YARN, Mesos, Kubernetes


   Getting started with Spark
   --------------------------	
    1. Using your Big Data Lab (that is assigned to you)

	-> You will land up on a Windows server. 
	-> On the desktop of the Window server you find Oracle VM Virtualbox
	-> On the desktop of the Window server you find a "Word" document with userids and passwords.

	-> Connect to Ubuntu VM using Oracle VM Virtualbox

		-> Start the PySpark shell by opening a terminal and enter the command "pyspark"
		-> Start the Spyder IDE by clicking on the icon.

    2. Installing your own dev environment.

	-> Make sure you have Anaconda Navigator Installed. 

	-> Try installing pyspark using pip install
		-> Open an anaconda terminal and use the 
	-> Follow the instruction provided in the document to sepup PySpark to work with Spayder and Jupyter Notebook.


	



   Spark Architecture
   ------------------















	

		
			      




























