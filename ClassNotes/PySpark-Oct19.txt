
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark


  Prerequisites
  -------------
	-> Python
        -> SQL
	 
  Spark
  -----

	Spark is written is Scala programming language

  	Spark is an open-source distributed computing framework for Big-data Analytics

	Spark is an in-memory framework

		in-memory => Spark can persist intermediate results of the tasks in memory and subsequent
                             tasks can directly work on in-memory results. 


	Spark is a unified framework
	
		-> Spark provides a consistent set of APIs for processing different Analytics workloads
		   using the same execution engine and some well define data abstractions. 

		Batch Processing 		=> Spark Core API (RDD API)
		Structured Data Processing	=> Spark SQL
		Streaming data processing	=> Spark Streaming (Structured Streaming)
		Predictive analytics		=> Spark MLLib
		Graph Parallel computations	=> Spark GraphX


	Spark is a polyglot
		-> Spark supports Scala, Java, Python and R


	Spark supports multiple cluster managers
		-> Local, Spark Standalone, YARN, Mesos, Kubernetes 

       

   Getting Started with Spark
   --------------------------

     1. Working in your vLab

     2. Setup PySpark dev environement on your personal machine.

	-> Install and setup Anaconda distribution for Python
		download: https://www.anaconda.com/download
		
	-> Install and Setup PySpark ro work with 'Spyder' and 'Jupyter Notebooks'
		
		1. pip install pyspark

		OR

		2. Follow the instructions shared in the attached document.
		<Github>/Pyspark-JupyterNotebooks-Windows-Setup.pdf

      3. Using Databricks community edition (free edition)

		Sign-up: https://www.databricks.com/try-databricks

			=> Fill the details with valid email-address
			=> Next page, click on "Get started with Community Edition" link 
			   (Don't click on 'Continue' button)

		Login: https://community.cloud.databricks.com/login.html



   Spark Architecture
   ------------------
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


   RDD (Resilient Distributed Dataset)
   -----------------------------------

   -> RDD is the fundamental data abstraction of Spark core API

   -> RDD is a collection of distributed in-memory partitions
	-> Each partition is a collection of objects (of some type)

   -> RDD has two components:
	  Data: Is created when RDD is executed. Is a collection of in-memory partitions	
	  Meta-Data: Lineage DAG

   -> RDDs are immutable

   -> RDD are lazily evaluated
	-> Transformations does not cause execution
	-> Only action commands trigger execution	



   Creating an RDD
   ---------------

     Three ways to create an RDD:

	1. Create an RDD from some external data source (like text files)

		rddFile = sc.textFile(<dataPath>, n)    n=numPartitions
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		NOTE: To get the partition-count of an RDD:
			 rddFile.getNumPartitions()

		-> The default number of partitions is determined by the value of 'sc.defaultMinPartitions'
		   'sc.defaultMinPartitions' = 2 if number-of-cores >= 2, else 1

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,5,6,6,5,7,8,9,9], 3)

		-> The default number of partitions is determined by the value of 'sc.defaultParallelism'
		   'sc.defaultParallelism'= number of cores allocated


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split())
		

   RDD Operations
   --------------

    Two types of operations

	1. Transformation
		-> Creates an RDD
		-> Does not cause execution
		-> Returns an RDD

	2. Action
		-> Executes an RDD DAG
		-> Launches Jobs to the cluster
		-> Generates output


   RDD Lineage DAG
   ---------------
   Transformations (data loading commands) create RDD Lineage DAGs
   Maintained by driver
   Represents a logical-plan on how to create RDD partition
   Contains the hierarchy of dependencies all the way from very first RDD.   
	

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG: (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG: (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

        
  Types of Transformations
  ------------------------

	1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<data-path>, 4)
	rdd2 = rdd1.t2(...) 
	rdd3 = rdd1.t3(...) 
	rdd4 = rdd3.t4(...) 
	rdd5 = rdd3.t5(...) 
	rdd6 = rdd5.t6(...) 
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  --> instruction to Spark to save rdd6 partitions
	rdd7 = rdd6.t7(...) 

	rdd6.collect()
	Lineage of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile -> t3 -> t5 -> t6] ==> collect

	rdd7.collect()
	Lineage of rdd6: (4) rdd7 -> rdd6.t7
		[t7] ==> collect

	rdd6.unpersist()


	Storage Levels
        --------------
	MEMORY_ONLY		=> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x Replicated
	DISK_ONLY		=> Disk Serialized 1x Replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x Replicated	
	
	Commands
	---------
	rdd1.cache()    	=> in-memory persistence
	rdd1.persist()		=> in-memory persistence
	rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	rdd1.unpersist()



   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.



  RDD Transformations & Actions
  -----------------------------

  1. map		P: U -> V
			Object to object transnformation
			Transforms each input object by applying the function
			input RDD: N objects, output RDD: N object

	rdd1.map(lambda x: x > 8).collect()


 2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output
			input RDD: N objects, output RDD: < N object

	rddFile.filter(lambda x: len(x.split()) > 8).collect()


 3. glom		P: None
			Returns one list object per partition with all the objects of the partition

		rdd1		rdd2 = rdd1.glom()

		P0: 2,3,5,1,4 -> glom -> P0: [2,3,5,1,4]
		P1: 3,6,8,7,9 -> glom -> P1: [3,6,8,7,9]
		P2: 5,0,7,9,3 -> glom -> P2: [5,0,7,9,3]

		rdd1.count() = 15 (int)   rdd2.count() = 3 (list)

		rdd1.glom().map(sum).collect()

  4. flatMap		P: U -> Iterable[V]
			flapMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: > N object
	
		rddWords = rddFile.flatMap(lambda x: x.split())


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1.mapPartitions(lambda p: [sum(p)]).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x+10, p)).glom().collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartition but you get partition-index as an additional function parameter	

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).glom().collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x+10), p)).collect()


  7. distinct		P: None, Optional: numPartitons
			Returns the distinct objects of the RDD.

		rddWords.distinct().collect()
		rddWords.distinct(3).collect()



























































  


