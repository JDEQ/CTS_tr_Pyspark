
  Agenda (8 sessions)
  ------------------- 
    Spark - basics & architecture
    Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variables
    Spark-Submit
    Spark-SQL
	-> Working with DataFrames
	-> Integrations - MySQL & Hive
    Spark Streaming
	-> DStreams (introduction)
	-> Structured Streaming 

  Materials
  ---------
	=> PDF presentations
	=> Code Modules
        => Class Notes
	=> Github: https://github.com/ykanakaraju/pyspark
  
  Getting started with Spark
  --------------------------
	1. Using vLab
	      -> Follow the instruction given in the attached document.
              -> Three tools:
		   -> PySpark shell
		   -> Spyder IDE
		   -> Jupyter Notebooks

	2. Installing development environment in your personal machine.
		-> First make sure you have "Anaconda distribution"
		    => URL: https://docs.anaconda.com/anaconda/install/
                -> Follow the instruction given in the shared document to setup "PySpark" with Spyder & Jupyter

        3. Signup for Databricks community edition (free signup)
		Signup link: https://www.databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html
   

  Spark
  ------
        Cluster: Is a unified entity comprising of many nodes whose combined resources can be used to
                 distribute your storage or processing.

	=> Spark is written in Scala programming language

	=> Spark is a in-memory distributed computing framework.

	=> Spark is a 'unified' analytics framework

	    Spark provides a consistent set of APIs for processing different analytics workloads
	    using the same execution engine.

		Batch Analytics	of unstructured data    => Spark Core API
		Batch Analytics	of structured data	=> Spark SQL
		Streaming Analytics			=> Spark Streaming, Structured Streaming 
		Predictive Analytics			=> Spark MLLib
		Graph Parallel Computations		=> Spark GraphX

	=> Spark is a polyglot
		-> Supports Scala, Java, Python and R

	=> Spark jobs can be submitted to mutiple cluster managers
		=> local, spark standalone, YARN, Mesos, Kubernetes


  Spark Architecture (& Building Blocks)
  --------------------------------------

       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
	

   RDD (Resilient Distributed Dataset)
   ------------------------------------
    -> Fundamental in-memory data abstraction of Spark Core API

    -> RDD is a collection of distributed in-memory partitions.
	-> Partition is a collection of objects (of any type)

	RDD =>   Meta Data (Lineage DAG)
		 Data 	

    -> RDDs are immutable
   
    -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Actions commands 

  Creating RDDs
  -------------

    Three ways:

	1. Creating RDDs from external data files
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		=> default number fo partitions is given by sc.defaultMinPartitions

	2. Creating RDDs from programmatic data
		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0], 3)

		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0])
		=> default number fo partitions is given by sc.defaultParallelism

        3. By applying transformations on existing RDDs
		
		rdd2 = rdd1.map(lambda x: x*100)

  RDD Operations
  --------------
    Two things:

	1. Transformations
		-> Create the RDD lineage DAGs 
		-> Does not trigger the actual execution
		-> Every transformation returns an other RDD.

	2. Actions
		-> Trigges execution.
		-> Converts the logical plan to physical plan and sends the tasks to the executors.


  RDD Lineage DAG
  ---------------
   -> Are created at the driver when data-loading or transformation commands  
   -> Represents a 'logical plan' on how to create the RDD partitions
   -> Maintains the hierarchy of dependencies all the way from the very first RDD. 

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
	   Lineage DAG =>  rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   Lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    Lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

      
   RDD Persistence
   ---------------
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   --> instruction to spark to save the rdd6 partition
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> rdd6

	rdd7.collect()

	lineage of rdd7 => rdd7 -> rdd6.t7
		[t7] -> rdd7

	StorageLevels
        -------------
	1. MEMORY_ONLY	      -> default, Memory Serialized 1x replicated
	2. MEMORY_AND_DISK    -> Disk Memory Serialized 1x replicated
	3. DISK_ONLY	      -> Disk Serialized 1x replicated
	4. MEMORY_ONLY_2      -> Memory Serialized 2x replicated
	5. MEMORY_AND_DISK_2  -> Disk Memory Serialized 2x replicated
 

        Commands
        --------
		rdd1.cache()      -> memory only
		rdd1.persist()    -> memory only
		rdd1.persist( StorageLevel.DISK_ONLY )

		rdd1.unpersist()


   Executor Memory Structure
   -------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of transformations
   ------------------------

	Two types:

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


   RDD Transformations
   -------------------
  
    1. map		P: U => V
			Object to object transformation
			input RDD: N objects, output RDD: N objects
 	
		rddFile.map(lambda s: s.upper()).collect()

    2. filter		P: U -> Boolean
			Filters the objects of the input RDD based on the function output.
			input RDD: N objects, output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

    3. glom		P: None
			Returns one list object per partition with all the objects of the partition

		rdd1			rdd2 = rdd1.glom()
		P0: 1,2,1,3,4,5 -> glom -> P0: [1,2,1,3,4,5]
		P1: 4,3,5,4,6,7 -> glom -> P1: [4,3,5,4,6,7]
		P2: 3,1,3,5,6,7 -> glom -> P2: [3,1,3,5,6,7]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flattens the iterables produced by the function 
			input RDD: N objects, output RDD: >= N objects	

		rddWords.flatMap(lambda w: w.upper()).collect()

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 1,2,1,3,4,5 -> mapPartitions -> P0: 
		P1: 4,3,5,4,6,7 -> mapPartitions -> P1: 
		P2: 3,1,3,5,6,7 -> mapPartitions -> P2: 
		
		rdd1.mapPartitions(lambda p: [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()

   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Same as mapPartitions, but you get partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))] ) \
		    .filter(lambda t: t[0] == 0)\
		    .flatMap(lambda x: x[1])\
                    .map(lambda x: x*10)\
                    .collect()

   7. distinct		P: None, Optional: numPartitions
			returns an RDD with distinct objects (removes the duplicates in the output RDD)			 
		
			rddWords.distinct().collect()
			rddWords.distinct(4).collect()

   Types of RDDs	
   --------------
	1. Generic RDDs : RDD[U]
	2. Pair RDDS :	RDD[(K, V)]

   8. mapValues		P: U => V
			Applied only on Pair RDDs
			The 'value' part of the (K,V) pairs is transformed using the function.

		rdd4 = rdd2.mapValues(lambda v: (v, v+4) )

   9. sortBy		P: U => V, Optional: ascending (True/False), numPartition
			The elements of the RDD are sorted based on the function output

		rddWords.sortBy(lambda x: x[-1], True).collect()
		rdd1.sortBy(lambda x: x%5, False, 2).glom().collect()

   10. groupBy		P: U -> V
			Return a pair RDD where:
			key: unique value of the function output
			value: ResultIterable object with all the objects that produced the key

   		wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len) \
              			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: A list of ratios (eg: [0.5, 0.5] )
				Splits an rdd into multiple RDDs randomly in the given ratios.

		rddList = rdd1.randomSplit( [0.35, 0.15, 0.5])
		rddList = rdd1.randomSplit( [0.35, 0.15, 0.5], 353 )   # here 353 is a seed


   12. repartition		P: numPartitions
				Returns an RDD with specified number of partitions
				You may increase or decrease the number of partitions.
				Results in global shuffling 

		rdd5 = rdd1.repartition(5)

   13. coalesce			P: numPartitions
				Returns an RDD with specified number of partitions
				You can only decrease the number of partitions.
				Results in partition-merging 

		rdd2 = rdd1.coalesce(2)

	Recommendations
	---------------
	=> An RDD partition should be between 100 MB to 1 GB (ideally 128 MB)
	=> The number of partitions should be a multiple of number of cores allocated to your app
	=> If the number of partition is close to less than 2000, bump it up to 2000
	=> The number of CPU cores per executor should be 5


   14. partitionBy	P: numberOfPartitions, Optional: partition-function (default: hash)
			Applied on to PairRDDs

P: numPartitions, Optional: partitioning-function (default: hash)
				Applied only on Pair RDD
				Is used to control which keys go to which partition based on some partition
				function. 		
			
		rdd3 = rdd1.map(lambda x: (x, 0)).partitionBy(2).map(lambda x: x[0])

transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;      

rdd1 = sc.parallelize(transactions, 3) \
        .map(lambda d: (d['city'], d)) \
        .partitionBy(3, custom_partitioner) \
        .map(lambda x: x[1])

rdd1.glom().collect()



   15. union, intersection, subtract, cartesian

	let us say rdd1 has M partitions and rdd2 has N partitions

	 command			output partitions
         --------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd1.cartesian(rdd2)		M * N, wide


   ..ByKey transformations
   -----------------------
	=> Are wide transformations
	=> Applied only to pair RDDs

   16. sortByKey		P: None, Optional: ascending (True/False), numPartitions

   17. groupByKey		P: None, Optional: numPartitions 

	 
	  wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              		.flatMap(lambda x: x.split(" ")) \
              		.map(lambda x: (x, 1)) \
              		.groupByKey() \
              		.mapValues(len) \
              		.sortBy(lambda x: x[1], False, 1)

   18. reduceByKey	P: (U, U) => U	 Optional: numPartitions 
			Reduces all the values of each unique-key by applying the reduce function. 

	  wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              		.flatMap(lambda x: x.split(" ")) \
              		.map(lambda x: (x, 1)) \
              		.reduceByKey(lambda x, y: x + y) \
              		.sortBy(lambda x: x[1], False, 1)

   19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])


   20. joins		=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

			   RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup			Is used when you want to join RDDs with duplicate keys and want unique keys
				in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


	


  RDD Actions 
  -----------
 
   1. collect

   2. count

   3. saveAsTextFile

  4. reduce			P: U, U -> U
				Reduces the entire RDD to one final value of the same type by iterativly
				applying the reduce function on each partition and then further reduces the
				values produced at each partition to one final value. 

		rdd1

		P0: 6, 3, 4, 5, 2, 8, 7, 8, 9, 0	-> reduce -> 52   => 153
		P1: 7, 8, 3, 2, 1, 2, 1, 2, 4, 3	-> reduce -> 33
		P2: 5, 4, 6, 7, 8, 9, 6, 4, 2, 9, 0, 8  -> reduce -> 68	
	
		rdd1.reduce(lambda x, y: x + y)  

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )

   5. aggregate

	Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. take
		rdd1.take(10)

   7. takeOrdered

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)

   8. takeSample

		rdd1.takeSample(True, 20)
		rdd1.takeSample(True, 20, 4564)

		rdd1.takeSample(False, 20)

   9. countByValue
		rddWords.countByValue()

   10. countByKey
		rddPairs.countByKey()

   11. saveAsSequenceFile

   12. foreach		P: Some function that is applied on all the objects of the RDD, but does not return anything.


   13. saveAsSequenceFile

	

  Spark Closures
  ===============

	In spark, a 'closure' represents all the variables and methods that must be visible inside
        and executors for the tasks to complete their computations on RDD partitions

	=> Driver serializes the closure and a separate copy is sent to each executor. 

	c = 0

	def isPrime( n )
	   returns True or False

	def f1(n) :
	   global c
	   if (isPrime(n)) c += 1
	   return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)  # println 0

	
	Limitation: We can not use local variables (which are part of the closure) to implement
		    global counter.

	Solution: Use "Accumulator" variable

	
  Shared Variables
  ================

	A shared variable is a variable whose value is shared (written to / read from) by all the
	tasks running for the job. 

	
   1. Accumulator variable

	-> Maintained by the driver
	-> Not part of closure (not a local copy)
	-> All tasks can add to this accumulator. 
	-> All tasks share one copy of the variable maintained at the driver side.
	-> Used to implement global counter
	
	c = sc.accumulator(0) 	

	def isPrime(n):
		returns True if n is Prime
		else False		

	def f1(n):
		global c
		if (isPrime(n)) c.add(1)
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     # c = 80


   2. Broadcast Variable


	d = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, 6: f, 7: g, .. })   # 100 MB 

	def f1(n):
		global d
		return d.value[n]

 	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()



   Use-Case
   --------

    Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
    From cars.tsv dataset compute the average weight of each make of American origin cars
	-> Arrange the data in the DESC order of the average weight
	-> Save the output as a single text file.
	
      Try that yourself....



  ========================
    spark-submit	
  ========================

	=> is a single command that is used to submit any spark application (Scala, Java, Python or R)
           to any cluster manager.



	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executor-memory 10G
		--executor-cores 5
		--num-executors 10
                E:\Spark\wordcount.py [app args]
 
      spark-submit --master local[2] E:\Spark\wordcount.py [app args]

      spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput 1


  ================================
    Spark SQL (pyspark.sql)
  ================================

      => Spark's structured data processing API
	
		Structured file formats : Parquet (default), ORC, JSON, CSV (delimited text)
		             JDBC format: RDBMS and NoSQL
			     Hive format: Hive data warehouse 

    => SparkSession
		-> Starting point of execution
		-> Represents a 'user-session' with it's own configuration (within an application)
   
      	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate() 

   => DataFrame (DF)
	
	-> Is the main data abstraction of Spark SQL
	
	-> Is a collection of distributed in-memory partitions that are immutable and lazily-evaluated. 
	
	-> DF is a collection of "Row" objects
		-> Row is a collection of columns
		-> The columns are processed using Spark SQL internal type representation.

	-> DF has two components:
		Data       : "Row" objects
		Metadata   : Schema (StructType object)

			StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			)


  Basic steps in a Spark SQL application
  --------------------------------------

   1. Read/load data from some data source into a DataFrame

	df1 = spark.read.format("json").load(inputPath) 
	df1 = spark.read.json(inputPath)  	

   2. Apply transformation on the DF using DataFrame APIs or using SQL

	using DataFrame APIs
	---------------------
	df2 = df1.select("userid", "name", "age", "gender") \
         	.where("age is not null") \
         	.orderBy("gender", "age") \
         	.groupBy("age").count() \
         	.limit(4)

	using SQL
	---------
	df1.createOrReplaceTempView("users")
	spark.catalog.listTables()
	
	#spark.catalog.dropTempView("users")  -> to drop a tempview

	qry = """select age, count(*) as count
         	from users
         	where age is not null
         	group by age
         	order by age
         	limit 4"""        
         
	df3 = spark.sql(qry)
	df3.show()


   3. Write/save the DF into some external target 
	
	outputPath = "E:\\PySpark\\output\\json"

	df2.write.format("json").save(outputPath)
	df2.write.json(outputPath)


  SaveModes
  ---------
	1. errorifexists  (default)
	2. ignore
	3. append
	4. overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessible only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

  Working with different file formats
  -----------------------------------
   
     JSON
	  read
		df1 = spark.read.format("json").load(inputPath) 
		df1 = spark.read.json(inputPath) 
	  write
		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)

     Parquet
	  read
		df1 = spark.read.format("parquet").load(inputPath) 
		df1 = spark.read.parquet(inputPath) 
	  write
		df2.write.format("parquet").save(outputPath)
		df2.write.parquet(outputPath)

     ORC
	  read
		df1 = spark.read.format("orc").load(inputPath) 
		df1 = spark.read.orc(inputPath) 
	  write
		df2.write.format("orc").save(outputPath)
		df2.write.orc(outputPath)

     CSV (delimited text file)
	  read
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, sep="|", inferSchema=True)
	  write
		df2.write.csv(outputPath)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|")


  DataFrame transformation
  ------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequncy"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                 )


  2. where / filter

	df3 = df2.where("domestic = false and count > 1000")
	df3 = df2.filter("domestic = false and count > 1000")

	df3 = df2.where(col("count") > 1000)

  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")

	df3 = df2.orderBy(desc("count"), asc("origin"))


  4. groupBy => Returns a "pyspark.sql.group.GroupedData" object
	     => To get DF, apply some aggregation methods

	df3 = df2.groupBy("domestic", "highFrequncy").count()
	df3 = df2.groupBy("domestic", "highFrequncy").max("count")
	df3 = df2.groupBy("domestic", "highFrequncy").sum("count")
	df3 = df2.groupBy("domestic", "highFrequncy").avg("count")

	df3 = df2.groupBy("domestic", "highFrequncy") \
        	.agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		avg("count").alias("avg"))

  5. limit

	df2 = df1.limit(100)

  6. selectExpr

		df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 	"DEST_COUNTRY_NAME as destination",
                 	"count",
                 	"count + 10 as newCount",
                 	"count > 200 as highFrequncy",
                 	"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

  7. withColumn

	df3 = df1.withColumn("highFrequncy", expr("count > 200")) \
        	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        	.withColumn("count", expr("count * 10")) \
        	.withColumn("newCount", col("count") + 10)


	ageGroupDf = userDf.withColumn("ageGroup", when(col("age") < 13, "Child")
                                           	  .when(col("age") < 20, "Teenager")
                                                  .when(col("age") < 60, "Adult")
                                                  .otherwise("Senior"))

ageGroupDf.show()



   8. withColumnRenamed  

	df3 = df1.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")



   9. udf (user-defined-function)

	def get_age_group( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"

	get_age_group_udf = udf(get_age_group, StringType())    

	ageGroupDf = userDf.withColumn("ageGroup", get_age_group_udf(col("age")) )
	ageGroupDf.show()

        =============================================

	@udf(returnType = StringType())
	def get_age_group( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"

   	ageGroupDf = userDf.withColumn("ageGroup", get_age_group(col("age")) )
	
	 =============================================

	 def get_age_group( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"

	 userDf.createOrReplaceTempView("users")
	 spark.catalog.listTables()

	 spark.catalog.listFunctions()

	 spark.udf.register("get_age_group", get_age_group, StringType())

	 qry = "select id, name, age, get_age_group(age) as ageGroup from users"


   10. drop

	  df4 = df3.drop("newCount", "highFrequncy")

	  df4.printSchema()
	  df4.show(5)

   11. dropna

	  df5 = usersDf.dropna()
	  df6 = usersDf.dropna(subset=["age", "phone"])
	  df6.show()

   12. dropDuplicates

	 df4 = userDf.dropDuplicates()
	 df4.show()

	 df5 = userDf.dropDuplicates(["name", "age"])
	 df5.show()

   13. distinct()

	 df1.distinct().count()
	 df1.select("DEST_COUNTRY_NAME").distinct().count()

	Q: How many rows are there with uniques values for "DEST_COUNTRY_NAME" column?
		df1.select("DEST_COUNTRY_NAME").distinct().count()
		df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()

    14. union, intersect, subtract 

	 	df4 = df2.union(df3)
		df4.rdd.getNumPartitions()
		df4.show() # 15 rows
		df4.write.csv("E:\\PySpark\\output\\csv", mode="overwrite")

		df5 = df4.intersect(df3)
		df5.show()
		df5.rdd.getNumPartitions()

		df6 = df4.subtract(df3)
		df6.show()
		df6.rdd.getNumPartitions()

		
	        Note: The number of shuffle partitions of an output dataframe (created as a result of shuffle
		      operation such as subtract, intersect) is determined by "spark.sql.shuffle.partitions"
                      configuration property (of the sparkSession)

 		 -> spark.conf.set("spark.sql.shuffle.partitions", "10")
	         -> spark.conf.get("spark.sql.shuffle.partitions")

    15. sample

		df2 = df1.sample(True, 0.5)
		df2 = df1.sample(True, 0.5, 4646)
		df2 = df1.sample(True, 1.5, 4646)     # fraction > 1 allowed in withreplacement sampling

		df2 = df1.sample(False, 0.6)
		df2 = df1.sample(False, 0.6, 7676)
		df2 = df1.sample(False, 1.5, 4646)     # ERROR: fraction > 1 not allowed in without-replacement sampling
	

    16. randomSplit

		dfList = df1.randomSplit( [0.4, 0.3, 0.3], 4543534 )

		dfList[0].count()
		dfList[1].count()
		dfList[2].count()

    17. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(2, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()

    18. coalesce

	  	df6 = df2.coalesce(3)
		df6.rdd.getNumPartitions()

     19. join => discussed as separate topic.



  Creating an RDD from DF
  -----------------------
	rdd1 = df1.rdd
	rdd1.take(5)


  Creating a DF from programming data
  -----------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

	df1.show()
	df1.printSchema()


  Creating a DF frm RDD
  ----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()


  Creating DF with custom schema
  ------------------------------

	mySchema = StructType([ 
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True),
                StructField("age", IntegerType(), True) 
            ])

	df1 = rdd1.toDF(schema=mySchema)

        ================================================

        mySchema = StructType([ 
                StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
                StructField("DEST_COUNTRY_NAME", StringType(), True),
                StructField("count", IntegerType(), True) 
            ])

	df2 = spark.read.json(inputPath, schema=mySchema)
	df2 = spark.read.schema(mySchema).json(inputPath)


  Joins
  ------
	Supported joins => inner, left, right, full, left_semi, left_anti

	left_semi join
        --------------
	=> Is like inner join but the data comes from only the left-side table.
	=> Equivalent to the following sub-query:
		select * from emp where deptid in (select id from dept)

	left_anti join
	--------------
	=> Equivalent to the following sub-query:
		select * from emp where deptid not in (select id from dept)

	Explicitly enforcing a broadcast join
        -------------------------------------
	joinedDf = employee.join(broadcast(department), joinCol, "inner")


  Use-Case
  --------
    
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv datasets fetch the top 10 movies with highest average-user rating
      -> Consider only those movies that are rated by atleast 30 users
      -> data: movieId, title, numberOfRatings, averageRating
      -> Arrange the data in the DESC order of average-weight
      -> Save the output as a single pipe-separated CSV file with header.

    => Try it yourself...


   JDBC Format - MySQL Integration
   -------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


   Integrating Spark SQL with Apache Hive
   --------------------------------------

    => Hive a data warehousing platform built for Hadoop

   	-> Warehouse : A directory where hive maintains its data files
	-> Metastore : Stores the metadata information about the databases, tables etc. 


# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.catalog.currentDatabase()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


 =========================================
    Spark Streaming
 =========================================
  
   Two libraries
	1. Spark Streaming
	2. Structured Streaming (this is preferred)


  Spark Streaming (DStreams API)
  -------------------------------

     => microbatch based processing
     => Provides "seconds" scale latency.  (near-real-time processing)
     => does not support event-time processing


      StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 

	=> Sources: File, Socket, Rate, Kafka
	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch








   	












