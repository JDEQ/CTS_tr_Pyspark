
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - MySQL & Hive 
   Spark Streaming
	-> Structured Streaming	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ==============================================================

   Spark
   ------

	-> Spark is a unified in-memory distributed parallel computing framework

		in-memory computation -> Spark's allows you to persist intermediate results of tasks in memory
		so that subsequent tasks ca directly process these saved partitions. 

        -> Spark is written in SCALA language
	
	-> Spark is a polyglot
		-> Scala, Java, Python, R

	-> Spark appilication can run on multiple clusters managed by several cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes
  
  
  Spark unified stack
  -------------------

	-> Spark provides a consistent set of APIs for analyzing different analytics work loads on the same
	   execution engine using well defined dat abstractions.

		Batch Analytics			=> Spark SQL
		Streaming Analytics		=> Spark Streaming, Structured Streaming
		Predictive Analytics		=> Spark MLLib
		Graph Parallel Computations	=> Spark GraphX


  Getting started with Spark
  --------------------------
	
   1. Working on your Lab

   2. Setting up Spark dev environment on a personal machine. 
	
	-> Make sure you have Anaconda distribution installed. 
	-> Try 'pip install' in Anaconda command shell
		pip install pyspark
	-> If that is not working, try the instra=uction given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Sign-up to Databricks Community Edition
	
	Signup: https://www.databricks.com/try-databricks
		-> Make sure you select "Databrick Community Edition" link (in the second screen)

	Login: https://community.cloud.databricks.com/login.html


  Spark Architecture
  ------------------
   
   1. Cluster Manager

   2. Driver

   3. SparkContext

   4. Executor  
  
	
  RDD (Resilient Distributed Dataset)
  -----------------------------------

     => RDD is the fundamental data abstraction of Spark core.

     => RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects. 

     => RDDs are lazily evaluation

     => RDDs are immutable


  Creating RDDs
  -------------
	Three ways:

	1. Creating an RDD from some external data source  (such as a file)

		rdd1 = sc.textFile(<FilePath>, numPartitions)
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating an RDD from progrmmatic data

		rdd1 = sc.parallelize(<collection>, numPartitions)
		rdd1 = sc.parallelize([2,1,3,2,4,3,5,6,5,6,7,8,6], 3)


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------
	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


  RDD Lineage DAG
  ----------------

    RDD Lineage is a logical plan maintained by Driver
    RDD Lineage DAG is created when an RDD is created.
    RDD Lineage DAg tracks the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		rddFile Lineage : rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		rddWords Lineage : rddWords -> rddFile.flatMap -> sc.textFile
   	
	rddPairs = rddWords.map(lambda x: (x, 1))
  		rddPairs Lineage : rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
   		rddWc Lineage : rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

     
  RDD Transformations
  -------------------

    1. map		P: U -> V
			Object to object transformation
			Applies the function on each input object to transform it to the output object
			input RDD: N object, output RDD: N objects
   












  
