
  Agenda (PySpark)
  --------------------
  1. Python Crash Course - Basics of Python
  2. Spark - Basics & Architecture
  3. Spark APIs
  4. Spark Core - RDD API - Introduction & Basics
  5. Spark SQL in details
	Spark SQL Transformations
	Spark SQL Integrations with RDBMS & Hive

 ------------------------------------------------------
 
  Course Materials
  ----------------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> https://github.com/ykanakaraju/pyspark
	
   
  Installing Python Anaconda Distribution
  ---------------------------------------
	URL: https://www.anaconda.com/download
	https://www.anaconda.com/download#downloads

	Click on Download link
	Run the msi installer.

 

  Spark - Basics & Architecture
  -----------------------------
   
   Spark is a distributed computing framework for big data analytics.

     	-> Spark is written using Scala programming language

	-> Spark is an in-memory distributed computing framework

		Spark can persist intermediate partitions in-memory and subsequent tasks
		can directly process these stored partitions to advance the computations. 

        -> Spark is a unified framework.
		
		Spark provides a consistant set of APIs for processing different analytical
		workloads based on the same execution engine.

		Spark Core API (RDD)	-> Low level API for unstrauctured data processing
		Spark SQL (DataFrames)  -> Used for structured batch analytics
		Spark Streaming		-> Used for real-time data analytics
		Spark MLLib		-> Predictive analytics using Machine Learning
		Spark GraphX		-> Graph parallel computations.

	-> Spark is a polyglot		
		-> Supports Scala, Java, Python and R programming language.

	-> Spark can run on multiple clusters
		-> Local, Standalone Scheduler, YARN, Mesos, Kubernetes
           

   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client  : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


  RDD (Resilient Distributed Dataset)
  -----------------------------------

	-> RDD is the fundamental data abstraction of Spark.
	-> RDDs are Collection of distributed in-memory partitions.
	-> RDDs have to components
		RDD Loagical plan -> RDD Lineage DAG
		RDD runtime data  -> Collection of in-memory partitions

	-> RDDs are lazily evaluated.

	-> RDDs are immutable


  Creating RDDs
  -------------

    Three ways:

	1. Create an RDD from external dataset

		rddFile = sc.textFile(filePath, 4)

	2. Create an RDD from programmatics data

	        rdd1 = sc.parallelize( [3,2,1,4,2,4,6,5,3,5,4,5,7,8,5,7,8,9,6,7,1,2,1,2,3,2,5,0,2,3], 3 )

	3. By transforming existing RDDs.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------

    Two operations : 

	1. Transformations
		-> Returns an RDD
		-> Does not cause execution
		-> Causes creation of lineage DAGs by the driver

	2. Actions
		-> Triggers execution and launches Job on the cluster
		-> Returns some output
	

  RDD Lineage DAG
  ----------------
	
     RDD Lineage DAG is a logical plan on how to create the RDD
     RDD Lineage DAG maintains a hierarchy of depencies to create the RDD partitions (when asked for) all the way from the very first RDD.
	

	rddFile = sc.textFile(filePath, 4)
	Lineage DAG of rddFile : (4) rddFile -> sc.textFile on filePath

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage DAG of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile on filePath

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	


  Types of Transformations
  ------------------------
	-> Narrow transformations
		-> Transformations that does not cause data shuffle 	
		-> No data movement across the network 

	-> Wide transformations
		-> Transformations that does causes data shuffle 	
		-> Data moves across the network 

  RDD Transformations
  -------------------

   1. map		P: U -> V
			object to object transformation

		rddFile.map(len).collect()


   2. filter		P: U -> Boolean
			filter the objects into output based on the function

		rddFile.filter(lambda x: len(x) > 51).collect()

  3. glom		P: None
			Returns one list object per partition.
		
		rdd1		rdd2 = rdd1.glom()

		P0: 1,2,3,6,4 -> glom -> P0: [1,2,3,6,4]
		P1: 5,6,4,7,2 -> glom -> P1: [5,6,4,7,2]
		P2: 9,0,5,8,1 -> glom -> P2: [9,0,5,8,1]

		 rdd1.glom().map(lambda p: len(p)).collect()


  4. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1	    rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )

		P0: 1,2,3,6,4 -> mapPartitions -> P0: 16
		P1: 5,6,4,7,2 -> mapPartitions -> P1: 24
		P2: 9,0,5,8,1 -> mapPartitions -> P2: 23

		rdd1.mapPartitions(lambda p: [sum(p)]).collect()


  5. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions but gives the partition-index as an additional
			function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()


   6. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.

		 rddWords = rddFile.flatMap(lambda x: x.split(" "))

   Types of RDDs:
   --------------
	1. Generic RDDs: RDD[U]
	2. Pair RDD: RDD[(U, V)]


   7. mapValues		P: U -> V
			Applied only to Pair RDD
			Transforms only the value part of (K,V) pairs by applying the function


   8. distinct()	P: None, Optional: numPartitions
			Returns only distinct objects of the RDD.


   9. sortBy 		P: U -> V, Optional: ascending(True/False), numPartitions
 			Sorts the RDD based on function output

		rdd1.sortBy(lambda x: x%3, True, 5).glom().collect()

  NOTE:
  -----
	All ...ByKey transformations are wide transformations and applied only to Pair RDDs

  10: sortByKey	       P: None, Optional: ascending(True/False), numPartitions
		       Sorts the RDD by key	

		      rdd2.sortByKey(False, 2).glom().collect()

  11. groupBy		P: U -> V,  Optional: numPartitions
			Returns a Pair RDD where
				key: each unique value of the function output
				value: ResultIterable. The objects of the RD that created the key

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 ) \
        		.flatMap(lambda x: x.split()) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)


  12. groupByKey	P: None, Optional: numPartitions
			Applied to only Pair RDDs
			Groups the objects by Key

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 ) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

  13. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Applied to only Pair RDDs
			Reduces all the values of each unique key

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 ) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False, 1)


  14. repartition	P: numPartitions
			Used to increase or decrease the number of partitions of the RDD
			Causes global shuffle

		rdd2 = rdd1.repartition(5)
		rdd2 = rdd1.repartition(2)

  15. coalesce		P: numPartitions
			Used to only decrease the number of partitions of the RDD
			Causes partition merging

		rdd2 = rdd1.coalesce(2)


  16. partitionBy	P: numPartitions, Optional: partitioning function (default: hash)
			Applied only to pair RDDs
			Partitions the data based on the key applied to the partitioning function.
 

  RDD Persistence
  ---------------	
	rdd1 = sc.textFile(<dataPath>, 40)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )      
              --> instruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	rdd6 DAG => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile -> t3 -> t5 -> t6] -> rdd6 --> collected to the driver

	rdd7.collect()

	rdd7 DAG => rdd7 -> rdd6.t7
		[t7] -> rdd7 --> collected to the driver

	rdd6.unpersist()
		

     StorageLevels
     -------------
	MEMORY_ONLY   		-> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK 	-> Disk Memory Serialized 1x Replicated 
	DISK_ONLY		-> Disk Serialized 1x Replicated 
	MEMORY_ONLY_2		-> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated 

    Commands
    --------
	rdd1.cache()     -> MEMORY_ONLY   
	rdd1.persist()   -> MEMORY_ONLY   
	rdd1.persist( StorageLevel.MEMORY_AND_DISK ) 

	rdd1.unpersist()

			
  RDD Actions
  -----------

  1. collect

  2. count
 
  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			reduces an entire RDD to one object of the same type by iterativly applying the
			function first on each partition and then across results of each partition.


		rdd1 

		P0: 1,2,3,6,4 -> reduce -> 16 -> reduce -> 63
		P1: 5,6,4,7,2 -> reduce -> 24
		P2: 9,0,5,8,1 -> reduce -> 23

		rdd1.reduce(lambda x, y: x + y)

>>> rddWc.collect()
[('hadoop', 25), ('flatmap', 12), ('hdfs', 6), ('sadas', 1), ('das', 6), ('spark', 40), ('asd', 5), ('scala', 28), ('hive', 19), ('transformations', 10), ('d', 1), ('map', 6), ('groupby', 6), ('flume', 6), ('oozie', 6), ('sqoop', 6), ('mapreduce', 6), ('rdd', 43), ('actions', 10)]

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )

   5. take

		rdd1.take(20)

   6. takeOrdered

		rddWords.takeOrdered(30, len)
		rddWords.takeOrdered(30)

   7. takeSample
		
		rdd1.takeSample(True, 20)
		rdd1.takeSample(True, 20, 457)

		rdd1.takeSample(False, 20)
		rdd1.takeSample(False, 20, 457)

   8. countByValue

   9. countByKey


  Use-case
  --------
    Dataset: <github>/data_git/cars.tsv

	From cars.tsv file, find the average weight of all models of each make from among American cars.
	-> Arrange the data in the desc order of average weight
	-> Save the output in a single text file.

	=> Try to solve the problem yourself


  Spark-Submit
  ------------
    Spark-Submit is a single command to submit any spark application (Scala, Java, Python, R)
    to any cluster manager (local, spark standalone, YARN, Mesos, Kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]


  =====================================
      Spark SQL (pyspark.sql)
  =====================================

   -> High-level API built on top of Spark Core API 
   -> Spark's structured data processing API

	Data formats:  Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC format:   RDBMS & NoSQL databases
	Hive format:   Hive Warehouse

   SparkSession
   ------------
	-> Starting point of execution for Spark SQL 
	-> Represents a user session inside an application. 

	from pyspark.sql import SparkSession

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate()  

   DataFrame
   ---------
	DataFrame is a collection of distributed in-memory partitions.
	DataFrames are immutable and lazily evaluated. 

	DataFrame have two components:
	
		Data   : Collection of 'Row' objects 
		Schema : 'StructType' object

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)


  Basic steps in creating a Spark SQL application
  -----------------------------------------------

   1. Create a dataframe from some data source

	inputPath = "E:\\PySpark\\data\\users.json"
	df1 = spark.read.format("json").load(inputPath)
	df1 = spark.read.load(inputPath, format="json")
	df1 = spark.read.json(inputPath)

	df1.show()
	df1.printSchema()


   2. Apply transformations on the dataframe using transformation methods or using SQL

	Applying dataframe transformation methods 
        -----------------------------------------
	df2 = df1.select("userid", "name", "age", "phone", "gender") \
         	.filter("age is not null") \
         	.orderBy("gender", "age") \
         	.groupBy("age").count() \
         	.limit(4)

	Applying SQL to transform dataframes
	------------------------------------
	df1.createOrReplaceTempView("users")
	spark.catalog.listTables()

	spark.catalog.dropTempView("users")  => To drop a temp view

	qry = """select age, count(*) as count
			 from users
			 where age is not null
			 group by age
			 order by count"""

	df3 = spark.sql(qry)
	df3.show()

   3. Save/write the dataframe to some external file/database.

	outputPath = "E:\\PySpark\\output\\json"
	df3.write.format("json").save(outputPath)
	df3.write.save(outputPath, format="json")
	df3.write.json(outputPath)

 SaveModes
 ---------
    => Defines the behaviour when saving a DF to an existing directory

	-> ErrorIfExists (default)
	-> Ignore
	-> Append
	-> Overwrite

	df3.write.json(outputPath, mode="overwrite")
	df3.write.mode("overwrite").json(outputPath)


 LocalTempViews & GlobalTempViews
 --------------------------------
 	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

		df1.createOrReplaceTempView("users")

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

 		df1.createOrReplaceGlobalTempView("gusers")


 DataFrame Transformation API Methods
 ------------------------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")
	df2.show()	

	---------------------

	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"),
                  expr("DEST_COUNTRY_NAME as destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                )
	

  2. where / filter

	df3 = df2.where("highFrequency = false and count > 100")
	df3 = df2.filter("highFrequency = false and count > 100")

	df3 = df2.filter( col("count") > 1000 )
	df3.show()


  3. orderBy / sort

	df3 = df2.orderBy("count", "ORIGIN_COUNTRY_NAME")
	df3 = df2.sort("count", "ORIGIN_COUNTRY_NAME")
	df3 = df2.orderBy(col("count").desc(), col("ORIGIN_COUNTRY_NAME").asc())
	df3 = df2.orderBy(desc("count"), asc("ORIGIN_COUNTRY_NAME") )


  4. groupBy -> Returns a 'pyspark.sql.group.GroupedData' object
		Use some aggregation method to return a DataFrame.

	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg(   count("count").alias("count"), 
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 1).alias("avg"))



	df3.show()

  5. limit

	df2 = df1.limit(10)

  6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                  "DEST_COUNTRY_NAME as destination",
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

  7. withColumn & withColumnRenamed

	df2 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	-----------------

	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 65),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	usersDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	usersDf.show()

	userGroupsDf = usersDf.withColumn("ageGroup", when(col("age") < 13, "child")
						.when(col("age") < 20, "teenager")
						.when(col("age") < 60, "adult")
						.otherwise("senior"))

  8. udf (user defined function)


	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"

	get_age_group_udf = udf(getAgeGroup, StringType())

	userGroupsDf = usersDf.withColumn("ageGroup", get_age_group_udf(col("age")) )

	userGroupsDf.show()

	-----------------------------------

	@udf(returnType = StringType())
	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"

	#get_age_group_udf = udf(getAgeGroup, StringType())

	userGroupsDf = usersDf.withColumn("ageGroup", getAgeGroup(col("age")) )

	userGroupsDf.show()
       
        --------------------------------------

	spark.udf.register("get_age_group", getAgeGroup, returnType=StringType())

	qry = """select id, name, age, get_age_group(age) as ageGroup from users"""

	spark.sql(qry).show()
	spark.catalog.listFunctions()

   
  9. drop	=> drops/excludes one or more columns

	df3 = df2.drop('newCount', 'highFrequency')

	df3.show()
	df3.printSchema()


  10. dropna	=> drops rows with null values

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	usersDf.dropna(subset=["age", "phone"]).show()


  11. dropDuplicates	=> Drops the duplicate rows

 
	listUsers = [(1, "Raju", 5),
				 (1, "Raju", 5),
				 (3, "Raju", 5),
				 (4, "Raghu", 35),
				 (4, "Raghu", 35),
				 (6, "Raghu", 35),
				 (7, "Ravi", 70)]

	usersDf = spark.createDataFrame(listUsers).toDF("id", "name", "age")

	usersDf.show()

	usersDf.dropDuplicates().show()
	usersDf.dropDuplicates(["name", "age"]).show()


  12. distinct => returns distinct rows of the DataFrame

	usersDf.distinct().show()

	''' How many unique DEST_COUNTRY_NAME values are there in df1 '''

	df1.select("DEST_COUNTRY_NAME").distinct().count()
	df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()


  13. randomSplit => Splits the dataframe into multiple DFs randomly in the specified weights.

	dfList = df1.randomSplit([0.5, 0.3, 0.2], 7575)

	print( dfList[0].count(), dfList[1].count(), dfList[2].count() )


  14. sample

	df2 = df1.sample(True, 0.5)      # True: with-replacement, 0.5 is the fraction
	df2 = df1.sample(True, 0.5, 53)  # 53 is a seed
	df2 = df1.sample(True, 1.5, 53)  # fraction > 1 is allowed

	df2 = df1.sample(False, 0.5)      # False: with-out-replacement, 0.5 is the fraction
	df2 = df1.sample(False, 0.5, 53)  # 53 is a seed
	df2 = df1.sample(False, 1.5, 53)  # ERROR: fraction > 1 is NOT allowed	


  15. union, intersect, subtract

	df2 = df1.where("count > 1000")
	df2.count()  # 14 rows
	df2.show()
	df2.rdd.getNumPartitions()

	df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
	df3.count()  # 1 row
	df3.show()
	df3.rdd.getNumPartitions()

	-- union --
	df4 = df2.union(df3)
	df4.count()
	df4.rdd.getNumPartitions()


	spark.sql.shuffle.partitions
	----------------------------
	=> Is a configuration property that determines the number of partitions of a dataframe
	   created by a shuffle operation.

   		spark.conf.get("spark.sql.shuffle.partitions")
		spark.conf.set("spark.sql.shuffle.partitions", "10")

  16. repartition

	df2 = df1.repartition(6)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(3)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()

	df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
	df5.rdd.getNumPartitions()


  17. coalesce

	df3 = df2.coalesce(1)
	df3.rdd.getNumPartitions()

  18. Joins  => discussed as a separate topic


  19, partitionBy  -> to be discussed

  20. buckettedBy  -> to be discussed



  Working with Databricks
  -----------------------
   Sign up: https://www.databricks.com/try-databricks
   Log-in : https://community.cloud.databricks.com/login.html


  Working with different file formats
  -----------------------------------

   JSON

	read
	-----
	df1 = spark.read.format("json").load(inputPath)
	df1 = spark.read.load(inputPath, format="json")
	df1 = spark.read.json(inputPath)

	write
	-----
	df3.write.format("json").save(outputPath)
	df3.write.save(outputPath, format="json")
	df3.write.json(outputPath)

  Parquet

	read
	-----
	df1 = spark.read.format("parquet").load(inputPath)
	df1 = spark.read.load(inputPath, format="parquet")
	df1 = spark.read.parquet(inputPath)

	write
	-----
	df3.write.format("parquet").save(outputPath)
	df3.write.save(outputPath, format="parquet")
	df3.write.parquet(outputPath)

  ORC

	read
	-----
	df1 = spark.read.format("orc").load(inputPath)
	df1 = spark.read.load(inputPath, format="orc")
	df1 = spark.read.orc(inputPath)

	write
	-----
	df3.write.format("orc").save(outputPath)
	df3.write.save(outputPath, format="orc")
	df3.write.orc(outputPath)

 CSV
	read
	----
	df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputPath)
	df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
	df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
	-----
	df2.write.mode("overwrite").csv(outputPath, header=True)
	df2.write.mode("overwrite").csv(outputPath, header=True, sep="|")


 Text
	read
	----
	df1 = spark.read.text(inputPath)
	df1.printSchema()
  



  Create an RDD from a DataFrame
  ------------------------------
	rdd1 = df1.rdd


  Create a DataFrame from programmatic data
  -----------------------------------------

	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers)
	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])

	df2.show()
	df2.printSchema()

  
  Create a DataFrame from RDD
  ----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1.show()

	df1 = rdd1.toDF(["id", "name", "age"])
	df1.show()


  Create a DataFrame with custom/programmatic schema
  --------------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)

	--------------------------

	mySchema1 = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)])

	df1 = spark.createDataFrame(listUsers, schema=mySchema1)

	--------------------------

	mySchema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"  
	df1 = spark.read.json(inputPath, schema=mySchema)

	---------------------------

	mySchema1 = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)])

	df1 = spark.read.schema(mySchema1).json(inputPath)


  Joins
  -----

   Supported Joins: inner, left, right, full, left_semi, left_anti, cross
	
    left_semi join
    --------------
	-> Is like inner join, but fetches the data only from the left table
	-> Is equivalent to the following sub-query

		select * from emp where deptid in (select id from dept)
	
    left_anti join
    --------------
	-> Is equivalent to the following sub-query

		select * from emp where deptid not in (select id from dept)

    SQL approach
    ------------
	employee = spark.createDataFrame([
		(1, "Raju", 25, 101),
		(2, "Ramesh", 26, 101),
		(3, "Amrita", 30, 102),
		(4, "Madhu", 32, 102),
		(5, "Aditya", 28, 102),
		(6, "Pranav", 28, 100)])\
	  .toDF("id", "name", "age", "deptid")
	  
	department = spark.createDataFrame([
		(101, "IT", 1),
		(102, "ITES", 1),
		(103, "Opearation", 1),
		(104, "HRD", 2)])\
	  .toDF("id", "deptname", "locationid")

	employee.show()
	department.show()

	spark.catalog.listTables()

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*, dept.*
			from emp full outer join dept
			on emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)

	joinedDf.show()


    Using DF Transformation method
    -------------------------------

	joinCol = employee.deptid == department.id
	joinedDf = employee.join(department, joinCol)
	joinedDf = employee.join(department, joinCol, "left")
	joinedDf = employee.join(department, joinCol, "left_semi")
	joinedDf = employee.join(department, joinCol, "left_anti")

	joinedDf.show()



    Join Strategies
    ---------------
	-> Shuffle Joins (big table to big table)
		-> Shuffle hash Join
		-> Sort merge join

	-> Broadcast Joins (big table to small table)
		-> Controlled by "spark.sql.autoBroadcastJoinThreshold" property


	Enforcing broadcast join:
		df3 = df1.join(broadcast(df2), joinCol, joinType)


  Use-case-1
  -----------
    Dataset: <github>/data_git/cars.tsv    => USING SPARK SQL

	From cars.tsv file, find the average weight of all models of each make from among American cars.
	-> Arrange the data in the desc order of average weight
	-> Save the output in a csv file.

	=> Try to solve the problem yourself


  Use-case-2
  ----------

      Datasets: <github>/data_git/movielens/<files>
 
      From movies.csv and ratings.csv datasets, fetch the top "10 movies with heighest
      average user rating"

	-> Consider only those movies that have atleast 30 user-ratings
	-> Data:  movieId, title, totalRatings, averageRating
	-> Sort the data in the DESC order of the averageRating
	-> Round the averageRating to 4 decimal places (X.XXXX)
	-> Save the output as a single pipe-separated CSV file. 

	.. to be solved ..








	
  





















	

  


































          
  
    

   











  








































  



   







