POC:  Sneha - 2108194

 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> RDD shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	-> Dstreams API (introduction)


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 
  Spark
  ----- 

      Spark is wriiten in SCALA programming language

      Spark is a unified in-memory distributed computing framework.  
   
      Computing Cluster : A unified entity that consitutes many nodes whose cumulative resources 
      (RAM, CPU Cores, Disk) can be use to distribute your storage or processing.

      In-memory => The intermediate results (partitions) can be persisted in RAM and subsequent
      transformations that read those in-memory partitions (assuming you have enough RAM)
      

      Spark Unified Stack
	Spark comes with a unified set of APIs for performing computations on different Analytics
        Workloads.
	
		Batch Analytics of Unstructured data	-> Spark Core API
		Batch Analytics of Structured data	-> Spark SQL
		Stream Analytics			-> Spark Streaming
        	Predictive Analytics		        -> Spark MLlib
		Graph Parallel Computation		-> Spark GraphX
       

     Spark supports multiple cluster managers
        -> Local
	-> Spark Standalone
        -> YARN
	-> Mesos
	-> Kubernetes
     
     Spark is a polyglot
	-> Scala  (native language)
	-> Java
	-> Python 
	-> R


   Getting started with PySpark
   ----------------------------

    1. Working in your vLab

        -> Login in to Ubuntu VM
	
	  1.1 PySpark Shell
		=> pyspark   (type on a terminal)

	  1.2 Spyder IDE   
		=> Search for spyder and launch
		=> If spyder is not found, you can install it
			pip install spyder
			sudo apt install spyder

	  1.3 Jupyter notebooks
		=> Open a terminal
		=> Type :
			jupyter notebook
			jupyter notebook --allow-root

    2. Setup PySpark environment on your local machine	 

	 -> Make sure "Anaconda" is installed and configured. 
	 -> Try: "pip install pyspark"
	    OR
	 -> Follow the instructions given in the shared document
	   
  
    3. Databricks Community Edition  (Free Signup)

	     https://docs.databricks.com/getting-started/community-edition.html
	     
	     Signup link: https://www.databricks.com/try-databricks
		=> Make sure you click on "Community Edition" option 

	     Login: https://community.cloud.databricks.com/login.html


		Downloading output from Databricks
                ----------------------------------

		
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

		Example:
		/FileStore/tables/output/carsFeb23/part-00000
		https://community.cloud.databricks.com/files/tables/output/carsFeb23/part-00000?o=1072576993312365#tables/new/dbfs



   Spark Architecture (Building Blocks)
   ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks is reported to the driver.


    RDD (Resiliant Distributed Dataset)
    -----------------------------------

	=> Fundamental data abstraction for Spark Core API

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data     => A colletion of distributed in-memory partitions.
		           -> Each partition is a collection of objects.

        => RDD partitions are immutable

        => RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Action commands triggers exection


    Creating RDDs
    --------------

       Three ways

	1. Create an RDD from some external data source (such as a text file)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		=> default partitions: sc.defaultMinPartitions
				       [Equal to 2 if cores allocated >= 2, else 1]

	2. Create an RDD from programmatic data (such as a collection)

		rdd1 = sc.parallelize( [1,2,1,3,3,5,3,5,7,8,4,7,8], 2)

		=> default partitions: sc.defaultParallelism
				       [Equal to the number of cores allocated to your app]

	3. By applying transformations on existsing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))

    RDD Operations
    --------------

	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


    RDD Lineage DAG
    ---------------

	=> Lineage DAG is a logical plan of an RDD which maintains the hierarchy of dependencies all the 
           way upto very first RDD

        => Lineage DAGs are created when you perform a transformation

	=> Lineage DAGs are maintained by the Driver         

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage: (4) rdd1 -> sc.textFile  (E:\\Spark\\wordcount.txt)

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		Lineage: (4) rddWords -> rddFile.flatMap -> sc.textFile

        rddPairs = rddWords.map(lambda x: (x, 1))
		Lineage: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile
    
	rddWordCount = rddPairs.reduceByKey(lambda x, y: x + y)
		Lineage: (4) rddWordCount -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

   
  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Persistence
  ---------------

	rdd1 = sc.textFile( .., 3 )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )         -> instruction to save the rdd6 partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	Lineage DAG: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		-> [textFile, t3, t5, t6]  -> rdd6

	rdd7.collect()
	Lineage DAG: rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		-> [t7]  -> rdd7

	rdd6.unpersist()	


        Storage Levels
        --------------	
	1. MEMORY_ONLY	        => default. Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK      => Disk Memory Serialized 1x Replicated
        3. DISK_ONLY            => Disk Serialized 1x Replicated
        4. MEMORY_ONLY_2	=> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2    => Disk Memory Serialized 2x Replicated

	Commands
	--------

	rdd1.cache()     -> im-memory persistence
	rdd1.persist()   -> im-memory persistence
	rdd1.persist(StorageLevel.MEMORY_AND_DISK) -> using custom storage level

	rdd1.unpersist()
	

   Executors Memory Structure
   --------------------------
        Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Transformations
  -------------------
   => Transformations return a RDD.
   => Does not cause execution.


  1. map		P: U -> V
			Transforms each input object to corresponding output object	
			input RDD: N objects, output RDD: N objects		

		rdd2 = rddFile.map( lambda x: x.split(" ") )


  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be there in the output
			input RDD: N objects, output RDD: <= N objects	

		rdd2 = rdd1.filter(lambda x: x > 10)

  
  3. glom		P: None
			Returns a list object for every partition with all the elements of the partition
			input RDD: N objects, output RDD: equal to number of partitions

	rdd1			rdd2 = rdd1.glom()

	P0: 3,2,1,4,7  -> glom -> P0: [3,2,1,4,7]
	P1: 6,0,7,1,1  -> glom -> P1: [6,0,7,1,1]
	P2: 5,1,3,2,4  -> glom -> P2: [5,1,3,2,4]

	rdd1.count() = 15 (int)   rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			Flattens all the elements of the Iterables produced by the function
			input RDD: N objects, output RDD: >= N object

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Apply the transformation on the entire partition to produce the corresponding
			output partition.

	rdd1		rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )

	P0: 3,2,1,4,7  -> mapPartitions -> P0: 17
	P1: 6,0,7,1,1  -> mapPartitions -> P1: 15
	P2: 5,1,3,2,4  -> mapPartitions -> P2: 15

	rdd1.mapPartitions(lambda a: map(lambda x: x*10, a) ).glom().collect()


  6. mapPartitionsWithIndex  	P: Int, Iterable[U] -> Iterable[V]
			  Similar to mapPartitions, but you get partition index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x), p)).filter(lambda x: x[0] == 1).map(lambda x: x[1]).collect()

   
  7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD

		rddWords.distinct().collect()
		rddWords.distinct(3).collect()

        Types of RDDs
        -------------

	=> Generic RDDs: RDD[U]
              Pair RDDs: RDD[(K, V)]


  8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the valur part (only) of the Pairs by applying the function
			input RDD: N objects, output RDD: N object

		rdd4.mapValues(list).collect() 

  9. sortBy		 P: U -> V,  Optional: ascending (True/False), numPartitions
			 Sorts the RDD objects based on the function output.
			 input RDD: N objects, output RDD: N object

		rdd1.sortBy(lambda x: x%5).glom().collect()
		rdd1.sortBy(lambda x: x%5, False).glom().collect()
		rdd1.sortBy(lambda x: x%5, True, 5).glom().collect()

  10. groupBy		  P: U -> V    Optional: numPartitions
			  Will return a Pair RDD where
				key: unique values of the function output
				value: Objects that produced the key - ResultIterable
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
          		.flatMap(lambda x: x.split(" ")) \
          		.groupBy(lambda x: x) \
          		.mapValues(len) \
          		.sortBy(lambda x: x[1], False, 1)

  11. randomSplit	  P: List of weights (eg: [0.6, 0.4, 0.4])
			  Returns a list of RDDs split approximatly in the specified ratios randomly.

  
  12. repartition	  P: numPartitions
			  Returns an RDD with specified number of partitions
			  Can increase or decrease the number of partitions
			  Use global shuffling	  


  13. coalesce		  P: numPartitions
			  Returns an RDD with specified number of partitions
			  Can only decrease the number of partitions
			  Uses partition-merging

    Recommendations
    ---------------
     => Size of the partition should be 100 MB to 1 GB (ideal: 128 MB) 
     => The number of partitions should be a multiple of number of cores 
     => If the number of partitions is close to but less than 2000, bump it up to 2000.
     => Number of CPU cores per executor should be 5


  14. union, intersection, subtract, cartesian
   
	Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   command				partitions & type
           ------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide
   
 
  15. partitionBy       P: numPartitions, Optional: partitioning-function  (default: hash)
			Applied only on pair RDDs
			Partitioning is performed based on the 'key'

	 transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)

  
  ..ByKey Transformations
  -----------------------
	-> Wide transformations
	-> Applied only to PairRDD

  16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
			Sorts the objects based on the key.

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()
		rdd2.sortByKey(False, 4).glom().collect()


  17. groupByKey	P: None, Optional: numPartitions
			Returns a PairRDD where:
			  key: unique-key
			  value: Grouped values.

			WARNING: Avoid groupByKey.	

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
          		.flatMap(lambda x: x.split(" ")) \
          		.map(lambda x: (x, 1)) \
          		.groupByKey() \
          		.mapValues(sum) \
          		.sortBy(lambda x: x[1], False, 1)

    18. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Reduce values of each unique first for each partition and then across the
			outputs of partitions.

		wordcount = text_file.flatMap(lambda line: line.split(" "))  \
                		.map(lambda word: (word, 1)) \
                		.reduceByKey(lambda a, b: a + b, 1)

    19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))

		
   Use-Case
   --------
	From cars.tsv dataset, find the average-weight of all models of each make of American origin cars
	-> Arrange the data in the descending order of average weight
	-> Save the output in a single text file.

	Please try it yourself


  RDD Actions
  ===========

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one value of the same type by iterativly applying the
			function first at each partition and then on the outputs of each partition.
		rdd1			

		P0: 1,2,1,3,5,6,1 -> reduce ->  -17 => 4
		P1: 2,3,1,4,5,7,1 -> reduce ->  -19
		P2: 9,5,3,2,1,0,0 -> reduce ->  -2 
	
		rdd1.reduce( lambda x, y : x - y ) = 4
   
		rddWc.reduce(lambda a, b: (a[0] + "," + b[0], a[1] + b[1]) )

   5. aggregate		
	
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. first

   7. take(n)

   8. takeOrdered(n)

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)


   9. takeSample	=> rdd1.takeSample(withReplacement(True/False), n)
			   rdd1.takeSample(True, 10)

		  rdd1.takeSample(True, 10)		-> withReplacement sampling
		  rdd1.takeSample(True, 10, 345)	-> withReplacement sampling with seed
		  rdd1.takeSample(False, 100, 645)	-> withOutReplacement sampling with seed

  10. countByValue

  11. countByKey

  12. foreach	=> does not return anything
		   runs a function on all the objects of the RDD.

		rddWc.foreach(lambda p: print("Key: {0}, Value:{1}".format(p[0], p[1])))

  13. saveAsSequenceFile


    
   Spark-Submit command
   --------------------

   Is a single command to submit any Spark application (Java, Scala, Python or R) to any cluster manager (local, 
   spark standalone, yarn, mesos, kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt wordcount_output 2

	spark-submit --master yarn   \
		--deploy-mode cluster \
		--driver-memory 2G \
		--driver-cores 2 \
		--executor-memory 5G \
		--executor-cores 5 \
		--num-executors 10 \
		E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt wordcount_output 2


   Closures
   --------
	-> A closure represenrs all the code (variables and methods) that must be visible
	   inside each executor for the tasks to perform their computations on the RDD partitions.

        -> A closure is serialized and a separate copy of it is sent to every single executor.
		
		c = 0

		def f1(n):
		   if ( isPrime(n) )  c += 1
		   return n*10

		def isPrime(n):
		    return True if n is Prime
		    return False if n is not Prime

		rdd1 = sc.parallelize( range(1, 4001), 4 )
		rdd2 = rdd1.map( f1 )

		print( c )               // 0
   
		rdd2.collect()

	Limitation: local variables (which are part of closure) can not be used to implement
		    global counters. 

        Solution: Use "Accumulator" variables to implement global counters.

	


   Shared Variables
   ----------------

    1. Accumulators
	-> Is a shared variable that is accessible from all the executors.
	-> Not part of the closure
	-> Is maintained by the driver
	-> All tasks can write to this variable. 


		c = sc.accumulator(0)

		def f1(n):
		   global c
		   if ( isPrime(n) )  c.add(1)
	           return n*10

		def isPrime(n):
		    return True if n is Prime
		    return False if n is not Prime

		rdd1 = sc.parallelize( range(1, 4001), 4 )
		rdd2 = rdd1.map( f1 )

		print( c.value )               // 80
   
		rdd2.collect()



    2. Broadcast Variables
	-> A broadcast variable is used to save memory
	-> Driver sends one copy of the broadcast variable to each executor
	-> All tasks in that executor-node can access that single copy.
	-> Broadcast variable is not part of the closure.

		d = {1:a, 2:b, 3:c, 4:d, 5:e, 6:f, .......}    # 100 MB
		bc = sc.broadcast( d )

		def f1(n):
		   global bc
		   return bc.value[n]

		rdd1 = sc.parallelize([1,2,3,4,5,6,,7,8,9,10,...], 4)

		rdd2 = rdd1.map( f1 )

		rdd2.collect()      -> a,b,c,d,....
		

 ================================================
    Spark SQL   (pyspark.sql)
 ================================================
   
   -> Is a high-level API built on top of Spark Core.
   -> Is used to analyze structured data.
	
	Structured File Formats: Parquet (default), ORC, JSON, CSV
	JDBC Format: RDBMS, NoSQL
	Hive Format: Hive

   SparkSession
   ------------
	-> Starting point of execution for Spark SQL API
	-> Represents a user-session running inside (within the scope of) an application (sparkcontext)
	-> We can have multiple sessions running inside an application
     
	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate() 


	spark.conf.set(<config-param>, <value>)
	spark.conf.set("spark.sql.shuffle.partitions", 5)

  
   DataFrame (DF)
   --------------
	-> Is a collection on distributed in-memory partitions that are immutable and lazily evaluated. 
	-> Contains "Row" objects     (pyspark.sql.Row)

	DataFrame has two components
		-> Data : Collection of Rows
		-> Schema : StructType object

		StructType(
			List(
			   StructField(age,LongType,true),
			   StructField(gender,StringType,true),
 			   StructField(name,StringType,true),
			   StructField(phone,StringType,true),
			   StructField(userid,LongType,true)
			)
		)


   Steps working with Spark SQL
   ----------------------------

    1. Read/Load the source data into a dataframe

		inputPath = "E:/PySpark/data/users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)
	
		df1.show()
		df1.printSchema()

    2. Apply transformations on the DF using DF Transformation methods or using SQL

		using DF Transformation methods
		--------------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
         		.filter("age is not null") \
         		.sort("gender", "age") \
         		.groupBy("age").count() \
         		.limit(4)


		using SQL
		---------
	    	df1.createOrReplaceTempView("users")    # create a tempView
		spark.catalog.listTables()

		qry = """select age, count(*) as count
         		from users
         		where age is not null
         		group by age
         		order by count
         		limit 4"""

		df2 = spark.sql(qry)
		df2.show()

		spark.catalog.dropTempView("users")    # drop a tempView


    3. Write/Save the contents of the DF to some destination.  

		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)


  LocalTempView & GlobalTempView
  ------------------------------
        LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


  SaveModes
  ----------

	errorifexsists  (default)
	ignore	
	append
	overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  DF Transformations
  ------------------

   1. select

		df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")


		df2 = df1.select( col("DEST_COUNTRY_NAME").alias("destination"),
                  	column("ORIGIN_COUNTRY_NAME").alias("origin"),
                  	expr("count"),
                  	expr("count + 10 as newCount"),
                  	expr("count > 200 as highFreuency"),
                  	expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))


   2. where  / filter

		df3 = df2.where("highFrequency = True and domestic = False")
		df3 = df2.where( col("count") > 200 )	

   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.orderBy(col("count").desc(), col("origin").asc())
		df3 = df2.orderBy(desc("count"), asc("origin"))


		df3 = df2.sort(desc("count"), asc("origin"))

   4. groupBy   -> returns a pyspark.sql.group.GroupedData object
		-> Apply an aggregation method to return a DataFrame

		df3 = df2.groupBy("highFrequency", "domestic").count()
		df3 = df2.groupBy("highFrequency", "domestic").sum("count")
		df3 = df2.groupBy("highFrequency", "domestic").max("count")
		df3 = df2.groupBy("highFrequency", "domestic").avg("count")

		df3 = df2.groupBy("highFrequency", "domestic") \
        		.agg(   sum("count").alias("sum"),
              			max("count").alias("max"),
              			count("count").alias("count"),
              			round(avg("count"), 2).alias("avg") )
   5. limit 
		df3 = df2.limit(10) 
    
   6. selectExpr

	df2 = df1.select( expr("DEST_COUNTRY_NAME as destination"),
                  expr("ORIGIN_COUNTRY_NAME as origin"),
                  expr("count"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFreuency"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

	IS SAME AS:

	df2 = df1.selectExpr( "DEST_COUNTRY_NAME as destination",
                  "ORIGIN_COUNTRY_NAME as origin",
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFreuency",
                  "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

   7. withColumn

		df3 = df1.withColumn("newCount", expr("count + 10")) \
        		.withColumn("highFrequency", expr("count > 200")) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        		.withColumn("count", col("count").cast("int")) \
        		.withColumn("country", lit("India"))

		df4 = df3.withColumn("ageGroup", when(col("age") < 13, "child")
                                		.when(col("age") < 20, "teenager")
                                		.when(col("age") < 60, "adult")
                                		.otherwise("senior") )

   8. withColumnRenamed

		df3 = df1.withColumn("newCount", expr("count + 10")) \
        		.withColumn("highFrequency", expr("count > 200")) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        		.withColumn("count", col("count").cast("int")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


  9. udf  (user defined function)

	   def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"

	   getAgeGroupUdf = udf(getAgeGroup, StringType())
    
	   df4 = df3.withColumn("ageGroup", getAgeGroupUdf( col("age") ) )

           ---------------------------------------------------
	   @udf(returnType = StringType())
	   def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"
    
	   df4 = userDf.withColumn("ageGroup", getAgeGroup( col("age") ) )
           ---------------------------------------------------

	   spark.catalog.listFunctions()

     	   spark.udf.register("get_age_group", getAgeGroup, StringType())

	   qry = "select id, name, age, get_age_group(age) as ageGroup from users"
	   df4 = spark.sql(qry)
	   df4.show()


   10. drop

		df3 = df2.drop("newCount", "highFrequency") 
		df3.show(10)

   11. dropna

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()
	
		df3 = usersDf.dropna()
		df3 = usersDf.dropna(subset=['phone', 'age'])

   12, dropDuplicates

		listUsers = [(1, "Raju", 5),
             			(1, "Raju", 5),
             			(3, "Raju", 5),
             			(4, "Raghu", 35),
             			(4, "Raghu", 35),
             			(6, "Raghu", 35),
             			(7, "Ravi", 70)]

		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
		userDf.show()

		df3 = userDf.dropDuplicates()
		df3 = userDf.dropDuplicates(["name", "age"])

		df3.show()

    13. distinct

		df3 = userDf.distinct()
		df3.show()

    14. sample
		df2 = df1.sample(True, 0.5)
		df2 = df1.sample(True, 0.5, 4564)
		df2 = df1.sample(True, 1.5, 4564)    -> Allowed

		df2 = df1.sample(False, 0.5, 4564)
		df2 = df1.sample(False, 1.5, 4564)   -> Error: fraction should in in range [0,1]

    15. randomSplit

		dfArr = df1.randomSplit([0.6, 0.4], 4543)
		print( dfArr[0].count(), dfArr[1].count() )

    16. repartition

		df4 = df1.repartition(4)
		df4.rdd.getNumPartitions()

		df3 = df4.repartition(3)
		df3.rdd.getNumPartitions()

		df4 = df1.repartition(4, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df1.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()

    17. coalesce 

		df3 = df5.coalesce(3)
		df3.rdd.getNumPartitions()

    18. join  => taken as a separate topic



   show() command
   --------------
	df1.show()
	df1.show(30)
	df1.show(20, False)
	df1.show(truncate = False)


   Working with different file formats
   -----------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)
		df2.write.mode("overwrite").json(outputPath)
		df2.write.json(outputPath, mode="overwrite")

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath)
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")



  Create an RDD from a DafaFrame
  -------------------------------
	rdd1 = df1.rdd
	rdd1.take(5)


  Create a DF from programmatic data
  ----------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

	df2.show()
	df2.printSchema()



  Create a DF from an RDD
  -----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df2 = rdd1.toDF()
	df2 = rdd1.toDF(["id","name", "age"])

	df2.show()
	df2.printSchema()


  Create a DF with Programmatic Schema
  ------------------------------------ 

	mySchema = StructType([
            StructField("id", IntegerType(), True), 
            StructField("name", StringType(), True), 
            StructField("age", IntegerType(), True), 
        ])

	df2 = rdd1.toDF(schema = mySchema)

	df2.show()
	df2.printSchema()
   
        ----------------------------------------------
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True), 
            StructField("DEST_COUNTRY_NAME", StringType(), True), 
            StructField("count", IntegerType(), True), 
        ])


	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)

	df1.show()
	df1.printSchema()


   Joins
   -----

     Supported Joins: inner, left (left_outer), right (right_outer), full (full_outer), left_semi, left_anti.
 
     left_semi join
     --------------
	-> Similar to inner join, but data comes only from left side table.
	-> Equivalent to the folowing subquery:
		select * from emp where deptid in (select id from dept)


     left_anti join
     --------------
	-> Equivalent to the folowing subquery:
		select * from emp where deptid not in (select id from dept)


  ***  Adaptive Query Execution   ==> Look into this topic
       -> Introduced in Spark 3.x



  JDBC Format - Working with MySQL
  --------------------------------
   import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()

  Integrating with Hive
  ---------------------
# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listDatabases()
spark.catalog.currentDatabase()
spark.catalog.listTables()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL


moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


    Use-Case: Movie Data Analysis
    -----------------------------
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv files, find the top 10 movies with highest average user rating.
    -> Consider only those movies with rating count > 30
    -> Data required: movieId, title, totalRatings, averageRating
    -> Arrange the data in the DESC order of averageRating
    -> Save the output as a single pipe-separated CSV file with header.

    => Please try it yourself.
   
  
 ===================================
    Spark Streaming
 ===================================

   
     Spark's streaming data (real-time) analytics API

     Two libraries
	1. Spark Streaming (DStreams API)
	2. Structured Streaming (this is preferred)

  
    Spark Structured Streaming
    -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	-> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 

	=> Sources: File, Socket, Rate, Kafka
	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch 










































   


