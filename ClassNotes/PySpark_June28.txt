
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - MySQL & Hive 
   Spark Streaming
	-> Structured Streaming	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ==============================================================

   Spark
   ------

	-> Spark is a unified in-memory distributed computing framework

		in-memory computation -> Spark's allows you to persist intermediate results of tasks in memory
		so that subsequent tasks ca directly process these saved partitions. 

        -> Spark is written in SCALA language
	
	-> Spark is a polyglot
		-> Scala, Java, Python, R

	-> Spark appilication can run on multiple clusters managed by several cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes
  
  
  Spark unified stack
  -------------------

	-> Spark provides a consistent set of APIs for analyzing different analytics work loads on the same
	   execution engine using well defined dat abstractions.

		Batch Analytics			=> Spark SQL
		Streaming Analytics		=> Spark Streaming, Structured Streaming
		Predictive Analytics		=> Spark MLLib
		Graph Parallel Computations	=> Spark GraphX


  Getting started with Spark
  --------------------------
	
   1. Working on your Lab

	-> Try opening the email link in Edge browser (if Chorme is not working for you)
	-> Follow instaructions given in the email.
	-> You will land up on Windows server (click 'Remind me later' link if asks for Updates)
	-> Check your user-id in the "HadoopBig...." word document
	-> Click on "Oracle VM Virtualbox" icon and double click on the VM to lauch the Ubuntu lab

	-> Inside your Ubuntu lab:
		-> Open PySpark shell
			-> Open a terminal and type "pyspark"

		-> Open "Spyder" ide.	
			-> A spark app is already setup as reference.

   2. Setting up Spark dev environment on a personal machine. 
	
	-> Make sure you have Anaconda distribution installed. 
	-> Try 'pip install' in Anaconda command shell
		pip install pyspark
	-> If that is not working, try the instra=uction given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Sign-up to Databricks Community Edition 
	
	Signup: https://www.databricks.com/try-databricks
		-> Make sure you select "Databrick Community Edition" link (in the second screen)

	Login: https://community.cloud.databricks.com/login.html


  Spark Architecture
  ------------------
   
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
  
	
  RDD (Resilient Distributed Dataset)
  -----------------------------------

     => RDD is the fundamental data abstraction of Spark core.

     => RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects. 

     => RDDs are lazily evaluation

     => RDDs are immutable


  Creating RDDs
  -------------
	Three ways:

	1. Creating an RDD from some external data source  (such as a file)

		rdd1 = sc.textFile(<FilePath>, numPartitions)
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating an RDD from progrmmatic data

		rdd1 = sc.parallelize(<collection>, numPartitions)
		rdd1 = sc.parallelize([2,1,3,2,4,3,5,6,5,6,7,8,6], 3)


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------
	Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


  RDD Lineage DAG
  ----------------

    RDD Lineage is a logical plan maintained by Driver
    RDD Lineage DAG is created when an RDD is created.
    RDD Lineage DAg tracks the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		rddFile Lineage : rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		rddWords Lineage : rddWords -> rddFile.flatMap -> sc.textFile
   	
	rddPairs = rddWords.map(lambda x: (x, 1))
  		rddPairs Lineage : rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
   		rddWc Lineage : rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  RDD Persistence
  ---------------     
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist(StorageLevel.DISK_ONLY)  --> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	rdd6 DAG: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collect

	rdd7.collect()
	rdd7 DAG: (4) rdd7 -> rdd6.t7
		[t7] -> collect


	Storage Levels
        ---------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated		
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	---------
	1. cache    -> in-memory (Memory Serialized 1x Replicated)
	2. persist  -> can take storage-level as an argument.
	3. unpersist	


   Executors Memory Structure
   --------------------------
        Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  Types of Transformations
  ------------------------

     Types of transformtions

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformations
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

     
  RDD Transformations
  -------------------

    1. map		Parameter: U -> V
			Object to object transformation
			Applies the function on each input object to transform it to the output object
			input RDD: N object, output RDD: N objects

		rddFile.map(lambda s: s.split(" ")).collect()

    2. filter		P: U -> Boolean
			Only those those objects for which the function returns True, will be there in the
			output.
			input RDD: N object, output RDD: <= N objects

		rddFile.filter(lambda s: len(s.split(" ")) > 8).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition


		rdd1		      rdd2 = rdd1.glom()
		P0: 2,1,3,2,4 -> glom -> P0: [2,1,3,2,4]
		P1: 5,6,7,5,7 -> glom -> P1: [5,6,7,5,7]
		P2: 0,8,5,2,3 -> glom -> P2: [0,8,5,2,3]		

	       rdd1.count() = 15 (int)	rdd2.count() = 3 (list)

   		rdd1.glom().map(sum).collect()

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function. 
			input RDD: N object, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation.

		rdd1.mapPartitions(lambda p: [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but given partition-index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect() 
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct		P: None, Optional: numPartitions
			Returns the distinct objects of the RDD.
	
		rddFile.flatMap(lambda x: x).distinct().collect()
  
   Types of RDDs
   --------------
	1. Generic RDD:   RDD[U]		
	2. Pair RDD:	  RDD[(K, V)]		

   
   8. mapValues		P: U -> V
			Applied only to Pair RDDs
			Transforma only the value part of (K,V) pairs by applying the function

		rddWc.mapValues(lambda x: x*10).collect()


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the function output.
		
		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rdd1.sortBy(lambda x: x > 3, True, 2).glom().collect()


   10. groupBy		P: U -> V, Optional: numPartitions
			Returns a pair RDD where
				key: Each unique value of the function output
				value: A ResultIterable containing objects of the RDD grouped for the key.

		rddWords.groupBy(lambda x: x).mapValues(len).collect()

   11. randomSplit	P: list of weights (ex: [0.6, 0.4])
			Returns a list of RDDs split randomlt in the specified weights.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 64657)

   12. repartition	P: numPartitions
			Can be used to increase or decrease the number of output partititons
			Performs global shuffle

   13. coalesce		P: numPartitions
			Can be used to only decrease the number of output partititons
			Performs partition-merging
	
	Recommendations
  	---------------
	-> The size of the RDD partitions should be between 100 MB and 1 GB. 
	   Ideally, the size should be 128 MB (if you are using Hadoop)

        -> The number of partitions should be a multiple of number of Cores

	-> If the number of partitions is close to but less than 2000, bump it up to 2000.

	-> The number of CPU cores per executor should be 5


   14. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on PairRDDs
			The partitioning happens based on the key of the (K,V) pairs


   	transactions = [
    	 	{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 	{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 	{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 	{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 	{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 	{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 	{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 	{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 	{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 	{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


  15. union, intersection, subtract, cartesian
   
	Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   command				partitions & type
           ------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide


  ..ByKey transformations
  -----------------------
    -> Are wide transformations
    -> Are applied only to PairRDDs

  16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
		   	   Sorts the objects of the RDD by key.

		rdd4.sortByKey().glom().collect()
		rdd4.sortByKey(False).glom().collect()
		rdd4.sortByKey(False,2).glom().collect()


  17. groupByKey	P: None, Optional: numPartitions
			Groups the objects of the RDD by key
			Returns a Pair RDD
				key: Each unique of of input RDD
				value: ResultIterable with grouped values.

			WARNING: Avoid groupByKey if possible

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
				.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
				.groupByKey() \
				.mapValues(sum) \
				.coalesce(1)

   18. reduceByKey	P: (U, U) -> U,   Optional: numPartitions
			Reduces all the values of each unique key within each partition in the first stage
			and across partition in the second stage.

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y, 1)

    19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])



   RDD Actions
   ------------


  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one value (of the same type) by iterativly applying the
			function on each partition in the first stage and across partitions in the next stage.

			rdd1
			P0: 2,1,4,5,8 -> reduce -> 20 -> reduce -> 57
			P1: 5,7,3,2,1 -> reduce -> 18
			P2: 9,1,0,2,7 -> reduce -> 19

			n = rdd1.reduce(lambda x,y: x + y)

  5. aggregate
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))
			













			
	
	






























   






 































  
