
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> RDD shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	-> Dstreams API (introduction)


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

  Spark
  -----
     -> Used for big-data analytics.

     -> Spark is a in-memory distributed computing framework.

	 -> in-memory means intermediate results of transformations can be saved in memory (RAM)
	    and subsequent transformations can process those savs results. 

     -> Spark is a unified framework. 
	   
           -> Provides a set of consistent APIs for processing different analytical workloads using
	      the same execution engine and well-defines data abstractions.
	
		Batch Analytics	of unstructured data 	=> Spark Core API (RDD)
		Batch Analytics	of structured data 	=> Spark SQL (DataFrames)
          	Stream Analytics (real-time)		=> Structured Streaming, DStreams
		Predictive Analytics (ML)		=> Spak MLlib
		Graph Parallel Computations		=> Spark GraphX

      -> Spark is written in Scala language.

      -> Spark is a poliglot
	   => Supports scala, java, python & R

      -> Spark is very fast
	   -> Spark is 100 times faster than MapReduce if you use 100% in-memory
	   -> Spark is 6 to 7 times faster than MapReduce if you use disk based computation
  
  
  Getting started with Spark
  --------------------------

    1. Working in your vLab	
	  1.1 PySpark Shell
		=> pyspark   (type on a terminal)

	  1.2 Spyder IDE   
		=> Search for spyder and launch
		=> If spyder is not found, you can install it
			pip install spyder
			sudo apt install spyder

	  1.3 Jupyter notebooks
		=> Open a terminal
		=> Type :
			jupyter notebook
			jupyter notebook --allow-root
	
    2. Setup PySpark environment on your local machine.
	2.1 Make sure you have anaconda distribution installed.
		https://docs.anaconda.com/anaconda/install/

	2.2 Follow the step from this document:
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition
	     https://docs.databricks.com/getting-started/community-edition.html
	     
	     Signup link: https://www.databricks.com/try-databricks
		=> Make sure you click on "Community Edition" option 
	     Login: https://community.cloud.databricks.com/login.html

	
   Spark Architecture (Building Blocks)
   ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


  Spark Core API
  --------------
    => Is the low-level API
    => Takes care of all low-level operations
	-> memory-managenement
	-> monitoring
	-> fault-recovery.
    => RDD is the main data abstraction.
	
   
    RDD (Resilient distributed dataset)
    -----------------------------------

	=> Is the fundamental data abstraction of Spark

	=> RDD has two components

	   RDD metadata => RDD Lineage DAG (logical plan) maintained by Driver
	   RDD data => A colletion of distributed in-memory partitions.
		       -> Each partition is a collection of objects.

	=> RDDs are lazily evaluated
		-> RDD transformations does not cause execution
		-> RDD Action commands trigger execution.

	=> RDDs are immutable


   Creating RDDs
   -------------
    Three ways:

	1. Create an RDD from some external data file.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The number of partitions is based on 'sc.defaultMinPartitions'

	2.  Create an RDD from programmatic data

		rdd1 = sc.parallelize( range(1, 100), 4 )
		rdd2 = sc.parallelize( [1,1,2,3,2,4,5,6,4,6,7,7,8,8,0], 2 )

		rdd1 = sc.parallelize( range(1, 100))
		-> The number of partitions is based on 'sc.defaultParallelism'

	3. By applying transformations on existsing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))

	Note: To get the number of partitions of an RDD use:

		rddFile.getNumPartitions()


   RDD Operations
   --------------
     2 operations

	1. Transformations
		-> Create the RDD lineage DAGs 
		-> Does not trigger the actual execution
		-> Every transformation returns an other RDD.

	2. Actions
		-> Trigges execution.
		-> Converts the logical plan to physical plan and sends the tasks to the executors.

   RDD Lineage DAG
   ---------------
	=> Is the meta data of an RDD maiantained by the driver	

        rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
	     	Lineage DAG: (4) rdd1 -> sc.textFile on "E:\\Spark\\wordcount.txt"

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
		Lineage DAG: (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))	
		Lineage DAG: (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
		Lineage DAG: (4) rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile

		Tasks: (sc.textFile, flatMap, map, reduceByKey)

	rdd4.collect()


   RDD Persistence
   ===============

	rdd1 = sc.textFile( <File>, 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )        => instruction to the Spark to save rdd6 partition.
	rdd7 = rdd6.t7(...)

	rdd6.collect()

		Lineage DAG of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile -> t3 -> t5 -> t6] -> [rdd6]

	rdd7.collect()
	
		Lineage DAG of rdd7: rdd7 -> rdd6.t7
		[t7] -> [rdd7]	

	rdd6.unpersist()

	StorageLevel
	------------
	1. MEMORY_ONLY	      	=> default. Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	=> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		=> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	=> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2    => Disk Memory Serialized 2x Replicated		

	Commands
	--------
	rdd1.cache()     			=> in-memory persistence
	rdd1.persist()   			=> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()
	

   Executor's memory structure
   ===========================
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   =========================

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 

   
   RDD Transformations
   ===================

    Note: A transformation returns an RDD.

    1. map 		P: U -> V
			Transforms each input object to output object by applying the function.
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: x.split(" ") ).collect()

    2. filter		P: U -> Boolean 
			Only those input objects for which the function returns True will be in the output
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) > 51).collect()

    3. glom		P: None
			Returns one list object per partition with all the objects of the partition. 


		rdd1		rdd2 = rdd1.glom()

		P0: 1,4,2,5,7,8 -> glom -> P0: [1,4,2,5,7,8]
		P1: 5,7,2,1,4,3 -> glom -> P1: [5,7,2,1,4,3]
		P2: 9,0,5,7,1,4 -> glom -> P2: [9,0,5,7,1,4]

		rdd1.count() = 18 (int)	  rdd2.count() = 3 (list)
	
  		rdd1.glom().map(len).collect()

    4. flatMap		P: U -> Iterable[V]     (iterable -> sth that you can loop through)
			flatMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: >= N objects

		 rddFile.flatMap(lambda x: x.split(" ")).collect()


   5. mapPartitions	P: Iterable[U] : Iterable[V]
			Take an entire input partition as function input and returns the output partition
			partition to partition transformation.

		rdd1	 rdd2 = rdd1.mapPartitions( lambda p: [sum(p)] ) 						

		P0: 1,4,2,5,7,8 -> mapPartitions -> P0: 27
		P1: 5,7,2,1,4,3 -> mapPartitions -> P1: 22
		P2: 9,0,5,7,1,4 -> mapPartitions -> P2: 26

		rdd1.mapPartitions(lambda p: [sum(p)]).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()

   6. mapPartitionsWithIndex	p: Int, Iterable[U] : Iterable[V]
				Similar to mapPartitions, but we get the partition-index as an additional function
				parameter.

		rdd1.mapPartitionsWithIndex( lambda i, p : [(i, sum(p))] ).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x), p)) \
		    .filter(lambda t: (t[0] == 1)) \
                    .map(lambda x: x[1]) \
                    .collect()

   7. distinct		p: None, Optional: numPartitions
			Returns an RDD with distinct objects of the RDD.
			input RDD: N objects, output RDD: <= N objects

		rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(4)
		
   Types of RDDs
   -------------
	Generic RDDs:  RDD[U]
	Pair RDDs:     RDD[(K, V)] 	

	 
   8. mapValues		p: U -> V
			Applied only on pair RDDs
			Transforms only the 'Value' part by applying the function.
			input RDD: N objects, output RDD: N objects

		rdd3 = rdd2.mapValues(lambda x: (x, x+10))
		-> where rdd2 MUST be a pair RDD

   9. groupBy		p: U -> V, Optional: numPartitions
			Returns a pair RDD where:
				key: each unique function output 
				value: ResultIterable containing all the objects of the RDD that produced the key.
		
		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           		   .flatMap(lambda x: x.split(" ")) \
           		   .groupBy(lambda x: x, 1) \
           		   .mapValues(len)

   10. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			The objects of the RDD are sorted based on the value of the funcion output
			All the objects with the same function output will always be there in the same partition.

		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           			.flatMap(lambda x: x.split(" ")) \
           			.groupBy(lambda x: x) \
           			.mapValues(len) \
           			.sortBy(lambda x: x[1], False, 1)

    11. randomsplit	P: List of weights (ex: [0.5, 0.3, 0.2])
			Returns a list of RDDs split randomly in the specified weights 

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 464)


    12. repartition	P: numPartitions
			Is used to increase or decrease in number of partitions of the output RDD
			Causes global shuffle

		rddWords6 = rddWords.repartition(6)
		rddWords3 = rddWords6.repartition(3)

    13. coalesce	P: numPartitions
			Is used to decrease in number of partitions of the output RDD
			Cause partition-merging
	
		rddWords6 = rddWords.coalesce(6)

	Recommendations
        ---------------
	-> The size of each partition should be between 100 MB to 1 GB (ideally 128 MB)
	-> If the number of partitions is close to but less than 2000, bump it up to 2000
	-> The number of CPU cores per executor should be 5


    14. partitionBy	P: numPartitions, Optional: partitioning-function  (default: hash)
			Applied only on pair RDDs
			Partitioning is performed based on the 'key'

	 transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)



    15. union, intersection, subtract, cartesian

	   Let us say we have rdd1 with M partitions & rdd2 with N partitions.

	   command				partitions & type
           ------------------------------------------------------
	   rdd1.union(rdd2)			M + N, narrow
	   rdd1.intersection(rdd2)		M + N, wide
	   rdd1.subtract(rdd2)			M + N, wide
	   rdd1.cartesian(rdd2)			M * N, wide


    ..ByKey Transformations
    -----------------------
  	-> Are wide transformation
	-> Are applied only on Pair RDDs

    16. sortByKey	=> to be discussed.
	   

		
   
  RDD Actions
  ===========

   1. collect

   2. count

   3. saveAsTextFile

   






















