
  Kanakaraju (Trainer) => kanakaraju.y2@cognizant.com


  PySpark Curriculum
  ---------------------------------------	
    Spark Basics & Architecture
    Spark Core Basics (RDD)
    Spark Submit Command
    Spark Shared Variables
    Spark SQL DataFrame Operations
    Spark SQL Window Functions
    Spark SQL with JDBC/RDBMS
    Spark SQL with Hive
    Spark Structured Streaming

  PySpark on AWS Curriculum
  ----------------------------------------
     AWS Lambda & AWS CloudWatch
     (Integrating with with SNS, S3 & DynamoDB)
     AWS EMR
     (Cluster provisioning and running PySpark using Step Execution & Spark Submit) 
     AWS Glue & AWS Athena
     (Crawlers, Jobs, Workflows & ETL using PySpark)
     AWS Step Functions Basics
     Case-Study: Integrating AWS services
       -> Integrating Lambda, Glue & PySpark

   Pre-requisites
   --------------
	=> Python
	=> Basics of AWS - IAM, S3 etc.
  
  Materials
  ---------
        => PDF Presentations
	=> Code Module
	=> Class Notes Files
	=> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----

     -> Spark is a framework we use for big data analytics

     -> Spark is an in-memory distributed computing framework

     -> Spark is written in SCALA programming language

	
	   -> Spark is 100x times faster than MapReduce using 100% in-memory computation
	   -> Spark is 6 to 7 times faster than MapReduce even with disk based computation.

     	   Computing Cluster :  A collection of nodes whose cumulative resources can be used to distributed your stores
                         	and processing. 

     -> Spark is a unified framework.
	
         -> Spark provides a consistent set of APIs based on the same execution engine to process different analytical workloads.

	  Spark APIs
	  -----------
		1. Spark Core API    	-> Low level API, RDD API. Used for unstructured data processing
		2. Spark SQL		-> Structured data process
		3. Spark Streaming      -> Spark Streaming (legacy) and Structured Streaming (current)
		4. Spark MLlib		-> Machine learning workloads
		5. Spark GraphX		-> Graph Parallel computations.

     -> Spark is a polyglot
	  -> Spark apps can be written using Scala, Java, Python and R 

     -> Spark applications can run on mutiple cluster manager
	   -> local, spark standalone, YARN, Mesos, Kubernetes 
	

   Getting strated with Spark
   --------------------------

	1. Using vLab provided to you.

	2. Installing Spark on your personal machine. 
	
		-> Make sure to install Anaconda Distibution (such Anaconda Navigator)
		-> Make sure you are running Java 8 or above (JDK 1.8 or above)
		-> Open Anaconda Command Prompt and install pyspark
			pip install pyspark  

		-> If pip is not working, you can follow the instructions given in the shared document.

		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


	3. Signup to Databricks Community Edition. 

		Signup: https://www.databricks.com/try-databricks
		-> Click on "Continue with Community Edition" link (Do not click on Continue) in the second screen. 


    Spark Architecture
    -------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client  : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.

   
   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
	-> Fundamental data abstraction of Spark Core API

	-> Is a collection of distributed in-memory partitions.
		-> Each partition is a collection of objects (of some type)

	-> RDDs are immutable

	-> RDDs are lazily evaluted
		-> Transformations does not cause executions.
		-> Actions trigger execution. 
   

   Creating RDDs
   -------------

    Three ways to create an RDD:

	1. From some external data sources such as text files.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	 	default number of partitions => sc.defaultMinPartitions = 2, if cores >= 2, else 1.
 
	2. From programmatic data

		rdd1 = sc.parallelize(range(1, 100), 3)

		default number of partitions => sc.defaultParallelism = number of cores allocated

	3. By performing transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)
		


   RDD Operations
   --------------

	Two types of operations:

	1. Transformations
		-> Create an RDD
		-> Does not cause executions. Does not start jobs on the cluster.
		-> Only lineage DAGs are maintained by the driver

	2. Actions
		-> Executes the RDD
		-> Creates a physical plan and sends jobs to the cluster.


   RDD Lineage DAGs
   ----------------
    RDD Lineage DAG is maintained by driver for every RDD
    RDD lineage is crated when an RDD is created (by a data loading commands or transformations)
    RDD lineage DAG contains the heirarchy of dependencies all the way from the very first RDD.
	

	rddFile = sc.textFile( file, 4 )
	Lineage DAG of rddFile : (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage DAG of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWords : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile



  Types of RDD Transformations
  -----------------------------
	
	1. Narrow Transformations

           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations

           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<filePath>, 20)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   ----> instruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7(..)

	rdd6.collect()

	lineage of rdd6 => (20) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   transformations: [sc.textFile, t3, t5, t6] -> collected

        rdd7.collect()

	lineage of rdd7 => (20) rdd7 -> rdd6.t7
	   transformations: [t7] -> collected

	rdd6.unpersist()


   Storage Levels
   --------------

	MEMORY_ONLY	   => default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK    => Disk Memory Serialized 1x Replicated
	DISK_ONLY	   => Disk Serialized 1x Replicated
	MEMORY_ONLY_2      => Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2  => Disk Memory Serialized 2x Replicated

	
   Commands
   --------
	rdd1.cache()    => in-memory persistence
	rdd1.persist()  => in-memory persistence
	rdd1.persist( StrorageLevel.DISK_ONLY )

	rdd1.unpersist()


  RDD Transformations
  --------------------
   
  1. map		P: U -> V
			Object to object transformation. 
			Input RDD: N objects, Ouput RDD: N objects

		rddFile = sc.textFile("G:\\Spark\\wordcount.txt", 3)
		rdd2 = rddFile.map(lambda x: len(x.split(" ")))

 
  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output.
			Input RDD: N objects, Ouput RDD: <= N objects

   		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Returns a list object with all the objects of each partition


		rdd1		rdd2 = rdd1.glom()
		P0: 1,2,1,3,2,4 -> glom -> P0: [1,2,1,3,2,4]
		P1: 4,5,3,6,7,6 -> glom -> P1: [4,5,3,6,7,6]
		P2: 4,7,9,0,9,0 -> glom -> P2: [4,7,9,0,9,0]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			flatten the iterables produced by the function
		
		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions 	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation	

		rdd1	rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )
		P0: 1,2,1,3,2,4 -> mapPartitions -> P0: 15
		P1: 4,5,3,6,7,6 -> mapPartitions -> P1: 31
		P2: 4,7,9,0,9,0 -> mapPartitions -> P2: 29


		rdd1.mapPartitions(lambda p: [sum(p)]).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()


  6. mapPartitionsWithIndex    P: Int, Iterable[U] -> Iterable[V]
			 Similar to mapPartitions but you get the partition-index as additional fn parameter.

		rdd2 = rdd1 \
			.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x) , p)) \
			.filter(lambda x: x[0] == 1) \
			.map(lambda x: x[1])


  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD

		rddWords.flatMap(lambda x: x).distinct().collect()


   Types of RDDs
   -------------
	
	1. Generic RDDs   : RDD[U]		
	2. Pair RDDs      : RDD[(K, V)]		
	


  8. mapValues		P: U -> V
			Applied only to Pair RDDs
			Transforms only the value part of the (K, V) pairs by applying the function (on the value) 

  		rdd3.mapValues(lambda x: sum(x)).collect()

  9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the elements of the RDD based on the function output.

			rddWords.sortBy(lambda x: x[-1], False).collect()
			rdd1.sortBy(lambda x: x%2, True, 4).glom().collect()


  10. groupBy		P: U -> V, Optional: numPartitions   
			Groups the objects of the RDD based on funtion output.
			Returns a Pair RDD, where
				key: each unique value of the function output
				value: ResultIterable - grouped RDDs objects that produced the key. 

    

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 12) \
          		.flatMap(lambda x: x.split()) \
          		.groupBy(lambda x: x) \
          		.mapValues(len) \
          		.sortBy(lambda p: p[1], False, 1)

  11. repartition	P: numPartitons
			Used to increase or decrease the number of output partitions.
			Causes global shuffle



  12. coalesce		P: numPartitons
			Used to only decrease the number of output partitions.
			Causes partition merging

	
	Recommendations
	---------------
	
	-> The size of each partition should be between 100 MB to 1 GB.
	   -> If you are using hadoop, use 128 MB partitions (ideal)
	-> The number of partitions should be a multiple of numbr of cores allocated to your application.
	-> The number of cores per executor should be 5.


  13. partitionBy	P: numPartitions, Optional: partitioning-function (default: hash)
			Applied only to pair RDDs. Partitioning happens based on the keys. 	

		rdd3 = rdd1.map(lambda x: (x, 2342)).partitionBy(3).map(lambda x: x[0])


  ..ByKey Transformations
  -----------------------
	-> Are wide transformtions
	-> Are applied to Pair RDDs only


  14. sortByKey		P: None, Optional: ascending (True/False), numPartitions
			Applied only to pair RDDs.
			Sorts the RDD based on the key.

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(False, 3).glom().collect()


  15. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where
				key: each unique key of the RDD objects
				value: grouped values of each unique key (ResultIterable)

			=> Avoid groupByKey if possible.

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 12) \
          		.flatMap(lambda x: x.split()) \
                	.map(lambda x: (x, 1)) \
          		.groupByKey() \
          		.mapValues(sum) \
          		.sortBy(lambda p: p[1], False, 1)


  16. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Reduce all the values of each key by iterativly applying the function	

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 12) \
          		.flatMap(lambda x: x.split()) \
                	.map(lambda x: (x, 1)) \
          		.reduceByKey(lambda x, y: x + y) \
          		.sortBy(lambda p: p[1], False, 1)



  Use Case
  ---------
		
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, get the average weight of all the models of each make 
	of American origin cars. Arrange in the DESC order of average weight. Save the
	output as a single text file. 

	=> Please try it yourself. 

 

  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one value of the same type
			
			rdd1
			P0: 2,3,5,8,0 -> reduce -> 18 -> reduce -> 58
			P1: 3,7,6,4,1 -> reduce -> 21
			P2: 9,4,3,2,1 -> reduce -> 19

			rdd1.reduce(lambda x, y: x + y)

			------------------------------------

			rddWc.collect()
			[('hadoop', 25), ('flatmap', 12), ('hdfs', 6), ('sadas', 1), ('das', 6), ('spark', 40), ('asd', 5), ('scala', 28), ('hive', 19), ('transformations', 10), ('d', 1), ('map', 6), ('groupby', 6), ('flume', 6), ('oozie', 6), ('sqoop', 6), ('mapreduce', 6), ('rdd', 43), ('actions', 10)]

			rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]))


  ========================
       Spark SQL
  ========================

        -> High Level API built on top of Spark Core API

	-> Spark's Structured data processing API

		File Formats: Parquet (default), ORC, JSON, CSV (delimited text), Text
		JDBC Format: RDBMS & NoSQL
		Hive Format: Hive Warehouse


   SparkSession
   ------------
	-> Starting point of execution for Spark SQL
	-> Represents a user-session (with its own configurations) within an application.

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()

	spark.conf.set("spark.sql.shuffle.partitions", "5")



   DataFrame (DF)
   ---------------
	-> Is a collection of distributed in-memory partitions
        -> Immutable and lazily-evaluated. 

	DataFrame has two components:	
		-> Data    : Collection of Row objects
		-> Schema  : StructType object

			StructType(
			     List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			     )
			)


  Basic Steps in a Spark SQL Application
  --------------------------------------

      1. Create a DataFrame from some external/internal data source.

		df1 = spark.read.format("json").load(inputPath)		
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)


      2. Apply transformations on the DF using DF Transformation methods or using SQL


	   Using DF Transformation methods
           -------------------------------
		df2 = df1.select("userid", "name", "gender", "age", "phone") \
        		.where("age is not null") \
       			.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


    	   Using SQL Approach
           -------------------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(1) as count
        		from users
        		where age is not null
        		group by age
        		order by age
        		limit 4"""        
        
		spark.sql(qry).show()


      3. Save the DF to some external destination (such as a file or database)
	
   		df3.write.format("json").save(ouputPath)
		df3.write.save(ouputPath, format="json")
		df3.write.json(ouputPath)

		df3.write.mode("overwrite").json(ouputPath)

  
  LocalTempView & GlobalTempView
  ------------------------------

	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")

	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView ("gusers")		

  SaveModes
  ---------
    
    => Controls the behaviour when you are writing to an existing directory.

	1. errorIfExists  (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.json(ouputPath, mode="ignore")
	df3.write.mode("ignore").json(ouputPath)


  DF Transformations
  -------------------

   1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME",
                 	"DEST_COUNTRY_NAME",
                 	"count")

	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"),
                  column("DEST_COUNTRY_NAME").alias("destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                )

   2. where / filter 

	df3 = df2.filter("highFrequency = true and destination = 'United States'")
	df3 = df2.where("highFrequency = true and destination = 'United States'")

	df3 = df2.where( col("count") > 1000 )


   3. orderBy / sort

	df3 = df2.sort("count", "origin")
	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))


   4. groupBy  => returns pyspark.sql.group.GroupedData object
		  apply an aggregation function to return a DataFrame

	df3 = df2.groupBy("domestic", "highFrequency").count()
	df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	df3 = df2.groupBy("domestic", "highFrequency").avg("count")
	df3 = df2.groupBy("domestic", "highFrequency").max("count")

	df3 = df2.groupBy("domestic", "highFrequency") \
        	.agg( 	count("count").alias("count"),
              		sum("count").alias("sum"),
              		round(avg("count"), 2).alias("avg"),
              		max("count").alias("max")
		    )


   5. limit 

		df2 = df1.limit(10)


   6. selectExpr

		df2 = df1.selectExpr( "ORIGIN_COUNTRY_NAME as origin",
                  "DEST_COUNTRY_NAME as destination",
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"
                )


  7. withColumn  &  withColumnRenamed

 	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


	df4 = df3.withColumn("ageGroup", when( col("age") < 13, "child")
                                	.when( col("age") < 20, "teenager")
                                	.when( col("age") < 60, "adult")
                                	.otherwise("senior"))


  8. udf  (user defined function)

	

	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"
		
	get_age_group = udf(getAgeGroup, StringType())

	df4 = df3.withColumn("ageGroup", get_age_group( col("age") ))
		
	df4.show()    

	------------------------------------------

	@udf (returnType = StringType())
	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"


	df4 = df3.withColumn("ageGroup", getAgeGroup( col("age") ))

	------------------------------------------

	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"

	spark.udf.register("get_age_group", getAgeGroup, StringType())

	qry = "select id, name, age, get_age_group(age) as ageGroup from users"
	df5 = spark.sql(qry)
	df5.show()

  

  9. drop => drops (excludes) columns

	df3 = df2.drop("newCount", "highFrequency")
	df3.show()


  10. dropna  => drops Rows with null values in any or specified columns.


	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	df3 = usersDf.dropna(subset=["phone", "age"])
	df3.show()


  11. dropDuplicates => Drops rows with duplicate values (in all or specified columns)

		listUsers = [(1, "Raju", 5),
				 (1, "Raju", 5),
				 (3, "Raju", 5),
				 (4, "Raghu", 35),
				 (4, "Raghu", 35),
				 (6, "Raghu", 35),
				 (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	df3 = userDf.dropDuplicates()

	output
	+---+-----+---+
	| id| name|age|
	+---+-----+---+
	|  3| Raju|  5|
	|  6|Raghu| 35|
	|  1| Raju|  5|
	|  7| Ravi| 70|
	|  4|Raghu| 35|
	+---+-----+---+


	df3 = userDf.dropDuplicates(["name", "age"])

	output
	+---+-----+---+
	| id| name|age|
	+---+-----+---+
	|  1| Raju|  5|
	|  4|Raghu| 35|
	|  7| Ravi| 70|
	+---+-----+---+

  
  12. randomSplit  => Splits a DF randomly in the specified weights.

	dfList = df1.randomSplit([0.6, 0.4])  		 => every time output will be different
	dfList = df1.randomSplit([0.6, 0.4], 354)	 => seed: 354, gets same output across multiple executitons.
	print( dfList[0].count(), dfList[1].count() )


  13. union

	Adds the partitions of both the DFs to the output DataFrame

	df4 = df2.union(df3)


  14. repartition => can be used to increase or decrease the number of output partitions

	df2 = df1.repartition(4)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(2)
	df3.rdd.getNumPartitions()

	df4 = df1.repartition( 3, col("DEST_COUNTRY_NAME") )
	df4.rdd.getNumPartitions()

	df5 = df1.repartition(col("DEST_COUNTRY_NAME"))
	df5.rdd.getNumPartitions()

	spark.conf.get("spark.sql.shuffle.partitions")
	spark.conf.set("spark.sql.shuffle.partitions", "5")


  15. coalesce => can be used to decrease the number of output partitions

	df6 = df5.coalesce(3)
	df6.rdd.getNumPartitions()


  16. join

          -> to be discussed .....


         





  Working with different formats
  ------------------------------

  JSON

	read
		df1 = spark.read.format("json").load(inputPath)		
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
      		df3.write.format("json").save(ouputPath)
		df3.write.save(ouputPath, format="json")
		df3.write.json(ouputPath)
		df3.write.mode("overwrite").json(ouputPath)JSON

  PARQUET

	read
		df1 = spark.read.format("parquet").load(inputPath)		
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
      		df3.write.format("parquet").save(ouputPath)
		df3.write.save(ouputPath, format="parquet")
		df3.write.parquet(ouputPath)
		df3.write.mode("overwrite").parquet(ouputPath)

  ORC

	read
		df1 = spark.read.format("orc").load(inputPath)		
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
      		df3.write.format("orc").save(ouputPath)
		df3.write.save(ouputPath, format="orc")
		df3.write.orc(ouputPath)
		df3.write.mode("overwrite").orc(ouputPath)

  CSV

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)		
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

		// Applying custome schema for CSv without header
		df1 = spark.read.csv(inputPath, inferSchema=True, schema="origin STRING, dest STRING, count INT")

	write
      		df3.write.format("csv").save(ouputPath, header=True)
		df3.write.save(ouputPath, format="csv",  header=True)
		df3.write.csv(ouputPath, header=True, sep="|")


  TEXT
	read
		df1 = spark.read.text(inputPath)
		-> One column:  value: String

	write
        	df1.write.text(outputPath)



  Creating an RDD from a DataFrame
  ---------------------------------
       rdd1 = df1.rdd


  Creating a DataFrame from Programmatic data
  -------------------------------------------

	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]


	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame from an RDD
  ---------------------------------

	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()


	df1 = rdd1.toDF(["id", "name", "age"])

	df1.printSchema()
	df1.show()


  Creating a DataFrame with programmatic schema
  ---------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)
        --------------------------------
	mySchema = StructType([
        	StructField("id", IntegerType(), True),
        	StructField("name", StringType(), True),
        	StructField("age", IntegerType(), True)
        	])

        df1 = spark.createDataFrame(listUsers, schema=mySchema)
        ------------------------
	mySchema = StructType([
        	StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
        	StructField("DEST_COUNTRY_NAME", StringType(), True),
        	StructField("count", IntegerType(), True)
        	])

	df1 = spark.read.json(inputPath, schema=mySchema)


		 






  


	



















	