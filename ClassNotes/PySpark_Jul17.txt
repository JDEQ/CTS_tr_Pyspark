
  Agenda (PySpark)
  --------------------
  1. Python Crash Course - Basics of Python
  2. Spark - Basics & Architecture
  3. Spark APIs
  4. Spark Core - RDD API - Introduction & Basics
  5. Spark SQL in details
	Spark SQL Transformations
	Spark SQL Integrations with RDBMS & Hive

 ------------------------------------------------------
 
  Course Materials
  ----------------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> https://github.com/ykanakaraju/pyspark
	
   
  Installing Python Anaconda Distribution
  ---------------------------------------
	URL: https://www.anaconda.com/download
	https://www.anaconda.com/download#downloads

	Click on Download link
	Run the msi installer.

 

  Spark - Basics & Architecture
  -----------------------------
   
   Spark is a distributed computing framework for big data analytics.

     	-> Spark is written using Scala programming language

	-> Spark is an in-memory distributed computing framework

		Spark can persist intermediate partitions in-memory and subsequent tasks
		can directly process these stored partitions to advance the computations. 

        -> Spark is a unified framework.
		
		Spark provides a consistant set of APIs for processing different analytical
		workloads based on the same execution engine.

		Spark Core API (RDD)	-> Low level API for unstrauctured data processing
		Spark SQL (DataFrames)  -> Used for structured batch analytics
		Spark Streaming		-> Used for real-time data analytics
		Spark MLLib		-> Predictive analytics using Machine Learning
		Spark GraphX		-> Graph parallel computations.

	-> Spark is a polyglot		
		-> Supports Scala, Java, Python and R programming language.

	-> Spark can run on multiple clusters
		-> Local, Standalone Scheduler, YARN, Mesos, Kubernetes
           

   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client  : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


  RDD (Resilient Distributed Dataset)
  -----------------------------------

	-> RDD is the fundamental data abstraction of Spark.
	-> RDDs are Collection of distributed in-memory partitions.
	-> RDDs have to components
		RDD Loagical plan -> RDD Lineage DAG
		RDD runtime data  -> Collection of in-memory partitions

	-> RDDs are lazily evaluated.

	-> RDDs are immutable


  Creating RDDs
  -------------

    Three ways:

	1. Create an RDD from external dataset

		rddFile = sc.textFile(filePath, 4)

	2. Create an RDD from programmatics data

	        rdd1 = sc.parallelize( [3,2,1,4,2,4,6,5,3,5,4,5,7,8,5,7,8,9,6,7,1,2,1,2,3,2,5,0,2,3], 3 )

	3. By transforming existing RDDs.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  RDD Operations
  --------------

    Two operations : 

	1. Transformations
		-> Returns an RDD
		-> Does not cause execution
		-> Causes creation of lineage DAGs by the driver

	2. Actions
		-> Triggers execution and launches Job on the cluster
		-> Returns some output
	

  RDD Lineage DAG
  ----------------
	
     RDD Lineage DAG is a logical plan on how to create the RDD
     RDD Lineage DAG maintains a hierarchy of depencies to create the RDD partitions (when asked for) all the way from the very first RDD.
	

	rddFile = sc.textFile(filePath, 4)
	Lineage DAG of rddFile : (4) rddFile -> sc.textFile on filePath

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage DAG of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile on filePath

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	


  Types of Transformations
  ------------------------
	-> Narrow transformations
		-> Transformations that does not cause data shuffle 	
		-> No data movement across the network 

	-> Wide transformations
		-> Transformations that does causes data shuffle 	
		-> Data moves across the network 

  RDD Transformations
  -------------------

   1. map		P: U -> V
			object to object transformation

		rddFile.map(len).collect()


   2. filter		P: U -> Boolean
			filter the objects into output based on the function

		rddFile.filter(lambda x: len(x) > 51).collect()

  3. glom		P: None
			Returns one list object per partition.
		
		rdd1		rdd2 = rdd1.glom()

		P0: 1,2,3,6,4 -> glom -> P0: [1,2,3,6,4]
		P1: 5,6,4,7,2 -> glom -> P1: [5,6,4,7,2]
		P2: 9,0,5,8,1 -> glom -> P2: [9,0,5,8,1]

		 rdd1.glom().map(lambda p: len(p)).collect()


  4. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1	    rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )

		P0: 1,2,3,6,4 -> mapPartitions -> P0: 16
		P1: 5,6,4,7,2 -> mapPartitions -> P1: 24
		P2: 9,0,5,8,1 -> mapPartitions -> P2: 23

		rdd1.mapPartitions(lambda p: [sum(p)]).collect()


  5. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions but gives the partition-index as an additional
			function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()


   6. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.

		 rddWords = rddFile.flatMap(lambda x: x.split(" "))

   Types of RDDs:
   --------------
	1. Generic RDDs: RDD[U]
	2. Pair RDD: RDD[(U, V)]


   7. mapValues		P: U -> V
			Applied only to Pair RDD
			Transforms only the value part of (K,V) pairs by applying the function


   8. distinct()	P: None, Optional: numPartitions
			Returns only distinct objects of the RDD.


   9. sortBy 		P: U -> V, Optional: ascending(True/False), numPartitions
 			Sorts the RDD based on function output

		rdd1.sortBy(lambda x: x%3, True, 5).glom().collect()

  NOTE:
  -----
	All ...ByKey transformations are wide transformations and applied only to Pair RDDs

  10: sortByKey	       P: None, Optional: ascending(True/False), numPartitions
		       Sorts the RDD by key	

		      rdd2.sortByKey(False, 2).glom().collect()

  11. groupBy		P: U -> V,  Optional: numPartitions
			Returns a Pair RDD where
				key: each unique value of the function output
				value: ResultIterable. The objects of the RD that created the key

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 ) \
        		.flatMap(lambda x: x.split()) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)


  12. groupByKey	P: None, Optional: numPartitions
			Applied to only Pair RDDs
			Groups the objects by Key

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 ) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

  13. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Applied to only Pair RDDs
			Reduces all the values of each unique key

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 ) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False, 1)


  14. repartition	P: numPartitions
			Used to increase or decrease the number of partitions of the RDD
			Causes global shuffle

		rdd2 = rdd1.repartition(5)
		rdd2 = rdd1.repartition(2)

  15. coalesce		P: numPartitions
			Used to only decrease the number of partitions of the RDD
			Causes partition merging

		rdd2 = rdd1.coalesce(2)


  16. partitionBy	P: numPartitions, Optional: partitioning function (default: hash)
			Applied only to pair RDDs
			Partitions the data based on the key applied to the partitioning function.
 

  RDD Persistence
  ---------------	
	rdd1 = sc.textFile(<dataPath>, 40)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )      
              --> instruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	rdd6 DAG => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile -> t3 -> t5 -> t6] -> rdd6 --> collected to the driver

	rdd7.collect()

	rdd7 DAG => rdd7 -> rdd6.t7
		[t7] -> rdd7 --> collected to the driver

	rdd6.unpersist()
		

     StorageLevels
     -------------
	MEMORY_ONLY   		-> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK 	-> Disk Memory Serialized 1x Replicated 
	DISK_ONLY		-> Disk Serialized 1x Replicated 
	MEMORY_ONLY_2		-> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated 

    Commands
    --------
	rdd1.cache()     -> MEMORY_ONLY   
	rdd1.persist()   -> MEMORY_ONLY   
	rdd1.persist( StorageLevel.MEMORY_AND_DISK ) 

	rdd1.unpersist()

			
  RDD Actions
  -----------

  1. collect

  2. count
 
  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			reduces an entire RDD to one object of the same type by iterativly applying the
			function first on each partition and then across results of each partition.


		rdd1 

		P0: 1,2,3,6,4 -> reduce -> 16 -> reduce -> 63
		P1: 5,6,4,7,2 -> reduce -> 24
		P2: 9,0,5,8,1 -> reduce -> 23

		rdd1.reduce(lambda x, y: x + y)

>>> rddWc.collect()
[('hadoop', 25), ('flatmap', 12), ('hdfs', 6), ('sadas', 1), ('das', 6), ('spark', 40), ('asd', 5), ('scala', 28), ('hive', 19), ('transformations', 10), ('d', 1), ('map', 6), ('groupby', 6), ('flume', 6), ('oozie', 6), ('sqoop', 6), ('mapreduce', 6), ('rdd', 43), ('actions', 10)]

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )

   5. take

		rdd1.take(20)

   6. takeOrdered

		rddWords.takeOrdered(30, len)
		rddWords.takeOrdered(30)

   7. takeSample
		
		rdd1.takeSample(True, 20)
		rdd1.takeSample(True, 20, 457)

		rdd1.takeSample(False, 20)
		rdd1.takeSample(False, 20, 457)

   8. countByValue

   9. countByKey


  Use-case
  --------
    Dataset: <github>/data_git/cars.tsv

	From cars.tsv file, find the average weight of all models of each make from among American cars.
	-> Arrange the data in the desc order of average weight
	-> Save the output in a single text file.

	=> Try to solve the problem yourself


  Spark-Submit
  ------------
    Spark-Submit is a single command to submit any spark application (Scala, Java, Python, R)
    to any cluster manager (local, spark standalone, YARN, Mesos, Kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]


  =====================================
      Spark SQL (pyspark.sql)
  =====================================

   -> High-level API built on top of Spark Core API 
   -> Spark's structured data processing API

	Data formats:  Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC format:   RDBMS & NoSQL databases
	Hive format:   Hive Warehouse

   SparkSession
   ------------
	-> Starting point of execution for Spark SQL 
	-> Represents a user session inside an application. 

	from pyspark.sql import SparkSession

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate()  


   DataFrame
   ---------

	DataFrame is a collection of distributed in-memory partitions.
	DataFrames are immutable and lazily evaluated. 

	DataFrame have two components:
	
		Data   : Collection of 'Row' objects 
		Schema : StructType object

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)


  Basic steps in creating a Spark SQL application
  -----------------------------------------------

   1. Create a dataframe from some data source

	inputPath = "E:\\PySpark\\data\\users.json"
	df1 = spark.read.format("json").load(inputPath)
	df1 = spark.read.load(inputPath, format="json")
	df1 = spark.read.json(inputPath)

	df1.show()
	df1.printSchema()


   2. Apply transformations on the dataframe using transformation methods or using SQL

	Applying dataframe transformation methods 
        -----------------------------------------
	df2 = df1.select("userid", "name", "age", "phone", "gender") \
         	.filter("age is not null") \
         	.orderBy("gender", "age") \
         	.groupBy("age").count() \
         	.limit(4)

	Applying SQL to transform dataframes
	------------------------------------
	df1.createOrReplaceTempView("users")
	spark.catalog.listTables()

	qry = """select age, count(*) as count
			 from users
			 where age is not null
			 group by age
			 order by count"""

	df3 = spark.sql(qry)
	df3.show()

   3. Save/write the dataframe to some external file/database.

	outputPath = "E:\\PySpark\\output\\json"
	df3.write.format("json").save(outputPath)
	df3.write.save(outputPath, format="json")
	df3.write.json(outputPath)


 LocalTempViews & GlobalTempViews
 --------------------------------
 	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

 SaveModes
 ---------
    => Defines the behaviour when saving a DF to an existing directory

	-> ErrorIfExists (default)
	-> Ignore
	-> Append
	-> Overwrite

	df3.write.json(outputPath, mode="overwrite")
	df3.write.mode("overwrite").json(outputPath)


 DataFrame Transformation API Methods
 ------------------------------------

  1. select

  2. where / filter

  3. orderBy

  4. groupBy -> Returns a 'pyspark.sql.group.GroupedData' object
		Use some aggregation method to return a DataFrame.

  5. limit




  Working with Databricks
  -----------------------



  Working with different file formats
  -----------------------------------


  


































          
  
    

   











  








































  



   







