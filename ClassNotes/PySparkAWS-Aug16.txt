
  PySpark Curriculum
  ---------------------------------------	
    Spark Basics & Architecture
    Spark Core Basics (RDD)
    Spark Submit Command
    Spark Shared Variables
    Spark SQL DataFrame Operations
    Spark SQL Window Functions
    Spark SQL with JDBC/RDBMS
    Spark SQL with Hive
    Spark Structured Streaming

  PySpark on AWS Curriculum
  ----------------------------------------
     AWS Lambda & AWS CloudWatch
     (Integrating with with SNS, S3 & DynamoDB)
     AWS EMR
     (Cluster provisioning and running PySpark using Step Execution & Spark Submit) 
     AWS Glue & AWS Athena
     (Crawlers, Jobs, Workflows & ETL using PySpark)
     AWS Step Functions Basics
     Case-Study: Integrating AWS services
       -> Integrating Lambda, Glue & PySpark

   Pre-requisites
   --------------
	=> Python
	=> Basics of AWS - IAM, S3 etc.
  
  Materials
  ---------
        => PDF Presentations
	=> Code Module
	=> Class Notes Files
	=> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----

     -> Spark is a framework we use for big data analytics

     -> Spark is an in-memory distributed computing framework

     -> Spark is written in SCALA programming language

	
	   -> Spark is 100x times faster than MapReduce using 100% in-memory computation
	   -> Spark is 6 to 7 times faster than MapReduce even with disk based computation.

     	   Computing Cluster :  A collection of nodes whose cumulative resources can be used to distributed your stores
                         	and processing. 

     -> Spark is a unified framework.
	
         -> Spark provides a consistent set of APIs based on the same execution engine to process different analytical workloads.

	  Spark APIs
	  -----------
		1. Spark Core API    	-> Low level API, RDD API. Used for unstructured data processing
		2. Spark SQL		-> Structured data process
		3. Spark Streaming      -> Spark Streaming (legacy) and Structured Streaming (current)
		4. Spark MLlib		-> Machine learning workloads
		5. Spark GraphX		-> Graph Parallel computations.

     -> Spark is a polyglot
	  -> Spark apps can be written using Scala, Java, Python and R 

     -> Spark applications can run on mutiple cluster manager
	   -> local, spark standalone, YARN, Mesos, Kubernetes 
	

   Getting strated with Spark
   --------------------------

	1. Using vLab provided to you.

	2. Installing Spark on your personal machine. 
	
		-> Make sure to install Anaconda Distibution (such Anaconda Navigator)
		-> Make sure you are running Java 8 or above (JDK 1.8 or above)
		-> Open Anaconda Command Prompt and install pyspark
			pip install pyspark  

		-> If pip is not working, you can follow the instructions given in the shared document.

		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


	3. Signup to Databricks Community Edition. 

		Signup: https://www.databricks.com/try-databricks
		-> Click on "Continue with Community Edition" link (Do not click on Continue) in the second screen. 


    Spark Architecture
    -------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client  : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.

   
   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
	-> Fundamental data abstraction of Spark Core API

	-> Is a collection of distributed in-memory partitions.
		-> Each partition is a collection of objects (of some type)

	-> RDDs are immutable

	-> RDDs are lazily evaluted
		-> Transformations does not cause executions.
		-> Actions trigger execution. 
   

   Creating RDDs
   -------------

    Three ways to create an RDD:

	1. From some external data sources such as text files.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	 	default number of partitions => sc.defaultMinPartitions = 2, if cores >= 2, else 1.
 
	2. From programmatic data

		rdd1 = sc.parallelize(range(1, 100), 3)

		default number of partitions => sc.defaultParallelism = number of cores allocated

	3. By performing transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)
		


   RDD Operations
   --------------

	Two types of operations:

	1. Transformations
		-> Create an RDD
		-> Does not cause executions. Does not start jobs on the cluster.
		-> Only lineage DAGs are maintained by the driver

	2. Actions
		-> Executes the RDD
		-> Creates a physical plan and sends jobs to the cluster.


   RDD Lineage DAGs
   ----------------
    RDD Lineage DAG is maintained by driver for every RDD
    RDD lineage is crated when an RDD is created (by a data loading commands or transformations)
    RDD lineage DAG contains the heirarchy of dependencies all the way from the very first RDD.
	

	rddFile = sc.textFile( file, 4 )
	Lineage DAG of rddFile : (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage DAG of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWords : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile



  Types of RDD Transformations
  -----------------------------
	
	1. Narrow Transformations


	2. Wide Transformations


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<filePath>, 20)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   ----> instruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7(..)

	rdd6.collect()

	lineage of rdd6 => (20) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   transformations: [sc.textFile, t3, t5, t6] -> collected

        rdd7.collect()

	lineage of rdd7 => (20) rdd7 -> rdd6.t7
	   transformations: [t7] -> collected

	rdd6.unpersist()


   Storage Levels
   --------------

	MEMORY_ONLY	   => default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK    => Disk Memory Serialized 1x Replicated
	DISK_ONLY	   => Disk Serialized 1x Replicated
	MEMORY_ONLY_2      => Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2  => Disk Memory Serialized 2x Replicated

	
   Commands
   --------
	rdd1.cache()    => in-memory persistence
	rdd1.persist()  => in-memory persistence
	rdd1.persist( StrorageLevel.DISK_ONLY )

	rdd1.unpersist()











 




	

















	





  RDD Transformations
  --------------------
   
  1. map		P: U -> V
			Object to object transformation. 
			Input RDD: N objects, Ouput RDD: N objects

		rddFile = sc.textFile("G:\\Spark\\wordcount.txt", 3)
		rdd2 = rddFile.map(lambda x: len(x.split(" ")))

 
  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output.
			Input RDD: N objects, Ouput RDD: <= N objects

   		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Returns a list object with all the objects of each partition


		rdd1		rdd2 = rdd1.glom()
		P0: 1,2,1,3,2,4 -> glom -> P0: [1,2,1,3,2,4]
		P1: 4,5,3,6,7,6 -> glom -> P1: [4,5,3,6,7,6]
		P2: 4,7,9,0,9,0 -> glom -> P2: [4,7,9,0,9,0]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			flatten the iterables produced by the function
		
		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions 	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation	

		rdd1	rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )
		P0: 1,2,1,3,2,4 -> mapPartitions -> P0: 15
		P1: 4,5,3,6,7,6 -> mapPartitions -> P1: 31
		P2: 4,7,9,0,9,0 -> mapPartitions -> P2: 29


		rdd1.mapPartitions(lambda p: [sum(p)]).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()


  6. mapPartitionsWithIndex    P: Int, Iterable[U] -> Iterable[V]
			 Similar to mapPartitions but you get the partition-index as additional fn parameter.

		rdd2 = rdd1 \
			.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x) , p)) \
			.filter(lambda x: x[0] == 1) \
			.map(lambda x: x[1])


  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD

		rddWords.flatMap(lambda x: x).distinct().collect()























  	


    





  RDD Actions
  -----------

  1. collect

  2. count



  
































		 






  


	



















	