
  Agenda (8 sessions)
  ------------------- 
    Spark - basics & architecture
    Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variables
    Spark-Submit
    Spark-SQL
	-> Working with DataFrames
	-> Integrations - MySQL & Hive
    Spark Streaming
	-> DStreams (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF presentations
	=> Code Modules
        => Class Notes
	=> Github: https://github.com/ykanakaraju/pyspark
  
  Getting started with Spark
  --------------------------
	1. Using vLab
	      -> Follow the instruction given in the attached document.
              -> Three tools:
		   -> PySpark shell
		   -> Spyder IDE
		   -> Jupyter Notebooks

	2. Installing development environment in your personal machine.
		-> First make sure you have "Anaconda distribution"
		    => URL: https://docs.anaconda.com/anaconda/install/
                -> Follow the instruction given in the shared document to setup "PySpark" with Spyder & Jupyter

        3. Signup for Databricks community edition (free signup)
		Signup link: https://www.databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html
   

  Spark
  ------
        Cluster: Is a unified entity comprising of many nodes whose combined resources can be used to
                 distribute your storage or processing.

	=> Spark is written in Scala programming language

	=> Spark is a in-memory distributed computing framework.

	=> Spark is a 'unified' analytics framework

	    Spark provides a consistent set of APIs for processing different analytics workloads
	    using the same execution engine.

		Batch Analytics	of unstructured data    => Spark Core API
		Batch Analytics	of structured data	=> Spark SQL
		Streaming Analytics			=> Spark Streaming, Structured Streaming 
		Predictive Analytics			=> Spark MLLib
		Graph Parallel Computations		=> Spark GraphX

	=> Spark is a polyglot
		-> Supports Scala, Java, Python and R

	=> Spark jobs can be submitted to mutiple cluster managers
		=> local, spark standalone, YARN, Mesos, Kubernetes


  Spark Architecture (& Building Blocks)
  --------------------------------------

       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
	

   RDD (Resilient Distributed Dataset)
   ------------------------------------
    -> Fundamental in-memory data abstraction of Spark Core API

    -> RDD is a collection of distributed in-memory partitions.
	-> Partition is a collection of objects (of any type)

	RDD =>   Meta Data (Lineage DAG)
		 Data 	

    -> RDDs are immutable
   
    -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Actions commands 

  Creating RDDs
  -------------

    Three ways:

	1. Creating RDDs from external data files
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		=> default number fo partitions is given by sc.defaultMinPartitions

	2. Creating RDDs from programmatic data
		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0], 3)

		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0])
		=> default number fo partitions is given by sc.defaultParallelism

        3. By applying transformations on existing RDDs
		
		rdd2 = rdd1.map(lambda x: x*100)

  RDD Operations
  --------------
    Two things:

	1. Transformations
		-> Create the RDD lineage DAGs 
		-> Does not trigger the actual execution
		-> Every transformation returns an other RDD.

	2. Actions
		-> Trigges execution.
		-> Converts the logical plan to physical plan and sends the tasks to the executors.


  RDD Lineage DAG
  ---------------
   -> Are created at the driver when data-loading or transformation commands  
   -> Represents a 'logical plan' on how to create the RDD partitions
   -> Maintains the hierarchy of dependencies all the way from the very first RDD. 

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
	   Lineage DAG =>  rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   Lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    Lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

      
   RDD Persistence
   ---------------
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   --> instruction to spark to save the rdd6 partition
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> rdd6

	rdd7.collect()

	lineage of rdd7 => rdd7 -> rdd6.t7
		[t7] -> rdd7

	StorageLevels
        -------------
	1. MEMORY_ONLY	      -> default, Memory Serialized 1x replicated
	2. MEMORY_AND_DISK    -> Disk Memory Serialized 1x replicated
	3. DISK_ONLY	      -> Disk Serialized 1x replicated
	4. MEMORY_ONLY_2      -> Memory Serialized 2x replicated
	5. MEMORY_AND_DISK_2  -> Disk Memory Serialized 2x replicated
 

        Commands
        --------
		rdd1.cache()      -> memory only
		rdd1.persist()    -> memory only
		rdd1.persist( StorageLevel.DISK_ONLY )

		rdd1.unpersist()


   Executor Memory Structure
   -------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of transformations
   ------------------------

	Two types:

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


   RDD Transformations
   -------------------
  
    1. map		P: U => V
			Object to object transformation
			input RDD: N objects, output RDD: N objects
 	
		rddFile.map(lambda s: s.upper()).collect()

    2. filter		P: U -> Boolean
			Filters the objects of the input RDD based on the function output.
			input RDD: N objects, output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

    3. glom		P: None
			Returns one list object per partition with all the objects of the partition

		rdd1			rdd2 = rdd1.glom()
		P0: 1,2,1,3,4,5 -> glom -> P0: [1,2,1,3,4,5]
		P1: 4,3,5,4,6,7 -> glom -> P1: [4,3,5,4,6,7]
		P2: 3,1,3,5,6,7 -> glom -> P2: [3,1,3,5,6,7]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flattens the iterables produced by the function 
			input RDD: N objects, output RDD: >= N objects	

		rddWords.flatMap(lambda w: w.upper()).collect()

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 1,2,1,3,4,5 -> mapPartitions -> P0: 
		P1: 4,3,5,4,6,7 -> mapPartitions -> P1: 
		P2: 3,1,3,5,6,7 -> mapPartitions -> P2: 
		
		rdd1.mapPartitions(lambda p: [sum(p)]).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()

   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Same as mapPartitions, but you get partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))] ) \
		    .filter(lambda t: t[0] == 0)\
		    .flatMap(lambda x: x[1])\
                    .map(lambda x: x*10)\
                    .collect()

   7. distinct		P: None, Optional: numPartitions
			returns an RDD with distinct objects (removes the duplicates in the output RDD)			 
		
			rddWords.distinct().collect()
			rddWords.distinct(4).collect()

   Types of RDDs	
   --------------
	1. Generic RDDs : RDD[U]
	2. Pair RDDS :	RDD[(K, V)]

   8. mapValues		P: U => V
			Applied only on Pair RDDs
			The 'value' part of the (K,V) pairs is transformed using the function.

		rdd4 = rdd2.mapValues(lambda v: (v, v+4) )

   9. sortBy		P: U => V, Optional: ascending (True/False), numPartition
			The elements of the RDD are sorted based on the function output

		rddWords.sortBy(lambda x: x[-1], True).collect()
		rdd1.sortBy(lambda x: x%5, False, 2).glom().collect()

   10. groupBy		P: U -> V
			Return a pair RDD where:
			key: unique value of the function output
			value: ResultIterable object with all the objects that produced the key

   		wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len) \
              			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: A list of ratios (eg: [0.5, 0.5] )
				Splits an rdd into multiple RDDs randomly in the given ratios.

		rddList = rdd1.randomSplit( [0.35, 0.15, 0.5])
		rddList = rdd1.randomSplit( [0.35, 0.15, 0.5], 353 )   # here 353 is a seed


   12. repartition		P: numPartitions
				Returns an RDD with specified number of partitions
				You may increase or decrease the number of partitions.
				Results in global shuffling 

		rdd5 = rdd1.repartition(5)

   13. coalesce			P: numPartitions
				Returns an RDD with specified number of partitions
				You can only decrease the number of partitions.
				Results in partition-merging 

		rdd2 = rdd1.coalesce(2)

	Recommendations
	---------------
	=> An RDD partition should be between 100 MB to 1 GB (ideally 128 MB)
	=> The number of partitions should be a multiple of number of cores allocated to your app
	=> If the number of partition is close to less than 2000, bump it up to 2000
	=> The number of CPU cores per executor should be 5


   14. partitionBy	P: numberOfPartitions, Optional: partition-function (default: hash)
			Applied on to PairRDDs

P: numPartitions, Optional: partitioning-function (default: hash)
				Applied only on Pair RDD
				Is used to control which keys go to which partition based on some partition
				function. 		
			
		rdd3 = rdd1.map(lambda x: (x, 0)).partitionBy(2).map(lambda x: x[0])

transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;      

rdd1 = sc.parallelize(transactions, 3) \
        .map(lambda d: (d['city'], d)) \
        .partitionBy(3, custom_partitioner) \
        .map(lambda x: x[1])

rdd1.glom().collect()



   15. union, intersection, subtract, cartesian

	let us say rdd1 has M partitions and rdd2 has N partitions

	 command			output partitions
         --------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd1.cartesian(rdd2)		M * N, wide


   ..ByKey transformations
   -----------------------
	=> Are wide transformations
	=> Applied only to pair RDDs

   16. sortByKey		P: None, Optional: ascending (True/False), numPartitions

   17. groupByKey		P: None, Optional: numPartitions 

	 
	  wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              		.flatMap(lambda x: x.split(" ")) \
              		.map(lambda x: (x, 1)) \
              		.groupByKey() \
              		.mapValues(len) \
              		.sortBy(lambda x: x[1], False, 1)

   18. reduceByKey	P: (U, U) => U	 Optional: numPartitions 
			Reduces all the values of each unique-key by applying the reduce function. 

	  wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              		.flatMap(lambda x: x.split(" ")) \
              		.map(lambda x: (x, 1)) \
              		.reduceByKey(lambda x, y: x + y) \
              		.sortBy(lambda x: x[1], False, 1)

   19. aggregateByKey


   20. joins


   21. cogroup
	















  RDD Actions 
  -----------
 
   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) => U
			reduces an entire RDD into one value of the same type by iterativly applying the 
			reduce function on each partition in the first stage and across partition outputs
			in the second stage

                rdd1				

		P0: 2, 1, 3, 2, 4, 5, 6, 7	 -> reduce -> -26 -> reduce => 93
		P1: 8, 2, 1, 2, 3, 2, 0, 9	 -> reduce -> -11
		P2: 5, 2, 7, 6, 3, 0, 7, 3, 2, 1 -> reduce -> -26


		rdd1.reduce( lambda x, y: x - y )     => 11

	
	
		
















	







