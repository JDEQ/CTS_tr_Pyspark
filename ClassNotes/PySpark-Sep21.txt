
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   
  Spark Basics
  -------------
     
    -> Spark is an in-memory distributed computing framework (cluster computing framework)
       for BigData analytics.   

    -> Spark is written in 'Scala'
    
	in-memory => ability to persist intermediate results in memory and allow subsequent tasks/stages
		     to read the data directly from persisted data. 

	Spark resorts to disk-based computation (disk-spills) if there is not enough RAM for persisting 
	intermediate results. 


   -> Spark is a unified framework

	  Spark provides a consistent set of APIs based on well-defined data structures and a common execution
	  engine to perform processing of different analytical workloads.  

	  	Batch Analytics			=> Spark SQL (and 'Spark Core')
	  	Streaming Analytics		=> Spark Streaming, Structured Streaming
	  	Predictive Analytics		=> Spark MLlib
		Graph Parallel Computations	=> Spark GraphX


   -> Spark is a polyglot	
	 Supports multiple programming language: Scala, Java, Python, R

   
   -> Spark applications can be submitted to multiple cluster managers.
		local, Spark Standalone, YARN, Mesos, Kubernetes


   Getting started with Spark
   --------------------------

    -> Make sure you have Anaconda installed.


    1. Downloading spark and working locally using shell.

	 Download URL: https://spark.apache.org/downloads.html
	 Download the xxxxx.tgz file and extract it to a suitable location.

	 Add <spark-dir>\bin folder to your PATH environment variable.

	-> Open a terminal (Anaconda Terminal) and type 'pyspark'
	-> This will launch the 'PySpark' shell.

	default Web UI location: http://localhost:4040/


   2. Making PySpark accessible from Spyder and Jupyter Notebooks

	-> Follow the instructions given in the shared document
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


   3. Using Databricks Community Edition (free version)

	Signup URL: https://www.databricks.com/try-databricks#account
	Provide a valid email.
	
	-> In the next page, click on "Get started with Community Edition" link (not Continue button)
    
	Login URL: https://community.cloud.databricks.com/login.html



   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
    

   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> Fundamental data abstraction of Spark.

	-> Represents a collection of in-memory partitions
		-> Partition is a collection of objects

	-> RDDs follow lazy evaluation
		-> Transformations does not cause execution
		-> Action command execute the RDDs

	-> RDDs partitions are immutable
    

   Creating RDDs
   -------------
    Three ways:

	1. Create an RDD from some external datasets.

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)	

		
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")

		Default number of partitions is determined by "sc.defaultMinPartitions" config parameter
		The default value of "sc.defaultMinPartitions" = 2 if number of cores >= 2
								 1 if number of cores == 1	


	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,6,7,6,8,7,9,0,7,9,0,0,6,6,4,3,3,2,1,5,6,7,8,4,6], 3)

		Default number of partitions is determined by "sc.defaultParallelism" config parameter
		The default value of "sc.defaultParallelism" = number of cores allocated.


	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))
	

     Note: Use rdd1.getNumPartitions() to get the partition-count of rdd1


   RDD Operations
   --------------
	
     Two types of operations


	1. Transformations
		-> Create RDDs
		-> Cause lineage DAGs to be created
		-> Does not cause execution
		
	2. Actions
		-> Execute RDDs
		-> Convert the logical plan to the physical plan



   RDD Lineage DAG
   ---------------
   RDD Lineage DAG is a logical plan on how to create the RDD
   Driver maintains Lineage DAGs for all RDDs
   A DAG maintains heirarchy of dependencies all the way from the very first RDD.
   Transformations & data loading commnads create DAG.


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage of rddFile : (4) rddFile -> sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage of rddPairs : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddWc = rddPairs.reduceByKey(lambda x, y : x + y)
	Lineage of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile on wordcount.txt

	[ sc.textFile -> flatMap -> map -> reduceByKey => rddWc ===> sent to driver ]

	rddWc.collect()


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<filePath>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  --> instruction to save rdd6 partitions
	rdd7 = rdd6.t7(...)
	
	rdd6.collect()
	rdd6 DAG : (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile -> t3 -> t5 -> t6] ==> collected

	rdd7.collect()
	rdd7 DAG : (4) rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[t7] ==> collected


	StorageLevels
	-------------
	MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK		-> Disk Memory Serialized 1x Replicated
	DISK_ONLY		-> Disk Serialized 1x Replicated
	MEMORY_ONLY_2		-> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated


	Commands
	--------		
	rdd1.cache()     			-> in-memory persistence
	rdd1.persist()				-> in-memory persistence
	rdd1.persist(StorageLevel.DISK_ONLY)

	rdd1.unpersist()


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.


  Types of Transformations
  ------------------------
	Two Types

	1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive



  Spark Application Execution Heirarchy
  --------------------------------------

	Spark Application (Each applicatio has a SparkContext)
	|
	|=> Jobs (Each action command spans a Job)
	    |
	    |=> Stages (one or more stages, Each wide transformation will cause stage transition)
		|
		|=> Tasks (one or more tasks, # tasks = #partitions in that stage)
		    |
		    |=> Transformations (that can run in parallel)
		

  RDD Transformations
  -------------------
  
  1. map		P: U -> V
			Transforms each object of the input RDD by applying the function
			object to object transformation
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: len(x.split())).collect()


  2. filter		P: U -> Boolean
			Only those objects for which the function return True will be in the output RDD.
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) > 51).collect()


  3. glom		P: None
			Return one list object per partition with all the objects of the partition

		
		rdd1		rdd2 = rdd1.glom()

		P0: 1,5,4,7,8 -> glom -> P0: [1,5,4,7,8]
		P1: 4,5,6,7,8 -> glom -> P1: [4,5,6,7,8]
		P2: 7,0,9,1,3 -> glom -> P2: [7,0,9,1,3]

		rdd1.count() = 15 (int)  rd2.count() = 3 (list)


  4. flatMap		P: U -> Iterable[V]
			flatMap flatten the iterables produced by the function.

		rddFile.flatMap(lambda x: x.split()).collect()



  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation


		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 1,5,4,7,8 -> mapPartitions -> P0: 25
		P1: 4,5,6,7,8 -> mapPartitions -> P1: 30
		P2: 7,0,9,1,3 -> mapPartitions -> P2: 20


		rdd1.mapPartitions(lambda p: [sum(p)] ).glom().collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).glom().collect()	



  6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
			Similar to mapPartitions, but you get the partition-index as additional function parameter.
				
		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).glom().collect()


  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.
		 
		rddWords.distinct().collect()
		rddWords.distinct(4).collect()


  Types of RDDs
  -------------

	1. Generic RDD => RDD[U]                
	2. Pair RDD    => RDD[(K, V)]		


  8. mapValues		P: U -> V
			Applied to only Pair RDDs
			Transforms the 'value' part of the (K,V) by applying the function.	

		rdd3.mapValues(len).collect()


  9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the value of function output

		rddWords.sortBy(lambda x: x[0]).collect()
		rddWords.sortBy(lambda x: x[0], False).collect()
		rddWords.sortBy(lambda x: x[0], True, 3).collect()

 
  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where
			   key: Each unique value of the function output
			   value: ResultIterable. objects of the RDD that produced the key

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split()) \
            		.groupBy(lambda x: x) \
            		.mapValues(len) \
            		.sortBy(lambda x: x[1], False, 1)


  11. randomSplit


  12. repartition


  13. coalesce




  
  



  

  RDD Actions
  -----------

  1. collect   	    => Returns a list with all the objects of all the partitions of the RDD.

  2. count	    => Returns the count of RDD objects

  3. saveAsTextFile => Saves the partitions as files in the specified (non-existing) directory
   


























