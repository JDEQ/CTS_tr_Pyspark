
  Agenda
  -------

   -> Spark - Basics & Architecture
   -> Spark Core API (RDD API)
	-> RDD Transformations and Actions
	-> Shared Variables
   -> Spark SQL (DataFrame API)
   -> Spark MLlib
   -> Introduction Spark Streaming 

   Materials
   ---------
	-> PDF presentations
	-> Source Code 
	-> Class Notes

   Prerequisites
   -------------
	-> Python Programming Language



   Cluster
   -------
	-> A group of nodes whose cumulative resources can be used to distribute the storage and processing. 

   Spark
   -----
	-> Spark is a unified in-memory distributed computing framework.

	-> Spark is written in Scala programming language.

	-> Spark is a polyglot
		-> Scala, Python, Java, R

	-> Spark can run on several cluster managers
		-> local, spark standalone, yarn, mesos, kubernetes	
      
	In-memory framework 
		-> The intermediate results of the tasks can be saved in RAM and subsequent tasks can 
		   perform transformation on these in-memory partitions. 

	Unified Framework
		-> Spark provides a consistent set of of APIs for performing different analytical workload
		   on the same execution engine.

		Batch Processing of unstructured data 	:  Spark Core API
		Batch Processing of structured data	:  Spark SQL
		Streaming Processing			:  Spark Streaming, Structured Streaming
		Predictive Analytics (ML)		:  Spark MLlib
		Graph Parallel Computations		:  Spark GraphX
 

    Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors
   
   
   Getting started with Spark
   --------------------------

	1. Working in your vLab:

		-> Login to CentOS 7  (there is an icon on the wondows desktop)
		-> Open a terminal
		-> Type the following
			$ jupyter notebook   (this may give an error)
			$ jupyter notebook --allow-root


		Launch PySpark Shell:
		---------------------
		$pyspark



	2. Using Databricks Community Edition  (Free cloud account)

		-> URL: https://databricks.com/try-databricks

		-> Signup with your details (give a valid email)
			-> From the email complete process.
			-> Login to Databricks account

		-> Read the Quick Start Tutorial

	3. If you Anaconda Navigator installed, you can setup Jupyter Notebook & Spyder with PySpark

		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
		
		Follow the instruction given in the document shared in the github.
		
		
   RDD (Resilient Distributed Dataset)
   -----------------------------------
		
	=> RDD is fundamental data abstraction	

	=> RDD is a collection of distributed in-memory partitions.
		-> Each partition is a collection of objects.

	=> RDD partitions are immutable.

	=> RDDs follow lazy evaluation model
		-> Transformation does not cause execution
		-> Only action commands trigger execution.

          	
   How to create RDD ?
   -------------------

	3 ways:

	1. create an RDD from external text file.

		rddFile = sc.textFile(file, 4)

	2. Create an RDD programmatic data
		
		rdd1 = sc.parallelize( [6,4,2,5,7,8,1,2,3,5,9,8,3,1,5], 3 )


	3. By applying transformations on existing RDDs
	
		rdd3 = rddFile.flatMap(lambda x: x.split(" "))


   What can you on a RDD ?
   -----------------------

	Two things:

	1. Transformations
		-> Returns an RDD
		-> Do not cause execution
		-> Only RDD lineage DAG is created at the driver side.
	
	2. Actions
		-> Returns some output
		-> Triggers execution of the RDD and creates a physical plan from the lineage and the
		   driver sends a set of tasks to the executors to materialize the RDD partitions.

   RDD Lineage DAG
   ---------------
    When a transformation creates an RDD, a lineage DAG is create a driver. 

    Lineage DAG of an RDD is a logical execution plan that tracks all the dependencies that caused
    the creation this RDD all the way from the very first RDD.

	rddFile = sc.textFile(file, 4)
	    Lineage: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	    Lineage: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	    Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)		
	    Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4.collect()   => action command that launches a job. 


  RDD Persistence
  ---------------
	rdd1 = sc.textFile( .. )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )    --> store the partitions of rdd6
	rdd7 = rdd6.t7( .. )

	rdd6.collect()
	-> Lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   sc.textFile (rdd) -> t3 (rdd3) -> t5 (rdd5) -> t6 -> rdd6 ==> collect()

	rdd7.collect()
	 -> Lineage:  rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		rdd6.t7 -> rdd7 ==> collect()

	rdd6.unpersist()           --> store the persisted partitions

	Storage Levels
        ---------------
	1. MEMORY_ONLY
	2. MEMORY_AND_DISK
	3. DISK_ONLY
	4. MEMORY_ONLY_SER	  (not supported in pyspark)
	5. MEMORY_AND_DISK_SER	  (not supported in pyspark)
	6. MEMORY_ONLY_2          (2 replicas are stored on two different executors)	
	7. MEMORY_AND_DISK_2

	Commands
        ---------
		-> rdd.persist()      // default persistence
		-> rdd.persist( StorageLevel.DISK_ONLY )
		-> rdd.cache()       // in-memory persistence (same as rdd.persist())

		-> rdd.unpersist()

    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  -------------------

   1. map			P: U -> V
				Object to object transformation
				input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x) > 51).collect()   // True, False, ...

   2. filter			P: U -> Boolean
				RDD objects that return TRue for the function will be in the output. 
				input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x) > 51).collect()


   3. glom			P: None
				Returns a list of all objects for every partition

		rdd1			rdd2 = rdd1.glom()

		P0: 6, 4, 2, 5, 7  -> glom ->  P0 : [6, 4, 2, 5, 7]
		P1: 8, 1, 2, 3, 5  -> glom ->  P1 : [8, 1, 2, 3, 5]
		P2: 9, 8, 3, 1, 5  -> glom ->  P2 : [9, 8, 3, 1, 5]

		rdd1.count() = 15 (int)		rdd2.count() = 3 (list)

		=> rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap			P: U -> Itarable[V]
				flapMap flattens the iterables produced by the function.
				input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies a function on the entire partition.

		rdd1	     rdd2 = rdd1.mapPartitions(lambda data: [len(data)])

		P0: 6, 4, 2, 5, 7  -> mapPartitions ->  P0 : 5
		P1: 8, 1, 2, 3, 5  -> mapPartitions ->  P1 : 5
		P2: 9, 8, 3, 1, 5  -> mapPartitions ->  P2 : 5

		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x) ).glom().collect()
		rdd1.mapPartitions(lambda x: [sum(x)] ).glom().collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Applies a function on the entire partition. You get partition-id as an
				additional function parameter.

	rdd1.mapPartitionsWithIndex(lambda pid, pdata : map(lambda a: (pid, a*10), pdata) ).glom().collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects of the input RDD

	  	rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(4)   // rdd2 will have 4 partitions

   8. sortBy			P: U -> V, Optional: ascending, numPartitions
				Elements of the output RDD are sorted based on the function output
				they produce.

		rddWords.sortBy(len, False).collect()   // DESC order based on length of the word
		rddWords.sortBy(lambda x: x[0] if len(x) < 2 else x[1]).collect()
		rdd1.sortBy(lambda x: x%5, True, 5).glom().collect()  // output RDD has 5 partitions.

   Types of RDDs
   --------------
	-> Genertic RDDs : RDD[U]
	-> Pair RDDs     : RDD[(U, V)]


   9. mapValues			P: U -> V
				Applied ONLY to Pair RDDs.
				The function is applied only to the value part. Transforms only the 'value'
				part of the (K, V) pairs. 

		rdd2.mapValues(lambda x: (x,x)).collect()
		rdd2.mapValues(lambda x: x*10).collect()

   10. groupBy			P: U -> V,  Optional: numPartitions
				Groups the elements of the RDD based on the function output.
				Produces a Pair RDD where:
				   key: unique function out
				   value: ResultIterable of all the objects of the RDD that produced the key.
				   
		rddWords.groupBy(lambda x: x, 2).mapValues(len).glom().collect()
  
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.groupBy(lambda x: x) \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of ratios (ex: [0.6, 0.4]), optional: seed
				Splits an inputRDD into a list of output RDDs approx. in the given ratios.

		rddList = rdd1.randomSplit([0.4, 0.3, 0.3])
		rddList = rdd1.randomSplit([0.6, 0.4], 464)		
 
   12. repartition		P: numPartitions
				Creates an output RDD with specified number of partitions
				Used to increase or decrease the number of partitions.
				Results in global shuffle.

		rdd2 = rdd1.repartition(5)   // rdd2 will have 5 partitions

   13. coalesce  		P: numPartitions
				Creates an output RDD with specified number of partitions
				Used to only decrease the number of partitions.
				Does partition merging.

		rdd2 = rdd1.coalesce(5)   // rdd2 will have 5 partitions	

        recommendations
	---------------
	-> Size of the partition:  should be approx. 128 MB  
	-> Number of Partitions: should be a multiple of cores (2x, 3x, 4x)
	-> Number of cores in each executor: 5


   14. partitionBy		P: numPartitions, optional: partitioning function.
				Applied ONLY on pair RDD
				Partitioning happens based on the key
				Used to control which objects go to which partitions.


   15. union, intersection, subtract, cartesian
				-> Works on two RDDs. 
				
	Let us say rdd1 has M partitions and rdd2 has N partitions.

	command					# of output partitions
        --------------------------------------------------------------
	rdd1.union(rdd2)			M + N, narrow
	rdd1.intersection(rdd2)			M + N, wide
	rdd1.subtract(rdd2)			M + N, wide
	rdd1.cartesian(rdd2)			M * N, wide


   ..ByKey transformations
   -----------------------
    -> Wide transformations
    -> Applied only to pair RDDs
    -> Perform some operattion based on the key.

   
   16. sortByKey		P: None, Optional: ascending, numPartitions
				Sorts the RDD by the key

   	rddPairs.sortByKey().collect()		 // asc sort based on key
	rddPairs.sortByKey(False).collect()      // desc sort based on key
	rddPairs.sortByKey(False, 3).collect()   // desc sort based on key as 3 partitions

   17. groupByKey		P: None, Optional: numPartitions
				The objects of the output RDD are grouped based on key. The output is
				a pair RDD with unique keys and grouped values.

				Avoid groupByKey as much as possible. 

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         			.flatMap(lambda x: x.split(" ")) \
         			.map(lambda x: (x, 1)) \
         			.groupByKey() \
         			.mapValues(sum) \
         			.sortBy(lambda x: x[1], False) \
         			.coalesce(1)

   18. reduceByKey		P: (U, U) -> U   optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the 
				reduce function on all the values of each unique key.

		rddPairs.reduceByKey(lambda x, y: x + y)
		rddPairs.reduceByKey(lambda x, y: x + y, 2)		

  		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         			.flatMap(lambda x: x.split(" ")) \
         			.map(lambda x: (x, 1)) \
         			.reduceByKey(lambda x, y: x + y) \
         			.sortBy(lambda x: x[1], False) \
         			.coalesce(1)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.  
  
	

  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce			P: (U, U) -> U
				It reduces the entoire RDD to a single value of the same type by iterativly
				applying the reduce function within each partition and then across
				partitions.

	P0: 9, 5, 2, 4, 3, 4  -> -9   -> 10
	P1: 8, 6, 4, 2, 0, 3  -> -7
	P2: 8, 6, 5, 4, 3, 2  -> -12

		rdd1.reduce(lambda x, y: x - y)
		rddPairs.reduce(lambda x, y: (x[0] + "." + y[0], x[1] + y[1]) )  				
                   # rddPairs => [('flatmap', 1), ('map', 1), ('groupby', 1), ('spark', 1), ..]

  5. aggregate:			
   
	
		rdd1.aggregate( (0,0), 
			        lambda z,v : (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1])
			      )	














 

