
  Agenda
  -------

   -> Spark - Basics & Architecture
   -> Spark Core API (RDD API)
	-> RDD Transformations and Actions
	-> Shared Variables
   -> Spark SQL (DataFrame API)
   -> Spark MLlib
   -> Introduction Spark Streaming 

   Materials
   ---------
	-> PDF presentations
	-> Source Code 
	-> Class Notes

   Prerequisites
   -------------
	-> Python Programming Language



   Cluster
   -------
	-> A group of nodes whose cumulative resources can be used to distribute the storage and processing. 

   Spark
   -----
	-> Spark is a unified in-memory distributed computing framework.

	-> Spark is written in Scala programming language.

	-> Spark is a polyglot
		-> Scala, Python, Java, R

	-> Spark can run on several cluster managers
		-> local, spark standalone, yarn, mesos, kubernetes	
      
	In-memory framework 
		-> The intermediate results of the tasks can be saved in RAM and subsequent tasks can 
		   perform transformation on these in-memory partitions. 

	Unified Framework
		-> Spark provides a consistent set of of APIs for performing different analytical workload
		   on the same execution engine.

		Batch Processing of unstructured data 	:  Spark Core API
		Batch Processing of structured data	:  Spark SQL
		Streaming Processing			:  Spark Streaming, Structured Streaming
		Predictive Analytics (ML)		:  Spark MLlib
		Graph Parallel Computations		:  Spark GraphX
 

    Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors
   
   
   Getting started with Spark
   --------------------------

	1. Working in your vLab:

		-> Login to CentOS 7  (there is an icon on the wondows desktop)
		-> Open a terminal
		-> Type the following
			$ jupyter notebook   (this may give an error)
			$ jupyter notebook --allow-root


		Launch PySpark Shell:
		---------------------
		$pyspark



	2. Using Databricks Community Edition  (Free cloud account)

		-> URL: https://databricks.com/try-databricks

		-> Signup with your details (give a valid email)
			-> From the email complete process.
			-> Login to Databricks account

		-> Read the Quick Start Tutorial

		To Download a file from databricks:
		-----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

		Example:
		/FileStore/tables/wordcount-5.txt
		https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


	3. If you Anaconda Navigator installed, you can setup Jupyter Notebook & Spyder with PySpark

		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
		
		Follow the instruction given in the document shared in the github.
		
		
   RDD (Resilient Distributed Dataset)
   -----------------------------------
		
	=> RDD is fundamental data abstraction	

	=> RDD is a collection of distributed in-memory partitions.
		-> Each partition is a collection of objects.

	=> RDD partitions are immutable.

	=> RDDs follow lazy evaluation model
		-> Transformation does not cause execution
		-> Only action commands trigger execution.

          	
   How to create RDD ?
   -------------------
	3 ways:

	1. create an RDD from external text file.

		rddFile = sc.textFile(file, 4)

	2. Create an RDD programmatic data
		
		rdd1 = sc.parallelize( [6,4,2,5,7,8,1,2,3,5,9,8,3,1,5], 3 )

	3. By applying transformations on existing RDDs
	
		rdd3 = rddFile.flatMap(lambda x: x.split(" "))


   What can you on a RDD ?
   -----------------------

	Two things:

	1. Transformations
		-> Returns an RDD
		-> Do not cause execution
		-> Only RDD lineage DAG is created at the driver side.
	
	2. Actions
		-> Returns some output
		-> Triggers execution of the RDD and creates a physical plan from the lineage and the
		   driver sends a set of tasks to the executors to materialize the RDD partitions.

   RDD Lineage DAG
   ---------------
    When a transformation creates an RDD, a lineage DAG is create a driver. 

    Lineage DAG of an RDD is a logical execution plan that tracks all the dependencies that caused
    the creation this RDD all the way from the very first RDD.

	rddFile = sc.textFile(file, 4)
	    Lineage: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	    Lineage: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	    Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)		
	    Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4.collect()   => action command that launches a job. 


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( .. )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )    --> store the partitions of rdd6
	rdd7 = rdd6.t7( .. )

	rdd6.collect()
	-> Lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   sc.textFile (rdd) -> t3 (rdd3) -> t5 (rdd5) -> t6 -> rdd6 ==> collect()

	rdd7.collect()
	 -> Lineage:  rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		rdd6.t7 -> rdd7 ==> collect()

	rdd6.unpersist()           --> store the persisted partitions

	Storage Levels
        ---------------
	1. MEMORY_ONLY		 (not supported in pyspark)
	2. MEMORY_AND_DISK	 (not supported in pyspark)
	3. DISK_ONLY
	4. MEMORY_ONLY_SER	  
	5. MEMORY_AND_DISK_SER	  
	6. MEMORY_ONLY_2          (2 replicas are stored on two different executors)	
	7. MEMORY_AND_DISK_2

	Commands
        ---------
		-> rdd.persist()      // default persistence
		-> rdd.persist( StorageLevel.DISK_ONLY )
		-> rdd.cache()       // in-memory persistence (same as rdd.persist())

		-> rdd.unpersist()

    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  -------------------

   1. map			P: U -> V
				Object to object transformation
				input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x) > 51).collect()   // True, False, ...

   2. filter			P: U -> Boolean
				RDD objects that return TRue for the function will be in the output. 
				input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x) > 51).collect()


   3. glom			P: None
				Returns a list of all objects for every partition

		rdd1			rdd2 = rdd1.glom()

		P0: 6, 4, 2, 5, 7  -> glom ->  P0 : [6, 4, 2, 5, 7]
		P1: 8, 1, 2, 3, 5  -> glom ->  P1 : [8, 1, 2, 3, 5]
		P2: 9, 8, 3, 1, 5  -> glom ->  P2 : [9, 8, 3, 1, 5]

		rdd1.count() = 15 (int)		rdd2.count() = 3 (list)

		=> rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap			P: U -> Itarable[V]
				flapMap flattens the iterables produced by the function.
				input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies a function on the entire partition.

		rdd1	     rdd2 = rdd1.mapPartitions(lambda data: [len(data)])

		P0: 6, 4, 2, 5, 7  -> mapPartitions ->  P0 : 5
		P1: 8, 1, 2, 3, 5  -> mapPartitions ->  P1 : 5
		P2: 9, 8, 3, 1, 5  -> mapPartitions ->  P2 : 5

		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x) ).glom().collect()
		rdd1.mapPartitions(lambda x: [sum(x)] ).glom().collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Applies a function on the entire partition. You get partition-id as an
				additional function parameter.

	rdd1.mapPartitionsWithIndex(lambda pid, pdata : map(lambda a: (pid, a*10), pdata) ).glom().collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects of the input RDD

	  	rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(4)   // rdd2 will have 4 partitions

   8. sortBy			P: U -> V, Optional: ascending, numPartitions
				Elements of the output RDD are sorted based on the function output
				they produce.

		rddWords.sortBy(len, False).collect()   // DESC order based on length of the word
		rddWords.sortBy(lambda x: x[0] if len(x) < 2 else x[1]).collect()
		rdd1.sortBy(lambda x: x%5, True, 5).glom().collect()  // output RDD has 5 partitions.

   Types of RDDs
   --------------
	-> Genertic RDDs : RDD[U]
	-> Pair RDDs     : RDD[(U, V)]


   9. mapValues			P: U -> V
				Applied ONLY to Pair RDDs.
				The function is applied only to the value part. Transforms only the 'value'
				part of the (K, V) pairs. 

		rdd2.mapValues(lambda x: (x,x)).collect()
		rdd2.mapValues(lambda x: x*10).collect()

   10. groupBy			P: U -> V,  Optional: numPartitions
				Groups the elements of the RDD based on the function output.
				Produces a Pair RDD where:
				   key: unique function out
				   value: ResultIterable of all the objects of the RDD that produced the key.
				   
		rddWords.groupBy(lambda x: x, 2).mapValues(len).glom().collect()
  
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.groupBy(lambda x: x) \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of ratios (ex: [0.6, 0.4]), optional: seed
				Splits an inputRDD into a list of output RDDs approx. in the given ratios.

		rddList = rdd1.randomSplit([0.4, 0.3, 0.3])
		rddList = rdd1.randomSplit([0.6, 0.4], 464)		
 
   12. repartition		P: numPartitions
				Creates an output RDD with specified number of partitions
				Used to increase or decrease the number of partitions.
				Results in global shuffle.

		rdd2 = rdd1.repartition(5)   // rdd2 will have 5 partitions

   13. coalesce  		P: numPartitions
				Creates an output RDD with specified number of partitions
				Used to only decrease the number of partitions.
				Does partition merging.

		rdd2 = rdd1.coalesce(5)   // rdd2 will have 5 partitions	

        recommendations
	---------------
	-> Size of the partition:  should be approx. 128 MB  
	-> Number of Partitions: should be a multiple of cores (2x, 3x, 4x)
	-> Number of cores in each executor: 5


   14. partitionBy		P: numPartitions, optional: partitioning function.
				Applied ONLY on pair RDD
				Partitioning happens based on the key
				Used to control which objects go to which partitions.


   15. union, intersection, subtract, cartesian
				-> Works on two RDDs. 
				
	Let us say rdd1 has M partitions and rdd2 has N partitions.

	command					# of output partitions
        --------------------------------------------------------------
	rdd1.union(rdd2)			M + N, narrow
	rdd1.intersection(rdd2)			M + N, wide
	rdd1.subtract(rdd2)			M + N, wide
	rdd1.cartesian(rdd2)			M * N, wide


   ..ByKey transformations
   -----------------------
    -> Wide transformations
    -> Applied only to pair RDDs
    -> Perform some operattion based on the key.

   
   16. sortByKey		P: None, Optional: ascending, numPartitions
				Sorts the RDD by the key

   	rddPairs.sortByKey().collect()		 // asc sort based on key
	rddPairs.sortByKey(False).collect()      // desc sort based on key
	rddPairs.sortByKey(False, 3).collect()   // desc sort based on key as 3 partitions

   17. groupByKey		P: None, Optional: numPartitions
				The objects of the output RDD are grouped based on key. The output is
				a pair RDD with unique keys and grouped values.

				Avoid groupByKey as much as possible. 

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         			.flatMap(lambda x: x.split(" ")) \
         			.map(lambda x: (x, 1)) \
         			.groupByKey() \
         			.mapValues(sum) \
         			.sortBy(lambda x: x[1], False) \
         			.coalesce(1)

   18. reduceByKey		P: (U, U) -> U   optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the 
				reduce function on all the values of each unique key.

		rddPairs.reduceByKey(lambda x, y: x + y)
		rddPairs.reduceByKey(lambda x, y: x + y, 2)		

  		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         			.flatMap(lambda x: x.split(" ")) \
         			.map(lambda x: (x, 1)) \
         			.reduceByKey(lambda x, y: x + y) \
         			.sortBy(lambda x: x[1], False) \
         			.coalesce(1)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.  
  
	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

	student_rdd.collect()

	avg_rdd = student_rdd.map(lambda x: (x[0], x[2])) \
            		.aggregateByKey( (0,0),
                             lambda z, v: (z[0] + v, z[1] + 1),
                             lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            		.mapValues(lambda x: x[0]/x[1])


   20. joins		-> join (inner), leftOuterJoin, rightOuterJoin, fullOuterJoin	
			   Applied on two pair RDDs
			   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

    21. cogroup		-> Is used when you want to join RDDs with duplicate keys.
			   cogroup => groupByKey on each RDD -> fullOuterJoin

		rdd1 = sc.parallelize([("key1", 10), ("key2", 12), ("key1", 7), ("key2", 6), ("key3", 6)])  
		rdd2 = sc.parallelize([("key1", 5), ("key2", 4), ("key2", 7), ("key1", 17), ("key4", 17)])
		
		grp = rdd1.cogroup(rdd2).mapValues(lambda x: (list(x[0]), list(x[1])))

   		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		-> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

   		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		-> (key1, [5,17]) (key2, [4,7]) (key4, [17])

       		-> (key1, ([10, 7], [5,17])) (key2, ([12, 6], [4,7]))  (key3, ([6], [])) (key4, ([], [17]))


  Use-Case
  ---------
   -> From cars.tsv, compute the average weight of each american make. 
   -> Arransge the data in the desc order of average weight
   -> Save the output as a single text file.

	--> Try it ..
	--> dataset:  https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce			P: (U, U) -> U
				It reduces the entoire RDD to a single value of the same type by iterativly
				applying the reduce function within each partition and then across
				partitions.

	P0: 9, 5, 2, 4, 3, 4  -> -9   -> 10
	P1: 8, 6, 4, 2, 0, 3  -> -7
	P2: 8, 6, 5, 4, 3, 2  -> -12

		rdd1.reduce(lambda x, y: x - y)
		rddPairs.reduce(lambda x, y: (x[0] + "." + y[0], x[1] + y[1]) )  				
                   # rddPairs => [('flatmap', 1), ('map', 1), ('groupby', 1), ('spark', 1), ..]

  5. aggregate		
   
	Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence Function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

		rdd1.aggregate( (0,0), 
			        lambda z,v : (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1])
			      )	

  6. first

  7. take
		rdd1.take(10)

  8. takeOrdered
		rdd1.takeOrdered(10)
		rdd1.takeOrdered(10, lambda x: x%3)

  9. takeSample
		rdd1.takeSample(True, 10)		// withReplacement = True
		rdd1.takeSample(False, 10)		// withReplacement = False
		rdd1.takeSample(False, 10, 453)		// 453 is a seed

  10. countByValue

  11. countByKey	=> applied to only Pair RDDs

  12. foreach		=> Does not return anything
			   Runs some function on all the elements of the RDD

  13. saveAsSequenceFile 

  	rdd2.saveAsSequenceFile("E:\\PySpark\\output\\seq")

 ===================================================

  spark-submit
  ------------

	-> Is a single command that is used to submit any spark application (scala, java, python, R)
	   to any cluster manager (local, spark, yarn, mesos, k8s)

		spark-submit --master yarn 
		        --deploy-mode cluster
			--driver-memory 2G
			--executor-memory 10G
			--num-executors 10
			E:\PySpark\wordcount.py <command-line-args>

		spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2

 =====================================================

  Shared Variables
  ----------------

  Closure : is those variables and functions which must be visible for the executor to perform its
	    computations on the RDD.

	-> This closure is serialized and a copy is sent to each executor.


           // The following code will not give desired result.
	   // as we can not local variables like this..

     	   counter = 0

           def isPrime(a):
		returns 0 if a is not prime
		returns 1 if a is prime

	   def f1(n):
		global counter
		if ( isPrime(n) == 1 ) counter = counter  + 1
		return n*2

           rdd1 = sc.parallelize( range(1, 4001), 4 )

	   rdd2 = rdd1.map( f1 )

	   rdd2.collect()  

           print("counter - " + counter )
	    
           => We CAN NOT use local variables which are part of the closure to implement global counters
              (i.e we can not use local variables to counts some thing across several executors)



   Accumulator Variable
   ---------------------
	
     -> Is a shared variable maintained by driver and is not part of function closure (and hence is
	not a local variable)
     -> All the distributed tasks can add to this accumulator
     -> Accumulators are used to implement counter.

	   counter = sc.accumulator(0)

           def isPrime(a):
		returns 0 if a is not prime
		returns 1 if a is prime

	   def f1(n):
		global counter
		if ( isPrime(n) == 1 ) counter.add(1)
		return n*2

           rdd1 = sc.parallelize( range(1, 4001), 4 )

	   rdd2 = rdd1.map( f1 )

	   rdd2.collect()  

           print("counter - " + counter.value )


  Broadcast Variable
  ------------------

	d1 = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5 : e , ......... })    

	def f1(n):
		global d1
		return d1[n]

	rdd1 = sc.parallelize([1,2,3,4,5,...], 4)

	rdd2 = rdd1.map( f1 )


  ===============================================
     Spark SQL
  ===============================================

   -> Is a high-level API built on top of Spark Core API

   -> Is Spark's structured data processing API.
	
	 Structured File Formats: Parquet (default), ORC, JSON, CSV (delimited text files)
	 Hive
	 JDBC Sources : RDBMS DBs, NoSQL DBs

   -> SparkSession
	-> Starting point of execution. 
	-> Represents a user-session inside a spark context. 
  
	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()   

   -> DataFrame (DF)

	-> Main data abstraction of Spark SQL
	-> Collection of distributed in-memory partitions that are immutable and lazily evaluated.
	-> DataFrame is a collection of 'Row' objects.

	-> DFs have two components:
		-> data   : set of partitions of Row objects
		-> schema : defines the structure of the Row.
			    StructType object

		   StructType(
			List(
			    StructField(age,LongType,true),
			    StructField(gender,StringType,true),
			    StructField(name,StringType,true),
			    StructField(phone,StringType,true),
			    StructField(userid,LongType,true)
			)
		   )


   Working with DataFrames
   -----------------------

	1. Read/load data from some structured source into a DataFrame.

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	2. Apply transformation on the DF using DF API transformations or by using SQL

		DataFrame API approach:

		    df2 = df1.select("userid", "name", "age") \
        		.where("age is not null") \
        		.groupBy("age").count() \
        		.orderBy("count") \
        		.limit(4)

                SQL Approach:

			df1.createOrReplaceTempView("users")

			qry = """select age, count(*) as count 
         			from users 
         			where age is not null
         			group by age
         			order by count
         			limit 4"""

			df3 = spark.sql(qry)

	3. Write/save the Dfs data into a structure destination.  

		outputDir = "E:\\PySpark\\output\\json"

		df3.write.format("json").save(outputDir)
		df3.write.json(outputDir)



  DataFrame Transformations
  -------------------------

   1. select

	df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

	df2 = df1.select( col("DEST_COUNTRY_NAME").alias("destination"), 
                  column("ORIGIN_COUNTRY_NAME").alias("origin"), 
                  expr("count"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )


   2. where / filter

	df3 = df2.where("highFrequency = true and domestic = false")
	df3 = df2.filter("highFrequency = true and domestic = false")

	df3 = df2.where(col("count") > 1000)

   3. orderBy / sort

	df3 = df2.orderBy("count", "destination")
	df3 = df2.orderBy(desc("count"), asc("destination"))

	df3 = df2.sort(desc("count"), asc("destination"))


   4. groupBy (with some aggregation method)

		=> groupBy returns a GroupedData object

	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")
	df3 = df2.groupBy("highFrequency", "domestic").min("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	 .agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		avg("count").alias("avg"),
              		max("count").alias("max"))

   5. limit

	df3 = df2.limit(5)

   6. selectExpr

		df2 = df1.select( expr("DEST_COUNTRY_NAME as destination"), 
                  expr("ORIGIN_COUNTRY_NAME as origin"), 
                  expr("count"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )

		is same as

		df2 = df1.selectExpr( "DEST_COUNTRY_NAME as destination", 
                  "ORIGIN_COUNTRY_NAME as origin", 
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"
                )

   7. withColumn
		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", expr("count > 200")) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))


   8. withColumnRenamed

		df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

   9. drop
	
		df3 = df2.drop("newCount", "highFrequency")

   10. union

		df5 = df4.union(df3)
		=> Here df4 and df3 should have same schema.

   11. sample

		df3 = df1.sample(True, 0.7)
		df3 = df1.sample(True, 1.7)    		// the sampling fraction can be > 1
		df3 = df1.sample(False, 0.7, 86787)	// with seed

		df3 = df1.sample(False, 0.7, 86787)	// withReplacement = False (without replacement)
		df3 = df1.sample(False, 1.7)  => Invalid // sample fraction should in [0,1] for without replacement	

   12. randomSplit
		
		dfList = df1.randomSplit([0.6, 0.4])
		dfList = df1.randomSplit([0.6, 0.4]. 345)

		dfList[0].count()
		dfList[1].count()	

   13. distinct
	
		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()


   14. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df1.repartition( col("DEST_COUNTRY_NAME") )
		df4.rdd.getNumPartitions()

		=> The number of output (shuffle) partitions is given by the value of 
		   "spark.sql.shuffle.partitions" wholde default value is 200.
		
		   spark.conf.set("spark.sql.shuffle.partitions", "5")
		   	

		df5 = df1.repartition( 4, col("DEST_COUNTRY_NAME") )
		df5.rdd.getNumPartitions()

   15. coalesce
 	df6 = df5.coalesce(2)
	df6.rdd.getNumPartitions()


   16. join
	  -> covered as a separate topic.


   LocalTempViews & GlobalTempViews
   ---------------------------------
	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.



  SaveModes
  ---------
	-> what should happen when you write to an existing directory.

	1. errorIfExists
	2. ignore
	3. append
	4. overwrite

	=> df3.write.json(outputDir, mode="overwrite")
	=> df3.write.mode("overwrite").json(outputDir)


  Working with different file formats
  -----------------------------------

   1. JSON

	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)	
	
	write
		df3.write.format("json").save(outputDir)
		df3.write.json(outputDir)
		df3.write.json(outputDir, mode="overwrite")

   2. Parquet

	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.parquet(inputPath)	
	
	write
		df3.write.format("parquet").save(outputDir)
		df3.write.parquet(outputDir)
		df3.write.parquet(outputDir, mode="overwrite")

   3. ORC

	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.orc(inputPath)	
	
	write
		df3.write.format("orc").save(outputDir)
		df3.write.orc(outputDir)
		df3.write.orc(outputDir, mode="overwrite")

   4. CSV  (delimited text file format)

	read
		df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df3.write.format("csv").option("header", True).save(outputDir)
		df2.write.csv(outputDir, mode="overwrite", header=True)
		df2.write.csv(outputDir, mode="overwrite", header=True, sep="|")


    Creating an RDD from a DataFrame
    --------------------------------
	rdd1 = df1.rdd
  

   Creating a DataFrame from Programmatic data
   -------------------------------------------

	list1 = [(1, 'Raju', 45),
         (2, 'Aditya', 35),
         (3, 'Amrita', 40),
         (4, 'Komala', 25),
         (5, 'Pravan', 15),
         (6, 'Ramesh', 30)]

	df1 = spark.createDataFrame(list1).toDF("id", "name", "age")


   Creating a DataFrame from an RDD
   --------------------------------

	rdd1 = spark.sparkContext.parallelize(list1)
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

	
	Applying custom schema
        -----------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)


  Joins
  -----    
   Supported Joins: inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

   left-semi join
   --------------
	-> is like inner join but the data only comes from left table. 
        -> is defined by the following subquery:		
		select * from emp where deptid IN (select id from dept)
	
   left-anti join
   --------------
	-> is defined by the following subquery:		
		select * from emp where deptid NOT IN (select id from dept)
    
    data:
    -----
     employee = spark.createDataFrame([
    	(1, "Raju", 25, 101),
    	(2, "Ramesh", 26, 101),
    	(3, "Amrita", 30, 102),
    	(4, "Madhu", 32, 102),
    	(5, "Aditya", 28, 102),
    	(6, "Pranav", 28, 100)])\
  	.toDF("id", "name", "age", "deptid")
  
     department = spark.createDataFrame([
    	(101, "IT", 1),
    	(102, "ITES", 1),
    	(103, "Opearation", 1),
    	(104, "HRD", 2)])\
  	.toDF("id", "deptname", "locationid")

	employee.show()
	department.show()
	

     SQL approach
     ------------

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")


	qry = """select *
         	from emp left anti join dept on
         	emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)

	joinedDf.show()


    DF API approach
    ---------------
	Supported Joins: inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]

	joinedDf = employee.join(department, joinCol, "left_anti")
	joinedDf.show()


	Broadcast Join
        --------------
	joinedDf = employee.join(broadcast(department), joinCol, "left_anti")
		

   Use-Case
   --------
   From movies.csv and ratings.csv fetch the top 10 movies with highest average rating.
	
   -> Consider only those movies with atleast 30 ratings
   -> Data: movieId, title, totalRatings, avgRating
   -> Arrange the data in the DESC order of average rating
   -> Save it as a single CSV file with "|" delimiter.  

	=> Try it...		
	=> datasets are available @ https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens


   Working with Hive
   -----------------
   => Hive is a data warehousing platform built on top of Hadoop

 	1. warehouse : Is a directory in which Hive stores all the data files of its managed tables
	2. metastore : Is an external RDBMS (usually) where Hive stores all its metadata. 

	spark = SparkSession \
    		.builder \
    		.appName("Datasorces") \
    		.config("spark.master", "local") \
    		.config("spark.sql.warehouse.dir", warehouse_location) \
    		.enableHiveSupport() \
    		.getOrCreate()
	
======================
spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
================================
      
    Working with MySQL
    -------------------
      => Check out the example in the GitHub

 ================================================

  Machine Learning & Spark MLlib
  ------------------------------

   ML Model  => Trained entity
		Trained based on some historical data
                   -> On how to derive an output from inputs 	
		   -> Identify the patterns in the data
		   -> Give recommendations ....
		Trained by "Algorithms" 
  
              model = algorithm.fit( training data )

  
   Terminology
   -----------
   
    1. Training Data -> the historic data that contains label and features. 

    2. Features	  -> inputs, dimensions
 
    3. Label      -> output   

    4. Algorithm  -> Is a iterative mathematical computation that establishes a relation between 
		      the label and features with a goal to minimize the loss. 

    5. Model      -> Is the result of the algorithmic training     

    6. Error	  -> The variance w.r.t a single data point.

    7. Loss       -> The cumulative average error w.r.t all the data


      	x	y	z (label)   prediction   error		
       -------------------------------------------------
	1000	100	2150		2200	  -50	
	500	1000	2100		2000	  100		
	1500	100	3000		3100      100
	1200	600	2950		3000	  -50
	2000	200	 ?
	----------------------------------------------
				 Loss: 	  300/4 = 75	

   Steps in a ML project
   ---------------------

   1. Data Collection

   2. Data Prepartition  (60% of the time is spent here)
	
	-> All data must be numeric
	-> There should no nulls/empty data etc.
        -> Cleanup all the outliers

	EDA: Exploratpry data analysis
	FE:  Feature Engineering

	Output: To create a "Feature Vector" that can be fit to an algorithm.

   3. Train the model using one or more Algorithms.

	Output: A trained model 

	-> Split the prepared data into 70% & 30% splits for train and validation datasets
	-> Train the model using 'train' dataset (70% data)

   4. Evaluate the model

	-> Get the predictions from the model on the 'validation' dataset (30% data)
	-> By comparing the predictions with the actual labels, accuracy be measured.

	Output: An evaluated model

   5. Deploy the model


  Types of Machine Learning
  -------------------------

    1. Supervised Learning

	-> We have labelled data.  (Data contains both label and features)

	1.1 Classification
		-> Label is one of two/few fixed values
		-> Ex: 1/0, [1,2,3,4,5]
		-> Ex: Survival Prediction, Email Spam prediction.  

        1.2 Regression
		-> Label is a continuous value
		-> House Price Pridiction

    2. Unsupervised Learning

	-> Data contains only features. There won't be any label

	2.1 Clustering   
		-> Ex: Customer Segmentation

		=> Collaborative Filtering	
		    -> Recommendation engined

	2.2  Dimensionality Reduction

    3. Reinforcement Laerning.
	-> Semi-supervised learning.
	





