
  Curriculum
  ----------

   Understanding Spark
   RDD API - Transformations & Action 
   Spark SQL - DataFrames   
   Machine Learning & Spark MLLib
   Spark Streaming - Introduction

 ------------------------------------------------------------------------

   Big Data -> A set of data sets that are huge and complex that the traditional
               data management systems can not reasonbly process. 

    1. Volumn
    2. Velocity - Data growth.
    3. Variety  - Unstructured data or Semi-Structured
    4. Varacity - Data in doubt.  

   Traditional Data Management Systems -> Single server based system. 
 
   Cluster based computational systems are the solution. 

   Cluster -> Is a group of connected nodes that form a unified system whose cumulative 
              resources can be used to distribute your storage and processing requirements. 
   
   Hadoop Framework --> Distributed Storage and Processing Framework
                    --> HDFS: (Storage) 128 MB blocks
                        MapReduce (Distributed Parallel Processing) 
                        YARN  ( Cluster Manager / Resource Manager )

   MapReduce -> Is a disk based execution framework

   MapReduce -> Is not good for
                -> Iterative processing (ML)
                -> Lot of small files 
                -> Ad-hoc queries

   Spark is solution to the limitations of MapReduce Framework 

  -----------------------------------------------------------------------------

   Spark -> Open source unified in-memory distributed processing framework.
         -> Very memory intensive (needs lot of RAM)
         -> Is a polyglot ( Scala, Python, Java, R )
    
   Hadoop
   ------
      -> Batch processing of unstructured data    :  MapReduce
      -> Batch processing of structured data      :  Hive, Pig, Impala, Drill ...
      -> Stream data processing                   :  Storm, Kafka
      -> Machine Learning                         :  Mahout
      -> Graph Parallel Computations              :  Giraph
                 

   Spark unified framework
   ----------------------- 
   Spark provides a consistent set of APIs to process various analytics work loads.
   
      -> Batch processing of unstructured data    :  Spark Core (RDD)
      -> Batch processing of structured data      :  Spark SQL
      -> Stream data processing                   :  Spark Streaming, Structured Streaming
      -> Machine Learning                         :  Spark MLlib
      -> Graph Parallel Computations              :  Spark GraphX
   
  ----------------------------------------------------------------------------------

   Spark Building Blocks
   ---------------------

    1. Cluster Manager
          -> Spark apps are submitted to the cluster manager
          -> CM will register the job and schedules it by allocating some executors to the job
        
          -> Spark Standalone Scheduler, YARN, Mesos, Kubernetes.

    2. Driver Process
          -> Master Process which manages all the tasks of the spark job
          -> This is program where main() method is defined.
          -> Contains a SparkContext, which represents the connection to the cluster. 
          -> Will analyize the DAG and determines which tasks to be launched and when
             on the executors and accordingly send the tasks to the executors.

         Modes: (deploy-modes)
           1. Client Mode: (default mode) driver runs on the client machine.
           2. Cluster Mode: driver runs as one of the processes on the cluster.

    3. Executor Processes
          -> Tasks are launched in these executors by the driver process
          -> Each tasks is functinally same, but runs on a small chunk of data. Several
             tasks are launched parallelly across multiple executors.

    4. Spark Context
          -> Runs in the driver process
          -> represents an application's connection to the cluster with a defined config. 
  
 ----------------------------------------------------------------

   Programming Lang      =>  Scala, Python, Java & R
   Spark High Level API  =>  Spark SQL, Spark Streaming, Spark MLlib, Spark GraphX
   Spark Core API        =>  RDD based API
   Resource Managers     =>  Standalone Scheduler, YARN, Mesos, Kubernites   
   Storage Layer         =>  Linux, HDFS, RDMS, NoSQL, Kafka

------------------------------------------------------------------

   RDD - Resilient Distributed Dataset
   -----------------------------------

    RDD is the core data abstraction of Spark.


    RDD -> Partitioned 
           -> Distributed Dataset as partitions
           -> Each partition contains a collection of objects

        -> Immutable: you can not change the contents of an RDD   
     
        -> Lazilily Evaluated.
            -> Execution will happen only when you have an action command.
 
        -> Resilient (Fail Safety)
            -> RDDs can recreate the missing partitions by recomputing the partitions
               by executing the required tasks from the RDD lineage DAGs.
   

  How to create an RDD
  --------------------

   1. An RDD can be created from an external data file:
        rdd = sc.textFile ( file ) 

   2. An RDD can be created by parallelizing programmatic data
        rdd = sc.parallelize( [1,2,3,4,5,6,7,8] )

   3. By applying transformations on existing RDDs, we can create an other RDD
        rdd2 = rdd1.<transformation> 


  What can you do with an RDD
  ---------------------------

   1. Transformations
       -> represent in-memory transformation of input RDDs into output RDD
       -> tranformations create RDD
       -> transformations does not cause execution. 
       -> transformation cause the lineage DAG (directed acyclic graph) to 
          be created (at the driver side)
     

   2. Actions
       -> Action command produce output such as sending the data from the RDD to the driver
          or saving the contents of RDD into a file. 

       -> Actions trigger execution. 

       -> When an action command is seen by the driver, it will analyze the DAG, and 
          launches the set of tasks that need to be performed on the executors to get
          the output.

rdd1 = sc.textFile( filePath, 3 )
rdd2 = rdd1.map(lambda x: x.split(" "))
rdd3 = rdd2.map(lambda x: len(x))
rdd3.collect()  <-- action  

 Lineage DAG:
 ------------
 rdd1 -->  sc.textFile
 rdd2 --> rdd1.map --> sc.textFile
 rdd3 --> rdd2.map --> rdd1.map --> sc.textFile



  rdd1 = sc.textFile(filePath, 9)
  rdd2 = rdd1.flatMap(lambda x: x.split(" "))
  rdd3 = rdd2.map(lambda x: (x, 1))
  rdd4 = rdd3.reduceByKey(lambda a,b: a + b)

  rdd4.collect

  rdd4  -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile 

  sc.textFile -> flatMap -> map -> reduceByKey -> rdd4


  Lineage of RDDs
  ---------------

  rdd1  -> sc.textFile
  rdd2  -> rdd1.flatMap -> sc.textFile
  rdd3  -> rdd2.map -> rdd1.flatMap -> sc.textFile
  rdd4  -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile


  ================================================================================
   
  Repository:  https://github.com/ykanakaraju/pyspark

  ===============================================================================
 
   Lab:
   ----
      CentOS --> Terminal --> pyspark
                          --> jupyter notebook --allow-root

  ==============================================================================

   RDD Parsistence
   ---------------

    rdd1 = sc.textFile( file )
    rdd2 = rdd1.t1()
    rdd2.persist()
    rdd3 = rdd2.t2()
    rdd4 = rdd2.t3()
    rdd5 = rdd2.t4()
    rdd6 = rdd5.t5()
    rdd7 = rdd6.t6()
    rdd8 = rdd6.t7()
    rdd8.persist()    --> The partitions of rdd8 are saved in memory, not GCed automatically.
    rdd9 = rdd8.t8()

    rdd9.collect()
       rdd9 => rdd8.t8 => rdd6.t7 => rdd5.t5 => rdd2.t4 => rdd1.t1 => sc.textFile
  

   Types of Persistence:
   ---------------------

     2 formats -> Deserialized format or serialized format

        ex: rdd8.persist( StorageLevel.MEMORY_ONLY )
            rdd8.cache() (same as above command ie storing in MEMORY_ONLY level)

            rdd8.unpersist()

    Storage Levels:
    ---------------

      1. MEMORY_ONLY :      Deserialized format in Storage memory ONLY.
      2. MEMORY_AND_DISK :  Deserialized format in MEMORY if available, else store on the disk.
      3. DISK_ONLY :        Deserialized format in disk only

      4. MEMORY_ONLY_SER :  Serialized format in Storage memory ONLY.    
      5. MEMORY_AND_DISK_SER

      6. MEMORY_ONLY_2
      7. MEMORY_AND_DISK_2   
 
  ==============================================================================

    Spark Executor Memory Structure
    -------------------------------

      Let us say we are requesting for exexutors with a memory of 10GB each.
      CM will allocate 10.3 GB exexutors to the Spark job.


      1. Reserved Memory: 300 MB


      2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB (unified memory area)
 
           2.1  Storage Memory (spark.memory.storageFraction : 0.5 ) => 3 GB  
                 -> Persist RDDs & Broadcast Variables

           2.2  Execution Memory => 3 GB
                 -> All RDD partition creation, running os tasks etc use this memory.
           

      3. User Memory   => 4 GB
           -> python/java/scala related code, objects use this memory

  ==============================================================================
    
    Types of Transformations
    ------------------------

      1. Narrow Transformations
           -> Simple and efficient
           -> Does not cause shuffling of the data across executors
           -> the computation of each partitions depends ONLY on the input partitions
              (partition-to-partition computations)
 

      2. Wide Transformations     
           -> Causes shuffling of the data across executors/nodes
           -> The computation of each partition requires all the partitions of the input RDD. 
           -> Every wide transformation results in stage-boundary.

        
   rdd9 => textFile, map, distict, map, flatmap, sortBy, filter

    stage 1: <textFile, map>  
    stage 2: <distinct, map, flatmap>
    stage 3: <sortBy, filter> 

   Job: Each action command results in a Job
    
   Stages: Each job is divided in stages. 
           Each stage has tasks that can run in parallel
           Stages have to launched one after other

   Tasks: Each Stage has a set of tasks that can run in parallel
          

   Job -> Stages -> Tasks


  RDD Transformations
  -------------------

  1. map             => F: U => V
                     => Transforms each object to another object by applying the function
                     => Input RDD Count: N, Output RDD Count: N 


  2. filter          => F: U => Boolean
                     => The output RDD will have only those elements that return 'true'
		     => Input RDD Count: N, Output RDD Count: <= N 


  3. glom            => F: None
                     => Will create an Array with all the elements of each partition of the RDD
                     => Input RDD Count: N, Output RDD Count: = number of partitions of input RDD


  4. distinct        => F: None
                     => Will output only distinct elements from the input RDD
		     => Input RDD Count: N, Output RDD Count: <= N 
                     => Results in global shuffle.

  
  5. flatMap         => F: U => Iterable[V]
                     => Will flatten all the elements of the iterables generated by the function. 
                     => Input RDD Count: N, Output RDD Count: >= N


  6. mapPartitions   => F: Iterable[U] => Iterable[V]
                     => Will take an entire partition (all the objects of each partition) 
                        as input an return an iterable.
                     => Input RDD Count: N, Output RDD Count: = number of partitions of input RDD

  
  7. mapPartitionsWithIndex => F: (Int, Iterable[U]) => Iterable[V]

  		     => Will take the partition index and the partition elements as functions inputs 
                        an return an iterable.

                     => Input RDD Count: N, Output RDD Count: = number of partitions of input RDD


   8. sortBy        => F: U => V, where V is the key based on which the elements are sorted.
                    => Will sort the elements of the RDD based on the output of the function.
		    => Input RDD Count: N, Output RDD Count: = N 


   9. randomSplit   => F: takes array of ratios
                    => The RDD is split into an array of RDDs based on the ratios specified. 
                    => You can specify a seed value as second argument to repeat the output across
                       multiple executions.

           ex: rddArr = wordsRdd.randomSplit( [0.5, 0.3, 0.2] )
               rddArr = wordsRdd.randomSplit( [0.5, 0.3, 0.2], 14 )


   Guidelines to determine how many partitions should you have.
   ------------------------------------------------------------
   -> The optimal size of each partition is ~ 128 MB ( 100MB to 200MB )
   -> The number partitions can be 2 to 3x the number of cores you have.
   -> If the number of partitions is close to 2000, then bump it up to 2000+ partitions

   
   10. repartition(n) => Is used to change the number of partitions of an RDD
                         Useful in the case narrow transformations. 

                         Is used to increase or decrease the number of partitions
                         Results in global shuffling.

   11. coalesce(n)  => Is used ONLY to decrese the number of partitions
                       Results in partition merging, does not global shuffling.
               
   
   Two types of RDDs
   -----------------   
       1. Generic RDD - RDD[U]
       2. Pair RDD    - RDD[(U, V)]
     

   12. partitionBy  => Allows you apply your own partitioning logic to partition the RDDs
                       (Hash Partitioner & Range Partitioners)
                       
                       Applied only on Pair RDD


   13. subtract, intersection, union, cartesian
                    => Applies on two RDDs
                    => ex: rdd3 = rdd1.union(rdd2)
  
          Let's say rdd1 has 2 partitions & rdd2 has 3 partitions:

          ex 1:  rdd3 = rdd1.subtract(rdd2)     -> rdd3 has 2 + 3 partitions. (wide)
          ex 2:  rdd4 = rdd1.intersection(rdd2) -> rdd4 has 2 + 3 partitions. (wide) 
          ex 3:  rdd5 = rdd1.union(rdd2)        -> rdd5 has 2 + 3 partitions. (narrow)
          ex 3:  rdd6 = rdd1.cartesian(rdd2)    -> rdd5 has 2 * 3 partitions. (wide) 



   xxxByKey => They work only on pair RDDs
   ---------------------------------------

   14. mapValues  => map function applied only on the values part of a pair RDD.
       

   15. groupByKey  => Applied on an RDD[(U, V)], groupBy returns an RDD of type RDD[(U, Iterable[V])]
                   
                   => It will aggregate all the values of each unique key, so that the output will
                      have only unique keys and aggregated value.

                   => Results in global shuffle. DO NOT USE IT.
    
   16. reduceByKey  => Take a reduce function as input and applies that reduce function
                       on all the values of each unique key in the RDD, so that the output will
                       have only unique keys and reduced values.

                       Is a two stage operation:
                          stage 1: Reduces the values in each partition (narrow)
                          stage 2: Reduce the outputs of all the partitions (wide)
   


   17. sortByKey      => sorts the data based on the key

   18. aggregateByKey => aggregateByKey(zero-value, seq-op, comb-op) 

                         zero-value: is the value you start with; is of the type of your final result.
                         seq-op: is a fold function, whose output is of the type of zero-value.
                                 is applied to indidual partitions.

                         comb-op: is a reduce operation that reduces all the values of each unique
                                  key across partitions (applied on the output of seq op)

	zero-value:  (0, 0)
    	seq-op: (lambda x, y: (x[0] + y, x[1] + 1) )
    	comb-op: (lambda x, y: (x[0] + y[0], x[1] + y[1])

			Stage 1: It will reduce the values in each patition (narrow)
                        Stage 2: It will reduce the partial results across partitions (shuffling)

  	P0 : (a, 10) (a, 20) (a, 25) (b, 12) (b, 25) (c, 12) (c, 7) 
    	stage 1:  (a, (55, 3)) (b, (27, 2))  (c, (19, 2))

  	P1 : (a, 13) (a, 25) (a, 15) (b, 2) (b, 5)
    	stage 1: (a, (53, 3))  (b, (7,2))

  	P2 : (a, 11) (a, 10) (a, 22) (b, 52) (b, 35) 
    	stage 1: (a, (43, 3))  (b, (87,2))


        stage 2:  (a, (151, 9)), (b, (121, 6)) ...   (aggregates outputs from stage 1)
    
  
  19. joins :  Joining two pair RDDs. Joind the two RDDs based on the key, and values are paired.
               
               RDD[(U, (V,W) )] = RDD[(U, V)].join( RDD[(U, W)] )

               Transformations:  join, leftOuterJoin, rightOuterJoin, fullOuterJoin
   

  20. cogroup :  groupByKey within each RDD + full outer join the grouped RDDs

      RDD1: (a, 1) (a, 2) (b, 1) (b, 5)  --> (a, Iterable(1,2)) (b, Iterable(1,5))
      RDD2: (a, 3) (a, 5) (b, 7) (b, 6)  --> (a, Iterable(3,5)) (b, Iterable(7,6))

          -> (a, (Iterable(1,2), Iterable(3,5)) ) (b, (Iterable(1,5), Iterable(7,6)) )

      rdd3 = rdd1.cogroup(rdd2)


  ------------------------------------------

  from cars.csv, show me all american makes with hieghest average weight in descending order.


filePath = "C:\\PySpark\\data\\cars.tsv"

zv = (0,0)

def seq_op(z, e):
    return (z[0] + e, z[1] + 1)

def comb_op(a, b):
    return (a[0] + b[0], a[1] + b[1])

output = sc.textFile( filePath, 3 ) \
            .map(lambda l: l.split("\t")) \
            .filter(lambda t: t[9] == 'American') \
            .map(lambda arr: (arr[0], int(arr[6])) ) \
            .aggregateByKey(zv, seq_op, comb_op) \
            .mapValues(lambda t: t[0]/t[1]) \
            .sortBy(lambda t: t[1], False)

for x in output.collect():
    print(x)

 -------------------------------------------------------------------------

  RDD Actions
  -----------


     => Produce some output.
     => Triggers execution of the tasks. 


   1. collect()    -> returns an Array witj all the elements of the RDD to the driver.
   2. count()
   3. countByKey()
   4. countByValue()
   5. first
   6. take(n)         -> returns first 'n' elements of the RDD
   7. takeOrdered(n)  -> returns first 'n' elements of the RDD in a sorted order.
   8. takeSample(replacementFlag, count, [seed])   
                      -> samples n elements from the RDD. 
                         ex: rdd1.takeSample(True, 10, 35)
 
           sampling:  withReplacement, withOutreplacement
                      withReplacement: True / False

           seed: allows you to repeat the same output across multiplt executions.

  9. reduce
  10. foreach

  11. saveAsTextFile

  12. saveAsSequenceFile
  13. saveAsObjectFile 

-------------------------------------------------------------------

  Spark Shared Variables
  ----------------------

   Closure: Reresents all the modules that must be visible to an executor to perform
            some tasks. 

            A closure represents a function that refers to external variables or methods. 


      rdd1 -> 1 to 9000, 9 partitions (each partition has 1000 objects)

      primes = sc.accumulator(0)

      def double(n):
         global primes
         if (isPrime(n)) primes.add(1) 

         return n*2

      def isPrime(n):
          return 1 if it is prime
          else return 0

      rdd1 = sc.parallelize( range(1, 9001), 9)
      rdd2 = rdd1.map( double )

      print(primes)    ==> ??

  
  Problems with local variables:
  ------------------------------

   1. A local copy of the variable if sent to every task. 
   2. The value of the local variables can not be sent back to the driver.
   3. Because of this local variables can not be used to implement counter.


  Spark provides two shared variable to overcome these problems:
  --------------------------------------------------------------

    1. Accumulator
         -> Is created by the sparkContext and maintained at the driver side.
         -> All tasks can add to /update the value of this variable
         -> Used to implement counters (like couting how many errors you are getting)

    2. Broadcast Variable

        dict = sc.broadcast({1: 'a', 2:'e', 3:'i', 4:'o', 5:'u'})   

        rdd1 -> [1,2,3,4,5,6,7]
        rdd2 = rdd1.map( lambda x: dict.value[x] )    

         ->one copy of the broadcast variable is stored in the storage memory of every executor.
         -> this save memory, because we do not need a separate local copy for every task
     
       
          Core Abstractions of Spark => RDD, Accumulator, Broadcast

            rdd = sc.textFile(....)
            acc = sc.accumulator( 0 )
            bv = sc.broadcast( { ..... } )

 =====================================================================================

   spark-submit
   ------------
  
      -> Single command that is used to submit :
               any spark application : Python, Scala, Java, R
               to 
               any cluster : Standalone, YARN, MESOS   

      
  spark-submit --master local --deploy-mode cluster --driver-memory 2G --executor-memory 2G 
  --driver-cores 2 --executor-cores 5 --total-executor-nodes 100


  To submit a PySpark application:

  ./bin/spark-submit --master local <python-file-path> <command-line-args>
   

 ================================================================================================

  SPARK SQL  (pyspark.sql)
  ---------
    
   -> Spark's Structured Data Processing API
   -> Built on top of Spark-Core

   -> SparkSession :   (from Spark 2.0 onwards)

        -> Represents a user session
        -> Encapsulates a sparkContext
             
        -> Within an application scope, you can have only one spark Context, but we can
           have multiple SparkSessions. 

        -> Is the starting point of any Spark SQL application.


   Data Abstractions => DataFrame  

   DataFrame --> Is a distributed, partitioned collection of Row objects
                 Row: pyspark.sql.Row

             --> In-memory distributed table

   Row -> Is a set of named columns with a defined schema

   Row[ name, age, salary ]  -> name, age, salary are columns that are stored in Spark SQL's own
                                data types like IntergerType, StringType.
   

   Special Optimizers are there that can optimize the execution flow of dataframes which brings a lot
   of efficiency compared to RDDs. 

         -> Catalyst Optimizer
         -> Tungston Optimizer          


   What types of data can be processed using Spark SQL
   ---------------------------------------------------

       1. Structured Data Formats => Parquet (default), JSON, CSV (delimited text), ORC
       2. Hive  
       3. JDBC:  -> Databases : RDBMS & NoSQL databases.

    
   Steps in Spark SQL Programs:
   ----------------------------

   1. Load   -> read the data from some external source into a dataframe

           df = spark.read.format("csv").option("header", "true").load(<file-path>)


   2. Transform  -> By using DataFrame API or by using SQL, you can process the data of a DF

           => DataFrame API
           => Using SQL     

           df2 = df.select(......)
                   .where(........)

   3. Save -> Save the data of a DF into a structured file format, hive or to a database. 
  
           df.write.format("json").save(<directory-path>)


 -------------------------------------------------------

   DATA in a DataFrame => Rows

      Row(age=25, gender='Male', name='Satya', phone='8501099876', userid=1), 
      Row(age=None, gender='Male', name='Vivek', phone='5676599876', userid=2), 
      Row(age=25, gender='Male', name='Rahim', phone=None, userid=3)

  
  METADATA (schema) of a DataFrame => StructType object
   
     StructType(
          List(StructField(age,LongType,true),
               StructField(gender,StringType,true),
               StructField(name,StringType,true),
               StructField(phone,StringType,true),
               StructField(userid,LongType,true)
              )
     )


 ===============================================

  Columns Functions
  -----------------

   -> col, column, expr
   -> lit
   -> cast
   -> alias



  DataFrame API Transformations
  -----------------------------

  1. select

          => Can take a string argumentts (column names) 
          => can take "column" arguments -  col, column, expr

  2. selectExpr

          => Takes string arguments and treats all the strings as expressions
             (similar to applying 'expr' )

  	  => lit (method) => return a column object with a literal value

  3. withColumn

          => Adds a new columns to the output dataframe.

	df2 = df.select(expr("*")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("country", lit("India")) \
        	.withColumn("frequent", expr("count > 20"))
 
  4. withColumnRenamed
 
         => Change the name of an existing column in the output dataframe

	df2 = df.select(expr("*")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("country", lit("India")) \
        	.withColumn("frequent", expr("count > 20")) \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

  5. drop 
        => to drop one or more columns in the output dataframe.

	df3 = df2.drop("frequent", "country")  


  6. where / filter
	
        => Takes a string arguments representing a filter condition
        => Column expresssion

	df.filter(col("count") < 2).show(2)
	df.filter("count < 2").show(2)

	df.where(col("count") < 2).show(2)
	df.where("count < 2").show(2)


  7. distinct

	df.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct()
        -> Returns a dataframe


  8. sample
        -> Samples a fraction of records from the input dataframe.

	seed = 35
	withReplacement = True
	fraction = 0.03

	df.sample(withReplacement, fraction, seed).show()


  9. randomSplit
     
        -> Splits a dataframe into multiple dataframes in the ratios specified

         dfArr = df2.randomSplit([0.5, 0.3, 0.2], 20) 
	 print( dfArr[0].count(), dfArr[1].count(), dfArr[2].count() )


  10. union  
         -> adding two dataframes with the same schema.


  11. orderBy / sort

	     df2.select("count", "destination") \
   		.sort("count", desc("destination")) \
   		.show(100, False)

  12. limit

 
  13. repartition & coalesce

        => You can repartition using a partitioned columns (with the same number of columns)
        
	df.rdd.getNumPartitions() 
	df.repartition(col("DEST_COUNTRY_NAME"))
	df.repartition(5)
	df.repartition(5, col("DEST_COUNTRY_NAME"))
	df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2)

  14. groupBy

	df.groupBy("InvoiceNo") \
  	  .agg(mean("Quantity").alias("avgQty"), sum("Quantity"), count("Quantity")) \
  	  .show()


  Joins
  -----
   
     -> Either use SQL approach or use DataFrame API
     -> inner, left_outer, right_outer, outer (full outer), left_semi, left_anti, cross
  
     DataFrame API:
     --------------

        joinEmpDept = employee["deptid"] == department["id"]

        joinedDf = emp.join(dept,  joinEmpDept, "inner")

  
     Left Semi Join  => Inner Join, but data is fetched ONLY from left side table
            Qry: select * from employee where deptid IN (select deptid from department)
     
     Left Anti Join
      	   Qry: select * from employee where deptid NOT IN (select deptid from department)


	
 ========================================================

  aggregations
  ------------

  count, countDistinct & approx_count_distinct

  sum, sumDistinct, first, last, min, max

  avg / mean

  variance (var_samp) , stddev (stddev_samp) , var_pop, var_samp, stddev_pop, stddev_samp

  skewness, kurtosis

  Covariance, Correlation    => measures the variance or similarity between two columns

 ========================================================

  Applying SQL on DataFrames:


   1. Local temporary views
	
	=> df.createOrReplaceTempView("users")
           spark.sql("select * from users").show()

        => The view is local to the "spark" spark session.
           It will be cleared when spark is closed
           Is not accessible from other sparksessions
         
   
   2. Global temporary views

	=> df.createGlobalTempView("g_users")
           spark.sql("select * from global_temp.g_users").show()
           spark.newSession().sql("select * from global_temp.g_users").show()

        => The view create at global scope and attached a system preserved temp database called
          "global_temp"
           Is accessible from all sparksessions
   
 =========================================================
  
  Conversions
  ------------


  1. Creating  a DF from an RDD[Row]

       dataframe = spark.createDataFrame( rdd[Row], schema)

  2. Get an RDD from a DataFrame

        rdd[Row] = dataframe.rdd


  3.  Converting  a list of tuples into a DF using toDF() method

     employee = [(1, "Raju", 25, 101),
            (2, "Ramesh", 26, 101),
            (3, "Amrita", 30, 102),
            (4, "Madhu", 32, 102),
            (5, "Aditya", 28, 102),
            (6, "Pranav", 28, 10000)]

    df = spark.createDataFrame(employee).toDF("id", "name", "age", "deptid")



























