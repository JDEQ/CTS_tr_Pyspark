
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API  
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - MySQL & Hive
   Spark Streaming
	-> Structured Streaming	

  Materials
  ---------
   => PDF Presentations
   => Code Modules 
   => Class Notes 
   => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark
   ------
	-> Spark is a unified in-memory distributed computing framework.
	-> Spark is written in SCALA language

	in-memory computation: Ability to persist intermediate results (partitions) in memory so that 
	subsequent tasks can directly be launched on these stored partitions. 

	-> Spark is a polyglot
		-> Scala, Java, Python, R and SQL

	-> Spark application can be executed on clusters with multiple cluster managers
		-> local
		-> Spark Stansalone Scheduler
		-> YARN (popular)
		-> Mesos
		-> Kubernetes


   Spark Unified Stack
   -------------------
	Provides a consistent set of API that can all run on the same execution engine using well defined
	data abstraction. 

		Batch Processing of unstructured data	 => Spark Core API (RDDs)
		Batch Processing of structured data	 => Spark SQL (DataFrames)
		Stream processing (real time)		 => Spark Streaming
		Predictive analytics (machine learning)  => Spark MLLib
		Graph parallel computations		 => Spark GraphX		


   Getting started with Spark
   --------------------------

	1. Working in CTS vLab

	2. Installing Spark Dev environemnt on your local machine

		-> Install 'Anaconda' distribution for python.
		-> Download and unzip Apache Spark
			https://spark.apache.org/downloads.html
		-> Install pyspark using pip
			pip install pyspark
		-> If pip does not work, follow the instructions given in the shared document.
			Github: Pyspark-JupyterNotebooks-Windows-Setup.pdf

	3. Databricks community edition

		-> Signup: https://www.databricks.com/try-databricks
		-> Login: https://community.cloud.databricks.com/login.html


   Spark Architecture
   ------------------
	
    
    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.



    RDD (Resilient Distributed Dataset)
    -----------------------------------

    -> Fundamental data abstraction of Spark Core API
    -> RDD is a collection of distributed in-memory partitions.
	-> Partition is a collection of objects.

    -> RDDs are immutable

    -> RDDs are lazily evaluation. 


    Creating RDDs
    -------------

     3 ways of creating RDDs:

	1. Create an RDD from external data files

		rddFile = sc.textFile(filePath, 4)

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([1,2,3,4,5,9,4,2,6,8,4,8,9,0,6,5,5], 2)

	3. Create an RDD by applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


    RDD Operations
    --------------

	Two operations

	1. Transformations
		-> Transformations does not cause executions
		-> Transformations only create lineage DAGs

	2. Actions
		-> Actions trigger execution
		-> Converts logical plan to physical plan and sends jobs to the cluster for execution.


    RDD Lineage DAG
    ---------------
    When an RDD is created, the driver creates a Lineage DAG for that RDD
    Lineage DAG is a logical plan on how to create RDD partitions
    Lineage DAG contains a hierarchy of all the RDDs all the way from the very first RDD.

	rddFile = sc.textFile(filePath, 4)
	rddFile Lineage DAG:  (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	rddWords Lineage DAG:  (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	rddPairs Lineage DAG:  (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
	rddPairs Lineage DAG:  (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


   RDD Transformations
   -------------------

   1. map		P: U -> V
			Object to object transformation
			input RDD: N objects, output RDD: N objects

		rdd1.map(lambda x: x > 7).collect()

   2. filter		P: U -> Boolean
			Only those objects for which the fn returns True will be in the output RDD
			input RDD: N objects, output RDD: <= N objects

		rdd2.filter(lambda x: sum(x) % 4 == 0).collect()
    
   


	

















