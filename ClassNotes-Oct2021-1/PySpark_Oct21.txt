
  Agenda
  ------
    -> Spark - Basics & Architecture
    -> Spark Core API
	-> RDD - Transformations & Actions
        -> Shared Variables
    -> Spark SQL
	-> DataFrames
    -> Machine Learning & Spark MLlib  
    -> Introduction to Spark Streaming 
  
  ===================================================
  
   Materials
   ---------
      
     => Daily Class Notes
     => PDF versions of presentations.
     => Lot of Code
	
     https://github.com/ykanakaraju/pyspark

  ===================================================

   Spark  -> Is unified in-memory distributed computing framework.
	  -> Is written in Scala

	  -> Spark supports multiple programming languages
		-> Scala, Java, Python, R

    Computing Cluster :  
    ------------------

       - Is a unified entity comprising of lot of machines (nodes) whose cumulative resources
	 can be used to distribute the storage or to distribute processing.

       -> Usually managed by a cluster manager which is reponsible for allocating resources to
	  various jobs running on the clsuter.

   In-memory distributed computing
   --------------------------------

       -> The results of the intermediate tasks can be stored in-memory and subsequent tasks
	  can read these results from memory. 

    Spark Unified Framework
   ------------------------   
    Spark unified framework provodes a set of consiste API to process different 
    analytical workloads running on the same execution engine. 


	Hadoop Ecosystem (except Spark)
        --------------------------------	
         Batch Analytics of Unstructured Data	 => MapReduce
	 Batch Analytics of Structured Data	 => Hive, Impala, Drill
	 Streaming Analytics (real time)	 => Kafka, Storm, Flink
	 Predictive Analytics (machine learning) => Mahout
	 Graph Parallel Computations             => Giraph

        Spark Unified Framework
        --------------------------------	
         Batch Analytics of Unstructured Data	 => Spark Core API (RDD)
	 Batch Analytics of Structured Data	 => Spark SQL (DataFrames)
	 Streaming Analytics (real time)	 => Spark Streaming, Structured Streaming
	 Predictive Analytics (machine learning) => Spark Mllib
	 Graph Parallel Computations             => Spark GraphX
	
	
   Spark Architecture
   ------------------    
    1. Cluster Manager (CM)	
	-> Applications are submitted to the CM
	-> CM schedules the job
        -> Allocates resource containers (driver & executors) to the applications
	
    2. Driver Process
	-> Master process
	-> Manages the user-code
	-> A sparkContext (or SparkSession) object is created
	-> Analyses the user code and sends tasks to the executors

	Deploy-modes
	    
            2.1 Client (default) -> The driver runs on the client machine
	    2.2 Cluster		 -> Driver runs as on of the processes in the cluster	
	
    3. Executors
	-> Executors execute the tasks sent by the driver
	-> All tasks does the same process but on different partitions of the data	
	-> Report the status of the task to the driver.

    4. SparkContext
	-> SparkContext represents an application 
	-> Represents a connection to the cluster.


   Getting Started with Spark
   --------------------------
   
     1. Downloading and setting up Spark

	   URL:  https://spark.apache.org/downloads.html		

           -> Download thes aprk xxx.tgz file and extract it to a suitable folder
           -> Setup environment variables
		SPARK_HOME  : spark installation folder (ex: E:\spark-3.0.0-bin-hadoop2.7)
	 	HADOOP_HOME : spark installation folder
		-> Add <SPARK_HOME>\bin folder to the PATH environment variable.

     2. Install an IDE and setup PySpark

	 -> Refer to the installation document shared with you to setup PySpark with
	    Spyder and Jupyter Notebooks.

	 -> The document assumes that you have "Anaconda Navigator" installed. 

     3. Databricks Community Edition Account (free account with 15 GB)

	  -> URL: https://databricks.com/try-databricks 
	  -> Read the "Explore the Quickstart Tutorial" 
	
       
   Spark Core API
   -------------- 
     => Is the low level API     
     => The fundamental data abstraction of Spark Core API is "RDD"


   RDD (resilient distributed dataset)
   -----------------------------------

     -> Is a collection of in-memory distributed partitions.
	   -> These partitions are created by reading data from some source data 
	      file from disk.	

	   -> Each partition is a collection of objects

    -> RDDs are immutable
   	    -> the content of RDD can not be changed
            -> We can apply transformations to create new RDDs.

    -> RDDs are lazily evaluated
	    -> RDDs are executed only when an action command is issued
	    -> Transformations does not cause execution.

    -> RDD :
	  Meta Data => Lineage DAG (logical plan)
	  Data	    => Set of in-memory distributed partition.

    -> RDDs are resilient
	    -> RDD operations do not fail because of missing in-memory partitions. RDDs can 
	       recreate missing partitions. 	


   Creating RDDs
   -------------
	There are three ways:

	1. RDDs can be created from external data files

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 5 )   
                -> Creates RDD with 5 partitions

		rdd1 =  sc.textFile( "E:\\Spark\\wordcount.txt" )

		-> The default number of partitions is determined by "sc.defaultMinPartitions"
		-> The default values of "sc.defaultMinPartitions" is 2 if you have atleast 2 cores.

	2. RDDs can be created from programmatic data

		rdd1 = sc.parallelize( [6,8,9,4,1,3,2,5,6,7,9,0,6,2,8,1,2,1,2,6], 3 )
		-> Creates RDD with 3 partitions

		rdd1 = sc.parallelize( [6,8,9,4,1,3,2,5,6,7,9,0,6,2,8,1,2,1,2,6])

		-> The default number of partitions is determined by "sc.defaultParallelism"
		-> The default values of "sc.defaultParallelism" is equal to the total number of core


	3. By applying transformations on existing RDDs

	        rdd2 = rdd1.map(lambda x: x.upper())

		-> the output rdd will have, by default, same number of partitions as input RDD.

   What can we do with RDDs
   ------------------------
	We can do two things with an RDD

	1. Transformations
		-> Does not cause execution
		-> Cause creation of lineage DAG (logical plan) only

	2. Actions
	        -> Actions cuase the logical plan of the RDD to be converted into physical execution plan
		-> Cause the execution of RDD and create output

   NOTE:
   -----
	rdd1.getNumPartitions()  => Gives the partition count of rdd1

   RDD Lineage
   -----------
	
     Lineage DAG represents a logical plan that tells how to create an RDD.
     It tracks all the parent RDDs and transformations all the way from the very first RDD.
	
     rdd1 = sc.textFile( file, 4 )
	==> Lineage:  rdd1 -> sc.textFile

     rdd2 = rdd1.map(lambda x: x.upper())
	==> Lineage:  rdd2 -> rdd1.map -> sc.textFile

     rdd3 = rdd2.filter(lambda x: len(x) > 50)
	==> Lineage:  rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

     rdd4 = rdd3.flatMap(lambda x: x.split(" "))
	==>  rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.textFile

 
     rdd4.collect()
	
	 rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.textFile  	
	 Tasks: sc.textFile (rdd1) -> map (rdd2) -> filter (rdd3) -> flatMap (rdd4) -> collect()


   Types of Transformations
   ------------------------

	1. Narrow Transformations
		-> Does not cause shuffling of the data from one partition to other partitions
		-> Partition to partition transformations
		-> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
		-> Causes shuffling of the data
		-> One output partition may need data from multiple input partitions
		-> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

        rdd1 = sc.textFile( file )
	rdd2 = rdd1.t2(....)
	rdd3 = rdd1.t3(....)
	rdd4 = rdd3.t4(....)
	rdd5 = rdd3.t5(....)
	rdd6 = rdd4.t6(....)
	rdd7 = rdd4.t7(....)
	rdd7.persist( StorageLevel.MEMORY_AND_DISK )     => instruction to not subject the rdd to GC.
	rdd8 = rdd7.t8(....)

	rdd8.collect()

	lineage of rdd8 => rdd8 -> rdd7.t8 -> rdd4.t7 -> rdd3.t4 -> rdd1.t3 -> sc.textFile
	execution plan => sc.textFile (rdd1) -> t3 (rdd3) -> t7 (rdd7) -> t8 (rdd8) --> collect

	rdd7.collect()
	lineage of rdd7 -> rdd4.t7 -> rdd3.t4 -> rdd1.t3 -> sc.textFile

	
	Persistence Type
        ----------------
		-> in-memory in deserilzed format
		-> in-memory in serialized format
		-> on-disk


        Persistence StorageLevel
        ------------------------

	1. MEMORY_ONLY    	   => (default) Stored in-memory (in serialized format)
					-> RDD may be fully persisted, partially persisted or
					   not persisted at all based on available memory. 

	2. MEMORY_AND_DISK	   => 

	3. DISK_ONLY		   	
	
	4. MEMORY_ONLY_2

	5. MEMORY_AND_DISK_2


       Persistence Commands
       --------------------

	-> persist( StorageLevel.MEMORY_AND_DISK )
	-> persist()   -> in-memory persistence in serialized format
	-> cache()     -> in-memory persistence in serialized format

	-> unpersist()


   Executor Memory Structure
   -------------------------


   RDD Transformations
   -------------------
	
     1. map		P: U -> V
			Element to element transformation
			input RDD: N elements, output RDD: N elements
			  

     2. filter		P: U -> Boolean
			The output RDD will have only those objects for which the function returns True
			input RDD: N elements, output RDD: <= N elements


    3. glom		P: None
			Will return an Array with all the elements of each partition
			input RDD: N elements, output RDD: = number of partitions of input RDD

	rdd1			rdd2 = rdd1.glom()
	
	P0:  6,5,6,4,3 -> glom -> P0: [6,5,6,4,3]
	P1:  5,6,3,2,1 -> glom -> P1: [5,6,3,2,1]
	P2:  4,4,3,6,1 -> glom -> P2: [4,4,3,6,1]
			
	rdd1.count -> 15	rdd2.count -> 3

		rdd1.glom().collect()
		=> [[6, 8, 9, 4, 1, 3], [2, 5, 6, 7, 9, 0], [6, 2, 8, 1, 2, 1, 2, 6]]
		rdd1.glom().map(lambda x: len(x)).collect()
		=> [6, 6, 8]
		rdd1.glom().map(len).collect()
		=> [6, 6, 8]

   4. flatMap		P: U -> Iterable[V]       (iterable -> some collection that can be looped)
			flatMap flattens the iterables produced by the function.
			input RDD: N elements, output RDD: >= N elements

 		
   5. mapPartitions	P: Iterator[U] -> Iterator[V]	
			Applies a function on the entire partition


	rdd1	    rdd2 = rdd1.mapPartitions(lambda x: 

	P0:  6,5,6,4,3 -> mapPartitions -> P0: 
	P1:  5,6,3,2,1 -> mapPartitions -> P1: 
	P2:  4,4,3,6,1 -> mapPartitions -> P2: 


		rdd1.mapPartitions(lambda x: [len(list(x))]).collect()
		rdd1.mapPartitions(lambda x: [sum(x)]).collect()


   6. mapPartitionsWithIndex    P: Int, Iterator[U] -> Iterator[V]
				Same as mapPartitions, but we partition-id also as function parameter
			

	rdd1.mapPartitionsWithIndex(lambda indx, iter :  [(indx, sum(iter))]).collect()

   
   7. distinct			P: None,  Optional: Number of output partitions
				Will return an RDD with distinct elements.
				input RDD: N elements, output RDD: <= N elements

   Types of RDDs
   -------------		
	1. Generic RDDs -> RDD[U]
	2. Pair RDDs	-> RDD[(U, V)]


   8. mapValues			P: U -> V
				Applied only to pair RDDs
				Applies a function on only the value part of the pair RDD
				Element to element transformation
				input RDD: N elements, output RDD: N elements

				rdd2.mapValues(lambda x: str(x) + str(x) ).collect()


   9. sortBy			P: U -> V
				The elements of the RDD are sorted based on the value of the function output.

				rddWords.sortBy(lambda x: len(x)).collect()
				rddWords.sortBy(lambda x: len(x), False).collect()
				rddWords.sortBy(lambda x: len(x), True, 5).collect()

   10. groupBy			P: U -> V
				Returns a 'pair RDD' where the 'key' is each unique values of the function
				output and 'value' is a (ResultIterable) collection of all elements of the
				RDD that produces the same key.

   11. randomSplit		P: An array of ratios  ex: [0.4, 0.3, 0.3]
				Return a array of RDDs randomly split in the specified ratios.

		rddArr = rdd1.randomSplit([0.6, 0.4])
		rddArr = rdd1.randomSplit([0.6, 0.4], 5464)

   12. repartition		P: Number of output partitions
				Used to increase or decrease the number of partitions of the output RDD
				causes global shuffle

   13. coalesce			P: Number of output partitions
				Used to decrease the number of partitions of the output RDD
				Causes partition merging

   14. partitionBy		P: Number of partitions, Optional: Custom Partitioning function
				-> Applied only to pair RDDs
				-> Controls which elements goes to which partition by applying default 
				   hash partitioner or custom partitioning function on the 'key' of the
				   (k, v)
			
	def city_partitioner(city): 
    		return len(city)

	rdd3 = sc.parallelize(transactions, num_partitions) \
         	.map(lambda e: (e['city'], e))

	rdd4 = rdd3.partitionBy(num_partitions, city_partitioner)


   15. union, intersection, subtract, cartesian

	Let us say we have rdd1 with M partitions, rdd2 with N partitions

	command				number of output partitions
        ------------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide

   




    Guidelines / Best Practices
    ---------------------------
	-> The optimal size of a partition is 128 MB  (anywhere between 100 MB to 150 MB )
	-> You can have as many partitions as upto 2 to 3 times available cores.
	-> Do not use too big partitions ( 500 MB, 1 GB etc) as these could result if shuffle 
           memory overflow.
        -> Do not use too small partitions (10 MB, 25 MB). These could results in too much 
	   of shuffling.
	-> Have atleast as many partitions as there are CPU cores. 


 
To Download a file from databricks:
-----------------------------------
/FileStore/<FILEPATH>
https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

Example:
/FileStore/tables/wordcount-5.txt
https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


/FileStore/tables/WordcountExample1Output/part-00000

https://community.cloud.databricks.com/files/tables/WordcountExample1Output/part-00000?o=4949609693130439#tables/new/dbfs




  RDD Actions
  -----------

	1. collect

	2. count

	3. saveAsTextFile





 






