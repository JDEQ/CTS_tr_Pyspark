
  Agenda
  ------
    -> Spark - Basics & Architecture
    -> Spark Core API
	-> RDD - Transformations & Actions
        -> Shared Variables
    -> Spark SQL
	-> DataFrames
    -> Machine Learning & Spark MLlib  
    -> Introduction to Spark Streaming 
  
  ===================================================
  
   Materials
   ---------
      
     => Daily Class Notes
     => PDF versions of presentations.
     => Lot of Code
	
     https://github.com/ykanakaraju/pyspark

  ===================================================

   Spark  -> Is unified in-memory distributed computing framework.
	  -> Is written in Scala

	  -> Spark supports multiple programming languages
		-> Scala, Java, Python, R

    Computing Cluster :  
    ------------------

       - Is a unified entity comprising of lot of machines (nodes) whose cumulative resources
	 can be used to distribute the storage or to distribute processing.

       -> Usually managed by a cluster manager which is reponsible for allocating resources to
	  various jobs running on the clsuter.

   In-memory distributed computing
   --------------------------------

       -> The results of the intermediate tasks can be stored in-memory and subsequent tasks
	  can read these results from memory. 

    Spark Unified Framework
   ------------------------   
    Spark unified framework provodes a set of consiste API to process different 
    analytical workloads running on the same execution engine. 


	Hadoop Ecosystem (except Spark)
        --------------------------------	
         Batch Analytics of Unstructured Data	 => MapReduce
	 Batch Analytics of Structured Data	 => Hive, Impala, Drill
	 Streaming Analytics (real time)	 => Kafka, Storm, Flink
	 Predictive Analytics (machine learning) => Mahout
	 Graph Parallel Computations             => Giraph

        Spark Unified Framework
        --------------------------------	
         Batch Analytics of Unstructured Data	 => Spark Core API (RDD)
	 Batch Analytics of Structured Data	 => Spark SQL (DataFrames)
	 Streaming Analytics (real time)	 => Spark Streaming, Structured Streaming
	 Predictive Analytics (machine learning) => Spark Mllib
	 Graph Parallel Computations             => Spark GraphX
	
	
   Spark Architecture
   ------------------    
    1. Cluster Manager (CM)	
	-> Applications are submitted to the CM
	-> CM schedules the job
        -> Allocates resource containers (driver & executors) to the applications
	
    2. Driver Process
	-> Master process
	-> Manages the user-code
	-> A sparkContext (or SparkSession) object is created
	-> Analyses the user code and sends tasks to the executors

	Deploy-modes
	    
            2.1 Client (default) -> The driver runs on the client machine
	    2.2 Cluster		 -> Driver runs as on of the processes in the cluster	
	
    3. Executors
	-> Executors execute the tasks sent by the driver
	-> All tasks does the same process but on different partitions of the data	
	-> Report the status of the task to the driver.

    4. SparkContext
	-> SparkContext represents an application 
	-> Represents a connection to the cluster.


   Getting Started with Spark
   --------------------------
   
     1. Downloading and setting up Spark

	   URL:  https://spark.apache.org/downloads.html		

           -> Download thes aprk xxx.tgz file and extract it to a suitable folder
           -> Setup environment variables
		SPARK_HOME  : spark installation folder (ex: E:\spark-3.0.0-bin-hadoop2.7)
	 	HADOOP_HOME : spark installation folder
		-> Add <SPARK_HOME>\bin folder to the PATH environment variable.

     2. Install an IDE and setup PySpark

	 -> Refer to the installation document shared with you to setup PySpark with
	    Spyder and Jupyter Notebooks.

	 -> The document assumes that you have "Anaconda Navigator" installed. 

     3. Databricks Community Edition Account (free account with 15 GB)

	  -> URL: https://databricks.com/try-databricks 
	  -> Read the "Explore the Quickstart Tutorial" 
	
       
   Spark Core API
   -------------- 
     => Is the low level API     
     => The fundamental data abstraction of Spark Core API is "RDD"


   RDD (resilient distributed dataset)
   -----------------------------------

     -> Is a collection of in-memory distributed partitions.
	   -> These partitions are created by reading data from some source data 
	      file from disk.	

	   -> Each partition is a collection of objects

    -> RDDs are immutable
   	    -> the content of RDD can not be changed
            -> We can apply transformations to create new RDDs.

    -> RDDs are lazily evaluated
	    -> RDDs are executed only when an action command is issued
	    -> Transformations does not cause execution.

    -> RDD :
	  Meta Data => Lineage DAG (logical plan)
	  Data	    => Set of in-memory distributed partition.

    -> RDDs are resilient
	    -> RDD operations do not fail because of missing in-memory partitions. RDDs can 
	       recreate missing partitions. 	


   Creating RDDs
   -------------
	There are three ways:

	1. RDDs can be created from external data files

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 5 )   
                -> Creates RDD with 5 partitions

		rdd1 =  sc.textFile( "E:\\Spark\\wordcount.txt" )

		-> The default number of partitions is determined by "sc.defaultMinPartitions"
		-> The default values of "sc.defaultMinPartitions" is 2 if you have atleast 2 cores.

	2. RDDs can be created from programmatic data

		rdd1 = sc.parallelize( [6,8,9,4,1,3,2,5,6,7,9,0,6,2,8,1,2,1,2,6], 3 )
		-> Creates RDD with 3 partitions

		rdd1 = sc.parallelize( [6,8,9,4,1,3,2,5,6,7,9,0,6,2,8,1,2,1,2,6])

		-> The default number of partitions is determined by "sc.defaultParallelism"
		-> The default values of "sc.defaultParallelism" is equal to the total number of core


	3. By applying transformations on existing RDDs

	        rdd2 = rdd1.map(lambda x: x.upper())

		-> the output rdd will have, by default, same number of partitions as input RDD.

   What can we do with RDDs
   ------------------------
	We can do two things with an RDD

	1. Transformations
		-> Does not cause execution
		-> Cause creation of lineage DAG (logical plan) only

	2. Actions
	        -> Actions cuase the logical plan of the RDD to be converted into physical execution plan
		-> Cause the execution of RDD and create output

   NOTE:
   -----
	rdd1.getNumPartitions()  => Gives the partition count of rdd1

   RDD Lineage
   -----------
	
     Lineage DAG represents a logical plan that tells how to create an RDD.
     It tracks all the parent RDDs and transformations all the way from the very first RDD.
	
     rdd1 = sc.textFile( file, 4 )
	==> Lineage:  rdd1 -> sc.textFile

     rdd2 = rdd1.map(lambda x: x.upper())
	==> Lineage:  rdd2 -> rdd1.map -> sc.textFile

     rdd3 = rdd2.filter(lambda x: len(x) > 50)
	==> Lineage:  rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

     rdd4 = rdd3.flatMap(lambda x: x.split(" "))
	==>  rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.textFile

 
     rdd4.collect()
	
	 rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.textFile  	
	 Tasks: sc.textFile (rdd1) -> map (rdd2) -> filter (rdd3) -> flatMap (rdd4) -> collect()


   Types of Transformations
   -------------------------

	-> Narrow Transformation


	-> Wide Transformation



   RDD Persistence
   ---------------

        rdd1 = sc.textFile( file )
	rdd2 = rdd1.t2(....)
	rdd3 = rdd1.t3(....)
	rdd4 = rdd3.t4(....)
	rdd5 = rdd3.t5(....)
	rdd6 = rdd4.t6(....)
	rdd7 = rdd4.t7(....)
	rdd7.persist( StorageLevel.MEMORY_AND_DISK )           ==> instruction to not subject the rdd to GC.
	rdd8 = rdd7.t8(....)

	rdd8.collect()

	lineage of rdd8 => rdd8 -> rdd7.t8 -> rdd4.t7 -> rdd3.t4 -> rdd1.t3 -> sc.textFile
	execution plan => sc.textFile (rdd1) -> t3 (rdd3) -> t7 (rdd7) -> t8 (rdd8) --> collect

	rdd7.collect()
	lineage of rdd7 -> rdd4.t7 -> rdd3.t4 -> rdd1.t3 -> sc.textFile

	
	Persistence Type
        ----------------
		-> in-memory deserilzed format
		-> in-memory serialized format
		-> on-disk


        Persistence StorageLevel
        ------------------------
	1. MEMORY_ONLY (default)
	2. MEMORY_AND_DISK
	3. DISK_ONLY
	4. MEMORY_ONLY_SER
	5. MEMORY_AND_DISK_SER
	6. MEMORY_ONLY_2
	7. MEMORY_AND_DISK_2




   RDD Transformations
   -------------------
	
     1. map		P: U -> V
			Element to element transformation
			input RDD: N elements, output RDD: N elements
			  

     2. filter		P: U -> Boolean
			The output RDD will have only those objects for which the function returns True
			input RDD: N elements, output RDD: <= N elements


    3. glom		P: None
			Will return an Array with all the elements of each partition
			input RDD: N elements, output RDD: = number of partitions of input RDD

	rdd1			rdd2 = rdd1.glom()
	
	P0:  6,5,6,4,3 -> glom -> P0: [6,5,6,4,3]
	P1:  5,6,3,2,1 -> glom -> P1: [5,6,3,2,1]
	P2:  4,4,3,6,1 -> glom -> P2: [4,4,3,6,1]
			
	rdd1.count -> 15	rdd2.count -> 3

		rdd1.glom().collect()
		=> [[6, 8, 9, 4, 1, 3], [2, 5, 6, 7, 9, 0], [6, 2, 8, 1, 2, 1, 2, 6]]
		rdd1.glom().map(lambda x: len(x)).collect()
		=> [6, 6, 8]
		rdd1.glom().map(len).collect()
		=> [6, 6, 8]

   4. flatMap		P: U -> Iterable[V]       (iterable -> some collection that can be looped)
			flatMap flattens the iterables produced by the function.
			input RDD: N elements, output RDD: >= N elements

 		
   5. mapPartitions	P: Iterator[U] -> Iterator[V]	
			Applies a function on the entire partition


	rdd1	    rdd2 = rdd1.mapPartitions(lambda x: 

	P0:  6,5,6,4,3 -> mapPartitions -> P0: 
	P1:  5,6,3,2,1 -> mapPartitions -> P1: 
	P2:  4,4,3,6,1 -> mapPartitions -> P2: 


		rdd1.mapPartitions(lambda x: [len(list(x))]).collect()
		rdd1.mapPartitions(lambda x: [sum(x)]).collect()


   6. mapPartitionsWithIndex    P: Int, Iterator[U] -> Iterator[V]
				Same as mapPartitions, but we partition-id also as function parameter
			

	rdd1.mapPartitionsWithIndex(lambda indx, iter :  [(indx, sum(iter))]).collect()

   
   7. distinct			P: None,  Optional: ???
				Will return an RDD with distinct elements.
				input RDD: N elements, output RDD: <= N elements
				



















  RDD Actions
  -----------

	1. collect

	2. count


