
  Agenda
  ------
    -> Spark - Basics & Architecture
    -> Spark Core API
	-> RDD - Transformations & Actions
        -> Shared Variables
    -> Spark SQL
	-> DataFrames
    -> Machine Learning & Spark MLlib  
    -> Introduction to Spark Streaming 
  
  ===================================================
  
   Materials
   ---------
      
     => Daily Class Notes
     => PDF versions of presentations.
     => Lot of Code
	
     https://github.com/ykanakaraju/pyspark

  ===================================================

   Spark  -> Is unified in-memory distributed computing framework.
	  -> Is written in Scala

	  -> Spark supports multiple programming languages
		-> Scala, Java, Python, R

    Computing Cluster :  
    ------------------

       - Is a unified entity comprising of lot of machines (nodes) whose cumulative resources
	 can be used to distribute the storage or to distribute processing.

       -> Usually managed by a cluster manager which is reponsible for allocating resources to
	  various jobs running on the clsuter.

   In-memory distributed computing
   --------------------------------

       -> The results of the intermediate tasks can be stored in-memory and subsequent tasks
	  can read these results from memory. 

    Spark Unified Framework
   ------------------------   
    Spark unified framework provodes a set of consiste API to process different 
    analytical workloads running on the same execution engine. 


	Hadoop Ecosystem (except Spark)
        --------------------------------	
         Batch Analytics of Unstructured Data	 => MapReduce
	 Batch Analytics of Structured Data	 => Hive, Impala, Drill
	 Streaming Analytics (real time)	 => Kafka, Storm, Flink
	 Predictive Analytics (machine learning) => Mahout
	 Graph Parallel Computations             => Giraph

        Spark Unified Framework
        --------------------------------	
         Batch Analytics of Unstructured Data	 => Spark Core API (RDD)
	 Batch Analytics of Structured Data	 => Spark SQL (DataFrames)
	 Streaming Analytics (real time)	 => Spark Streaming, Structured Streaming
	 Predictive Analytics (machine learning) => Spark Mllib
	 Graph Parallel Computations             => Spark GraphX
	
	
   Spark Architecture
   ------------------    
    1. Cluster Manager (CM)	
	-> Applications are submitted to the CM
	-> CM schedules the job
        -> Allocates resource containers (driver & executors) to the applications
	
    2. Driver Process
	-> Master process
	-> Manages the user-code
	-> A sparkContext (or SparkSession) object is created
	-> Analyses the user code and sends tasks to the executors

	Deploy-modes
	    
            2.1 Client (default) -> The driver runs on the client machine
	    2.2 Cluster		 -> Driver runs as on of the processes in the cluster	
	
    3. Executors
	-> Executors execute the tasks sent by the driver
	-> All tasks does the same process but on different partitions of the data	
	-> Report the status of the task to the driver.

    4. SparkContext
	-> SparkContext represents an application 
	-> Represents a connection to the cluster.


   Getting Started with Spark
   --------------------------
   
     1. Downloading and setting up Spark

	   URL:  https://spark.apache.org/downloads.html		

           -> Download thes aprk xxx.tgz file and extract it to a suitable folder
           -> Setup environment variables
		SPARK_HOME  : spark installation folder (ex: E:\spark-3.0.0-bin-hadoop2.7)
	 	HADOOP_HOME : spark installation folder
		-> Add <SPARK_HOME>\bin folder to the PATH environment variable.

     2. Install an IDE and setup PySpark

	 -> Refer to the installation document shared with you to setup PySpark with
	    Spyder and Jupyter Notebooks.

	 -> The document assumes that you have "Anaconda Navigator" installed. 

     3. Databricks Community Edition Account (free account with 15 GB)

	  -> URL: https://databricks.com/try-databricks 
	  -> Read the "Explore the Quickstart Tutorial" 
	
       
   Spark Core API
   -------------- 
     => Is the low level API     
     => The fundamental data abstraction of Spark Core API is "RDD"


   RDD (resilient distributed dataset)
   -----------------------------------

     -> Is a collection of in-memory distributed partitions.
	   -> These partitions are created by reading data from some source data 
	      file from disk.	

	   -> Each partition is a collection of objects

    -> RDDs are immutable
   	    -> the content of RDD can not be changed
            -> We can apply transformations to create new RDDs.

    -> RDDs are lazily evaluated
	    -> RDDs are executed only when an action command is issued
	    -> Transformations does not cause execution.

    -> RDD :
	  Meta Data => Lineage DAG (logical plan)
	  Data	    => Set of in-memory distributed partition.

    -> RDDs are resilient
	    -> RDD operations do not fail because of missing in-memory partitions. RDDs can 
	       recreate missing partitions. 	


   Creating RDDs
   -------------
	There are three ways:

	1. RDDs can be created from external data files

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 5 )   
                -> Creates RDD with 5 partitions

		rdd1 =  sc.textFile( "E:\\Spark\\wordcount.txt" )

		-> The default number of partitions is determined by "sc.defaultMinPartitions"
		-> The default values of "sc.defaultMinPartitions" is 2 if you have atleast 2 cores.

	2. RDDs can be created from programmatic data

		rdd1 = sc.parallelize( [6,8,9,4,1,3,2,5,6,7,9,0,6,2,8,1,2,1,2,6], 3 )
		-> Creates RDD with 3 partitions

		rdd1 = sc.parallelize( [6,8,9,4,1,3,2,5,6,7,9,0,6,2,8,1,2,1,2,6])

		-> The default number of partitions is determined by "sc.defaultParallelism"
		-> The default values of "sc.defaultParallelism" is equal to the total number of core


	3. By applying transformations on existing RDDs

	        rdd2 = rdd1.map(lambda x: x.upper())

		-> the output rdd will have, by default, same number of partitions as input RDD.

   What can we do with RDDs
   ------------------------
	We can do two things with an RDD

	1. Transformations
		-> Does not cause execution
		-> Cause creation of lineage DAG (logical plan) only

	2. Actions
	        -> Actions cuase the logical plan of the RDD to be converted into physical execution plan
		-> Cause the execution of RDD and create output

   NOTE:
   -----
	rdd1.getNumPartitions()  => Gives the partition count of rdd1

   RDD Lineage
   -----------
	
     Lineage DAG represents a logical plan that tells how to create an RDD.
     It tracks all the parent RDDs and transformations all the way from the very first RDD.
	
     rdd1 = sc.textFile( file, 4 )
	==> Lineage:  rdd1 -> sc.textFile

     rdd2 = rdd1.map(lambda x: x.upper())
	==> Lineage:  rdd2 -> rdd1.map -> sc.textFile

     rdd3 = rdd2.filter(lambda x: len(x) > 50)
	==> Lineage:  rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

     rdd4 = rdd3.flatMap(lambda x: x.split(" "))
	==>  rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.textFile

 
     rdd4.collect()
	
	 rdd4 -> rdd3.flatMap -> rdd2.filter -> rdd1.map -> sc.textFile  	
	 Tasks: sc.textFile (rdd1) -> map (rdd2) -> filter (rdd3) -> flatMap (rdd4) -> collect()


   Types of Transformations
   ------------------------

	1. Narrow Transformations
		-> Does not cause shuffling of the data from one partition to other partitions
		-> Partition to partition transformations
		-> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
		-> Causes shuffling of the data
		-> One output partition may need data from multiple input partitions
		-> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

        rdd1 = sc.textFile( file )
	rdd2 = rdd1.t2(....)
	rdd3 = rdd1.t3(....)
	rdd4 = rdd3.t4(....)
	rdd5 = rdd3.t5(....)
	rdd6 = rdd4.t6(....)
	rdd7 = rdd4.t7(....)
	rdd7.persist( StorageLevel.MEMORY_AND_DISK )     => instruction to not subject the rdd to GC.
	rdd8 = rdd7.t8(....)

	rdd8.collect()

	lineage of rdd8 => rdd8 -> rdd7.t8 -> rdd4.t7 -> rdd3.t4 -> rdd1.t3 -> sc.textFile
	execution plan => sc.textFile (rdd1) -> t3 (rdd3) -> t7 (rdd7) -> t8 (rdd8) --> collect

	rdd7.collect()
	lineage of rdd7 -> rdd4.t7 -> rdd3.t4 -> rdd1.t3 -> sc.textFile

	
	Persistence Type
        ----------------
		-> in-memory in deserilzed format
		-> in-memory in serialized format
		-> on-disk


        Persistence StorageLevel
        ------------------------

	1. MEMORY_ONLY    	   => (default) Stored in-memory (in serialized format)
					-> RDD may be fully persisted, partially persisted or
					   not persisted at all based on available memory. 

	2. MEMORY_AND_DISK	   => 

	3. DISK_ONLY		   	
	
	4. MEMORY_ONLY_2

	5. MEMORY_AND_DISK_2


       Persistence Commands
       --------------------

	-> persist( StorageLevel.MEMORY_AND_DISK )
	-> persist()   -> in-memory persistence in serialized format
	-> cache()     -> in-memory persistence in serialized format

	-> unpersist()


   Executor Memory Structure
   -------------------------


   RDD Transformations
   -------------------
	
     1. map		P: U -> V
			Element to element transformation
			input RDD: N elements, output RDD: N elements
			  

     2. filter		P: U -> Boolean
			The output RDD will have only those objects for which the function returns True
			input RDD: N elements, output RDD: <= N elements


    3. glom		P: None
			Will return an Array with all the elements of each partition
			input RDD: N elements, output RDD: = number of partitions of input RDD

	rdd1			rdd2 = rdd1.glom()
	
	P0:  6,5,6,4,3 -> glom -> P0: [6,5,6,4,3]
	P1:  5,6,3,2,1 -> glom -> P1: [5,6,3,2,1]
	P2:  4,4,3,6,1 -> glom -> P2: [4,4,3,6,1]
			
	rdd1.count -> 15	rdd2.count -> 3

		rdd1.glom().collect()
		=> [[6, 8, 9, 4, 1, 3], [2, 5, 6, 7, 9, 0], [6, 2, 8, 1, 2, 1, 2, 6]]
		rdd1.glom().map(lambda x: len(x)).collect()
		=> [6, 6, 8]
		rdd1.glom().map(len).collect()
		=> [6, 6, 8]

   4. flatMap		P: U -> Iterable[V]       (iterable -> some collection that can be looped)
			flatMap flattens the iterables produced by the function.
			input RDD: N elements, output RDD: >= N elements

 		
   5. mapPartitions	P: Iterator[U] -> Iterator[V]	
			Applies a function on the entire partition


	rdd1	    rdd2 = rdd1.mapPartitions(lambda x: 

	P0:  6,5,6,4,3 -> mapPartitions -> P0: 
	P1:  5,6,3,2,1 -> mapPartitions -> P1: 
	P2:  4,4,3,6,1 -> mapPartitions -> P2: 


		rdd1.mapPartitions(lambda x: [len(list(x))]).collect()
		rdd1.mapPartitions(lambda x: [sum(x)]).collect()


   6. mapPartitionsWithIndex    P: Int, Iterator[U] -> Iterator[V]
				Same as mapPartitions, but we partition-id also as function parameter
			

	rdd1.mapPartitionsWithIndex(lambda indx, iter :  [(indx, sum(iter))]).collect()

   
   7. distinct			P: None,  Optional: Number of output partitions
				Will return an RDD with distinct elements.
				input RDD: N elements, output RDD: <= N elements

   Types of RDDs
   -------------		
	1. Generic RDDs -> RDD[U]
	2. Pair RDDs	-> RDD[(U, V)]


   8. mapValues			P: U -> V
				Applied only to pair RDDs
				Applies a function on only the value part of the pair RDD
				Element to element transformation
				input RDD: N elements, output RDD: N elements

				rdd2.mapValues(lambda x: str(x) + str(x) ).collect()


   9. sortBy			P: U -> V
				The elements of the RDD are sorted based on the value of the function output.

				rddWords.sortBy(lambda x: len(x)).collect()
				rddWords.sortBy(lambda x: len(x), False).collect()
				rddWords.sortBy(lambda x: len(x), True, 5).collect()

   10. groupBy			P: U -> V
				Returns a 'pair RDD' where the 'key' is each unique values of the function
				output and 'value' is a (ResultIterable) collection of all elements of the
				RDD that produces the same key.

		rddFile = sc.textFile(file, 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.groupBy(lambda x: x) \
            		.mapValues(len) \
            		.sortBy(lambda x: x[1], False, 1)



   11. randomSplit		P: An array of ratios  ex: [0.4, 0.3, 0.3]
				Return a array of RDDs randomly split in the specified ratios.

		rddArr = rdd1.randomSplit([0.6, 0.4])
		rddArr = rdd1.randomSplit([0.6, 0.4], 5464)

   12. repartition		P: Number of output partitions
				Used to increase or decrease the number of partitions of the output RDD
				causes global shuffle

   13. coalesce			P: Number of output partitions
				Used to decrease the number of partitions of the output RDD
				Causes partition merging

   14. partitionBy		P: Number of partitions, Optional: Custom Partitioning function
				-> Applied only to pair RDDs
				-> Controls which elements goes to which partition by applying default 
				   hash partitioner or custom partitioning function on the 'key' of the
				   (k, v)
			
	def city_partitioner(city): 
    		return len(city)

	rdd3 = sc.parallelize(transactions, num_partitions) \
         	.map(lambda e: (e['city'], e))

	rdd4 = rdd3.partitionBy(num_partitions, city_partitioner)


   15. union, intersection, subtract, cartesian

	Let us say we have rdd1 with M partitions, rdd2 with N partitions

	command				number of output partitions
        ------------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide

   
    ...ByKey Transformation   
    	=> These are wide transformations
	=> These transformation work only on pair RDD


   16. sortByKey		P: None, Optional: Ascending (Boolean), # of partitions
				Elements of the RDD are sorted based on the key of the pairRDD

		 rddSorted = rddPairs.sortByKey()
		 rddSorted = rddPairs.sortByKey(False)
		 rddSorted = rddPairs.sortByKey(True, 6)		

   17. groupByKey		P: None, Optional: # of partitions
				Elements of the RDD are grouped based on the key of the pairRDD
				The output RDD will have unique keys ad grouped values.

				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]

				rddPairs.groupByKey()
				rddPairs.groupByKey(6)

		    rddFile = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda x: (x, 1)) \
            			.groupByKey() \
            			.mapValues(sum) \
            			.sortBy(lambda x: x[1], False, 1)

				NOTE: groupByKey() & groupBy() causes global shuffle, hence inefficient.
				Try to avoid it.

    18. reduceByKey		P: (U, U) -> U,  optionally: # of partitions	
				Reduces all the values of each unique key within partitions and then across
				partitions.

		rddFile = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda x: (x, 1)) \
            			.reduceByKey(lambda x, y: x + y) \
            			.sortBy(lambda x: x[1], False, 1)

    19. aggregateByKey		Reduces all the values of each unique-key to a value of different type
				(of the type of a zero-value)

			Three parameters:

			1. zero-value: starting value with which all the values of each unique key are merged.
			2. Sequence-function: merges all the values of each unique key with in every partition with zero-value.
			3. Combine-function: reduces the output produced by seg-fn across partitions.
		
	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 				
	def seq_fun(z, e):
    		return (z[0] + e, z[1] + 1)

	def comb_fun(a, b):
    		return (a[0] + b[0], a[1] + b[1])

	students_avg = student_rdd.map(lambda x: (x[0], x[2])) \
                .aggregateByKey( (0,0), seq_fun, comb_fun ) \
                .mapValues(lambda x: x[0]/x[1])

   20. joins   ->  join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V,W))]
			
     		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup	-> Is used to join RDDs which contain duplicate keys                     
                   -> groupByKey & fullOuterJoin.
		

 	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
      	=> [  (key1, [10, 7]), (key2, [12, 6]), ('key3', [6])  ] 
	
 	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
     	=> [  (key1, [5, 17]), (key2, [4, 7]), ('key4', [17])  ] 

   	=> [(key1, ([10, 7], [5, 17])), (key2, ([12, 6], [4, 7])), (key3, ([6], [])), (key3, ([], [17])) ]


    Guidelines / Best Practices
    ---------------------------
	-> The optimal size of a partition is 128 MB  (anywhere between 100 MB to 150 MB )
	-> You can have as many partitions as upto 2 to 3 times available cores.
	-> Do not use too big partitions ( 500 MB, 1 GB etc) as these could result if shuffle 
           memory overflow.
        -> Do not use too small partitions (10 MB, 25 MB). These could results in too much 
	   of shuffling.
	-> Have atleast as many partitions as there are CPU cores. 


 
To Download a file from databricks:
-----------------------------------
/FileStore/<FILEPATH>
https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

Example:
/FileStore/tables/wordcount-5.txt
https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


/FileStore/tables/WordcountExample1Output/part-00000

https://community.cloud.databricks.com/files/tables/WordcountExample1Output/part-00000?o=4949609693130439#tables/new/dbfs



  RDD Actions
  -----------

	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		=> reduces the entire RDD into a single value of the same type
				   by iterativly applying a reduce function.
				
				   -> a reduce function takes two input parameters and returns one output value

				   -> Two step process :

					step 1: reduces all the values in each partition (narrow)
					step 2: reduces all the reduced output values of each partition.


			P0: 2, 3, 5, 1, 2, 6, 5, 6     ->  -26  -> 31
			P1: 2, 4, 3, 5, 7, 8, 9, 6     ->  -40  
			P2: 8, 3, 1, 3, 2, 1, 4, 5, 6  ->  -17


	5. aggregate		=> reduces the entire RDD into one final of different type by using a 
                                   zero-value.

				1. zero-value: starting value with which elements are merged.
				2. Sequence-function: merges all the values with in every partition with zero-value.
				3. Combine-function: reduces the output produced by seg-fn across partitions. 
				
                      rdd1.aggregate(  (0,0), 
				      lambda z, e: (z[0] + e, z[1] + 1), 
                                      lambda a, b: (a[0] + b[0], a[1] + b[1])
                                    )

	6. first

        7. take(n)

        8. takeOrdered(n)

        9. takeSample(withReplacement: Boolean, n, [seed])

        10. countByValue

        11. countByKey

  	12. foreach     -> does not return anything
			-> applies a function on all the elements of the RDD

	13. saveAsSequenceFile 
       

   Reading From Files
   ------------------
	1. rdd1 = sc.textFile( <textFile>, 5 )
	2. rdd1 = sc.sequenceFIle( <sequenceFile>, <KeyType>, <ValueType> )
	3. rdd2 = sc.wholeTextFiles()
               

   Use-Case
   --------
   From cars.tsv dataset. get me the average weight of all the models in eack make of American cars
   Arrange the data in the desc order of average weight
   Save the output into a one text file

     --> please try yourself


  Env. Variable:
  --------------
   PYTHONPATH
   %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%




   




