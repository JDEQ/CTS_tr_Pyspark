
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - MySQL & Hive 
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark
   -----
    
   -> Spark is unified in-memory distributed computing framework   (cluster computing framework)
   -> Spark is used for big data analytics.
   -> Spark is written in SCALA language

    in-memory computation : ability of Spark to persist intermediate results (of tasks) and launch 
			    subsequent tasks directly on these persisted results (partitions)	

  Spark Unified Stack
  -------------------
     Spark provides a unified set of APIs for different analytics workloads based on the same execution. 

	
	Batch Analytics of unstructured data	=> Spark Core API (RDDs)
	Batch Analytics of structured data	=> Spark SQL (DataFrames)
	Streaming Analytics (real-time)		=> DStreams API (legacy), Structured Streaming
	Predictive Analytics (ML)		=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX	    

  Spark is a polyglot
	-> Spark applications can be written in Scala, Java, Python, R, SQL
	-> Spark's library to write apps in python is called "PySpark"

   Spark supports multiple Cluster managers
	-> local 
	-> Spark standalone scheduler
	-> YARN  (most popular)
	-> Mesos
	-> Kubernetes


  Getting started with Spark
  --------------------------
    1. Working in your vLab

	-> You will be landing on a windows server
	-> You find a word doc with instructions and login credentials
	-> Double-Click on the "Oracle VM Virtualbox" to launch the vLab
	-> Login to the VM. This is your lab

		-> Open a terminal and launch pyspark shell.
			pyspark

		-> Open Spyder IDE to write pyspark program

   2. Installing Spark & Spyder locally on Windows.

	-> Make sure that you have Anaconda installed.
	-> Download Spark and add the SPARK_HOME env. variable. 
	-> pip install pyspark
		if pip is not working ...
		-> Follow the instaructions provided in the shared document.
		   githud document:   Pyspark-JupyterNotebooks-Windows-Setup.pdf

   3. Signup for Databricks community edition. 
	
	Signup URL: https://www.databricks.com/try-databricks
	Login URL: https://community.cloud.databricks.com/login.html



  Spark Architecture
  ------------------

    
    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.



  RDD (Resilient Distributed Dataset)
  -----------------------------------

    -> RDD is the fundamental data abstraction of Spark core API
    -> RDD is collection of distributed in-memory partitions. 
	-> Each partition has a collection of objects

    -> RDDs are immutable

    -> RDDs use lizy evaluation
	 -> Transformations does not execution
	 -> Only action commands trigger execution.

    
   Creating RDDs
   -------------
       3 ways:

	1. Create an RDD from some external data source (such as a file)

		rddFile = sc.textFile( filePath, 4 )

	2. Create an RDD from programmatic data

		 rdd2 = sc.parallelize([2,1,3,2,4,6,7,7,4,6,7,8,9,0], 2)

	3. Create an RDD by applying a transformation on existing RDD

		rddWords = rddFile.flatMap(lambda x: x.split(" "))
	

   RDD Operations
   ---------------
	Two operations:

	1. Transformations
		-> Creates an RDD
		-> Does not cause execution (no job is created)
		-> Creates the Lineage DAG of the RDD

	2. Actions
		-> Causes execution of the RDD
		-> Launches a job on the cluster


   RDD Lineage DAGs
   -----------------
 	RDD Lineage DAG is logical plan maintained by Driver
	RDD Lineage DAG contains the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile( filePath, 4 )
		rddFile Lineage DAG:  (4) rddFile -> sc.textFile


	rddWords = rddFile.flatMap(lambda x: x.split(" "))
		rddWords Lineage DAG:  (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
		rddPairs Lineage DAG:  (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
		rddWc Lineage DAG:  (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


   RDD Transformations
   -------------------

	-> to be discussed ...








     

	


















   









    










