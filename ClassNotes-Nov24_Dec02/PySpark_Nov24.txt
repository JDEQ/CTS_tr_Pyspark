
  Agenda (7 sessions * 4 hours each)
  -----------------------------------
   1. Spark - Basics & Architecture
   2. Spark Core API
	-> RDD Transformations & Actions
   3. Spark SQL 
   4. Spark MLlib (& Machine Learning)
   5. Spark Streaming (Introduction)


   Materials
   ---------
	-> PDF versions of the presentations
	-> Lot of Code Modules
        -> Class Notes
	
   Cluster
   -------	
      Is a unified entity comprising of multiple nodes whose cumulative resources can
      be used to distribute your storage and processing. 
  
   Spark
   -----
      => Is a unified in-memory distributed computing framework	
      => Written in Scala

      => In-memory distributed computing
	    -> The intermediate results (partitions) can be saved in-memory and subsequent tasks
               can be aluncjed on them. 
       
      => Unified Framework
	    -> Spark comes with a consistent set of API running on the same execution engine to
	       process different types of analytical work loads. 
		
		Batch Processing of unstructurted data	=> Spark Core
		Batch Processing of structurted data	=> Spark SQL
		Stream Processing			=> Spark Streaming
		Predictive analytics (ML)		=> Spark MLlib
		Graph Parallel Computations		=> Spark GraphX

      => Spark is a polyglot
	    -> We can write Spark application using Scala, Python, Java and R

      => Spark Layered Architecture

	  => Programming Languages:	Scala, Python, Java, R & SQL  
	  => Spark High Level APIs:	Spark SQL, Spark Streaming, Spatk MLlib, Spark GraphX
	  => Spark Low Level API:	Spark Core (RDD API) 
	  => Resource Managers : 	Spark Standalone, YARN, Mesos, Kubernetes
	  => Storage Layer:   		Linux, HDFS, S3, Kafka, RDBMS, NoSQL

    	
    Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the node in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link betweeb the driver and several tasks running in the executors


     Getting started with Spark
     --------------------------
	1. Working with Spark Shell

		-> Make sure you have Java (jdk 1.8.x) running
		-> Download Spark binaries from https://spark.apache.org/downloads.html
		-> Extract it to a suitable location.

		-> Add the necessary environment variables:	
			SPARK_HOME   -> D:\spark-3.2.0-bin-hadoop2.7
			HADOOP_HOME  -> D:\spark-3.2.0-bin-hadoop2.7
			PYTHONPATH   -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%
			Add SPARK_HOME to the PATH Environment variable

                -> Launch PySpark shell.

	2. Install an IDE

		-> Install Anaconda Navigator (https://www.anaconda.com/products/individual)
		-> Follow the instructions given in the GitHub to setup PySpark with Jupytor Notebook / Spyder

        3. Signup to Databrick Community Edition (Free Cloud Account)
		URL: https://databricks.com/try-databricks

		-> Read the "Quick Start Tutorial"  (Explore the Quickstart Tutorial link)


     RDD ( Resilient Distributed Dataset)
     -------------------------------------

	-> A collection of in-memory partitions
		-> Each partition is a collection of objects

	-> RDDs are immutable (you can not change the content of RDDs)

	-> RDD has two components:

		1. Lineage DAG  : meta data info about the logical plan on how to create the RDD
		2. Data		: in-memory partitions

	-> RDDs follow lazy evaluation
		-> Transformations does not cause execution. 
		-> Only action commands cause execution.
		
    
    How to create RDDs ?
    ---------------------

	Three ways:

	1. Creating from external data file.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")

		   -> The default partition count is given by "sc.defaultMinPartitions"
		      whose value is 2 if you have atleast two CPU cores.

		rdd1 = sc.textFile(filePath, 4)


	2. Creating from programmatic data

		rdd10 = sc.parallelize( range(1, 101) )

		-> The default partition count is given by "sc.defaultParallalism"
		      whose value is equal to the CPU cores allocated to your application.

		rdd10 = sc.parallelize( range(1, 101), 5 )


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())


    What can you do with RDDs ?
   ----------------------------
	Only two things:

	1. Transformations
		-> The output is an RDD
		-> Does not cause execution. Only 'Lineage DAG' is created.

	2. Actions
		-> Produces some output
		-> Triggers the execution of the rdd
	

    RDD Lineage DAG
    ----------------  
    
	NOTE: Use rdd1.getNumPartitions() to see the partition count

	Leneage DAG contains all the dependencies (parent RDDs) of the RDD (hierarchy) which caused the 
	creation of the current RDD all the way from the very first RDD.

	rdd1 = sc.textFile(filePath, 4)
		Lineage DAG: rdd1 -> sc.textFile	

    	rdd2 = rdd1.map(lambda x: x.upper())
		Lineage DAG: rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
   		Lineage DAG: rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.map(lambda x: (x, 1))
		Lineage DAG: rdd4 ->  rdd3.map -> rdd2.flatMap -> rdd1.map -> sc.textFile

        














