
  Agenda (7 sessions * 4 hours each)
  -----------------------------------
   1. Spark - Basics & Architecture
   2. Spark Core API
	-> RDD Transformations & Actions
   3. Spark SQL 
   4. Spark MLlib (& Machine Learning)
   5. Spark Streaming (Introduction)


   Materials
   ---------
	-> PDF versions of the presentations
	-> Lot of Code Modules
        -> Class Notes
	
   Cluster
   -------	
      Is a unified entity comprising of multiple nodes whose cumulative resources can
      be used to distribute your storage and processing. 
  
   Spark
   -----
      => Is a unified in-memory distributed computing framework	
      => Written in Scala

      => In-memory distributed computing
	    -> The intermediate results (partitions) can be saved in-memory and subsequent tasks
               can be aluncjed on them. 
       
      => Unified Framework
	    -> Spark comes with a consistent set of API running on the same execution engine to
	       process different types of analytical work loads. 
		
		Batch Processing of unstructurted data	=> Spark Core
		Batch Processing of structurted data	=> Spark SQL
		Stream Processing			=> Spark Streaming
		Predictive analytics (ML)		=> Spark MLlib
		Graph Parallel Computations		=> Spark GraphX

      => Spark is a polyglot
	    -> We can write Spark application using Scala, Python, Java and R

      => Spark Layered Architecture

	  => Programming Languages:	Scala, Python, Java, R & SQL  
	  => Spark High Level APIs:	Spark SQL, Spark Streaming, Spatk MLlib, Spark GraphX
	  => Spark Low Level API:	Spark Core (RDD API) 
	  => Resource Managers : 	Spark Standalone, YARN, Mesos, Kubernetes
	  => Storage Layer:   		Linux, HDFS, S3, Kafka, RDBMS, NoSQL

    	
    Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the node in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link betweeb the driver and several tasks running in the executors


     Getting started with Spark
     --------------------------
	1. Working with Spark Shell

		-> Make sure you have Java (jdk 1.8.x) running
		-> Download Spark binaries from https://spark.apache.org/downloads.html
		-> Extract it to a suitable location.

		-> Add the necessary environment variables:	
			SPARK_HOME   -> D:\spark-3.2.0-bin-hadoop2.7
			HADOOP_HOME  -> D:\spark-3.2.0-bin-hadoop2.7
			PYTHONPATH   -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%
			Add SPARK_HOME to the PATH Environment variable

                -> Launch PySpark shell.

	2. Install an IDE

		-> Install Anaconda Navigator (https://www.anaconda.com/products/individual)
		-> Follow the instructions given in the GitHub to setup PySpark with Jupytor Notebook / Spyder

        3. Signup to Databrick Community Edition (Free Cloud Account)
		URL: https://databricks.com/try-databricks

		-> Read the "Quick Start Tutorial"  (Explore the Quickstart Tutorial link)


     RDD ( Resilient Distributed Dataset)
     -------------------------------------

	-> A distributed collection of in-memory partitions
		-> Each partition is a collection of objects

	-> RDDs are immutable (you can not change the content of RDDs)

	-> RDD has two components:
		1. Lineage DAG  : meta data info about the logical plan on how to create the RDD
		2. Data		: in-memory partitions

	-> RDDs follow lazy evaluation.
		-> Transformations does not cause execution. 
		-> Only action commands cause execution.
		
    
    How to create RDDs ?
    ---------------------

	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.

		rdd1 = sc.parallelize( range(1, 101), 3 ) // 3 partitions are created.

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


    What can you do with RDDs ?
   ----------------------------

	Only Two things:

	1. Transformations
	    -> Transformations does not cause execution 
	    -> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
	    -> Triggers the actual execution of the RDDs and some output is generated
	    -> Causes the driver the convert the logical plan to a physical execution ad several
	       tasks are sent to the executor.
	

    RDD Lineage DAG
    ----------------  
    
	NOTE: Use rdd1.getNumPartitions() to see the partition count

	-> Is a logical plan maintained by the driver which contains all the dependencies (parent RDDs)
           that caused the creation of this RDD all the way from the very first RDD.

	rdd1 = sc.textFile(filePath, 4)
		Lineage DAG: rdd1 -> sc.textFile	

    	rdd2 = rdd1.map(lambda x: x.upper())
		Lineage DAG: rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
   		Lineage DAG: rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.map(lambda x: (x, 1))
		Lineage DAG: rdd4 ->  rdd3.map -> rdd2.flatMap -> rdd1.map -> sc.textFile

    
    RDD Persistence
    ---------------	
	rdd1 = sc.textFile( ..., 10 )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)  
	rdd6.persist(StorageLevel.MEMORY_ONLY)    --> instruction to spark to persist the RDD partitions.
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	Lineage of rdd6 :  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> collect()

	rdd7.collect()
	Lineage of rdd7 :  rdd7 -> rdd6.t7
	rdd6 -> t7 (rdd7) -> collect()
	
	Commands
	--------
		rdd1.persist()  // default storage level
		rdd1.persist( StorageLevel.MEMORY_AND_DISK )
		rdd1.cache()
		
		rdd1.unpersist()

	Storage levels
        --------------
	1. MEMORY_ONLY		-> deserialised format, memory only
	2. MEMORY_AND_DISK	-> deserialised format
	3. DISK_ONLY
	4. MEMORY_ONLY_SER	-> serialized format
	5. MEMORY_AND_DISK_SER
	6. MEMORY_ONLY_2
	7. MEMORY_AND_DISK_2


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Transformations
   -------------------
      
      => A transformation returns 'RDD'

    1. map			P: U -> V
				Transform each elements of the RDD by applying the function
				Element -> Element transformation
				input RDD: N objects, output RDD: N objects

    2. filter			P: U -> Boolean
				Only those elements that return Truw will be in the output RDD
				input RDD: N objects, output RDD: <= N objects

    3. glom			P: None
				Will return an list with all the elements of each partition.
	
		rdd1			rdd2 = rdd1.glom()
			
		P0 :  3,2,1,4,2 -> glom() -> P0 : [3,2,1,4,2]
		P1 :  5,2,4,3,0 -> glom() -> P1 : [5,2,4,3,0]
		P2 :  6,2,4,1,8 -> glom() -> P2 : [6,2,4,1,8]
		
		rdd1.count : 15 (int)	  rdd2.count(): 3 (array)

    4. flatMap			P: U -> Iterable[V]	(function should return some collection)
				Flattens the iterables produced by the functions
				nput RDD: N objects, output RDD: >= N objects
				
		rddWords = rddFile.flatMap(lambda x: x.split(" "))


    5. mapPartitions		P: Iterator[U] -> Iterator[V]
				Applies a function to entire partitions.


		rdd1			rdd2 = rdd1.mapPartitions( lambda x:  )
			
		P0 :  3,2,1,4,2 -> mapPartitions() -> P0 : 12
		P1 :  5,2,4,3,0 -> mapPartitions() -> P1 : 14
		P2 :  6,2,4,1,8 -> mapPartitions() -> P2 : 21

		rdd1.mapPartitions(lambda x: [sum(x)]).collect()
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).collect()


    6. mapPartititonsWithIndex		P: Int, Iterator[U] -> Iterator[V]
					Applies a function to entire partitions. 
					We get the partition-id as an additional function input.

		rdd1.mapPartitionsWithIndex(lambda pid, pdata: [(pid, sum(pdata))]).collect()
		rdd1.mapPartitionsWithIndex(lambda id, x: map(lambda a: (id, a*10), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distict elements of the RDD.

		rddWords.distinct().collect()
		rddWords.distinct(3).collect()





























