
   Agenda  (7 sessions of 4 hours each)
   ------------------------------------

   	- Spark - Basics & architecture
	- Spark Core API (RDD API) - Low Level API
	    - RDD Transformations & Actions	
	- Spark SQL - DataFrames API
	- Machine Learning & Spark MLlib
	- Introduction Spark Streaming

  ---------------------------------------------------

   What is Big Data ?

	-> Any data that is so big and complex that it become hard to store and process
	   using onhand data management systems and traditional application models.

   
   Computing Cluster
   	-> A unified entity containing a lot of nodes whose cumulative resources can be used
	   distribute storage and process.
   
   Hadoop      
       	-> is an opensource framework for storage and processing of big data.
	-> runs on a cluster made of commodity hardware.

	-> Provides two frameworks for big data

	1. HDFS (Hadoop distributed file system)
		=> Distrinuted Storage Solutions
		-> Splits the file into blocks of 128 MB each.
		-> Spread the bloks of the file across many nodes in the cluster	
		-> Each block is replicated in three different nodes for fail-safety.

	2. MapReduce 
		-> Distributed Processing Solutions
		-> Distributes the processing across many machines (Map phase) and then
		   aggregates the results produced by all these mapper instances (reduce phase) 		    
  
   MapReduce is not good at few use-cases:

	-> Not good with lot of small files
	-> Not good with ad-hoc querying or random-access of data
	-> Not good with iterative computations.

	
   What is Spark ?
   ---------------

	-> Spark is a unified in-memory distributed processing framework
	-> Spark is written in Scala
	-> Spark supports multiple languages: Scala, Java, Python & R
	-> Spark applications can run on mulitple cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes. 
	
	In-memory framework
	--------------------
	   -> The inetermediate partitions can be persisted in memory and further tasks
	      can be launched on these in-memory persisted partitions.

	Unified framework
	------------------

	   -> Spark provides a cosnistenet set of APIs for different analytics workloads
	      running on the same execution engine. 


		Hadoop Ecosystem
                -----------------
		Batch Analytics of unstructured data  	: MapReduce
		Batch Analytics of Structured data	: Hive, Impala, Drill, HBase
		Streaming (real time) Analytics		: Kafka, Storm, Samza
		Predictive Analytics using ML		: Mahout
		Graph parallel computations		: Giraph

		Spark Framework	
		---------------
		Batch Analytics of unstructured data  	: Spark Core API
		Batch Analytics of Structured data	: Spark SQL
		Streaming (real time) Analytics		: Spark Streaming, Structured Streaming
		Predictive Analytics using ML		: Spark MLlib
		Graph parallel computations		: Spark GraphX

	
	Spark Layered Structure
	-----------------------

	Programming Lang  : Scala, Java, Python, R
	Spark High Level  : Spark SQL, Spark Streaming, Sparl MLlib, Spark GraphX
	Spark Low Level	  : Spark Core API (RDDs)
	Cluster Managers  : Spark Standalone, YARN, Mesos, Kebernetes  
	Storage Layer     : HDFS, Linux, RDBMS, NoSQL, Kafka


   Getting started with Spark
   --------------------------
	Pre-requisite. 
		-> Install Anaconda navigator
			Download and install from the following URL
			https://www.anaconda.com/products/individual-d#windows

	1. Install Spark and run PySpark Shell
		https://spark.apache.org/downloads.html

		Download Spark xxx.tgz file and extract it to a suitable folder.

		Setup your environment varibles:
		-> Add SPARK_HOME & HADOOP_HOME env. variables and point them to Spark installation folder. 
		-> Add bin folder of the spark installation folder to the PATH environment var. 
		-> PYTHONPATH -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH% 
	 
	2. Using an IDE (such as Spyder or PyCharm)		
		-> Follow the steps mentioned in the document shared with you.

	3. Signup to free Databricks community edition account
		https://databricks.com/try-databricks
		-> Spend some time exploring the Quickstart tutorial


    Spark Architecture
    ------------------
  
     1. Cluster manager (CM)
	-> Jobs are submitted to CM
	-> CM schedules the job and allocates resources to the job

     2. Driver Process
	-> Driver process is the first process that gets created.
	-> Driver process manages the user-code and lauches tasks on the cluster.
	-> Driver contains a "SparkContext" object  (or "SparkSession" in the case of Spark SQL).
	
	Deploy-modes:
	
	1. Client Mode (default) -> the driver process runs on the client machine
	2. Cluster Mode		 -> the driver runs on one the node in the cluster

     3. Executor Processes

	-> Different tasks as per the programming logic are sent to be executed on various
	   executors allocated by the Cm to the application
	-> All tasks does the same process (logic) but of different partitions of data.
	-> report the status of the tasks to the driver

      4. SparkContext
	 -> represents an application contextand connection to the cluster.	
	 -> is the first objects that gets created (starting point of Spark core application).
	 -> is the link between driver and different tasks running on the cluster.



    RDD (resilient distributed dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
		-> Each partition is a collection of objects	

	-> RDDs are immutable
		-> You can not change the content of a partition.

	-> RDD has two components
		1. RDD lineage DAG -> logical plan that describes how to compute the RDD partitions
		2. RDD DAG	   -> in-memory partitions

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-. Only action commands trigger execution


   How to create RDDs?
   -------------------

	There are three ways:

	1. We can create RDDs from external data

		rdd1 = sc.textFile( <filePath> )


	2. We can create RDD from programmatic data using parallelize

		rdd10 = sc.parallelize([2,3,2,4,5,6,7,8,9,6,6,7])


	3. By applying transformations on an existing RDD

		rdd2 = rdd1.map(lambda x: x.upper())


   What are RDD Lineage DAGs
   -------------------------
	-> Maintained by the driver
	-> Is a DAG of dependencies that caused the creation of the RDD all the way from the 
	   first RDD.

	rdd1 = sc.textFile( filePath )
	Lineage DAG : 	rdd1 -> sc.textFile	

	rdd2 = rdd1.map(lambda x: x.upper())
	Lineage DAG : 	rdd2 -> rdd1.map -> sc.textFile	   

	rdd3 = rdd2.filter(lambda a: len(a) > 50) 
	Lineage DAG : 	rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile	 
	
	rdd4 = rdd3.map(lambda x: len(x))
	Lineage DAG : 	rdd4 -> rdd3.map -> rdd2.filter -> rdd1.map -> sc.textFile
		
	rdd3.collect()  => sc.textFile -> map -> filter -> rdd3
	
   What can you do with an RDD ?
   -----------------------------

	Only two things:

	1. Transformations
		-> Does not cause execution
		-> They do not produce any output. They produce only other RDDs
		-> They cause the creation of lineage DAGs

	2. Actions
		-> Trigger execution
		-> Converts the lineage DAg (logical plan) into a physical execution plan

 





























