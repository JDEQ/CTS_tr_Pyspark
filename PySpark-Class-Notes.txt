
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark

	-> Spark is an open source framework for big data analytics
	-> Spark is written in Scala

	-> Spark is an in-memory distributed computing framework
	
	   in-memory: ability to persist intermediate results and subsequent operations
		      can directly work on these persisted intermediate results. 

	-> Spark is a polyglot
		-> Supports Scala, Java, Python, R

	-> Spark is a unified framework

	-> Spark can run on multiple cluster managers
		local, spark standalone scheduler, YARN, Mesos, Kubernetes. 	


   Spark Unified Framework
   -----------------------

	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming, Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib
	Graph parallel computations  		-> Spark GraphX


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 



   Getting started with Spark
   --------------------------

	1. Setting up local dev environment on your personal machine.

		-> Install Anaconda distribution for Python. 
		   URL: https://www.anaconda.com/download

		-> Follow the instrunctions given in the shared document 
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

	* 2. Signup to Databricks Community Edition (free edition)
 		
		Signup: https://www.databricks.com/try-databricks

			Screen 1: Fill up the details with valid email address
			Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button)

		Login: https://community.cloud.databricks.com/login.html



		Downloading a file from Databricks
		----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439

		Example:
		file path to be downloaded: dbfs:/FileStore/ctsdatasets/output/wc/part-00000
		https://community.cloud.databricks.com/files/ctsdatasets/output/wc/part-00000?o=1072576993312365




   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Action commands trigger execution.


  Creating RDDs
  -------------
	
	Three ways:

	1. Create an RDD from external data files.

		rddFile = sc.textFile( <filePath> , 4 )

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,4,3,1,5,6,7,8,6,8,9,0,7,5,4,6,8], 2)
		
		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster
		

  RDD Lineage DAG
  ---------------
  Driver maintains a Lineage DAG for every RDD
  Lineage DAG is a heirarchy of dependencies of RDDs all the way upto the very first RDD	
  Lineage DAG is a logical plan on how to create the RDD.


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rddFile -> (4) rddFile -> sc.textFile on E:\\Spark\\wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG of rddWords -> (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x,1))
	Lineage DAG of rddPairs -> (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
	Lineage DAG of rddWc -> (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  Types of Transformations
  ------------------------

     Two types:

	 1. Narrow Transformations

           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations

           -> In wide transformations, the computation of a single partition depends on multiple
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive
	

  RDD Execution Flow
  ------------------

	Application (ex: PySpark Shell or Your App in Spyder/PyCharm)
	|
	|--> Jobs (Each action command launches one job)
		|
		|--> Stages (one or more stages per Job. Each wide transformation causes a new stage.)
			|
			|--> Tasks (one task per partition of the RDD in that stage)
				|
				|--> Transformations (one or more transformations per task)	


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.DISK_ONLY )   ------> Instruction to Spark to save the rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	Lineage DAG of rdd6 => (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6]  -> collect

	rdd7.collect()
	Lineage DAG of rdd7 => (4) rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[t7]  -> collect

       	Storage Levels
       	--------------	
	MEMORY_ONLY		=> default, Memory Serialized 1x replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x replicated
	DISK_ONLY		=> Disk Serialized 1x replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x replicated	
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x replicated	


	Commands
	--------	
	rdd1.cache()     -> in-memory persistence
	rdd1.persist()	 -> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()



   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.



  RDD Transformations
  -------------------

  => Transformations create RDD object
  => RDD object represents a Lineage DAG (maintained by the Driver)


  1. map		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects
			
	 rdd2 = rdd1.map(lambda x: (x%5, x))


  2. filter		P: U -> Boolean
			Objects for which the function returns True will be in the output RDDs
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()


  3. glom 		P: None
			Return one list object per partition with all the objects of that partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,4,5,6 -> glom -> P0: [3,2,1,4,5,6]
		P1: 5,6,7,8,9,3 -> glom -> P1: [5,6,7,8,9,3]
		P2: 4,7,6,2,1,0 -> glom -> P2: [4,7,6,2,1,0]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			fatMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()



  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			similat to mapPartitions, but we get the partition index as an additional parameter.

		rdd1 \
		.mapPartitionsWithIndex(lambda i,p: map(lambda x: (i, x), p)) \
		.filter(lambda x: x[0] == 1) \
		.map(lambda x: x[1]) \
		.collect()


  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

		rddWords.distinct().collect()



  Types of RDDs
  -------------

    Two types of RDDs:

	Generic RDD:  RDD[U]				
	Pair RDD:     RDD[(K, V)]			


  8. mapValues		P: U -> V
			Applied only on Pair RDD
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs


  9. sortBy		P: U -> V,  Optional: ascending (True/False), numPartitions
			Sorts the objects of RDD based on the function output that they generate.

		rdd1.sortBy(lambda x: x%2).glom().collect()
		rdd1.sortBy(lambda x: x%2, False).glom().collect()
		rdd1.sortBy(lambda x: x%2, True, 2).glom().collect()


  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where
				key: Each unique value of the function output
				value: 'ResultIterable' object containing RDD objects that produced the key

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 12) \
        		.flatMap(lambda x: x.split()) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

  11. repartition	P: numPartitions
			Is used to increase or decrease the number of output partitions	
			Global shuffle	

			
		rdd2 = rdd1.repartition(5)



  12. coalesce		P: numPartitions
			Is used to only decrease the number of output partitions	
			Partition merging


		rdd2 = rdd1.coalesce(5)


		Recommendations
		---------------
		-> The size of each partition should be between 100 MB to 1 GB
		   (Ideally 128 MB if you are running on Hadoop)
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor should be 5


  13. union, intersection, subtract

		-> Operate on two generic RDDs.

		Assume rdd1 has M partitions and rdd2 has N partitions

		command				output
		----------------------------------------------
		rdd1.union(rdd2)		M+N, narrow
		rdd1.intersection(rdd2)		M+N, wide
		rdd1.subtract(rdd2)		M+N, wide


  14. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           

  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])

	
  ..ByKey Transformations
  -----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only
	

  15. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(False, 3).glom().collect()


  16. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: do not use groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)


  17. reduceByKey	P: (U, U) => U, Optional: numPartitions
			Reduces all the 'values' of 'each unique-key' within each partition, and then, across partitions

		wordcountRdd = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
                 	.flatMap(lambda x: x.split()) \
                 	.map(lambda x: (x, 1)) \
                 	.reduceByKey(lambda x, y: x + y) \
                 	.sortBy(lambda x: x[1], False, 1)



  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) => U
			Reduces an entire RDD to one value of the same type by iterativly applying the function
			on each partition in the first stage and then across partitions in the second stage.

		rdd1		

		P0: 2,3,5,1,4 -> reduce -> -11 -> reduce -> 30
		P1: 3,6,8,7,9 -> reduce -> -27
		P2: 5,0,7,9,3 -> reduce -> -14

		rdd1.reduce(lambda x, y: x - y)


		rddWc.collect()
		[('hadoop', 25), ('flatmap', 12), ('hdfs', 6), ('sadas', 1), ('das', 6), ('spark', 40), ('asd', 5), ('scala', 28), ('hive', 19), ('transformations', 10), ('d', 1), ('map', 6), ('groupby', 6), ('flume', 6), ('oozie', 6), ('sqoop', 6), ('mapreduce', 6), ('rdd', 43), ('actions', 10)]

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )
		('hadoop,flatmap,hdfs,sadas,das,spark,asd,scala,hive,transformations,d,map,groupby,flume,oozie,sqoop,mapreduce,rdd,actions', 242)

  5. take(n)

		rdd1.take(5)  => returns a list with first 5 objects.

  6. takeOrdered

		rddWords.takeOrdered(20)   => returns a list with first 5 ordered objects.
		rddWords.takeOrdered(20, lambda x: len(x))

  7. takeSample	

	
		rddWords.takeSample(True, 20)   	=> withReplacement = True	
		rddWords.takeSample(True, 20, 53)	=> 53 is a seed here

		rddWords.takeSample(False, 20)   	=> withReplacement = False	
		rddWords.takeSample(False, 20, 53)	=> 53 is a seed here

  8. countByValue

		rdd1.countByValue()

  9. countByKey


  10. foreach  	 => Runs a function on all objects of the RDD, but does not return any value.


  11. saveAsSequenceFile



  Use-Case
  --------
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, find out the average-weight of all the models of each make 
	of Amerigin origin cars. 
	-> Arrange the data in the DESCENDING order of average weight
	-> Save the output as a single text file.

	=> Try it yourself



  Closures
  --------
  In Spark, a closure constitutes all the variables and methods which must be visible for the executor 
  to perform its computations on the RDD. 

  -> This closure is serialized and sent to each executor.


	c = 0

	def is_prime(n):
	  return True if n is prime
	  else
	  return False

	def f1(n):
	  global c
	  if is_prime(n) :
	     c += 1 
	  return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map(f1)
	rdd2.collect()

	print(c)	// 0

	

	Limitation: We can not use local variables (that are part of a closure) to implement global counters. 
	Solutions: Use an 'Accumulator' variable.


  Shared Variables
  ================ 

  1. Accumulator Variable
	
	-> Is a shared variable that is not part of the closure.
	-> Maintained by driver.
	-> All tasks can add to it using 'add' method.	
	-> Only driver can read the value of accumulator. Tasks can only write to it.
	-> Use accumulators to implement global counters. 


	c = sc.accumulator(0)

	def is_prime(n):
	  return True if n is prime
	  else
	  return False

	def f1(n):
	  global c
	  if is_prime(n) :
	     c.add(1)
	  return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map(f1)
	rdd2.collect()

	print(c.value)	  // 170


  2. Broadcast variable

	-> broadcast variable is a shared variable, hence, not a part of closure
	-> Driver sends a copy of the broadcast variable to every executor
	-> All the tasks within that executor can read from the one copy (of the executor)
	-> You can convert large immutable collections into broadcast variables. 


	d = sc.broadcast({ 1:a. 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, ........})   #100 MB

	def f1(n):
	   global d
	   return d.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,6,.....], 4)
	rdd2 = rdd1.map(f1)
	rdd2.collect()       



  Spark-submit command
  --------------------
   
    -> Is a single command to send any spark application (python, scala, java, R) to any cluster manager.

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory	2G \
		--executor-memory 10G \
		--driver-cores 2 \
		--executor-cores 5 \
		--num-executors 8 \
		E:\\Spark\\wordcount.py [app args]


	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wc_out_2 1

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


  ===============
    Spark SQL
  ===============

    
    => This is the main API of Spark for data analysis/data engineering/data science
	
	-> Is a high-level API built on top of Spark Core.	
	-> Spark Structured/Semi-structured data processing API


    => Supported Formats:
	
	File Formats: Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format:  RDBMS, NoSQL Databases
	Hive Format:  Hive warehouse


   => SparkSession
	
	-> Represents a user-session inside an application
	-> An application can contain multiple sessions.
	-> SparkSession is the starting point of executions

		application => SparkContext
		session	=> SparkSession

	In Databricks:
		A cluster -> Represents an application
		Notebooks -> Represent Sessions (in the application)	

		spark = SparkSession \
			.builder \
			.appName("Basic Dataframe Operations") \
			.config("spark.master", "local[*]") \
			.getOrCreate()    

  
   => DataFrame

	-> Is the main data abstraction	

	-> DataFrame represents a collection of in-memory partitions that are immutable and lazily evaluated
	   -> each partition contains "Row" objects

        -> DataFrame has two components

		-> Data   : Row objects
		-> Schema : StructType object

		StructType(
		    [
			StructField('age', LongType(), True), 
			StructField('gender', StringType(), True), 
			StructField('name', StringType(), True), 
			StructField('phone', StringType(), True), 
			StructField('userid', LongType(), True)
		    ]
		)


  
   Basic Steps in Spark SQL 		
   ------------------------
	
	1. Load/Read the data into a DataFrame	

		df1 = spark.read.format("json").load("/FileStore/users.json")
		df1 = spark.read.load("/FileStore/users.json", format="json")
		df1 = spark.read.json("/FileStore/users.json")
		
		Applying schema
		---------------
		users_schema = StructType(
		    [
			StructField('age', LongType(), True), 
			StructField('gender', StringType(), True), 
			StructField('name', StringType(), True), 
			StructField('phone', StringType(), True), 
			StructField('userid', LongType(), True)
		    ])

		df1 = spark.read.schema(users_schema).json("/FileStore/users.json")


	2. Transform the DataFrame using Transformations API or using SQL

		Using Transformation methods
		----------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("gender", "age").count() \
        		.limit(4)


		Using SQL
		---------
		df1.createOrReplaceTempView("users")

		df3 = spark.sql("""SELECT age, count(1) as count
        			   FROM users
        			   WHERE age IS NOT NULL
        			   GROUP BY gender, age
        			   ORDER BY gender, age
        			   LIMIT 4""")
		display(df3)	


 	3. Save/Write the DataFrame into a target location 
	
		df3.write.format("json").save("dbfs:/FileStore/output/json")
		df3.write.save("dbfs:/FileStore/output/json", format="json")
		df3.write.json("dbfs:/FileStore/output/json")

 
  Save Modes
  ----------
	- Define what should happen when you are writing to an existing directory
  		- ErrorIfExists (default)
  		- Ignore
  		- Append   (appends additional files to the existing directory)
  		- Overwrite (overwrites old directory)


	df3.write.mode("append").format("json").save("dbfs:/FileStore/output/json")
	df3.write.mode("overwrite").format("json").save("dbfs:/FileStore/output/json")

	df3.write.format("json").save("dbfs:/FileStore/output/json", mode="append")


  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")



  DataFrame Transformations
  -------------------------

   1. select
	
		df2 = df1.select("ORIGIN_COUNTRY_NAME",
				"DEST_COUNTRY_NAME",
				"count")
		----------------------------------------
		df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
			expr("DEST_COUNTRY_NAME as destination"),
			expr("count").cast("int"),
			expr("count+10 as newCount"),
			expr("count > 100 as highFrequency"),
			expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME").alias("domestic"))


		df2.show()
		df2.printSchema()


  2. where / filter

	df3 = df2.where("count > 500 and domestic = false")
	df3 = df2.filter("count > 500 and domestic = false")
	df3 = df2.where( col("count") > 200 )

  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")
	df3 = df2.sort(desc("count"), asc("origin"))


  4. groupBy   => returns a 'pyspark.sql.group.GroupedData' object (not a DataFrame)
		  Apply aggregation methods to return a DataFrame


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"),2).alias("avg"))

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	expr("sum(count) as sum"),
              		expr("max(count) as max"),
              		expr("avg(count) as avg"))


  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
			"DEST_COUNTRY_NAME as destination",
			"count",
			"count+10 as newCount",
			"count > 100 as highFrequency",
			"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
	df2.show()


  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
		.withColumn("highFrequency", expr("count > 100")) \
		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
		.withColumn("Country", lit("India"))

	----------------------

	df4 = df3.withColumn("ageGroup", when( col("age") < 13, "child")
                                	.when( col("age") < 20, "teenager")
                                	.when( col("age") < 60, "adult")
                                	.otherwise("senior"))

	df4 = df3.withColumn("ageGroup", when( expr("age < 13"), "child")
                                	.when( expr("age between 13 and 19"), "teenager")
                                	.when( expr("age < 60"), "adult")
                                	.otherwise("senior"))  


  8. drop


  9. dropDuplicates

 
  10. distinct

  
  11. repartition


  12. coalesce


  13. udf (User Defined Function)


	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"    

	get_age_group = udf(getAgeGroup, StringType())

	df4 = df3.withColumn("ageGroup", get_age_group(col("age")))

	---------------------------

	@udf(returnType = StringType())
	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"    


	df4 = df3.withColumn("ageGroup", getAgeGroup(col("age")))
	
	------------------------------

	spark.udf.register("get_age_group", getAgeGroup, StringType())

	df3.createOrReplaceTempView("users")

	qry = """select id, name, age, get_age_group(age) as ageGroup
		   from users"""       

	spark.sql(qry).show()

  14. join



   Working with different file formats
   -----------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)


   CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")

   Text
	read
		df1 = spark.read.text(inputPath)
		=> df1 will have one columns called 'value' of 'string' type

	write
		df1.write.text(outputPath)
		=> You can only save a DF with a single text column in 'text' format.


  Creating an RDD from DataFrame
  ------------------------------
   
        rdd1 = df1.rdd


  Creating a DataFrame from programmatic data
  -------------------------------------------
   
     	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])


  Creating a DataFrame from RDD
  -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame with custom/programmatic schema
  ----------------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)  
	-----------------------------------------------------
        filePath = "E:\\PySpark\data\\flight-data\\2015-summary-nh.csv"

	mySchema = "origin STRING, destination STRING, count INT"
	df1 = spark.read.csv(filePath, schema=mySchema) 
	-----------------------------------------------------
	mySchema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

	df1 = spark.read.json(filePath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()
	-----------------------------------------------------
	mySchema = StructType([
				 StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
				 StructField("DEST_COUNTRY_NAME", StringType(), True),
				 StructField("count", IntegerType(), True)
			   ])

	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()


  Use-Case
  --------

    Dataset: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average user-rating
	-> Consider only those movies that are rated by atleast 50 users
	-> Data: movieId, title, totalRatings, averageRating
	-> Arrange the data in the DESC order of averageRating
	-> Save the output as a single pipe-separated CSV file with header
	-> Use only DF transformation methods (not SQL)

	=> Try it yourself




































