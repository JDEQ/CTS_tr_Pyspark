
  Agenda (PySpark)
  -----------------
    Spark - Basics & Architecture
    Spark Core API 
	-> RDD - Transformations & Actions
	-> Shared Variables
    Spark SQL
	-> DataFrame Operations
    Spark Streaming
	-> Structured Streaming	
    Introduction to Spark MLlib (based on time)
   

  Materials
  ---------
	PDF Presentations
	Code Modules
	Class Notes (daily basis)
	Github: https://github.com/ykanakaraju/pyspark


  Spark
  ------

	Spark is an open source framework for big data analytics using a distributed computing
	architecture and performs parallel in-memory processing of data using simple programming
	constructs. 

        Cluster => Is a unified entity comprising of many nodes whose combined resources can be used
	           to distribute the processing. 

        In-Memory => Spark can persist intermediate results of tasks in the RAM and can launch subsequent
		     task on these in-memory results. 

	     -> In-memory computations are subjected to availability of RAM
	     -> Spark is 100 times faster than MapReduce if you use 100% in-memory computatition
	     -> Spark is 6 to 7 times faster than MapReduce even of disk-based computions is used.


	=> Spark is written in 'SCALA'

	=> Spark supports multiple programming language (polyglot)
	    -> Scala, Java, Python and R



	=> Spark is a unified framework.
		
	     => Provides a consistent set of APIs for processing of different analytics workloads
	        based on the same execution engine. 

		Spark Unified Stack
		-------------------		
		Batch Processing of unstructured data	=> Spark Core (RDD API)
		Batch Processing of structured data	=> Spark SQL
		Stream Processing (Real time analytics) => Spark Streaming, Structured Streaming
		Predictive Analytics (ML)		=> Spark MLlib
		Graph Parallel Computations		=> Spark GraphX	

   
   Getting started with PySpark  
   -----------------------------
	1. Using the vLab  	
	    -> Click on the Oracle Virtual Box and click on Ubuntu VM
	    -> This launches the vLab (Ubuntu VM)

            => Start 'PySpark' shell  (type 'pyspark')
	    => Start 'Spyder' IDE
		-> Open a terminal and type 'spyder'
		   (If spyder is not installed, you can install it using a command mentioned there itself)

        2. Working on your own personal machine
		
		-> Install 'Anaconda Navigator' 
		   URL: https://www.anaconda.com/products/distribution#windows

		-> Setup PySpark with Spyder
		    -> Follow the instructions given in the shared document.
		    https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

        3. Signup to Databrick Community Edition (free account)
		
		Sign-up: https://databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html
			Read: Guide: Quickstart tutorial


   Spark Architecture
   ------------------

   	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
   


  Spark Core API (RDD API)
  ------------------------

	=> Low  Level API which used RDD as the fundamental data abstraction
        => All other APIs are built on top of Spark core
	=> Responsible of memory-management, job scheduling, fail-safety etc.


  RDD (resilient distributed dataset)
  -----------------------------------
  
	-> RDD is a collection of distributed in-memory partitions.
		-> Partition is a collection of objects

	-> RDDs are immutable

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only Actions trigger execution.

	-> RDDs are resilient to missing in-memory partitions


   How to create RDDs?
   -------------------

	Three ways to create an RDD:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile(<filePath>, 4)

	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize([4,2,3,4,5,6,0,7,8,9,7,8,9,0,3,2,1,2,3,2,0,4,6,5,7], 3)

        3. By applying transformations on existing RDDs

		rdd3 = rdd2.map(lambda x: x*2)



  
   What can we do eith an RDD?
   ---------------------------

    Two things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts the logical Plan into a physical execution plan.


   RDD Lineage DAG
   ---------------

    -> RDD Lineage DAG is a logical plan maintained by the Driver
    -> RDD Lineage contains the hierarchy of dependencies all the way from the very first RDD.
    -> RDD Lineage DAGs are create by transformations and data loading commands.
    -> Transformations does not cause execution


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	   => lineage of rddFile : (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   => lineage of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	  => lineage of rddPairs : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	  => lineage of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	  	

  RDD Transformations
  -------------------





    






 

