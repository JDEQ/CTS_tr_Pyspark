
  Agenda (PySpark)
  -----------------
    Spark - Basics & Architecture
    Spark Core API 
	-> RDD - Transformations & Actions
	-> Shared Variables
    Spark SQL
	-> DataFrame Operations
    Spark Streaming
	-> Structured Streaming	
    Introduction to Spark MLlib (based on time)
   

  Materials
  ---------
	PDF Presentations
	Code Modules
	Class Notes (daily basis)
	Github: https://github.com/ykanakaraju/pyspark


  Spark
  ------

	Spark is an open source framework for big data analytics using a distributed computing
	architecture and performs parallel in-memory processing of data using simple programming
	constructs. 

        Cluster => Is a unified entity comprising of many nodes whose combined resources can be used
	           to distribute the processing. 

        In-Memory => Spark can persist intermediate results of tasks in the RAM and can launch subsequent
		     task on these in-memory results. 

	     -> In-memory computations are subjected to availability of RAM
	     -> Spark is 100 times faster than MapReduce if you use 100% in-memory computatition
	     -> Spark is 6 to 7 times faster than MapReduce even of disk-based computions is used.

	=> Spark is written in 'SCALA'

	=> Spark supports multiple programming language (polyglot)
	    -> Scala, Java, Python and R

	=> Spark is a unified framework.
		
	     => Provides a consistent set of APIs for processing of different analytics workloads
	        based on the same execution engine. 

		Spark Unified Stack
		-------------------		
		Batch Processing of unstructured data	=> Spark Core (RDD API)
		Batch Processing of structured data	=> Spark SQL
		Stream Processing (Real time analytics) => Spark Streaming, Structured Streaming
		Predictive Analytics (ML)		=> Spark MLlib
		Graph Parallel Computations		=> Spark GraphX	

   
   Getting started with PySpark  
   -----------------------------
	1. Using the vLab  	
	    -> Click on the Oracle Virtual Box and click on Ubuntu VM
	    -> This launches the vLab (Ubuntu VM)

            => Start 'PySpark' shell  (type 'pyspark')
	    => Start 'Spyder' IDE
		-> Open a terminal and type 'spyder'
		   (If spyder is not installed, you can install it using a command mentioned there itself)

        2. Working on your own personal machine
		
		-> Install 'Anaconda Navigator' 
		   URL: https://www.anaconda.com/products/distribution#windows

		-> Setup PySpark with Spyder
		    -> Follow the instructions given in the shared document.
		    https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

        3. Signup to Databrick Community Edition (free account)
		
		Sign-up: https://databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html
			Read: Guide: Quickstart tutorial


   Spark Architecture
   ------------------

   	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
   


  Spark Core API (RDD API)
  ------------------------

	=> Low  Level API which used RDD as the fundamental data abstraction
        => All other APIs are built on top of Spark core
	=> Responsible of memory-management, job scheduling, fail-safety etc.


  RDD (resilient distributed dataset)
  -----------------------------------
  
	-> RDD is a collection of distributed in-memory partitions.
		-> Partition is a collection of objects

	-> RDDs are immutable

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only Actions trigger execution.

	-> RDDs are resilient to missing in-memory partitions


   How to create RDDs?
   -------------------

	Three ways to create an RDD:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile(<filePath>, 4)

	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize([4,2,3,4,5,6,0,7,8,9,7,8,9,0,3,2,1,2,3,2,0,4,6,5,7], 3)

        3. By applying transformations on existing RDDs

		rdd3 = rdd2.map(lambda x: x*2)



  
   What can we do eith an RDD?
   ---------------------------

    Two things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts the logical Plan into a physical execution plan.


   RDD Lineage DAG
   ---------------

    -> RDD Lineage DAG is a logical plan maintained by the Driver
    -> RDD Lineage contains the hierarchy of dependencies all the way from the very first RDD.
    -> RDD Lineage DAGs are create by transformations and data loading commands.
    -> Transformations does not cause execution


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	   => lineage of rddFile : (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   => lineage of rddWords : (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	  => lineage of rddPairs : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	  => lineage of rddWc : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	  	

   RDD Persistence
   ----------------

	rdd1 = sc.textFile( .. )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist()   ------> instruction to Spark to persist RDD partitions. 
	rdd7 = rdd6.t7( .. )

	rdd6.collect()	
	   	lineage of rdd6:  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		(sc.textFile, t3, t5, t6) -> collect

	rdd7.collect()
		lineage of rdd6:  rdd7 -> rdd6.t7	
		(t7) -> collect
	
	rdd6.unpersist()


	Storage Levels
        --------------
	 1. MEMORY_ONLY		: Default, Memory Serialized 1x Replicated
	 2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated
	 3. DISK_ONLY		: Disk Serialized 1x Replicated
	 4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated
	 5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()     -> MEMORY_ONLY		
	rdd1.persist()   -> MEMORY_ONLY		
	rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	rdd1.unpersist()

    Executor's memory structure
    ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)
	


   Types of Transformations
   ------------------------
	
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 

		
   RDD Transformations
   -------------------
   
     -> A transformation returns an RDD
     -> Transformations does not cause execution. They create a lineage DAG


     1. map		P: U -> V
			Object to object transformation
			input RDD: N objects, output RDD: N objects

     		rddFile.map(lambda x: len(x)).collect()

    2. filter		P: U -> Boolean
			Will only put those objects for which the function returns True in the output RDD.
			input RDD: N objects, output RDD: <= N objects

  		rddFile.filter(lambda x: len(x.split(" ")) > 8 ).collect()

    3. glom		P: None
			Returns one list object per partition with all the elements of the partition

		rdd1			   rdd2 = rdd1.glom()
		P0: 2,1,3,2,5,6 -> glom -> P0: [2,1,3,2,5,6]
		P1: 3,4,2,0,2,4 -> glom -> P1: [3,4,2,0,2,4]
		P2: 6,4,7,1,1,0 -> glom -> P2: [6,4,7,1,1,0]

		rdd1.count() = 18 (int)	  rdd2.count() = 3 (list)	

		rdd1.glom().collect()

    4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function to return multiple objects
			from a single iterable object
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

    5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Each input partition is transformed using the function

		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )  
		P0: 2,1,3,2,5,6 -> mapPartitions -> P0: 19
		P1: 3,4,2,0,2,4 -> mapPartitions -> P1: 15 
		P2: 6,4,7,1,1,0 -> mapPartitions -> P2: 19

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()

    6. mapPartitionsWithIndex 	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions but we get the partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [ (i, sum(p)) ]).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()

    7. distinct			P: None,  Optional: numPartitions
				Returns the distinct objects of the input RDD.
				input RDD: N objects, output RDD: <= N objects
		
		rddWords.distinct().collect()

    8. sortBy			P: U -> V,   Optional: ascending (True/False), numPartitions
				Sorts the elements of the RDD based on the function output.

		rdd1.sortBy(lambda x: x%2).glom().collect()
		rdd1.sortBy(lambda x: x%2, False).glom().collect()
		rdd1.sortBy(lambda x: x%2, False, 1).glom().collect()

    Types of RDDs
    -------------
	1. Generic RDD :   RDD[U]
	2. Pair RDD :  RDD[(K, V)]


    9. mapValues

    10. groupBy




    


	




 









  







