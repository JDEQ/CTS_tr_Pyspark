
  Agenda - PySpark
  -----------------

	Spark - Basics & Architecture
	Spark Core API - RDDs
		-> RDD Transformations & Actions
		-> Shared Variables
	Spark SQL - DataFrames
	Spark MLlib & Machine Learning 
	Introduction to Spart Streaming


   Materials
   ---------
	-> PDF presentations
	-> Code & Data
	-> Class Notes

	URL: https://github.com/ykanakaraju/pyspark


   Cluster
   --------
	-> A group of nodes whose combined resources can be used to distribute our
	   storage and processing across many nodes.


   What is Spark ?
   --------------
	-> Spark framework is written in SCALA language

    	-> Spark is a unified in-memory distributed computing framework.


	Spark in-memory computation
        ---------------------------
	-> Spark can store intermediate results in memory. 
	    -> If there is not enough memory spark can store the results on the disk as well.


	Spark Unified Framework
	-----------------------
	-> Spark provides a set of consistent APIs running on the same execution engine for
	   performing different types of anlytical workloads.	

	
		Batch Processing of Unstructured Data	-> Spark Core API (RDDs)
		Batch Processing of Structured Data	-> Spark SQL
		Streaming Analytics (real time)		-> Spark Streaming
		Predictive Analytics (Machine Learning) -> Spark MLlib
		Graph Parallel Computations		-> Spark GraphX


   Spark Architecture
   ------------------

     1. Cluster Manager (CM)
	-> Spark Jobs are submitted to a cluster manager
	-> CM schedules the job, launches the job
	-> CM allocates some executors to the application

	=> Supported:
		Spark Standalone, YARN< Mesos, Kubernetes.

     2. Driver
	-> When an application is launched, a drive process is created
	-> The first object that gets created when an application is launched is "SparkContext"
	-> Is the master process
	-> Will store all the data and meta data related to the user program
	-> Will analyze the user code and sends tasks on the executors
    
     3. Executors
	-> Executors receives tasks from the driver
	-> All tasks does the same process on different partitions of data
	-> After the task completion, the status is reported to the driver.
	
     4. SparkContext
	-> Is an application context
	-> Is a link between the driver process and several tasks that are running on the cluster.


   Getting started with Spark
   --------------------------

      1. Downloading & setting up Spark 
	
	    URL: https://spark.apache.org/downloads.html
            -> Download the tar (.tgz) file and extract it to a suitable location.
	    -> Setup SPARK_HOME & HADOOP_HOME environment variable poiting to Spark installation dir.
	    -> PYTHONPATH: %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%

      2. Installing and setup PySpark with Spyder
	   -> Install "Anaconda Navigator"
	   -> Follow the step mentioned in the document shared on the GitHub

      3. Signup to Databricks Community Account
           URL: https://databricks.com/try-databricks

	   -> Signup to the free account (15 GB space)
	   -> Login to Databricks account
	   -> Read the "Quick Start Tutorial"


   Resilient Distributed Datasets (RDD)
   ------------------------------------

	-> RDD is a fundamental in-memory data abstraction of Spark Core API

	-> RDD is a collection of in-memory distributed partitions.
		-> Partition is a collection of objects. 

	-> RDDs have two aspects:

		Meta Data : Lineage DAG of RDD (Logical Plan)
			    -> describes how the RDD has to be created

		Data : The actual in-memory partitions

	-> RDDs are immutable
		-> RDD partition's data can not be changed.

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.

	
   How to create RDDs ?
   --------------------	
	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.


		rdd1 = sc.parallelize( range(1, 101), 3 )

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


   What can we do with an RDD ?
   -----------------------------

	Only Two things:

	1. Transformations
		-> Transformations does not execution 
		-> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
		-> Triggers the actual execution of the RDDs and some output is generated
		-> Cause the driver the convert the ogical plan to a physical execution ad several
		   tasks are sent to the executor.

   RDD Lineage DAG
   ---------------

	-> Lineage DAG of an RDD tracks all the dependencies as to how to create RDD all way from
	   the very first RDD. This is a logical plan maintained by the driver process.

	rddFile = sc.textFile( file )
		Lineage:   rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage:  rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

        rdd4.collect()  --> returns an Array()
 	sc.textFile (rddFile) -> map (rdd2) -> flatMap (rdd3) -> filter (rdd4) ==> driver
    
     

   RDD Transformations & Actions
   -----------------------------

    
	  	
	




	









 

	




