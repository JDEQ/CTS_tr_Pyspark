
  Agenda - PySpark
  -----------------

	Spark - Basics & Architecture
	Spark Core API - RDDs
		-> RDD Transformations & Actions
		-> Shared Variables
	Spark SQL - DataFrames
	Spark MLlib & Machine Learning 
	Introduction to Spart Streaming


   Materials
   ---------
	-> PDF presentations
	-> Code & Data
	-> Class Notes

	URL: https://github.com/ykanakaraju/pyspark


   Cluster
   --------
	-> A group of nodes whose combined resources can be used to distribute our
	   storage and processing across many nodes.


   What is Spark ?
   --------------
	-> Spark framework is written in SCALA language

    	-> Spark is a unified in-memory distributed computing framework.


	Spark in-memory computation
        ---------------------------
	-> Spark can store intermediate results in memory. 
	    -> If there is not enough memory spark can store the results on the disk as well.


	Spark Unified Framework
	-----------------------
	-> Spark provides a set of consistent APIs running on the same execution engine for
	   performing different types of anlytical workloads.	

	
		Batch Processing of Unstructured Data	-> Spark Core API (RDDs)
		Batch Processing of Structured Data	-> Spark SQL
		Streaming Analytics (real time)		-> Spark Streaming
		Predictive Analytics (Machine Learning) -> Spark MLlib
		Graph Parallel Computations		-> Spark GraphX


   Spark Architecture
   ------------------

     1. Cluster Manager (CM)
	-> Spark Jobs are submitted to a cluster manager
	-> CM schedules the job, launches the job
	-> CM allocates some executors to the application

	=> Supported:
		Spark Standalone, YARN< Mesos, Kubernetes.

     2. Driver
	-> When an application is launched, a drive process is created
	-> The first object that gets created when an application is launched is "SparkContext"
	-> Is the master process
	-> Will store all the data and meta data related to the user program
	-> Will analyze the user code and sends tasks on the executors

	Deploy-mode:

	1. client -> (default) the driver runs on the client machine
	2. cluster -> the driver also runs on one of nodes in the cluster
	
    
     3. Executors
	-> Executors receives tasks from the driver
	-> All tasks does the same process on different partitions of data
	-> After the task completion, the status is reported to the driver.
	
     4. SparkContext
	-> Is an application context
	-> Is a link between the driver process and several tasks that are running on the cluster.


   Getting started with Spark
   --------------------------

      1. Downloading & setting up Spark 
	
	    URL: https://spark.apache.org/downloads.html
            -> Download the tar (.tgz) file and extract it to a suitable location.
	    -> Setup SPARK_HOME & HADOOP_HOME environment variable poiting to Spark installation dir.
	    -> PYTHONPATH: %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%

      2. Installing and setup PySpark with Spyder
	   -> Install "Anaconda Navigator"
	   -> Follow the step mentioned in the document shared on the GitHub

      3. Signup to Databricks Community Account
           URL: https://databricks.com/try-databricks

	   -> Signup to the free account (15 GB space)
	   -> Login to Databricks account
	   -> Read the "Quick Start Tutorial"

To Download a file from databricks:
-----------------------------------

/FileStore/<FILEPATH>
https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

Example:
/FileStore/tables/wordcount-5.txt
https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


/FileStore/tables/WordcountExample1Output/part-00000

/FileStore/tables/demo_wordcount_out/part-00001

https://community.cloud.databricks.com/files/tables/demo_wordcount_out/part-00001?o=4949609693130439#tables/new/dbfs


   Resilient Distributed Datasets (RDD)
   ------------------------------------

	-> RDD is a fundamental in-memory data abstraction of Spark Core API

	-> RDD is a collection of in-memory distributed partitions.
		-> Partition is a collection of objects. 

	-> RDDs have two aspects:

		Meta Data : Lineage DAG of RDD (Logical Plan)
			    -> describes how the RDD has to be created

		Data : The actual in-memory partitions

	-> RDDs are immutable
		-> RDD partition's data can not be changed.

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.

	-> RDDs are resilient
		-> RDD are resilient to missing in-memory partitions. Any missing partition
		   does not cause failure of the job. RDDs can recreate the missing partitions
		   on-the-fly.  

	
   How to create RDDs ?
   --------------------	
	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.


		rdd1 = sc.parallelize( range(1, 101), 3 )

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


   What can we do with an RDD ?
   -----------------------------
	Only Two things:

	1. Transformations
	    -> Transformations does not execution 
	    -> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
	    -> Triggers the actual execution of the RDDs and some output is generated
	    -> Cause the driver the convert the ogical plan to a physical execution ad several
	       tasks are sent to the executor.

   RDD Lineage DAG
   ---------------

	-> Lineage DAG of an RDD tracks all the dependencies as to how to create RDD all way from
	   the very first RDD. This is a logical plan maintained by the driver process.

	rddFile = sc.textFile( file )
		Lineage:   rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage:  rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

        rdd4.collect()  --> returns an Array()

 	sc.textFile (rddFile) -> map (rdd2) -> flatMap (rdd3) -> filter (rdd4) ==> driver


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd5.persist( StorageLevel.MEMORY_AND_DISK )    --> instruction to spark to not delete those partitions
	rdd6 = rdd5.t6(..)

	rdd6.collect()	
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect

	rdd5.collect()	
	
       Types of persistence:
	 
             -> Persist in-memory as deserialized objects
	     -> Persist in-memory as serialized data (bytearrays)
	     -> Persist on disk

     Storage Levels
     ---------------
	1. MEMORY_ONLY	        -> Only in the RAM
	2. MEMORY_AND_DISK	-> Tries to store in memory if available, else on disk
	3. DISK_ONLY
	4. MEMORY_ONLY_SER	-> (default) Only in the RAM in serliazed format	
	5. MEMORY_AND_DISK_SER	
	6. MEMORY_ONLY_2
	7. MEMORY_AND_DISK_2	
 
    Commands
    --------
	rdd1.persist()
	rdd1.cache()    -> is similar to the above
	rdd1.persist( StotageLavel.MEMORY_AND_DISK )
	rdd1.unpersist()	

    Executor Memory Structure
    --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations & Actions
   -----------------------------

    1. map		P: U -> V 
			Element to element transformation
			input RDD: N objects, output RDD: N objects   
  
    2. filter		P: U -> Boolean
			Filters the elements that return True for the function.
  			input RDD: N objects, output RDD: <= N objects 

    3. glom		P: None
			Will return one array per partition of the input RDD
	

	rdd1			rdd2 = rdd1.glom()

	P0:  2,3,1,3,2,4,3 -> glom -> P0: [2,3,1,3,2,4,3]
	P1:  4,2,1,3,2,6,5 -> glom -> P1: [4,2,1,3,2,6,5]
	P2:  5,6,7,8,1,2,3 -> glom -> P2: [5,6,7,8,1,2,3]

	rdd1.count() = 21		rdd2.count() = 3

	rdd1.glom().collect()
	rdd1.glom().map(lambda x: sum(x)).collect()


   4. flatMap			P: U -> Iterable[V]   ( U -> Collection[V] )
				flatMap flattens all the iterable elements returned by the function.
				input RDD: N objects, output RDD: >= N objects 


	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	rddWords.flatMap(lambda x: x.upper()).collect()
	rdd1.flatMap(lambda x: range(1, x+1)).collect()


   5. mapPartitions		P: Iterator[U] -> Iterator[V]	
				Applies a function on the entire partition. All the elements of each 
				partition is passed as function input.

	rdd1.mapPartitions(lambda a: [sum(a)] ).collect()


   6. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V]
				The function take two params - partition-index & partition-data.	

	rdd1.mapPartitionsWithIndex(lambda id, data : [(id, len(list(data)))] ).collect()


   7. distinct			P: None, Optional: number of output partitions
				Returns the distinct elements of the RDD

   8. sortBy			P: U -> V, optional: ascending (True/False), numPartitions
				Elements of the RDD are sorted based the function output they produce.
	
	rdd1.sortBy(lambda x: x%5).glom().collect()
	rdd1.sortBy(lambda x: x%5, False).glom().collect()
	rdd1.sortBy(lambda x: x%5, True, 5).glom().collect()
	rdd1.sortBy(lambda x: x%5, numPartitions=5).glom().collect()  
	// specifying partition count by named parameter


   Types of RDDs
   -------------
	-> Generic RDDs : RDD[U]
	-> Pair RDDs:     RDD[(U, V)]


   9. mapValues			P: U -> V
				Applied ONLY to pair RDDs
				Applies the function on the values part of the pair RDDs.

	rddPairs2.mapValues(lambda x: x.upper()).collect()


   10. groupBy			P: U -> V, optional: numPartitions

				Returns a pair RDD, where the 'key' is the unique values of the function output
				and the 'value' is an iterable object (ResultIterable) contains all the elements 
				of the RDD that produced the function output.

   	rdd1.groupBy(lambda x: x%3 ).mapValues(list).collect()
	rddWords.groupBy(lambda x: x).mapValues(len).collect()
	rdd1.groupBy(lambda x: x%3, 2).mapValues(list).collect()

	rdd = sc.textFile(file, 4) \
        .flatMap(lambda x: x.split(" ")) \
        .groupBy(lambda x: x) \
        .mapValues(len) \
        .sortBy(lambda x: x[1], False, 1)


   11. randomSplit		P: Array of ratios
				Returns an Array of RDDs split approximatly in the specified ratios.

	rddArr = rdd1.randomSplit([1,2,3])
	rddArr = rdd1.randomSplit([1,2,3], 3538)   // here 3538 is a seed
		-> A seed fixes the randomness and produce the same output every time.

   12. repartition		P: numPartitions
				Creates an RDD with specified number of partitions
				Is used to increase or decrease the number of output partitions.
				Causes global shuffle

   13. coalesce			P: numPartitions
				Creates an RDD with specified number of partitions
				Is used only to decrease the number of output partitions.
				Cause partition merging.

   14. partitionBy		P: numPartitions, Optional: partitioning function
				Applied only on pair RDDs
				Used to control which elements goto which partition based on a partition function
				applied to the keys of the (K,V) pairs.
   			
	rddPairs.partitionBy(4, lambda x: len(x)).glom().collect()


   15. union, intersection, subtract, cartesian   	P: rdd
	
	Let us say rdd1 has M partitions and rdd2 has N partitions:

	Command					Number of output partitions
        ------------------------------------------------------------------
	rdd1.union(rdd2)			M + N, narrow
	rdd1.intersection(rdd2)			M + N, wide
	rdd1.subtract(rdd2)			M + N, wide
	rdd1.cartesian(rdd2)			M * N, wide
				
	
   ...ByKey Transformations    
   	=> Applied only to Pair RDD
	=> Are wide transformations
	

   16. sortByKey		P: None, Optional: Sorting Order, numPartitions
				Sorts the RDD based on the keys of the pair RDD.

	rddPairs.sortByKey().collect()
	rddPairs.sortByKey(False).collect()
	rddPairs.sortByKey(False, 5).collect()


   17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD where the key is the unique key of input RDD and values are
				aggregated values for that key. Output will have unique keys and aggregated values.

				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]

				Note: Avoid groupByKey if possible.

		rdd = sc.textFile(file, 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda a: (a,1)) \
        		.groupByKey() \
        		.mapValues(sum)

   18. reduceByKey		P: (U, U) -> U,    optional: numPartitions
				Wil reduce all the values of each unique key with in each partition
				in the first stage (narrow), and then reduces the outputs across partitions	
				to one final value. 

		rdd = sc.textFile(file, 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y, 1)

   19. aggregateByKey		Will reduce all thr values of each unique key into a value of the type
				of zero-value. (zero-value can be of different type than RDD elements).
	
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence Function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
	

	avg_rdd = student_rdd.map(lambda x: (x[0], x[2])) \
            	.aggregateByKey( (0,0),
                            lambda z, v: (z[0] + v, z[1] + 1), 
                            lambda a, b: (a[0] + b[0], a[1] + b[1])
                            ) \
            	.mapValues(lambda x: x[0]/x[1])


   20. join transformations	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		names1 = sc.parallelize(["vijay", "aditya", "raju", "amrita"]).map(lambda a: (a, 1))
		names2 = sc.parallelize(["amrita", "raju", "pavan", "pranav"]).map(lambda a: (a, 2))

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup   		=> Is used when you wantto join RDDs with duplicate keys.
				=> groupByKey on each RDD -> fullOuterJoin on those grouped RDDs


	   rdd1 => [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
			-> (key1, [10, 7]), (key2, [12, 6]), (key3, [6])
	
	   rdd2 => [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
			-> (key1, [5, 17]), (key2, [4, 7]), (key4, [17])

	   rdd3 = rdd1.cogroup(rdd2)
		-> (key1, ([10, 7], [5, 17])), (key2, ([12, 6], [4, 7])) (key3, ([6], [])), (key4, ([], [17]))



   RDD Actions
   ------------

	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		P: (U, U) -> U
				Will reduce the entire RDD into one final value by iterativly applying the 
				reduce function in each partition first, and then across partitions.

		P0: 6, 3, 1, 2, 3, 4    => -7   => 42
		P1: 6, 8, 9, 0, 6, 3    => -20
		P2: 4, 5, 7, 4, 8, 9, 0 => -29

		rdd1.reduce(lambda x, y: x - y)

       5. aggregate		Will reduce entire RDD into one final value of a type different 
				from the elements of the RDD.	
		
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence Function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

		rdd1.aggregate( (0,0), 
				lambda z, v: (z[0] + v, z[1] + 1), 
				lambda a, b: (a[0] + b[0], a[1] + b[1]) )

	6. take(n)   => Returns an array with first n elements of the RDD


	7. takeOrdered(n, [optioning fn]) -> returns first n elements of the sorted RDD.

		rdd1.takeOrdered(10)
		rdd1.takeOrdered(10, lambda x: x%2)


	8. takeSample(withReplcement, n, [seed])

		rdd1.takeSample(True, 6, 678)   - withReplacement: True, 678 is a seed
		rdd1.takeSample(False, 6, 678)  - withReplacement: False, 678 is a seed

	9. countByValue

	10. countByKey

	11. first

	12. saveAsSequenceFile

		rddWordCounts.coalesce(1).saveAsSequenceFile("E:\\PySpark\\output\\seq")

	13. foreach

		rddWordCounts.foreach(lambda a: print("key = " + a[0] + ", value = " + str(a[1])) )
		

   Use-Case
   --------
   From cars.tsv file find average weight of each of the makes of American cars.
   Arransge the data in the descending order of averahe weight
   Save the output as one text file. 

        --> Please try yourself



   Closure
   -------
	-> In Spark, a closure constitutes all the variables and methods which must be visible 
           for the executor to perform its computations on the RDD. 

	-> This closure is serialized and sent to each executor.

   
   Shared Variables
   ----------------

    => Because of closure, we CAN NOT use local variables to implement global counter. To solve this
       Spark provide a special shared variable called "accumulator"

       
	counter = 0

	def isPrime(n) :
	   returns 1 if n is Prime
	   else returns 0
	
	def f1(n) :
	   if (isPrime(n) == 1) counter = counter + 1
	   retunr n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()
 

	print("counter = " + counter)     // this will not fetch the total count

  
   1. Accumulator

	-> Accumulator is maintained by the driver and is not part of the function closure.
	-> Accumulators can be used to implement global counters.
	
	counter = sc.accumulator(0)

	def isPrime(n) :
	   returns 1 if n is Prime
	   else returns 0
	
	def f1(n) :
	   if (isPrime(n) == 1) counter.add(1)
	   retunr n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect() 

	print("counter = " + counter)     // this will not fetch the total count


   2. Broadcast

	-> A single copy of a "broadcast variable" is sent to every executor node
	-> Saved in the storage memory  (along-side the RDD paertitions)
	-> All the tasks that are running in that executor can lookup from that one copy.


	d1 = sc.broadcast({1: 'A', 2: 'B', 3: 'C', ....})   

	def f1 (n) :
	    global d1
            return d1.value[n]

	rdd1 = sc.parallelize( [2,1,3,2,5,6,8,9,8], 3 )

	rdd2 = rdd1.map( f1 )


  Spark-submit command
  --------------------

    -> Is a single command that is used to submiy any spark application (scala, java, python or R)
       to any cluster manager (standalone, local, yarn, mesos, k8s).	


	spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--executory-memory 5G
		--executor-cores 5
		--num-executors 20
		wordcount.py <command-line-args> 

 ==============================
     Spark SQL (pyspark.sql)
 ==============================

   -> Spark SQL is a structured data processing API

   -> Data that we can process is:	
	Structured file formats:  Parquet (default), ORC, JSON, CSV (delimited text)
	Hive
	JDBC Source : RDBMS databases, NoSQL databases

   -> SparkSession
	-> starting point of any SparkSQL application.
	-> represents a user-session within an application.
	-> an application can have multiple SparkSessions

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()
   
   -> DataFrame (DF)
	-> Data abstraction of Spark SQL

	-> DF is a collection of distributed, immutable, lazily evaluated in-memory partitions.
	-> DF is collection of "Row" objects

	-> Row object is an object of type "StructType" (which represents a schema)
	
		DataFrame has two things:
		1. Data 	-> Collection of Rows
		2. Meta Data 	-> Schema

		Schema => StructType(
			     List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			     )
			 )


   LocalTempViews & GlobalTempViews
   ---------------------------------
	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.



   Steps to work with Spark SQL
   ----------------------------

	1. Read/Load the data from a datasource (external data source or programmatic data) into a 
	   DataFrame.

		inputFilePath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputFilePath)		


	2. Apply transformations on the DF using either DataFrame API or using SQL

		Using DataFrame API:

			df2 = df1.select("userid", "name", "age", "phone") \
        			.where("age is not null") \
        			.orderBy("age", "name") \
        			.groupBy("age").count() \
        			.orderBy("count") \
        			.limit(4)

		Using SQL

			df1.createOrReplaceTempView("users")
		
			qry = """select age, count(*) as count 
         			from users 
         			where age is not null
         			group by age
         			order by count
         			limit 4"""

			df3 = spark.sql(qry)


	3. Write/Save the dataframe as a structured file or into any structured data destination.  
 
		df3.write.format("json").save(outputPath)
	


  DataFrame Transformations
  -------------------------

   1. select

		df2 = df.select( < a list of strings > )
		df2 = df.select( < a list of Column objects > )

		df2 = df1.select("ORIGIN_COUNTRY_NAME",
                 		 "DEST_COUNTRY_NAME",
                 		 "count")

		df2 = df1.select(col("DEST_COUNTRY_NAME").alias("destination"),
                 	col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 	expr("count"),
                 	expr("count + 10 as newCount"),
                 	expr("count > 200 as highFrequency"),
                 	expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

   2. where / filter

		df3 = df2.where("highFrequency = true and domestic = false")
		df3 = df2.filter("highFrequency = true and domestic = false")
		df3 = df2.where(col("count") > 500)

   3. orderBy / sort
		df3 = df2.orderBy("count", "destination")
		df3 = df2.orderBy(col("count").asc(), col("destination").desc() )
		df3 = df2.sort(desc("count"), asc("destination"))


   4. groupBy with some aggregation function

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")
		df3 = df2.groupBy("domestic", "highFrequency").max("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
         		.agg(   count("count").alias("count"), 
              			sum("count").alias("sum"), 
              			avg("count").alias("avg"), 
              			max("count").alias("max"))

		
		df3 = df2.groupBy("domestic", "highFrequency") \
         		.agg(   expr('count("count") as count'), 
              			sum("count").alias("sum"), 
              			avg("count").alias("avg"), 
              			max("count").alias("max"))

   5. limit
		
		df2 = df1.limit(10)

   6. selectExpr

	df2 = df1.select(expr("DEST_COUNTRY_NAME as destination"),
                 expr("ORIGIN_COUNTRY_NAME as origin"),
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

	is same as:

	
	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                 "ORIGIN_COUNTRY_NAME as origin",
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

   7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
                 .withColumn("highFrequency", col("count") > 200) \
                 .withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", col("count") > 200) \
         	.withColumn("domestic", col("DEST_COUNTRY_NAME") == col("ORIGIN_COUNTRY_NAME"))

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", col("count") > 200) \
         	.withColumn("domestic", col("DEST_COUNTRY_NAME") == col("ORIGIN_COUNTRY_NAME")) \
         	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

   8. drop
		df4 = df3.drop("newCount", "highFrequency")

   9. union

	flightsList = [("India", "UK", 234),
               ("India", "Japan", 234),
               ("India", "Australia", 234),
               ("India", "France", 234)]

	dfFlights = spark.createDataFrame(flightsList) \
            .toDF("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

	dfFlights.show()

	df5 = dfFlights.union(df1)

   10. randomSplit

	df100, df101 = df1.randomSplit([0.6, 0.4])
	
	dfArr = df1.randomSplit([0.6, 0.4], 57)    
	// 57 is a seed (any number) to produce the same output for multiple executions

	dfArr[0].count()
	dfArr[1].count()


   11. sample

	df4 = df1.sample(True, 0.7)
	df4 = df1.sample(True, 1.7, 5676)   // with replacement sampling

	df4 = df1.sample(False, 0.7, 567)   // with out replacement sampling


   12. repartition & coalesce

		df2 = df1.repartition(3)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()
   	
		spark.conf.set("spark.sql.shuffle.partitions", 5)
		=> default value of "spark.sql.shuffle.partitions" is 200.
	  	=> Command to set the config value of spark config parameter.

		df2 = df1.repartition(col("DEST_COUNTRY_NAME"))
		-> numpartitions = 5 (the value of spark.sql.shuffle.partitions)

		df2 = df1.repartition(8, col("DEST_COUNTRY_NAME"))

   13. distinct

	  df4.select("ORIGIN_COUNTRY_NAME").distinct().count()



   Working with different Structured File Formats
   -----------------------------------------------

    1. JSON
	read:
		df1 = spark.read.format("json").load(inputFilePath)
		df1 = spark.read.json(inputFilePath)

	write:
		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)

   2. CSV
	read:
		df1 = spark.read.format("csv").load(inputFilePath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True, sep="|")

	write:
		df2.write.mode("overwrite").format("csv").save(outputPath, header=True)
		df2.write.mode("overwrite").csv(outputPath, header=True)
		df2.write.mode("append").csv(outputPath, header=True, sep="|")

   3. parquet
	read:
		df1 = spark.read.format("parquet").load(inputFilePath)
		df1 = spark.read.parquet(inputFilePath)
	
	write:
		df2.write.format("parquet").save(outputPath)
		df2.write.parquet(outputPath)

   4. ORC
	read:
		df1 = spark.read.format("orc").load(inputFilePath)
		df1 = spark.read.orc(inputFilePath)
	write:
		df2.write.format("orc").save(outputPath)
		df2.write.mode("overwrite").orc(outputPath)

   
  Converting  a DataFrame into an RDD
  -----------------------------------
	
	rdd1 = df1.rdd

	df2.rdd.getNumPartitions()


  Save Modes
  ----------
	-> Define the behaviour when you are saving a DF to a existing directory.

	1. errorIfExists  (default)
	2. ignore
	3. append
	4. overwrite
		
	=> df2.write.mode("overwrite").json(outputPath) 


  Creating DataFrames from programmatic data
  ------------------------------------------
	usersList = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Raghu", 25),
             (4, "Ravi", 15),
             (5, "Ramya", 25),
             (6, "Raghava", 35),
             (7, "Radhika", 45)]

	df1 = spark.createDataFrame(usersList).toDF("id", "name", "age")
	
	df1.show()
	df1.printSchema()

   
   Creating DataFrames from RDD
   -----------------------------

    method 1:
    ---------

	usersList = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Raghu", 25),
             (4, "Ravi", 15),
             (5, "Ramya", 25),
             (6, "Raghava", 35),
             (7, "Radhika", 45)]

	rdd1 = spark.sparkContext.parallelize(usersList)
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")


   method 2
   --------

	rddRows = rdd1.map(lambda t: Row(t[0], t[1], t[2]))
	rddRows.collect()

	schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rddRows, schema)


  Applying schema programmatically while reading data
  ----------------------------------------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.schema(mySchema).json(inputFilePath)


   Joins
   -----

    Supported Joins => inner, left_outer, right_outer, full_outer, left_semi, left_anti 	


    left_semi join:

	-> Similar to inner join but the data only comes from the left side table.
	-> Is equivalent to the following sub-query
		select * from emp where deptid IN (select deptid from dept)

    left_anti join:

	-> fetches rows from the first table whose join column is not there in the second table.
	-> Is equivalent to the following sub-query
		select * from emp where deptid NOT IN (select deptid from dept)


	SQL Approach:
        -------------

		employee.createOrReplaceTempView("emp")
		department.createOrReplaceTempView("dept")

		qry = """select emp.*
        		from emp left anti join dept
        		on emp.deptid = dept.id"""
        
		dfJoined = spark.sql(qry)

		dfJoined.show()

	DataFrame API approach     
	-----------------------
	Supported Joins => inner, left_outer, right_outer, full_outer, left_semi, left_anti 


	joinCol = employee["deptid"] == department["id"]
	dfJoined = employee.join(department, joinCol, "left_outer")

   
   Working with MySQL
   ------------------
      --> refer to the code from Github


   Working with Hive
   -----------------	
	-> Hive is hadoop's data warehousing platform
	warehouse directory -> is the directory where Hive stores all the data of managed table. 


   Use-Case
   --------
	From movies.csv and ratings.csv datasets, fetch the top 10 movies with heighest average
	movie rating.

	-> Consider the movies that are rated by atleast 30 users. 
	-> Data Required: movieId, title, totalNumberOfRatings, averageRating
	-> Arrange the data in the desc order of averageRating
	-> Save the output in one Pipe-Seprated CSV file. 
	
        => Try it yourself...

  --------------------------------------------
      Machine Learning & Spark MLlib
  --------------------------------------------

     An ML Model  => is a learned entity. 
			-> Learns from historical data.
			-> Trained by algorithms (iterative mathematical computations)

     historical data (training data)  ==>  several inputs (features) and one output (label)

   
         x	y	z (label)   prediction	error
	---------------------------------------------
	1000	100	2150		2100	  50
	2000	1000	4900		5000	 100
	1500	500	3400		3500	 100
	1200	600	3100		3000	 100
	1100	200	 ?? 
	---------------------------------------------
				Loss:    350/4 => 86
      
         Model 1:    z = 2x + y   	Loss: 86
	 Model 2:    z = 1.9x + y	Loss: 75
	 Model 3:    z = 1.85x + 1.1y	Loss: 79
	 Model 4:    z = 1.91x + y	Loss: 74   --> final model.

   Terminology
   -----------

   1. Features   	-  inputs, dimensions
   2. Label	 	-  output 
   3. Training data     -  contains features and label
   4. Algorithm		-  Mathematical compuation that iteratively process the training data	
			   with a goal to establish a relation between the label and feature
			   with minimal value of a loss function.
   5. Learning/Training - The iterative algorithmic process
   6. ML Model		- The final output (which is a kind of relation between features and label)
			  of the training.
   7. error		- deviation between the prediction and actual value of a single data point
   8. loss		- cumulative error.
 
  
   Types of Machine Learning
   --------------------------
	
	1. Supervised Learning
	     -> Data => both features & label  (labelled data)

	     1.1 Classification
		  -> Label is one of two/few fixed values.
		  -> Label: 1/0, Yes/No, Spam/Not-Spam, [1,2,3,4,5]
		  -> Ex: Email Spam Prediction, Survival Prediction

	     1.2 Regression
		  -> Label is a continuous value
		  -> Ex: House Price Prediction

	2. Unsupervised Learning
	      -> Data does not contain label. 
	      -> Trying tp understand trhe patterns in the data.

	      2.1 Clustering 
		   
	      2.2 Dimesionality Reduction 	

	3. Reinforcement Learning

	    -> Semi-supervised learning


   Steps in any ML project
   -----------------------

    1. Data Collection

    2. Data Preparation  (Pre-processing) 
   
    	-> Prepare the data to make suitable to fit to an algorithm. 
	-> Creating the "Feature Vector"

	EDA (Exploratory Data Analysis)
	FE (Feature Engine)

	-> We should not any null values or empty values.
	-> All data should be in numerical format.
	
    3. Train out Model using one or more algorithms

	-> Split the training data into training and validation sets (70%, 30%)
	-> Train the model using the training dataset (70%)	

    4. Evaluate the model

    5. Deploy the model  (& continuous improvement)


    Machine Learning Libraries
    --------------------------
	Machine Learning => Spark MLlib, SciKit Learn (SKLearn), SAS, PyTorch
	Deep Learning => Tensor Flow, Keras, PyTorch


    Spark MLlib  
    -----------
	Two Libraries:

	1. pyspark.mllib    (based on RDDs) - legacy API (not in active use)
	2. pyspark.ml	    (based on DataFrames) - current library
   
     
        Different Tools
        ---------------
	1. Data 	-> DataFrames
	2. Features	-> Feature Selectors, Feature Transformers, Feature Extractors
	3. Algorithms	-> Classification, Regression, Clustering, Collaborative Filtering
	4. Pipeline	-> The approach to ML model building
	5. Model Selection Tools  -> Train-Validation Split, Cross-Validation
	6. Utilities	-> LinAlg, Statistics 


   Spark MLlib Components
   ----------------------
	
     1. DataFrames 	-> All the data is in the form of DataFrames.
 
     2. Feature Vector	-> Vector object containing all the features in numerical format.	
		-> Dense Vector  :  Vectors.dense(0,0,0,0,2,1,0,0,0,7,8,0,0,3,0)	
		-> Sparse Vector :  Vectors.sparse(15, [4,5,9,10,13], [2,1,7,8,3])

     3. Estimator

	     -> input:  DataFrame
	     -> output: Model
	     -> method: fit

		<model> = <estimator>.fit( <dataFrame> )
		lrModel = lr.fit( inputDf )

	      -> All ML agorithms, few Feature selectors (RFormula), few Feature Tranformers (StringIndexer, oneHotEncoder)

     4. Transformer

	     -> input: DataFrame
	     -> output: DataFrame
	     -> method: transform

		outputDF = <transformer>.transform( inputDF )

	     -> All Models, A lot of Feature Tranformers 9tokenizer, hashingTF

    5. Pipeline

	   Set of stages containing estimators and transformers.

	   pl = Pipeline.setStages([T1, T2, T3, E1])	
           plModel = pl.fit( df1 )

	   df1 -> T1 -> df2 -> T2 -> df3 -> T3 -> df4 -> E1 -> plModel

   
   Mini Project - Tital Passenger Survival Prediction
   --------------------------------------------------
    URL: https://www.kaggle.com/c/titanic/data

   PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs."Braund, Mr. Owen  John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S
9,1,3,"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)",female,27,0,2,347742,11.1333,,S
10,1,2,"Nasser, Mrs. Nicholas (Adele Achem)",female,14,1,0,237736,30.0708,,C
11,1,3,"Sandstrom, Miss. Marguerite Rut",female,4,1,1,PP 9549,16.7,G6,S
12,1,1,"Bonnell, Miss. Elizabeth",female,58,0,0,113783,26.55,C103,S

              

    Label:  Survived

    Features: 
 	Numerical: Pclass,Age,SibSp,Parch,Fare     -> Double format
	Categorical: Sex, Embarked
		=> StringIndexer + oneHotEncoder   -> Vector objects

		-> VectorAssembler -> FeatureVector
	
		-> RandomForest -> Model

  ====================================================================

   Spark Streaming
   ---------------

	Spark Streaming libraries:

	1. Spark Streaming ( RDDs - DStreams API)
	2. Spark Structured Streaming ( DataFrames )

	
     Spark Streaming :

	 -> Micro-batch based processing


     Spark Streaming nativly supports two types streams:

	1. Socket Streams
	2. File Streams
   

 =============================================================

   



