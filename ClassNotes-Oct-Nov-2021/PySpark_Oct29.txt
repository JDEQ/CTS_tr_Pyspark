
  Agenda - PySpark
  -----------------

	Spark - Basics & Architecture
	Spark Core API - RDDs
		-> RDD Transformations & Actions
		-> Shared Variables
	Spark SQL - DataFrames
	Spark MLlib & Machine Learning 
	Introduction to Spart Streaming


   Materials
   ---------
	-> PDF presentations
	-> Code & Data
	-> Class Notes

	URL: https://github.com/ykanakaraju/pyspark


   Cluster
   --------
	-> A group of nodes whose combined resources can be used to distribute our
	   storage and processing across many nodes.


   What is Spark ?
   --------------
	-> Spark framework is written in SCALA language

    	-> Spark is a unified in-memory distributed computing framework.


	Spark in-memory computation
        ---------------------------
	-> Spark can store intermediate results in memory. 
	    -> If there is not enough memory spark can store the results on the disk as well.


	Spark Unified Framework
	-----------------------
	-> Spark provides a set of consistent APIs running on the same execution engine for
	   performing different types of anlytical workloads.	

	
		Batch Processing of Unstructured Data	-> Spark Core API (RDDs)
		Batch Processing of Structured Data	-> Spark SQL
		Streaming Analytics (real time)		-> Spark Streaming
		Predictive Analytics (Machine Learning) -> Spark MLlib
		Graph Parallel Computations		-> Spark GraphX


   Spark Architecture
   ------------------

     1. Cluster Manager (CM)
	-> Spark Jobs are submitted to a cluster manager
	-> CM schedules the job, launches the job
	-> CM allocates some executors to the application

	=> Supported:
		Spark Standalone, YARN< Mesos, Kubernetes.

     2. Driver
	-> When an application is launched, a drive process is created
	-> The first object that gets created when an application is launched is "SparkContext"
	-> Is the master process
	-> Will store all the data and meta data related to the user program
	-> Will analyze the user code and sends tasks on the executors
    
     3. Executors
	-> Executors receives tasks from the driver
	-> All tasks does the same process on different partitions of data
	-> After the task completion, the status is reported to the driver.
	
     4. SparkContext
	-> Is an application context
	-> Is a link between the driver process and several tasks that are running on the cluster.


   Getting started with Spark
   --------------------------

      1. Downloading & setting up Spark 
	
	    URL: https://spark.apache.org/downloads.html
            -> Download the tar (.tgz) file and extract it to a suitable location.
	    -> Setup SPARK_HOME & HADOOP_HOME environment variable poiting to Spark installation dir.
	    -> PYTHONPATH: %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%

      2. Installing and setup PySpark with Spyder
	   -> Install "Anaconda Navigator"
	   -> Follow the step mentioned in the document shared on the GitHub

      3. Signup to Databricks Community Account
           URL: https://databricks.com/try-databricks

	   -> Signup to the free account (15 GB space)
	   -> Login to Databricks account
	   -> Read the "Quick Start Tutorial"

To Download a file from databricks:
-----------------------------------

/FileStore/<FILEPATH>
https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

Example:
/FileStore/tables/wordcount-5.txt
https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


/FileStore/tables/WordcountExample1Output/part-00000

/FileStore/tables/demo_wordcount_out/part-00001

https://community.cloud.databricks.com/files/tables/demo_wordcount_out/part-00001?o=4949609693130439#tables/new/dbfs




   Resilient Distributed Datasets (RDD)
   ------------------------------------

	-> RDD is a fundamental in-memory data abstraction of Spark Core API

	-> RDD is a collection of in-memory distributed partitions.
		-> Partition is a collection of objects. 

	-> RDDs have two aspects:

		Meta Data : Lineage DAG of RDD (Logical Plan)
			    -> describes how the RDD has to be created

		Data : The actual in-memory partitions

	-> RDDs are immutable
		-> RDD partition's data can not be changed.

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.

	-> RDDs are resilient
		-> RDD are resilient to missing in-memory partitions. Any missing partition
		   does not cause failure of the job. RDDs can recreate the missing partitions
		   on-the-fly.  

	
   How to create RDDs ?
   --------------------	
	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.


		rdd1 = sc.parallelize( range(1, 101), 3 )

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


   What can we do with an RDD ?
   -----------------------------

	Only Two things:

	1. Transformations
	    -> Transformations does not execution 
	    -> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
	    -> Triggers the actual execution of the RDDs and some output is generated
	    -> Cause the driver the convert the ogical plan to a physical execution ad several
	       tasks are sent to the executor.

   RDD Lineage DAG
   ---------------

	-> Lineage DAG of an RDD tracks all the dependencies as to how to create RDD all way from
	   the very first RDD. This is a logical plan maintained by the driver process.

	rddFile = sc.textFile( file )
		Lineage:   rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage:  rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

        rdd4.collect()  --> returns an Array()
 	sc.textFile (rddFile) -> map (rdd2) -> flatMap (rdd3) -> filter (rdd4) ==> driver


   Types of Transformations
   ------------------------

	1. Narrow Transformations
		-> Does not cause shuffling of the data from one partition to other partitions
		-> Partition to partition transformations
		-> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
		-> Causes shuffling of the data
		-> One output partition may need data from multiple input partitions
		-> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd5.persist()      ---> instruction to spark to not delete those partitions
	rdd6 = rdd5.t6(..)

	rdd6.collect()	
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect

	rdd5.collect()
	
	

   Executor Memory Structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations & Actions
   -----------------------------

    1. map		P: U -> V 
			Element to element transformation
			input RDD: N objects, output RDD: N objects   
  
    2. filter		P: U -> Boolean
			Filters the elements that return True for the function.
  			input RDD: N objects, output RDD: <= N objects 

    3. glom		P: None
			Will return one array per partition of the input RDD
	

	rdd1			rdd2 = rdd1.glom()

	P0:  2,3,1,3,2,4,3 -> glom -> P0: [2,3,1,3,2,4,3]
	P1:  4,2,1,3,2,6,5 -> glom -> P1: [4,2,1,3,2,6,5]
	P2:  5,6,7,8,1,2,3 -> glom -> P2: [5,6,7,8,1,2,3]

	rdd1.count() = 21		rdd2.count() = 3

	rdd1.glom().collect()
	rdd1.glom().map(lambda x: sum(x)).collect()


   4. flatMap			P: U -> Iterable[V]   ( U -> Collection[V] )
				flatMap flattens all the iterable elements returned by the function.
				input RDD: N objects, output RDD: >= N objects 


	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	rddWords.flatMap(lambda x: x.upper()).collect()
	rdd1.flatMap(lambda x: range(1, x+1)).collect()


   5. mapPartitions		P: Iterator[U] -> Iterator[V]	
				Applies a function on the entire partition. All the elements of each 
				partition is passed as function input.

	rdd1.mapPartitions(lambda a: [sum(a)] ).collect()


   6. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V]
				The function take two params - partition-index & partition-data.	

	rdd1.mapPartitionsWithIndex(lambda id, data : [(id, len(list(data)))] ).collect()


   7. distinct			P: None, Optional: number of output partitions
				Returns the distinct elements of the RDD



			

	
	








   RDD Actions
   ------------

	1. collect

	2. count

	3. saveAsTextFile


	









 

	




