
   Agenda  (7 sessions of 4 hours each)
   ------------------------------------

   	- Spark - Basics & architecture
	- Spark Core API (RDD API) - Low Level API
	    - RDD Transformations & Actions	
	- Spark SQL - DataFrames API
	- Machine Learning & Spark MLlib
	- Introduction Spark Streaming

  ---------------------------------------------------

   What is Big Data ?

	-> Any data that is so big and complex that it become hard to store and process
	   using onhand data management systems and traditional application models.

   
   Computing Cluster
   	-> A unified entity containing a lot of nodes whose cumulative resources can be used
	   distribute storage and process.
   
   Hadoop      
       	-> is an opensource framework for storage and processing of big data.
	-> runs on a cluster made of commodity hardware.

	-> Provides two frameworks for big data

	1. HDFS (Hadoop distributed file system)
		=> Distrinuted Storage Solutions
		-> Splits the file into blocks of 128 MB each.
		-> Spread the bloks of the file across many nodes in the cluster	
		-> Each block is replicated in three different nodes for fail-safety.

	2. MapReduce 
		-> Distributed Processing Solutions
		-> Distributes the processing across many machines (Map phase) and then
		   aggregates the results produced by all these mapper instances (reduce phase) 		    
  
   MapReduce is not good at few use-cases:

	-> Not good with lot of small files
	-> Not good with ad-hoc querying or random-access of data
	-> Not good with iterative computations.

	
   What is Spark ?
   ---------------

	-> Spark is a unified in-memory distributed processing framework
	-> Spark is written in Scala
	-> Spark supports multiple languages: Scala, Java, Python & R
	-> Spark applications can run on mulitple cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes. 
	
	In-memory framework
	--------------------
	   -> The inetermediate partitions can be persisted in memory and further tasks
	      can be launched on these in-memory persisted partitions.

	Unified framework
	------------------

	   -> Spark provides a cosnistenet set of APIs for different analytics workloads
	      running on the same execution engine. 


		Hadoop Ecosystem
                -----------------
		Batch Analytics of unstructured data  	: MapReduce
		Batch Analytics of Structured data	: Hive, Impala, Drill, HBase
		Streaming (real time) Analytics		: Kafka, Storm, Samza
		Predictive Analytics using ML		: Mahout
		Graph parallel computations		: Giraph

		Spark Framework	
		---------------
		Batch Analytics of unstructured data  	: Spark Core API
		Batch Analytics of Structured data	: Spark SQL
		Streaming (real time) Analytics		: Spark Streaming, Structured Streaming
		Predictive Analytics using ML		: Spark MLlib
		Graph parallel computations		: Spark GraphX

	
	Spark Layered Structure
	-----------------------

	Programming Lang  : Scala, Java, Python, R
	Spark High Level  : Spark SQL, Spark Streaming, Sparl MLlib, Spark GraphX
	Spark Low Level	  : Spark Core API (RDDs)
	Cluster Managers  : Spark Standalone, YARN, Mesos, Kebernetes  
	Storage Layer     : HDFS, Linux, RDBMS, NoSQL, Kafka


   Getting started with Spark
   --------------------------
	Pre-requisite. 
		-> Install Anaconda navigator
			Download and install from the following URL
			https://www.anaconda.com/products/individual-d#windows

	1. Install Spark and run PySpark Shell
		https://spark.apache.org/downloads.html

		Download Spark xxx.tgz file and extract it to a suitable folder.

		Setup your environment varibles:
		-> Add SPARK_HOME & HADOOP_HOME env. variables and point them to Spark installation folder. 
		-> Add bin folder of the spark installation folder to the PATH environment var. 
		-> PYTHONPATH -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH% 
	 
	2. Using an IDE (such as Spyder or PyCharm)		
		-> Follow the steps mentioned in the document shared with you.

	3. Signup to free Databricks community edition account
		https://databricks.com/try-databricks
		-> Spend some time exploring the Quickstart tutorial


    Spark Architecture
    ------------------
  
     1. Cluster manager (CM)
	-> Jobs are submitted to CM
	-> CM schedules the job and allocates resources to the job

     2. Driver Process
	-> Driver process is the first process that gets created.
	-> Driver process manages the user-code and lauches tasks on the cluster.
	-> Driver contains a "SparkContext" object  (or "SparkSession" in the case of Spark SQL).
	
	Deploy-modes:
	
	1. Client Mode (default) -> the driver process runs on the client machine
	2. Cluster Mode		 -> the driver runs on one the node in the cluster

     3. Executor Processes

	-> Different tasks as per the programming logic are sent to be executed on various
	   executors allocated by the Cm to the application
	-> All tasks does the same process (logic) but of different partitions of data.
	-> report the status of the tasks to the driver

      4. SparkContext
	 -> represents an application contextand connection to the cluster.	
	 -> is the first objects that gets created (starting point of Spark core application).
	 -> is the link between driver and different tasks running on the cluster.



    RDD (resilient distributed dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
		-> Each partition is a collection of objects	

	-> RDDs are immutable
		-> You can not change the content of a partition.

	-> RDD has two components
		1. RDD lineage DAG -> logical plan that describes how to compute the RDD partitions
		2. RDD DAG	   -> in-memory partitions

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands trigger execution

	-> RDDs are resilient
		-> RDDs are resilient to the missing in-memory partitions. RDDs can recreate
		   such missing partition on the fly and continue the tasks on them. 


   How to create RDDs?
   -------------------

	There are three ways:

	1. We can create RDDs from external data

		rdd1 = sc.textFile( <filePath> )

		-> default number of partitions is decided by a config property of SparkContext
		   called "sc.defaultMinPartitions" whose value is 2 if the number of cores 
		   allocated is atleast 2.

		rdd1 = sc.textFile( filePath, 4 )    # 4 partitions


	2. We can create RDD from programmatic data using parallelize

		rdd1 = sc.parallelize([2,3,2,4,5,6,7,8,9,6,6,7])

		-> default number of partitions is decided by a config property of SparkContext
		   called "sc.defaultParallelism" whose value is equal to the number of cores 
		   allocated to the application.


	3. By applying transformations on an existing RDD

		rdd2 = rdd1.map(lambda x: x.upper())


   What are RDD Lineage DAGs
   -------------------------
	-> Maintained by the driver
	-> Is a DAG of dependencies that caused the creation of the RDD all the way from the 
	   first RDD.

	rdd1 = sc.textFile( filePath )
	Lineage DAG : 	rdd1 -> sc.textFile	

	rdd2 = rdd1.map(lambda x: x.upper())
	Lineage DAG : 	rdd2 -> rdd1.map -> sc.textFile	   

	rdd3 = rdd2.filter(lambda a: len(a) > 50) 
	Lineage DAG : 	rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile	 
	
	rdd4 = rdd3.map(lambda x: len(x))
	Lineage DAG : 	rdd4 -> rdd3.map -> rdd2.filter -> rdd1.map -> sc.textFile
		
	rdd3.collect()  => sc.textFile -> map -> filter -> rdd3
	
   What can you do with an RDD ?
   -----------------------------

	Only two things:

	1. Transformations
		-> Does not cause execution
		-> They do not produce any output. They produce only other RDDs
		-> They cause the creation of lineage DAGs

	2. Actions
		-> Trigger execution
		-> Converts the lineage DAg (logical plan) into a physical execution plan

 

  Types of Transformations
  ------------------------

	Two types:

	1. Narrow transformations		
		-> Data shuffling does not happen
		-> The computation of each partition is dependent on only its input partition
		-> partition to partition tranformations
		-> map, filter, flatmap, glom, mapPartitions, ...

	2. Wide transformations
		-> Data shuffling happens
		-> The computation of each partition dependends on multiple input partitions
		-> output RDD may have different number of partitions than input RDD
		-> distinct, sortBy, groupBy, ...ByKey 


  RDD Persistence
  ---------------

      	rdd1 = sc.textFile( file )
	rdd2 = rdd1.t2(..)	
	rdd3 = rdd1.t3(..)	
	rdd4 = rdd3.t4(..)	
	rdd5 = rdd3.t5(..)	
	rdd6 = rdd5.t6(..)	
	rdd7 = rdd6.t7(..)
	rdd7.persist()     ---> instruction to spark to not GC rdd7 automatically. 	
	rdd8 = rdd7.t8(..)

	rdd7.collect()
	
	Lineage rdd7 => rdd7 -> rdd6.t7	-> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> t7 (rdd7) ---> collect()
	
        rdd8.collect()

	Lineage of rdd8 => rdd8 -> rdd7.t8 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		rdd7.t8 -> rdd8 -> collect()

	
	Persistence   -->  in-memory deserialized
			   in-memory serialized
			   on-disk
	
    Commands
    ---------
	rdd.persist()
	rdd.persist( StorageLevel.MEMORY_AND_DISK )
	rdd.cache()
	rdd.unpersist()  // delete the persisted partitions

   Storage Levels
   --------------
	MEMORY_ONLY (default)   -> stores in RAM as deserialized object
				
	MEMORY_AND_DISK		-> stores in RAM if available, or stores on disk.

	DISK_ONLY

	MEMORY_ONLY_SER		-> stores in RAM in serialized format.

	MEMORY_AND_DISK_SER

	MEMORY_ONLY_2

	MEMORY_AND_DISK_2	
      

  Executor Memory Structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Transformations
  -------------------
	-> Transformations does not cause execution.
	-> The output of every transformation is 

   1. map  		=> P: U -> V 
			   Element to element transformation
			   input RDD: N elements, output RDD: N elements 

   2. filter		=> P: U -> Boolean
			   Only those elements for which the function returns True will be
			   in the output partition.
			   input RDD: N elements, output RDD: <= N elements 

   3. glom		=> P: None
			   Transform all the elements of each partition into one array object
			   in the output RDD

	    rdd1		rdd2 = rdd1.glom()
	P0: 1,2,4,3,5,7  --> glom  --> P0: [1,2,4,3,5,7]
	P1: 1,2,8,9,4,7  --> glom  --> P1: [1,2,8,9,4,7]
	P2: 8,8,1,1,3,3  --> glom  --> P2: [8,8,1,1,3,3]		
	rdd1.count = 18			rdd2.count = 3

   4. flatMap		=> P: U -> Iterable[V]
			 Flattens all the elements of the iterables produced by the function.
			 input RDD: N elements, output RDD: >= N elements 

   5. distinct		=> P: None, optional: number of output partitions.
			Return distinct elements from the RDD (removes the dups)
			   
		
   6. mapPartitions	=> P: Iterator[U] -> Iterator[V]
			   The function is applied to the entire partition.

		rdd1		rdd2 = rdd1.mapPartitions( lambda x: ... )

	P0: 1,2,4,3,5,7  --> mapPartitions  --> P0:  2,4,8,6,10,14
	P1: 1,2,8,9,4,7  --> mapPartitions  --> P1: 
	P2: 8,8,1,1,3,3  --> mapPartitions  --> P2: 		
	rdd1.count = 18			


   7. mapPartitionsWithIndex	=> p: Int, Iterator[U] -> Iterator[V]
				The function is applied to the entire partition.

   8. sortBy

   9. groupBy
























