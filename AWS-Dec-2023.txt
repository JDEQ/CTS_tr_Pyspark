    
  Agenda
  ---------   
   1. PySpark Basics & SparkSQL Basics	8 hours
   2. AWS Basics & AWS Infra		4 hours
   3. Amazon S3				4 hours
   4. AWS Lambda			4 hours
   5. AWS Glue				8 hours
   6. AWS Athena 			2 hours
   7. Project (CDC)			4 hours


  Course Materials
  ----------------
    -> PDF Presentations
    -> Code Modules, Instructions, Scripts, Notebooks
    -> Class Notes
    -> Github: 	https://github.com/ykanakaraju/pyspark
		https://github.com/ykanakaraju/aws-cts  


  Spark
  -----

   -> Spark is an in-memory distributed computing framework for performing big data analytics.

   -> Spark is written in SCALA language

   -> Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

   -> Spark has unified stack for data analytics

   -> Spark APIs
	-> Spark Core API  : Low-level API. Uses RDDs
	-> Spark SQL	   : Structured/semi-structured data processing (Batch)
	-> Spark Streaming : DStreams API, Structured streaming
	-> Spark MLLib	   : Predictive analytics using ML
	-> Spark GraphX	   : Graph Processing

    
  Spark Architecture
  ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


  RDD (Resilient Distributed Dataset)
  -----------------------------------
	
   -> RDD is the fundamental data abstraction of Spark Core

   -> RDD represents a collection of distributed in-memory partitions
	 -> Partition is a collections of objects (of some type)

   -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
	-> Actions trigger execution.

   -> RDDs are immutable
	-> We can not change the content of an RDD once created. 
	-> We can only transform to other RDDs

   -> RDDs are resilient
	-> RDDs can recompute (missing) partitions at run time 


  Creating RDDs
  -------------

    Three ways:

	1. Creating an RDD from external file

		rddFile = sc.textFile( <filePath>, n )
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		=> default number of partitions is as per the value of "sc.defaultMinPartitions"
		=> "sc.defaultMinPartitions" = 2 if core >= 2, else 1


	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize(<some python collection>, n)
		rdd1 = sc.parallelize([3,2,1,4,2,3,5,4,6,7,8,9,0,8,9,0,8,6,4,1,2,3,2,4,6,5,7,8,9,6,7,5], 3)

		=> default number of partitions is as per the value of "sc.defaultParallelism"
		=> "sc.defaultMinPartitions" = number of cores allocated to your application


	3. Creating an RDD by applying transformations on exiting RDD

		rddWords = rddFile.flatMap(lambda x: x.split())

		-> By default the output RDD will have the same number of partitions as input partitions


  RDD Operations
  --------------
	Two operations

	1. Transformations
		-> Creates an RDD Lineage DAG
		-> Does not cause execution

	2. Actions
		-> Executes an RDD and launchs a Job in the cluster
		-> Produces some output
		-> Converts logical plan (DAG) to a physical plan for execution. 


  RDD Lineage DAG   
  ---------------
   (DAG: Directed Acyclic Graph)
   -> Represents a logical plan on how to execute the RDD.
   -> Lineage DAG stores the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rddFile => (4) rddFile -> sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG of rddWords => (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddPairs => (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc => (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile

  
  
  RDD Execution Flow
  ------------------

	Application (ex: PySpark Shell or You App in Spyder/PyCharm)
	|
	|--> Jobs (Each action command launches one job)
		|
		|--> Stages (one or more stages per Job; wide transformation causes stage transition)
			|
			|--> Tasks (one task per partition of the RDD in that stage)
				|
				|--> Transformations (one or more transformations per task)	

  

  Getting started with Spark
  --------------------------
 
    1. Creating a local development environment

	-> Install Anaconda distribution  
	-> Follow the instructions given in the document:
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    2. Signup to Databricks Community Edition (free edition)
	
	-> Signup: https://www.databricks.com/try-databricks
	   -> Click on "Get started with Community Edition" in second screen
  
	-> Login: https://community.cloud.databricks.com/login.html


 
  Spark SQL
  =========

   -> Spark's structured / semi-structured data processing API
   
	File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format  : RDBMS, NoSQL
	Hive Format  : Hive Warehouse	

   -> SparkSession

	-> Starting point of execution
	-> Represents a session inside an application
	-> Each session can have its own configuration. 

		spark = SparkSession \
    			.builder \
    			.appName("Basic Dataframe Operations") \
    			.config("spark.master", "local[*]") \
    			.getOrCreate()  

   -> DataFrames

		Data   => Rows
		Schema => StructType

			StructType(
			     [
				StructField('age', LongType(), True), 
				StructField('gender', StringType(), True), 
				StructField('name', StringType(), True), 
				StructField('phone', StringType(), True), 
				StructField('userid', LongType(), True)	
			     ]
			)




   Spark SQL Basic Steps
   ---------------------

    1. Reading/Loading data into a DF

		df1 = spark.read.format("json").load(input_path)
		df1 = spark.read.load(input_path, format="json")
		df1 = spark.read.json(input_path)


    2. Transforming a DF 

		Using DF Transformation methods
		-------------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)
            
		display(df2)


		Using SQL
		-------------------------------
		df1.createOrReplaceTempView("users")

		df3 = spark.sql("""SELECT age, count(1) as count
        			   FROM users
        			   WHERE age IS NOT NULL
        			   GROUP BY gender, age
        			   ORDER BY gender, age
        			   LIMIT 4""")
		display(df3)



    3. Writing/Saving the DF data into some destination

		df2.write.format("json").save(output_path)
		df2.write.save(output_path, format="json")
		df2.write.json(output_path)

		df2.write.mode("overwrite").json(output_path)


  Save Modes
  ----------
	- Define what should happen when you are writing to an existing directory
  		- ErrorIfExists (default)
  		- Ignore
  		- Append   (appends additional files to the existing directory)
  		- Overwrite (overwrites old directory)


	df3.write.mode("append").format("json").save("dbfs:/FileStore/output/json")
	df3.write.mode("overwrite").format("json").save("dbfs:/FileStore/output/json")

	df3.write.format("json").save("dbfs:/FileStore/output/json", mode="append")



  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")


  Working with different file formats
  -----------------------------------

  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)


   CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")

   Text
	read
		df1 = spark.read.text(inputPath)
		=> df1 will have one columns called 'value' of 'string' type

	write
		df1.write.text(outputPath)
		=> You can only save a DF with a single text column in 'text' format.



 DataFrame Transformations
 -------------------------

 1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


  2. where / filter

	df3 = df2.where("highFrequency = true and destination = 'United States'")
	df3 = df2.filter("highFrequency = true and destination = 'United States'")

	df3 = df2.where( col("count") > 100 )

	df3.show(5)


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))


  4. groupBy  => returns a "GroupedData" object
		 apply some aggregation method to return a DataFrame


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	  .agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 2).alias("avg")
		   )


  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	   df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 		"DEST_COUNTRY_NAME as destination",
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 200 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")


  7. withColumn 

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) \
        	.withColumn("country", lit("India") )

	df3 = df2.withColumn("ageGroup", when(col("age") < 13, "child")
                                 	.when(col("age") < 20, "teenager")
                                 	.when(col("age") < 60, "adult")
                                 	.otherwise("senior"))

  8. withColumnRenamed

	df3 = df1.withColumn("newCount", col("count") + 10) \
		.withColumn("highFrequency", expr("count > 200")) \
		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


  9. udf (user-defined-function)

	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"		
	   
	get_age_group_udf = udf(getAgeGroup, StringType())    
		
	df3 = df2.withColumn("ageGroup", get_age_group_udf(col("age")) )

	-------------------------------------------

	@udf(returnType = StringType())
	def get_age_group( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"
		  
	df3 = df2.withColumn("ageGroup", get_age_group(col("age")) )
	
	-------------------------------------------

	spark.udf.register("get_age_group_udf", get_age_group, StringType())

	qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"

	spark.sql(qry).show()


  10. drop	=> excludes the specified columns of the input DF in the output DF.

	df3 = df2.drop("newCount", "highFrequency")
	df3.show()


  11. dropna	=> drops the rows with NULL values in any or specified columns

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	usersDf.dropna().show()
	usersDf.dropna(subset=["name", "phone"]).show()


  12. dropDuplicates   => drops the duplicate rows

	listUsers = [(1, "Raju", 5),
				 (1, "Raju", 5),
				 (3, "Raju", 5),
				 (4, "Raghu", 35),
				 (4, "Raghu", 35),
				 (6, "Raghu", 35),
				 (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	userDf.dropDuplicates().show()
	userDf.dropDuplicates(["name", "age"]).show()


  13. distinct	 => returns distinct rows of the DF

	userDf.distinct().show()


	Q: How many UNIQUE values are there in DEST_COUNTRY_NAME column?
	-----------------------------------------------------------------
	df1.select("DEST_COUNTRY_NAME").distinct().count()
	df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()


 14. randomSplit => Spalit the DF into a list of DFs in the specified weights. 

	dfList = df1.randomSplit([0.6, 0.4])
	dfList = df1.randomSplit([0.6, 0.4], 345)   => 345 is a seed

	print( dfList[0].count(),  dfList[1].count() )


 15. union, intersect, subtract


	df2 = df1.where("count > 1000")
	df2.show()
	df2.count()   # 14 rows
	df2.rdd.getNumPartitions()

	df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
	df3.show()
	df3.count()   # 1 rows
	df3.rdd.getNumPartitions()

	df4 = df2.union(df3)
	df4.rdd.getNumPartitions()
	df4.show()

	df5 = df4.subtract(df2)   # df4: 2, df2: 1
	df5.show()
	df5.rdd.getNumPartitions()

	df6 = df4.intersect(df2)
	df6.show()
	df6.rdd.getNumPartitions()


  spark.sql.shuffile.partitions
  -----------------------------
	=> This config parameter determines the number of shuffle partitions created
	=> default value is 200

	spark.conf.get("spark.sql.shuffle.partitions")
	spark.conf.set("spark.sql.shuffle.partitions", "3")


 16. repartition

	df2 = df1.repartition(6)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(3)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()

	df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
	df5.rdd.getNumPartitions()


 17. coalesce

	df6 = df5.coalesce(3)
	df6.rdd.getNumPartitions()


 18. join 



  Joins
  -----
	Supported Joins: inner, left, right, full, left_semi, left_anti, cross


	left_semi Join
	--------------
	Is similar to inner join but the data is fetched only from the left table.

	Equivalent to the following sub-query:
	
	     select * from emp where deptid in (select id from dept)


	left_anti Join
	--------------
	Equivalent to the following sub-query:
	
	     select * from emp where deptid not in (select id from dept)


	SQL Way
	-------
	employee = spark.createDataFrame([
		(1, "Raju", 25, 101),
		(2, "Ramesh", 26, 101),
		(3, "Amrita", 30, 102),
		(4, "Madhu", 32, 102),
		(5, "Aditya", 28, 102),
		(6, "Pranav", 28, 100)])\
	  .toDF("id", "name", "age", "deptid")
	  
	department = spark.createDataFrame([
		(101, "IT", 1),
		(102, "ITES", 1),
		(103, "Opearation", 1),
		(104, "HRD", 2)])\
	  .toDF("id", "deptname", "locationid")

	employee.show()
	department.show()

	spark.catalog.listTables()

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*, dept.*
		 from emp join dept
		 on emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)

	joinedDf.show()


	DF Transformation method
        ------------------------
	joinCol = employee.deptid == department.id
	joinedDf = employee.join(department, joinCol, "full")
	joinedDf.show()


  Use-Case
  --------

   datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

   From movies.csv and ratings.csv files, fetch the top 10 movies with heighest average user rating.
	-> Consider only those movies with atleast 30 ratings
	-> Data: movieId, title, totalRatings, averageRating
	-> Arrange the data in the DESC order of averageRating
	-> Save the output as a single pipe-separated CSV file with header. 
	-> Use DF transformation methods only.

























