
  Agenda (PySpark)
  ------------------
  Spark - Basics & Architecture
  Spark Core API (Low-level API)
	-> RDD - Transformations and Actions
	-> Shared variable
  Spark SQL (DataFrames)
  Spark Streaming
  Introduction to Spark MLlib


  Materials
  ---------
    -> PDF presentations. 
    -> Code Modules
    -> Class Notes
    -> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----------

   => Spark is a unified in-memory distributed computing framework  (Cluster computing)

   => Spark is used for Big Data Analytics
   
   => Spark is written in Scala programming language.
   
   => Spark is a polyglot
	-> Scala, Java, Python, R 


   => Cluster: Is a unified entity containing many nodes whose cumulative resources can be used
	       to distribute your storage and processing. 

	       -> A cluster is managed by a Cluster Manager.

   => In-memory computing : The output results of tasks can be persisted (saved) is RAM and subsequent
		tasks can directly act up on these stored results. 

		-> Very Fast 

		-> Is upto 100x faster than MapReduce if you use 100% in-memory computatition
		   Is upto 6 to 7x faster than Mapreduce even if you use disk-based computatitions.		 

   Spark Unified Framework
   -----------------------
    -> Spark provides a consistent set of API for processing different analytics workload using the
       same execution engine. 

	Batch processing of unstructured data	=> Spark Core API (RDD) - Low-level API
	Batch processing of structured data	=> Spark SQL (DataFrames)
	Stream processing			=> Spark Streaming, Structured Streaming
	Predictive Analytics (Machine Learning)	=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX


  Spark Architecture
  ------------------

   1. Cluster Manager
	-> Spark jobs are submitted to the CM (using spark-submit command)
	-> CM allocates resources (container to launch driver and executors) to the job
	-> Supports: local, Spark Standlone, YARN, Mesos, Kubernetes

   2. Driver process
	-> Master process
	-> Manages the user code
	-> Contains a SparkContext (represents an application)
	-> Sends tasks to the executors

	Deploy Modes:
	1. Client (default) : Driver runs on the client machine
	2. Cluster : Driver runs on one of the nodes on the cluster.

  3. SparkContext
	-> Created inside a driver process
	-> represents an application
	-> link between the driver and executors

  4. Executors
	-> Run the tasks sent by the driver
	-> Report the status back to the Driver.
	
 

   Getting started with Spark
   --------------------------

   1. Working with your vLab

	-> Open your email and follow the instructions. 
	-> Click on the [[ vLab - 'Bigdata Enbl' ]] icon.
	    -> You will go to Windows Server
        -> Here, Click on 'CentOS 7' icon and enter you password (refer to README.txt for password)
	-> We will then go to CentOS lab

	Launch in PySPark Shell:
		
	-> Open a Terminal
	-> Type "pyspark"

        Launch Jupyter Notebook:
	
	-> Open a Terminal
	-> Type: "jupyter notebook --allow-root"


   2. Setting up PySpark on your personal machine

	-> Install 'Anaconda Navigator'  (url: https://www.anaconda.com/products/individual)
	-> Follow instaructions given in the shared document.
	    https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
   3. Databricks Community Edition Signup

	-> Register: https://databricks.com/try-databricks	
	-> Login: https://community.cloud.databricks.com/login.html


  
   RDD (Resilitent distributed dataset)
   ------------------------------------

   -> Is the fundamental data abstraction.

   -> RDD is a collection of distributed, in-memory partititons
	-> Each partition is a collection of objects. 

   -> RDDs are lazily evaluated

   -> RDDs are immutable

   -> RDDs are resilient to missing in-memory partitions.   


  How to create RDDs?
  -------------------
    3 ways:

	1. From some external data file

	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		=> The default number of partitions is as per sc.defaultMinPartitions (=2)

	    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic data

	    rdd1 = sc.parallelize(range(1, 100))
		=> The default number of partitions is as per sc.defaultParallelism (= number of cores allocated)

	    rdd1 = sc.parallelize(range(1, 100), 3)

       3. By applying transformations on existing RDDs

	    rdd2 = rdd1.map(lambda x: x+10)


  What can you do with an RDD ?
  -----------------------------

    Two things:

    1. Transformations	
	 -> Output: RDD
	 -> Causes only the lineage DAG of the RDD (logical plan) gets created at the driver side.

    2. Actions
	-> Triggers the execution of the RDD
	-> Converts the logical plan to physical plan causes the tasks to be launched on the cluster.
	

  RDD Lineage
  ------------
   Every RDD has a lineage DAG which is a logical plan.

   A lineage DAG contains the heirarchy of the RDD which caused the creation of that RDD all the way from
   the very first RDD.

   These Lineage DAGs are created when you perform transformations. 


     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
        Lineage DAG: (4) rddFile -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd3 = rdd2.flatMap(lambda x: x.split(" "))
        Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd4.collect()



   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD



   RDD Transformations
   -------------------

   => Every transformation returns RDD
   => Transformation only cause creation of Lineage DAGs. 
    

   1. map		P: U -> V  
			Object to Object transformation
			input RDD: N objects, outputRDD: N objects.

	rdd1.map(lambda x: x > 8).collect()


  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output RDD
   			input RDD: N objects, outputRDD: <= N objects.

	rddFile.filter(lambda x:  len(x.split(" ")) > 8).collect()


  3 . glom		P: None
			Returns a list object per partition with all the elements of the partition.
			input RDD: N objects, outputRDD: equal to number of partitions
		
		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,4,2,5 -> glom -> P0: [3,2,1,4,2,5]
		P1: 5,3,6,8,0,9 -> glom -> P1: [5,3,6,8,0,9]
		P2: 4,1,3,2,6,9 -> glom -> P2: [4,1,3,2,6,9]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

		rdd1.glom().map(len).collect()

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the elements of the iterables produced by the function.
			input RDD: N objects, outputRDD: >= N objects
	
		 rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation
			input RDD: N objects, outputRDD: M objects

	rdd1.mapPartitions(lambda p : map(lambda x: x*10, p) ).glom().collect()	
	rdd1.mapPartitions(lambda p : [sum(p)] ).glom().collect()	


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
			Partition to partition transformation
			Similar to mapPartitions, but we get the partition-index as additional function partition. 
			input RDD: N objects, outputRDD: M objects


   	rdd1.mapPartitionsWithIndex( lambda i, p : [ (i, sum(p)) ] ).collect()
	rdd1.mapPartitionsWithIndex(lambda i,p : map(lambda x: (i, x*10), p) ).collect()


   7. distinct		P: None. Optional: numPartitions
			Returns an RDD with distinct objects
			input RDD: N objects, outputRDD: <= N objects

	
   Types of RDDs
   ---------------

	=> Generic RDDs: RDD [U]
   	=> Pair RDDs: RDD[(K, V)]


   8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the 'value' part of the (K, V) pairs by applying the function.
			input RDD: N objects, outputRDD: N objects

		rdd4.mapValues(sum).collect()


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Elements of the RDD are sorted based on the function output.
			input RDD: N objects, outputRDD: N objects

	rdd3.sortBy(lambda x: x[1] % 2).collect()
	rdd3.sortBy(lambda x: x[1] % 2, False).collect()   # DESC sort
	rdd3.sortBy(lambda x: x[1] % 2, True, 5).collect()











