
  Agenda (PySpark)
  ------------------
  Spark - Basics & Architecture
  Spark Core API (Low-level API)
	-> RDD - Transformations and Actions
	-> Shared variable
  Spark SQL (DataFrames)
  Spark Streaming
  Introduction to Spark MLlib


  Materials
  ---------
    -> PDF presentations. 
    -> Code Modules
    -> Class Notes
    -> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----------

   => Spark is a unified in-memory distributed computing framework  (Cluster computing)

   => Spark is used for Big Data Analytics
   
   => Spark is written in Scala programming language.
   
   => Spark is a polyglot
	-> Scala, Java, Python, R 


   => Cluster: Is a unified entity containing many nodes whose cumulative resources can be used
	       to distribute your storage and processing. 

	       -> A cluster is managed by a Cluster Manager.

   => In-memory computing : The output results of tasks can be persisted (saved) is RAM and subsequent
		tasks can directly act up on these stored results. 

		-> Very Fast 

		-> Is upto 100x faster than MapReduce if you use 100% in-memory computatition
		   Is upto 6 to 7x faster than Mapreduce even if you use disk-based computatitions.		 

   Spark Unified Framework
   -----------------------
    -> Spark provides a consistent set of API for processing different analytics workload using the
       same execution engine. 

	Batch processing of unstructured data	=> Spark Core API (RDD) - Low-level API
	Batch processing of structured data	=> Spark SQL (DataFrames)
	Stream processing			=> Spark Streaming, Structured Streaming
	Predictive Analytics (Machine Learning)	=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX


  Spark Architecture
  ------------------

   1. Cluster Manager
	-> Spark jobs are submitted to the CM (using spark-submit command)
	-> CM allocates resources (container to launch driver and executors) to the job
	-> Supports: local, Spark Standlone, YARN, Mesos, Kubernetes

   2. Driver process
	-> Master process
	-> Manages the user code
	-> Contains a SparkContext (represents an application)
	-> Sends tasks to the executors

	Deploy Modes:
	1. Client (default) : Driver runs on the client machine
	2. Cluster : Driver runs on one of the nodes on the cluster.

  3. SparkContext
	-> Created inside a driver process
	-> represents an application
	-> link between the driver and executors

  4. Executors
	-> Run the tasks sent by the driver
	-> Report the status back to the Driver.
	
 

   Getting started with Spark
   --------------------------

   1. Working with your vLab

	-> Open your email and follow the instructions. 
	-> Click on the [[ vLab - 'Bigdata Enbl' ]] icon.
	    -> You will go to Windows Server
        -> Here, Click on 'CentOS 7' icon and enter you password (refer to README.txt for password)
	-> We will then go to CentOS lab

	Launch in PySPark Shell:
		
	-> Open a Terminal
	-> Type "pyspark"

        Launch Jupyter Notebook:
	
	-> Open a Terminal
	-> Type: "jupyter notebook --allow-root"


   2. Setting up PySpark on your personal machine

	-> Install 'Anaconda Navigator'  (url: https://www.anaconda.com/products/individual)
	-> Follow instaructions given in the shared document.
	    https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
   3. Databricks Community Edition Signup

	-> Register: https://databricks.com/try-databricks	
	-> Login: https://community.cloud.databricks.com/login.html


  
   RDD (Resilitent distributed dataset)
   ------------------------------------

   -> Is the fundamental data abstraction.

   -> RDD is a collection of distributed, in-memory partititons.
	-> Each partition is a collection of objects. 

   -> RDDs are lazily evaluated

   -> RDDs are immutable

   -> RDDs are resilient to missing in-memory partitions.   


  How to create RDDs?
  -------------------
    3 ways:

	1. From some external data file

	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		=> The default number of partitions is as per sc.defaultMinPartitions (=2)

	    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic data

	    rdd1 = sc.parallelize(range(1, 100))
		=> The default number of partitions is as per sc.defaultParallelism (= number of cores allocated)

	    rdd1 = sc.parallelize(range(1, 100), 3)

       3. By applying transformations on existing RDDs

	    rdd2 = rdd1.map(lambda x: x+10)


  What can you do with an RDD ?
  -----------------------------

    Two things:

    1. Transformations	
	 -> Output: RDD
	 -> Causes only the lineage DAG of the RDD (logical plan) gets created at the driver side.

    2. Actions
	-> Triggers the execution of the RDD
	-> Converts the logical plan to physical plan causes the tasks to be launched on the cluster.
	

  RDD Lineage
  ------------
   Every RDD has a lineage DAG which is a logical plan.

   A lineage DAG contains the heirarchy of the RDD which caused the creation of that RDD all the way from
   the very first RDD.

   These Lineage DAGs are created when you perform transformations. 


     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
        Lineage DAG: (4) rddFile -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd3 = rdd2.flatMap(lambda x: x.split(" "))
        Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd4.collect()



   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ...., 4 )
	rdd2 = rdd1.t2( ... ) 
	rdd3 = rdd1.t3( ... ) 
	rdd4 = rdd3.t4( ... ) 
	rdd5 = rdd3.t5( ... ) 
	rdd6 = rdd5.t6( ... ) 
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  -> is an instruction to spark to persist rdd6 partitions
	rdd7 = rdd6.t7( ... ) 

        rdd6.collect()
	        lineage of rdd6 =>  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect()

        rdd7.collect()
    		lineage of rdd6 =>  rdd7 -> rdd6.t7

       rdd6.unpersist()
    
   Storage Level
   --------------     
	1. MEMORY_ONLY	      : default, Memory Serialized 1x Replication
	2. MEMORY_AND_DISK    : memory preferred, disk otherwise - Disk Memory Serialized 1x Replication
	3. DISK_ONLY	      : saved on the disk, Disk Serialized 1x Replication
	4. MEMORY_ONLY_2      : Memory Serialized 2x Replication (2 copies on two different executors)
	5. MEMORY_AND_DISK_2  : Disk Memory Serialized 2x Replication
   
   Commands
   ---------
	rdd1.persist()
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )
	rdd1.cache()

	rdd1.unpersist()	


   RDD Transformations
   -------------------

   => Every transformation returns RDD
   => Transformation only cause creation of Lineage DAGs. 
    

  1. map		P: U -> V  
			Object to Object transformation
			input RDD: N objects, outputRDD: N objects.

	rdd1.map(lambda x: x > 8).collect()


  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output RDD
   			input RDD: N objects, outputRDD: <= N objects.

	rddFile.filter(lambda x:  len(x.split(" ")) > 8).collect()


  3 . glom		P: None
			Returns a list object per partition with all the elements of the partition.
			input RDD: N objects, outputRDD: equal to number of partitions
		
		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,4,2,5 -> glom -> P0: [3,2,1,4,2,5]
		P1: 5,3,6,8,0,9 -> glom -> P1: [5,3,6,8,0,9]
		P2: 4,1,3,2,6,9 -> glom -> P2: [4,1,3,2,6,9]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

		rdd1.glom().map(len).collect()

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the elements of the iterables produced by the function.
			input RDD: N objects, outputRDD: >= N objects
	
		 rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation
			input RDD: N objects, outputRDD: M objects

	rdd1.mapPartitions(lambda p : map(lambda x: x*10, p) ).glom().collect()	
	rdd1.mapPartitions(lambda p : [sum(p)] ).glom().collect()	


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
			Partition to partition transformation
			Similar to mapPartitions, but we get the partition-index as additional function partition. 
			input RDD: N objects, outputRDD: M objects


   	rdd1.mapPartitionsWithIndex( lambda i, p : [ (i, sum(p)) ] ).collect()
	rdd1.mapPartitionsWithIndex(lambda i,p : map(lambda x: (i, x*10), p) ).collect()


   7. distinct		P: None. Optional: numPartitions
			Returns an RDD with distinct objects
			input RDD: N objects, outputRDD: <= N objects

		rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(4)

	
   Types of RDDs
   ---------------

	=> Generic RDDs: RDD [U]
   	=> Pair RDDs: RDD[(K, V)]


   8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the 'value' part of the (K, V) pairs by applying the function.
			input RDD: N objects, outputRDD: N objects

		rdd4.mapValues(sum).collect()


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Elements of the RDD are sorted based on the function output.
			input RDD: N objects, outputRDD: N objects

	rdd3.sortBy(lambda x: x[1] % 2).collect()
	rdd3.sortBy(lambda x: x[1] % 2, False).collect()    # DESC sort
	rdd3.sortBy(lambda x: x[1] % 2, True, 5).collect()  # output RDD will have 5 partitions

  
  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD, where
				key:  is the unique function function
				value: ResultIterable containing objects of the RDD that returned the key

	rdd1.groupBy(lambda x: x > 5).mapValues(list).collect()
	rdd1.groupBy(lambda x: x > 5, 2).mapValues(list).collect()

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        	.flatMap(lambda x: x.split(" ")) \
        	.groupBy(lambda x: x, 1) \
        	.mapValues(len) \
        	.sortBy(lambda x: x[1], False)

  11. randomSplit	P: List of ratios (ex: [0.6, 0.4]). Optional: seed
			retuns a list of RDDs randomly split in the specified ratio.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 79798)   # seed is a random number eg: 79798

  12. repartition	P: numPartitions
			Used to increase or decrease the number of partitions
			repartition results in global shuffle.
		

  13. coalesce		P: numPartitions
			Used to decrease the number of partitions
			results in partition merging
	
    Recommendations :
	-> The size of each partition should be around 128 MB
	-> The number of partitions should be a multiple of number of cores
	-> If the number of partitions is close to but less than 2000, bump it up to 2000
	-> The number of cores in each executor should be 5 


   14. union, intersection, subtract, cartesian

	 Let us say we have rdd1 with M partitions, and rdd2 with N partitions

	 command			number of output partitions
	-----------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


   15. partitionBy 		P: numPartitions  Optional: partition-function ( U -> Int )
				Applied on on Pair RDDs
				Is used to control which elements should go to which partition based on the 
				value of the key. 

	transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


    ..ByKey Transformations
    ------------------------
     -> Wide transformations
     -> Are applied only on pair RDD


    16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the elements of the output RDD base on the key. 

		rdd2.sortByKey().collect()
		rdd2.sortByKey(False).collect()
		rdd2.sortByKey(True, 5).collect()


   17. groupByKey	P: None, optional: numPartitions
			Returns a pair RDD where elements are grouped by the key.

			Note: Avoid groupByKey if possible.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x,1)) \
        		.groupByKey() \
        		.mapValues(sum)

   18. reduceByKey	P: (U, U) -> U,  optional: numPartitions	

			Reduces all the values of each unique key by iterativly applying the function
			within each partition in the first stage and across partitions in the second stage.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x,1)) \
        		.reduceByKey(lambda x, y: x + y)

   19. aggregateByKey	Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
                 
	student_rdd.collect()

	output = student_rdd.map(lambda t : (t[0], t[2])) \
            	.aggregateByKey( (0,0),
                            lambda z,v: (z[0] + v, z[1] + 1),
                            lambda x, y: (x[0] + y[0], x[1] + y[1])) \
            	.mapValues(lambda p: p[0]/p[1])


   20. joins  => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		  RDD[(U, V)].join(RDD[(U, W)]) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup =>  Is used when you want to join RDDs and want only unique keys in the output
		   -> groupByKey -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> groupByKey: (key1, [10, 7]) (key2, [12,6]) (key3, [6])

	
		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> groupByKey: (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		=> cogroup: (key1, ([10, 7], [5, 17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile	
		
	rdd1.saveAsTextFile("E:\\PySpark\\output\\rdd1")	

    4. reduce		P: (U, U) -> U
			Reduces the entore RDD into one final value of the same type by iterativly
			applying the reduce function within each partition in the first stage and
                        across partitions in the second stage.

		rdd1			

		P0: 9, 5, 7, 3, 1, 8, 9	    -> reduce -> -24 => 2
		P1: 7, 6, 4, 1, 4, 0, 8	    -> reduce -> -16
		P2: 7, 0, 6, 8, 1, 1, 0, 1  -> reduce -> -10

		rdd1.reduce( lambda x, y : x if (x > y) else y )  

   5. aggregate  Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )


   6. take
		l1 = rdd1.take(10)  => returns a list object to the driver

   7. takeOrdered
		rdd1.takeOrdered(15)
		 rddWords.takeOrdered(50, lambda x: len(x))

   8. takeSample
		rdd1.takeSample(True, 10)    	  -> withReplacement: true
		rdd1.takeSample(True, 100, 75)	  -> You can sample more objects than the size of the RDD
					             75 is a seed
		rdd1.takeSample(False, 10, 75)	  -> withReplacement: false

   9. countByValue

   10. countByKey

   11. first
  
   12. foreach   => take a function as parameter
		    applies the function on all the objects of the RDD
		    does not return any value.

		rdd10.foreach(lambda a : print("key: " + str(a[0]) + ", value:" + str(a[1])))
   
  Use-Case
  --------
	
    Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

    From cars.tsv dataset, fetch the average weight of each make of "American" origin cars
    Arrange the data in the DESC order of the average weight
    Save the output as a single text file.

    => Try it yourself

   

    


	


	    














 

