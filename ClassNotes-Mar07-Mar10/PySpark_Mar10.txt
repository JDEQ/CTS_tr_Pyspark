
  Agenda (PySpark)
  ------------------
  Spark - Basics & Architecture
  Spark Core API (Low-level API)
	-> RDD - Transformations and Actions
	-> Shared variables
  Spark SQL (DataFrames)
  Spark Streaming
  Introduction to Spark MLlib


  Materials
  ---------
    -> PDF presentations. 
    -> Code Modules
    -> Class Notes
    -> GitHub: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----------

   => Spark is a unified in-memory distributed computing framework  (Cluster computing)

   => Spark is used for Big Data Analytics
   
   => Spark is written in Scala programming language.
   
   => Spark is a polyglot
	-> Scala, Java, Python, R 


   => Cluster: Is a unified entity containing many nodes whose cumulative resources can be used
	       to distribute your storage and processing. 

	       -> A cluster is managed by a Cluster Manager.

   => In-memory computing : The output results of tasks can be persisted (saved) is RAM and subsequent
		tasks can directly act up on these stored results. 

		-> Very Fast 

		-> Is upto 100x faster than MapReduce if you use 100% in-memory computatition
		   Is upto 6 to 7x faster than Mapreduce even if you use disk-based computatitions.		 

   Spark Unified Framework
   -----------------------
    -> Spark provides a consistent set of API for processing different analytics workload using the
       same execution engine. 

	Batch processing of unstructured data	=> Spark Core API (RDD) - Low-level API
	Batch processing of structured data	=> Spark SQL (DataFrames)
	Stream processing			=> Spark Streaming, Structured Streaming
	Predictive Analytics (Machine Learning)	=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX


  Spark Architecture
  ------------------

   1. Cluster Manager
	-> Spark jobs are submitted to the CM (using spark-submit command)
	-> CM allocates resources (container to launch driver and executors) to the job
	-> Supports: local, Spark Standlone, YARN, Mesos, Kubernetes

   2. Driver process
	-> Master process
	-> Manages the user code
	-> Contains a SparkContext (represents an application)
	-> Sends tasks to the executors

	Deploy Modes:
	1. Client (default) : Driver runs on the client machine
	2. Cluster : Driver runs on one of the nodes on the cluster.

  3. SparkContext
	-> Created inside a driver process
	-> represents an application
	-> link between the driver and executors

  4. Executors
	-> Run the tasks sent by the driver
	-> Report the status back to the Driver.
	
 

   Getting started with Spark
   --------------------------

   1. Working with your vLab

	-> Open your email and follow the instructions. 
	-> Click on the [[ vLab - 'Bigdata Enbl' ]] icon.
	    -> You will go to Windows Server
        -> Here, Click on 'CentOS 7' icon and enter you password (refer to README.txt for password)
	-> We will then go to CentOS lab

	Launch in PySPark Shell:
		
	-> Open a Terminal
	-> Type "pyspark"

        Launch Jupyter Notebook:
	
	-> Open a Terminal
	-> Type: "jupyter notebook --allow-root"


   2. Setting up PySpark on your personal machine

	-> Install 'Anaconda Navigator'  (url: https://www.anaconda.com/products/individual)
	-> Follow instaructions given in the shared document.
	    https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
   3. Databricks Community Edition Signup

	-> Register: https://databricks.com/try-databricks	
	-> Login: https://community.cloud.databricks.com/login.html


  
   RDD (Resilitent distributed dataset)
   ------------------------------------

   -> Is the fundamental data abstraction.

   -> RDD is a collection of distributed, in-memory partititons.
	-> Each partition is a collection of objects. 

   -> RDDs are lazily evaluated

   -> RDDs are immutable

   -> RDDs are resilient to missing in-memory partitions.   


  How to create RDDs?
  -------------------
    3 ways:

	1. From some external data file

	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		=> The default number of partitions is as per sc.defaultMinPartitions (=2)

	    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic data

	    rdd1 = sc.parallelize(range(1, 100))
		=> The default number of partitions is as per sc.defaultParallelism (= number of cores allocated)

	    rdd1 = sc.parallelize(range(1, 100), 3)

       3. By applying transformations on existing RDDs

	    rdd2 = rdd1.map(lambda x: x+10)


  What can you do with an RDD ?
  -----------------------------

    Two things:

    1. Transformations	
	 -> Output: RDD
	 -> Causes only the lineage DAG of the RDD (logical plan) gets created at the driver side.

    2. Actions
	-> Triggers the execution of the RDD
	-> Converts the logical plan to physical plan causes the tasks to be launched on the cluster.
	

  RDD Lineage
  ------------
   Every RDD has a lineage DAG which is a logical plan.

   A lineage DAG contains the heirarchy of the RDD which caused the creation of that RDD all the way from
   the very first RDD.

   These Lineage DAGs are created when you perform transformations. 


     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
        Lineage DAG: (4) rddFile -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd3 = rdd2.flatMap(lambda x: x.split(" "))
        Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile ("E:\\Spark\\wordcount.txt")

     rdd4.collect()



   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ...., 4 )
	rdd2 = rdd1.t2( ... ) 
	rdd3 = rdd1.t3( ... ) 
	rdd4 = rdd3.t4( ... ) 
	rdd5 = rdd3.t5( ... ) 
	rdd6 = rdd5.t6( ... ) 
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  -> is an instruction to spark to persist rdd6 partitions
	rdd7 = rdd6.t7( ... ) 

        rdd6.collect()
	        lineage of rdd6 =>  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect()

        rdd7.collect()
    		lineage of rdd6 =>  rdd7 -> rdd6.t7

       rdd6.unpersist()
    
   Storage Level
   --------------     
	1. MEMORY_ONLY	      : default, Memory Serialized 1x Replication
	2. MEMORY_AND_DISK    : memory preferred, disk otherwise - Disk Memory Serialized 1x Replication
	3. DISK_ONLY	      : saved on the disk, Disk Serialized 1x Replication
	4. MEMORY_ONLY_2      : Memory Serialized 2x Replication (2 copies on two different executors)
	5. MEMORY_AND_DISK_2  : Disk Memory Serialized 2x Replication
   
   Commands
   ---------
	rdd1.persist()
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )
	rdd1.cache()

	rdd1.unpersist()	


   RDD Transformations
   -------------------

   => Every transformation returns RDD
   => Transformation only cause creation of Lineage DAGs. 
    

  1. map		P: U -> V  
			Object to Object transformation
			input RDD: N objects, outputRDD: N objects.

	rdd1.map(lambda x: x > 8).collect()


  2. filter		P: U -> Boolean
			Only those objects for which the function returns True will be in the output RDD
   			input RDD: N objects, outputRDD: <= N objects.

	rddFile.filter(lambda x:  len(x.split(" ")) > 8).collect()


  3 . glom		P: None
			Returns a list object per partition with all the elements of the partition.
			input RDD: N objects, outputRDD: equal to number of partitions
		
		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,4,2,5 -> glom -> P0: [3,2,1,4,2,5]
		P1: 5,3,6,8,0,9 -> glom -> P1: [5,3,6,8,0,9]
		P2: 4,1,3,2,6,9 -> glom -> P2: [4,1,3,2,6,9]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

		rdd1.glom().map(len).collect()

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the elements of the iterables produced by the function.
			input RDD: N objects, outputRDD: >= N objects
	
		 rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation
			input RDD: N objects, outputRDD: M objects

	rdd1.mapPartitions(lambda p : map(lambda x: x*10, p) ).glom().collect()	
	rdd1.mapPartitions(lambda p : [sum(p)] ).glom().collect()	


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
			Partition to partition transformation
			Similar to mapPartitions, but we get the partition-index as additional function partition. 
			input RDD: N objects, outputRDD: M objects


   	rdd1.mapPartitionsWithIndex( lambda i, p : [ (i, sum(p)) ] ).collect()
	rdd1.mapPartitionsWithIndex(lambda i,p : map(lambda x: (i, x*10), p) ).collect()


   7. distinct		P: None. Optional: numPartitions
			Returns an RDD with distinct objects
			input RDD: N objects, outputRDD: <= N objects

		rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(4)

	
   Types of RDDs
   ---------------

	=> Generic RDDs: RDD [U]
   	=> Pair RDDs: RDD[(K, V)]


   8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the 'value' part of the (K, V) pairs by applying the function.
			input RDD: N objects, outputRDD: N objects

		rdd4.mapValues(sum).collect()


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Elements of the RDD are sorted based on the function output.
			input RDD: N objects, outputRDD: N objects

	rdd3.sortBy(lambda x: x[1] % 2).collect()
	rdd3.sortBy(lambda x: x[1] % 2, False).collect()    # DESC sort
	rdd3.sortBy(lambda x: x[1] % 2, True, 5).collect()  # output RDD will have 5 partitions

  
  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD, where
				key:  is the unique function function
				value: ResultIterable containing objects of the RDD that returned the key

	rdd1.groupBy(lambda x: x > 5).mapValues(list).collect()
	rdd1.groupBy(lambda x: x > 5, 2).mapValues(list).collect()

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        	.flatMap(lambda x: x.split(" ")) \
        	.groupBy(lambda x: x, 1) \
        	.mapValues(len) \
        	.sortBy(lambda x: x[1], False)

  11. randomSplit	P: List of ratios (ex: [0.6, 0.4]). Optional: seed
			retuns a list of RDDs randomly split in the specified ratio.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 79798)   # seed is a random number eg: 79798

  12. repartition	P: numPartitions
			Used to increase or decrease the number of partitions
			repartition results in global shuffle.
		

  13. coalesce		P: numPartitions
			Used to decrease the number of partitions
			results in partition merging
	
    Recommendations :
	-> The size of each partition should be around 128 MB
	-> The number of partitions should be a multiple of number of cores
	-> If the number of partitions is close to but less than 2000, bump it up to 2000
	-> The number of cores in each executor should be 5 


   14. union, intersection, subtract, cartesian

	 Let us say we have rdd1 with M partitions, and rdd2 with N partitions

	 command			number of output partitions
	-----------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


   15. partitionBy 		P: numPartitions  Optional: partition-function ( U -> Int )
				Applied on on Pair RDDs
				Is used to control which elements should go to which partition based on the 
				value of the key. 

	transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


    ..ByKey Transformations
    ------------------------
     -> Wide transformations
     -> Are applied only on pair RDD


    16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the elements of the output RDD base on the key. 

		rdd2.sortByKey().collect()
		rdd2.sortByKey(False).collect()
		rdd2.sortByKey(True, 5).collect()


   17. groupByKey	P: None, optional: numPartitions
			Returns a pair RDD where elements are grouped by the key.

			Note: Avoid groupByKey if possible.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x,1)) \
        		.groupByKey() \
        		.mapValues(sum)

   18. reduceByKey	P: (U, U) -> U,  optional: numPartitions	

			Reduces all the values of each unique key by iterativly applying the function
			within each partition in the first stage and across partitions in the second stage.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x,1)) \
        		.reduceByKey(lambda x, y: x + y)

   19. aggregateByKey	Is used to aggregate all the values of each unique key to a type
			different that the values of (k,v) pairs.		
			Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
                 
	student_rdd.collect()

	output = student_rdd.map(lambda t : (t[0], t[2])) \
            	.aggregateByKey( (0,0),
                            lambda z,v: (z[0] + v, z[1] + 1),
                            lambda x, y: (x[0] + y[0], x[1] + y[1])) \
            	.mapValues(lambda p: p[0]/p[1])


   20. joins  => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		  RDD[(U, V)].join(RDD[(U, W)]) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup =>  Is used when you want to join RDDs and want only unique keys in the output
		   -> groupByKey -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> groupByKey: (key1, [10, 7]) (key2, [12,6]) (key3, [6])

	
		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> groupByKey: (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		=> cogroup: (key1, ([10, 7], [5, 17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile	
		
	rdd1.saveAsTextFile("E:\\PySpark\\output\\rdd1")	

    4. reduce		P: (U, U) -> U
			Reduces the entore RDD into one final value of the same type by iterativly
			applying the reduce function within each partition in the first stage and
                        across partitions in the second stage.

		rdd1			

		P0: 9, 5, 7, 3, 1, 8, 9	    -> reduce -> -24 => 2
		P1: 7, 6, 4, 1, 4, 0, 8	    -> reduce -> -16
		P2: 7, 0, 6, 8, 1, 1, 0, 1  -> reduce -> -10

		rdd1.reduce( lambda x, y : x if (x > y) else y )  

   5. aggregate  Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

                3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.

		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )


   6. take
		l1 = rdd1.take(10)  => returns a list object to the driver

   7. takeOrdered
		rdd1.takeOrdered(15)
		 rddWords.takeOrdered(50, lambda x: len(x))

   8. takeSample
		rdd1.takeSample(True, 10)    	  -> withReplacement: true
		rdd1.takeSample(True, 100, 75)	  -> You can sample more objects than the size of the RDD
					             75 is a seed
		rdd1.takeSample(False, 10, 75)	  -> withReplacement: false

   9. countByValue

   10. countByKey

   11. first
  
   12. foreach   => takes a function as parameter
		    applies the function on all the objects of the RDD
		    does not return any value.

		rdd10.foreach(lambda a : print("key: " + str(a[0]) + ", value:" + str(a[1])))
   
  Use-Case
  --------
	
    Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

    From cars.tsv dataset, fetch the average weight of each make of "American" origin cars
    Arrange the data in the DESC order of the average weight
    Save the output as a single text file.

    => Try it yourself


  Closure 
  --------
	=> A closure is all the code (variables and methods) that must be visible inside an executor for
	   a task to perform its computatitions on the RDD. 

	=> The closure is serialized and a copy of the code is sent to every executor node.
	
	c = 0

	def isPrime( n ) :
		return True if n is prime
		return Flase if n is not prime

	def f1( n ):
		global c
		if (isPrime( n ))  c += 1
		return n*2
 
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
   
	l1 = rdd2.collect()

	print(c)      // 0

	
	Limitation: You can not use local variables which are part of the closure to implement
	global counters.


  Shared Variables
  ----------------

    1. Accumulator Variable

	-> Is a shared variable that can be added to by all the tasks.
	-> Is a maintained by driver and there is only one copy.
	-> Accumulator is not part of closure.
	-> Accumulators are used to implement global counter.

	c = sc.accumulator(0)

	def isPrime( n ) :
		return True if n is prime
		return Flase if n is not prime

	def f1( n ):
		global c
		if (isPrime( n ))  c.add(1)
		return n*2
 
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
   
	l1 = rdd2.collect()

	print(c)      // 90


  2. Broadcast Variable
	
	-> A large immutable collection can be used as broadcast variable
	-> Driver send only one copy per executor and all the tasks in that executor can shared this variable
	-> Can save a lot of execution memory.

	d = {1:a, 2:b, 3:c, 4:d, 5:e, 6:f, .......}   # 100 MB
	bc = sc.broadcast( d )

	def f1( n ):
		global bc
		return bc.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,..], 4)
	rdd2 = rdd1.map( f1 )    

	rdd2.collect()    

  ===============================================================
      spark-submit command
  ===============================================================

     Is a single command that is used to submit any spark application (scala, python, java, R)
     to any cluster manager (local, spark-standalone, yarn, mesos, kubernetes)

     => spark-submit [options] <app jar | python file | R file> [app arguments]

     => spark-submit --master yarn 
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 3
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		E:\\PySpark\\wordcount.py [app arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wordcount_output 1

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py
		


  ===============================================================
      Spark SQL  (pyspark.sql)
  =============================================================== 
		
     -> Spark's structured data processing API
   
     -> Supported Data Sources  
		
	-> Structured data formats => Parquet (default), ORC, JSON, CSV (delimited text file)
	-> JDBC Data sources => RDBMS DBs, NoSQL Databases
	-> Hive (data warehousing platform on Hadoop)

     SparkSession
     ------------
	-> Starting point of execution

	-> SparkSession represents a 'user sesion' with its own configuration inside
	   an application. 

	-> Introduced from Spark 2.0 onwards.

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate() 

   DataFrame
   ---------
	-> DataFrame is a collection of distributed in-memory partitions that are immutable and 
	   lazily evaluated. 

	-> DataFrame is collection of "Row" object
             
	-> DataFrames has two components:
		-> Data   : Collection of Rows
		-> Schema : StructType object

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)

   
    Steps in a Spark SQL program
    -----------------------------

    1. Read/load the data from some data-source into a DataFrame

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()
	

    2. Transform the DataFrame using DF API methods or using SQL

	 DF API Method:
	 -------------
		df2 = df1.select("userid", "name", "age", "gender") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)

	SQL method
        ----------

		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count 
         		from users
         		where age is not null
         		group by age
         		order by count
         		limit 4"""
         
		df3 = spark.sql(qry)
		df3.show()


    3. Write/save to an structured file format or to a DB or Hive etc.
	
   		df3.write.json("E:\\PySpark\\output\\json")
		df3.write.format("json").save("E:\\PySpark\\output\\json")	
		df3.write.save("E:\\PySpark\\output\\json", format="json")

		df3.write.mode("overwrite").json("E:\\PySpark\\output\\json")


  Save Modes
  ----------

	default: error if the directory already exists.

	-> ignore
	-> append
	-> overwrite

	df3.write.json("E:\\PySpark\\output\\json", mode="overwrite")
	df3.write.mode("overwrite").json("E:\\PySpark\\output\\json")
 

   LocalTempView & GlobalTempView
   ------------------------------
    
	LocalTempView 
		-> created at Session scope
		-> command:  df1.createOrReplaceTempView("users")   
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> command:  df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


  DF Transformations
  ------------------
  
   => Every transformation returns a DataFrame.

   1. select
	
	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(
                 col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 column("DEST_COUNTRY_NAME").alias("destination"), 
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"),
                 lit("India").alias("country")
              )

   2. where / filter

		df3 = df2.where("domestic = false and count > 300")
		df3 = df2.filter("domestic = false and count > 300")

		df3 = df2.where( col("count") > 300 )

   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.orderBy(desc("count"), asc("origin"))

   4. groupBy => returns a GroupedData object

	  df3 = df2.groupBy("domestic", "highFrequency").count()
	  df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	  df3 = df2.groupBy("domestic", "highFrequency").max("count")
	  df3 = df2.groupBy("domestic", "highFrequency").avg("count")

	  df3 = df2.groupBy("domestic", "highFrequency") \
        	.agg( 	count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		avg("count").alias("avg")
		    )

   5. limit
		df2 = df1.limit(10)

   6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin", 
                  "DEST_COUNTRY_NAME as destination", 
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

   7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int"))

	users2 = userDf.withColumn("ageGroup", 
                           when(userDf["age"] <= 12, "child")
                           .when(userDf["age"] < 20, "teenager")
                           .when(userDf["age"] < 60, "adult")
                           .otherwise("senior"))

   8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


   9. dropDuplicates

	listUsers = [(1, "Raju", 5),
             (1, "Raju", 5),
             (3, "Raju", 5),
             (4, "Raghu", 35),
             (4, "Raghu", 35),
             (6, "Raghu", 35),
             (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	userDf2 = userDf.dropDuplicates()

	output:
	+---+-----+---+
	| id| name|age|
	+---+-----+---+
	|  3| Raju|  5|
	|  6|Raghu| 35|
	|  1| Raju|  5|
	|  7| Ravi| 70|
	|  4|Raghu| 35|
	+---+-----+---+

	userDf2 = userDf.dropDuplicates(["name", "age"])

	output
	+---+-----+---+
	| id| name|age|
	+---+-----+---+
	|  1| Raju|  5|
	|  4|Raghu| 35|
	|  7| Ravi| 70|
	+---+-----+---+

   10. dropna

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	usersDf.dropna().show()  			 # drops any row that has a null in any columns
	usersDf.dropna(subset=['phone', 'age']).show()   # drops any row that has a null in the specified columns


   11. udf   (user defined functions)

	 def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"
    
	getAgeGroupUDF = udf(getAgeGroup, StringType()) 

	usersDf2 = usersDf.withColumn("ageGroup", getAgeGroupUDF(col("age")))
	usersDf2.show()

       ==============================================
	 creating udf with annotation
       ==============================================

	@udf(returnType=StringType()) 
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"


	usersDf2 = usersDf.withColumn("ageGroup", getAgeGroup(col("age")))

	=====================================================
	registering udf as a temp function as use it with sql
	=====================================================
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"


	spark.udf.register("getAgeGroupUDF", getAgeGroup, StringType())

	usersDf.createOrReplaceTempView("users")
	qry = "select id, name, age, getAgeGroupUDF(age) as ageGroup from users"
	spark.sql(qry).show(truncate=False)


    12. drop

		df4 = df3.drop("newCount", "highFrequency")
		df4.printSchema()
   
    13. union, subtract, intersect

		df6 = df4.union(df5) 
		df6 = df4.intersect(df5) 
		df7 = df6.subtract(df5) 

    14. sample
		df4 = df1.sample(True, 0.5)
		df4 = df1.sample(True, 1.5, 343)

		df4 = df1.sample(False, 0.75, 345)
		df4 = df1.sample(False, 1.5)  # error. fraction should be in [0,1] range

    15. distinct

		df1.select("ORIGIN_COUNTRY_NAME").count()
		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

    16. randomSplit

		dfList = df1.randomSplit([0.6, 0.4], 4565)

		dfList[0].count()
		dfList[1].count()

    17. repartition

		df4 = df1.repartition(4)
		df5 = df4.repartition(2)
		df6 = df4.repartition(3, col("ORIGIN_COUNTRY_NAME"))
		df6 = df4.repartition(col("ORIGIN_COUNTRY_NAME"))
			=> here the number of partitions is governed by a confuration called "spark.sql.shuffle.partitions"
			   whose default value is 200.

			   spark.conf.set("spark.sql.shuffle.partitions", "10")

     18. coalesce

		df7 = df6.coalesce(2)
		df7.rdd.getNumPartitions()

     19. joins  => discussed as a separate topic.  
		
		Supported Joins: inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

		joinCol = employee["deptid"] == department["id"]
		joinedDf = employee.join(department, joinCol, "left_outer")


   show() command
   ---------------

	df1.show(n = 20, truncate = True, verticle = False)

	df1.show()
	df1(40, False)
	df1(4, True, True)


   Working with different file formats
   -----------------------------------

     json
     -----
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.json(outputPath)
		df3.write.format("json").save(outputPath)	
		df3.write.save(outputPath, format="json")

		df3.write.mode("overwrite").json(outputPath)

     parquet (default)
     -----------------
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.parquet(outputPath)
		df3.write.format("parquet").save(outputPath)	
		df3.write.save(outputPath, format="parquet")

		df3.write.mode("overwrite").parquet(outputPath)

     orc
     ----
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.orc(outputPath)
		df3.write.format("orc").save(outputPath)	
		df3.write.save(outputPath, format="orc")

		df3.write.mode("overwrite").orc(outputPath)

     csv (delimited text file)
     -------------------------
	read
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)

		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.csv(outputPath, header=True)
		df3.write.csv(outputPath, header=True, sep="|")

		df3.write.format("csv").save(outputPath, header=True)	
		df3.write.save(outputPath, format="csv", header=True)

		df3.write.mode("overwrite").csv(outputPath)


   Creating an RDD from DataFrame
   ------------------------------

	rdd1 = df1.rdd     => return RDD of Row objects
	rdd1.take(5)
	rdd2 = rdd1.map(lambda r : (r["DEST_COUNTRY_NAME"], r["ORIGIN_COUNTRY_NAME"], r["count"]))


   Creating a DataFrame from programmatic data
   --------------------------------------------

	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Rajesh", 25),
             (4, "Ramya", 35),
             (5, "Ravi", 45)]


	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

	df2.show()
	df2.printSchema()


   Creating a DafaFrame from RDD
   ------------------------------
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Rajesh", 25),
             (4, "Ramya", 35),
             (5, "Ravi", 45)]

	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df2 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df2 = spark.createDataFrame(rdd1).toDF("id", "name", "age")


   Creating a DataFrame with custom schema
   --------------------------------------- 
  	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df2 = spark.createDataFrame(listUsers, schema=mySchema)
	
       ------------------------------------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"
	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)


   Joins
   ------
    Supported Joins: inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

    left-semi join
    ---------------
	-> Similar to inner join but data comes only from left-side table.
	-> Equivalent to sub-query:
		select * from emp where deptid in (select id from dept)

    left-anti join
    ---------------
	-> Equivalent to sub-query:
		select * from emp where deptid not in (select id from dept)

    	SQL Way
	--------
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*
         	from emp left anti join dept
         	on emp.deptid = dept.id"""
         
	joinedDf = spark.sql(qry)

	joinedDf.show()


	DF API Way
	---------
	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")
	joinedDf.show()
   

   Use-Case
   --------
   
    datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens
 
     From movies.csv and ratings.csv datasets find the top 10 movies with highest average user-rating.
    
    -> Consider only those moves with atleast 30 user ratings
    -> Data: movieId, title, totalRatings, averageRating
    -> Sort the data in the DESC order of averageRating
    -> Save the output as a single pipe-separated CSV file with header. 

    => Try to do it yourself.


   JDBC Format - Working with MySQL
   --------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


     Hive Format - Working with Hive
     --------------------------------
	Hive is a data warehousing platform built on top of Hadoop	
        Warehouse: Is a directory where hive stores all its data file.


# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


=======================================================
    Spark Streaming
=======================================================

   Two APIs
   --------

	Spark Streaming	 
		-> Built on top of Spark low-level API (RDDs)
		-> DStream API
		-> micro-batch based processing
		-> Provides 'seconds' scale latency (1 sec, 2 sec)
		-> near real time processing
		-> Streaming sources nativly supported: Sockets, File streams
		-> Event-time processing is not supported

	Structured Streaming  (IMP)
		-> Built on top of Spark SQL
		-> DataFrames based API
		-> micro-batch as well as continuous processing
		-> Provides 'milli-seconds' scale latency (10 ms, 1 ms)
		-> real time processing
		-> Streaming sources nativly supported: Sockets, File streams, Kafka
		-> Event-time processing is supported.

 
   Spark Streaming
   ---------------- 

       StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	-> Is a continuous flow of RDDs.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 
   

   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch
  
   

   
  




 



















   














     