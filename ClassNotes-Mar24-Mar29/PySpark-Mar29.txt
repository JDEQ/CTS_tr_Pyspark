
 Agenda (PySpark)
 ----------------
  Spark - Basics & Architecture
  Spark Core API (Low Level API)
	-> RDD - Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> Working with DataFrames
  Spark Streaming
	-> Spark Streaming (DStreams API)
	-> Structured Streaming (Based on DataFrames)
  Introduction to Spark MLlib


  Materials
  ---------
     -> PDF presentations
     -> Code Modules
     -> Class Notes
     => Github: 
	
  ===================================================

   Spark
   -----

     Apache Spark is an open-source distributed computing engine/framework to process data using in-memory
     computation on a cluster using simple programming constructs.  

     -> Spark is written in 'Scala' programming language

     Cluster : Is a unified entity consisting of many nodes whose cumulative resources can be used to  
               distribute your processing or storage.

     Distributed Computing : distributing the computations on many containers (executors) and perform the 
	       computations parallelly across many machines. 

     In-Memory : Abilty to save the intermediate results of task in the RAM and performing subsequent tasks on
		these persisted results. 

     => Spark is a unified computing engine. 

        Spark Unified Stack : 
        --------------------

	   -> Spark provides a consistent set of APIs for processing different analytics workloads using 
	      the same execution engine. 	

		-> Batch Analytics of unstructured data	   : Spark Core API (RDD API)
		-> Batch Analytics of structured data	   : Spark SQL
		-> Streaming Analytics (real-time)	   : Spark Streaming, Structured Streaming
		-> Predictive Analytics (machine learning) : Spark MLlib
		-> Graph Parallel Computations             : Spark GraphX

     => Spark is a polyglot
	-> Spark programs can be written in Scala, Java, Python and R.


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Spark application are submitted to a CM.
		-> Responsible for allocating resources (executors) for the spark application. 
		-> Supports multiple CMs : local, Spark Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process that manages the entire execution
		-> Manages the user-code and maintains the metadata of the application
		-> Can be launched in two deploye modes:
			client (default) : Driver runs on the client machine
			cluster : Driver runs on one of the nodes on the cluster
		-> Analyzes the user code and send the tasks to run on the executors		

	3. Executors
		-> Will receive tasks from the driver
		-> Tasks are executed and status is reported back to driver.

	4. SparkContext
		-> Runs the driver process
		-> Is the starting point of execution. 
		-> Is an application context
		-> Is a link between the driver and several tasks that are running on the cluster


    Getting started with Spark
    --------------------------     
     1. Working in your vLab:
	=> You connect to a Windows server.
	=> Double-Click on the "CentOS 7" icon on the desktop (of the window server)
		-> refer to "README.txt" file username & password
        => This will connect to CentOS lab

	1.1 Connect to PySpark shell:	
        	-> Open a Terminal 
		-> type : "pyspark"   
		-> this launches pyspark shell.

		-> Open a FireFox browser
		-> type "localhost:4040" in the address bar
		-> this launches the WebUI of PySpark Shell

	1.2 Connect to Jupyter Notebooks
		-> Open a terminal
		-> type:   "jupyter notebook --allow-root"
		-> This launches JN envirionment.

    2. Setting up your own PySpark development environment on your personal machine.

	-> Make sure that you have Anaconda Navigator installed
		https://www.anaconda.com/products/individual

	-> Setup "PySpark" with Jupyter Notebooks / Spyder
		Follow the steps mentioned in the document
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (free account) 

	   URL: https://databricks.com/try-databricks
	   Login URL: https://community.cloud.databricks.com/login.html


  RDD (Resilient distributed dataset)
  ------------------------------------	
  
    -> Fundamental in-memory data abstraction in Spark
	
    -> Is a collection of distributed in-memory partitions
	-> each partition is a collection of objects.

    -> RDDs follow 'Lazy Evaluation' 

    -> RDDs are immutable
	-> partition data con not be changed.


   How to create RDDs?
   -------------------

	Three ways:

	1. Create an RDD from some external file

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data
	
		rdd1 = sc.parallelize( [4,5,1,3,2,0,3,4,5,6,7,8,0,8,7,6,3,1,4,2,6,4,5,7,8,9,0], 2)

	3. By applyign transformations on existing RDD 

		rdd2 = rdd1.map( lambda x: x*2 )               


   What can we do with RDDs?
   ------------------------
	Two things:

	1. Transformation
		-> Returns an RDD
		-> Create the Lineage DAG of the RDD (at the driver)
		-> Does not cause execution.

	2. Actions
		-> Causes execution
		-> Converts the logical plan to pysical execution plan and launches a job on the cluster.

   RDD Lineage
   -----------
    -> RDD lineage is a 'logical plan' on how to create the RDD
    -> RDD lineage tracks all the dependent RDDs (hierarchy) all the way from the very firts RDD.
	
    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)     
       rddFile lineage => (4) rddFile -> sc.textFile
	
    rdd2 = rddFile.flatMap(lambda x: x.split(" "))
	rdd2 lineage => rdd2 -> rddFile.flatMap -> sc.textFile
  
    rdd3 = rdd2.map(lambda x: x.upper())
	rdd3 lineage => rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4 = rdd3.filter(lambda x: len(x) > 3)
	rdd4 lineage => rdd4 -> rdd3.filter -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4.collect()


  Types of Transformations
  ------------------------
	
     	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

  Executor's memory structure
  ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

  RDD Persistence
  ---------------
	
	rdd1 = sc.textFile( ..., 4 )
	rdd2 = rdd1.t2(....)	
	rdd3 = rdd1.t3(....)	
	rdd4 = rdd3.t4(....)	
	rdd5 = rdd3.t5(....)	
	rdd6 = rdd5.t6(....)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   ---> instrauction to spark to persist the rdd partitions.	
	rdd7 = rdd6.t7(....)

	rdd6.collect()	
		rdd6 lineage => (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile, t3, t5, t6 => rdd6

        rdd7.collect()
		rdd7 lineage => (4) rdd7 -> rdd6.t7
		t7 => rdd7

	rdd6.unpersist()
	
	Storage Levels
        ---------------
	1. MEMORY_ONLY	    	-> default, Memory Serialized 1x Replication
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replication
	3. DISK_ONLY		-> Disk Serialized 1x Replication
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replication
	5. MEMORY_AND_DISK_2    -> Disk Memory Serialized 2x Replication

	Commands
        ---------
		rdd1.persist()  -> memory-only persistence
		rdd1.persist( StorageLevel.MEMORY_AND_DISK ) 
		rdd1.cache()    -> memory-only persistence

		rdd1.unpersist()
		
  
  RDD Transformations
  --------------------
   
   => All transformations return an RDD
   => Only created the RDD lineage DAG
   => Does not cause execution. 

  
  1. map		P: U -> V
			object to object transformations
			Transforms the objects of the input RDD to an object in the output RDD by applying the function.
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x)).collect()


  2. filter		P: U -> Boolean
			Only the objects for which the function return True will be in the output RDDs
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: x[-1] == "d").collect()

  3. glom		P: None
			Returns one list object per partition with all the elements of the partition
	
		rdd1			rdd2 = rdd1.glom()
		P0: 3,1,2,4,2,5 -> glom -> P0: [3,1,2,4,2,5]
		P1: 7,8,9,0,3,6 -> glom -> P1: [7,8,9,0,3,6]
		P2: 4,6,5,8,9,0 -> glom -> P2: [4,6,5,8,9,0]
		
		rdd1.count(): 18 (int)	rdd2.count(): 3 (list)

  4. flatMap		P: U -> Iterable[V] 
			flatMap flattens the iterables to unwarp the object from the iterables.
			input RDD: N objects, output RDD: >= N objects

	 rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation			
		
		rdd1	  rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )

		P0: 3,1,2,4,2,5 -> mapPartitions -> P0: 17
		P1: 7,8,9,0,3,6 -> mapPartitions -> P1: 33
		P2: 4,6,5,8,9,0 -> mapPartitions -> P2: 32

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()
		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get partition-index as additional parameter.  

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x*10), p)).collect()


  7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects
				input RDD: N objects, output RDD: <= N objects

                  rdd1.distinct().collect()
		  rdd1.distinct(4).collect()

  Types of RDDs
  -------------
	=> Generic RDDs : RDD[U]
	=> Pair RDDs : RDD[(U, V)]


  8. mapValues			P: U -> V
				Applied only on Pair RDDs
				Transforms the 'value' part of the (K,V) pairs by applying the function.
				input RDD: N objects, output RDD:  N objects
  		

		rdd2.mapValues(lambda x : x*10).collect()
		rdd3.mapValues(list).collect()

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				The object of the RDD are sorted based on the function output.

		rddWords.sortBy(lambda x: x[-1]).collect()
		rdd2.sortBy(lambda x: x[1], False).collect()
		rdd2.sortBy(lambda x: x[1], False, 5).collect()

   10. groupBy			P: U -> V, Optional: numPartitions
				The elements of the RDD are grouped based on the function output
				Returns a pairs RDD where:
				    key: unique value of the function output
				    value: ResultIterable object containing all the object of the RDD that returned the key.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: List of ratios (e.g: [0.6, 0.4]). Optional: seed
				Split the RDD into multiple RDDs in the specified ratios. 

		rddList = rdd1.randomSplit( [0.4, 0.3, 0.3] )	
		rddList = rdd1.randomSplit( [0.5, 0.5], 46532 )

   12. repartition 		P: numPartition
				Is used to increase or decrease the number of partitions
				Causes global shuffle.

		rdd2 = rdd1.repartition(10)
		

   13. coalesce 		P: numPartitions
				Is used only to decrease the number of partitions
				Cause partition merging.

		rdd2 = rdd1.coalesce(10)

	Recommendations for good performence
        -------------------------------------
	-> The size of each partition should be around 128 MB
	-> The number of partitions should be a multiple of number of CPU core allocated.
	-> If the number of partitions is less than but close to 2000, bump it up to 2000 or more. 
	-> The number of cores in each executor should be 5


   14. partitionBy 		P: numPartitions  Optional: partition-function ( U -> Int )
				Applied on on Pair RDDs
				Is used to control which elements should go to which partition based on the 
				value of the key. 

	transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


   15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command			number of partitions
	---------------------------------------------
	rdd1.union(rdd2)	   M + N partitions, narrow
	rdd1.intersection(rdd2)	   M + N partitions, wide
	rdd1.subtract(rdd2)	   M + N partitions, wide
	rdd1.cartesian(rdd2)	   M * N partitions, wide


   ..ByKey transformations
   -----------------------
     -> Are wide transformations
     -> Applies only to Pair RDDs


   16. sortByKey		P: None, Optional: ascending (True/False), numPartition
				Sorts the elements of the RDD based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 4).collect()

    17. groupByKey		P: None, Optional: numPartition
				Returns a Pair RDD where
				key: Unique-key of the input RDD
				value: grouped values with the same key (ResultIterable)

				NOTE: Avoid groupByKey if possible. 

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		 .flatMap(lambda x: x.split(" ")) \
        		 .map(lambda x: (x, 1)) \
        		 .groupByKey() \
        		 .mapValues(sum) \
        		 .sortBy(lambda x: x[1], False) \
        		 .coalesce(1)

    18. reduceByKey		P: U, U -> U, Optional: numPartition
				Reduce all the values of each unique key of the RDD by iterativly applying
				the function withih each partition and then across partitions.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y)

    11. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
                 
	student_rdd.collect()

	output = student_rdd.map(lambda t : (t[0], t[2])) \
            	.aggregateByKey( (0,0),
                            lambda z,v: (z[0] + v, z[1] + 1),
                            lambda x, y: (x[0] + y[0], x[1] + y[1])) \
            	.mapValues(lambda p: p[0]/p[1])


    12. joins    =>  join (inner-join), leftOuterJoin, rightOuterJoin, fullOuterJoin

		     RDD[(K, V)].join( RDD[(K, W)]) => RDD[(K, (V,W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)


    13. cogroup		-> Is used when you join RDDs with duplicate keys and you need unique-keys in the 
			   output. 
				=> groupByKey (on each RDD) -> fullOuterJoin	

	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
    	=> (key1, [5,17]) (key2, [4,7]) (key4, [17])

	cogrouped => (key1, ([10, 7], [5,17])) (key2, ([12, 6],[4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------

  1. collect	-> returns a list to the driver with all the objects of the RDD

  2. count

  3. saveAsTextFile 

  4. reduce			P: (U, U) -> U
				reduces an entire RDD to one value of the same type by iterativly applying the
				reduce function.
		rdd1
		P0: 9, 5, 6, 4, 2, 4 -> reduce -> -12 -> 41
		P1: 7, 8, 9, 0, 8, 9 -> reduce -> -27
		P2: 0, 5, 6, 7, 8, 0 -> reduce -> -26

		rdd1.reduce(lambda x, y: x if (x > y) else y )

   5. aggregate	
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

   6. take	
		rdd1.take(10)	

   7. takeOrdered
		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: x%2)

   8. takeSample
		rdd1.takeSample(True, 10)		# True: with-replacement-sampling
		rdd1.takeSample(True, 100)
		rdd1.takeSample(True, 100, 4565)

		rdd1.takeSample(False, 100, 4565)	# False: with-out-replacement-sampling

   9. countByValue
	-> returns a dict object with object counts

   10. countByKey

   11. foreach 		Takes some function as a parameter and runs that function on all the values
                        of the RDD, but does not return any value. 
    
   12. first

  
   Use-Case
   --------

   dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
   		
   From cars.tsv dataset, find the average weight of each make of 'American' orogin cars. 
   Arrange the data in the DESC order of average weight
   Save the output as a single text file. 
		
       => Try to do it yourself


  Closure
  -------
    -> A closure is a the code (all the variables and methods) that must be visible inside an 
       execution for a task to perform its computations on the RDD. 

    -> The closure is serialized and copy is sent to every executor.

	c = 0

	def isPrime(n):
		# returns True if n is prime
		# returns True if n is not prime

	def f1(n) :
		global c
		if (isPrime(n)) c += 1
		return n*2	

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)      // c = 0

        Limitation: 
	We can not use 'local variables' (which is part of closure) to implement global counters.

	Use 'accumulators' to implement global counter.


  Shared Variables
  -----------------

    1. Accumulator Variable

		-> Is a shared variable that all the tasks can add to
		-> One copy maintained by the driver
		-> Not part of closure
		-> Used to implement global counters.

	c = sc.accumulator(0)

	def isPrime(n):
		# returns True if n is prime
		# returns True if n is not prime

	def f1(n) :
		global c
		if (isPrime(n)) c.add(1)
		return n*2	

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)      // c = 80


   2. Broadcast Variable

	
	d = sc.broadcast({1:a, 2:b, 3:c, 4:d, 5:e, 6:f, ...})      # 100 MB	

	def f1( n ):
		global d
		return d.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,6,...], 4)

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

 ======================================================
   spark-submit  command
 ======================================================  

   spark-submit is a single command that is used to submit any spark program (Scala, Java, Python, R)
   to any cluster manager (local, spark standalone, YARN, Mesos, Kubernetes)


    spark-submit [options] <app jar | python file | R file> [app arguments]


   => spark-submit --master yarn
		   --deploy-mode cluster
		   --driver-memory 2G
		   --driver-cores 4
		   --executor-memory 10G
		   --executor-cores 5
		   --num-executors 20
		    E:\\PySpark\\wordcount.py wordcount.txt wcout


   	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 1
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


  ===========================================
     Spark SQL  (pyspark.sql)
  ===========================================
	
    Spark's high-level API for processing structured data

	-> Structured File Formats: Parquet (default), ORC, JSON, CSV (delimited text)
	-> JDBC Format: RDBMS Databases, NoSQL
	-> Hive Format: Work with Hive data.

   SparkSession
   ------------
	-> Starting point of execution.
	-> Represents a user-session inside an application 
	-> An application (sc) can have multiple sessions (SparkSession)
	-> Introduced in Spark 2.0 (prior to that we used to have sqlContext)

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()   

   DataFrame (DF)
   --------------
	-> Spark SQL's data abstraction 

	-> DF is a collection of distributed in-memory partitions that are immutable and laziliy-evaluated. 

	-> DF is a collection of 'Row' objects 
	-> DF is processed using Spark SQL's SQL optimazation engine (such as Catalyst & Tungston)

	-> DF contains two components:
		1. Data   : Row objects
		2. Schema : StructType object

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
                )

        
   Steps in a Spark SQL program
   ----------------------------

     1. Read/load the data from some data source into a DataFrame.

		inputPath = "E:\\PySpark\\data\\users.json"

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()


     2. Transform the DF using DF API methods or using SQL

	    Using DF API methods
            --------------------

		df2 = df1.select("userid", "name", "age", "gender") \
         		.where("age is not null") \
         		.orderBy("gender", "age") \
         		.groupBy("age").count() \
         		.limit(4)

	    Using SQL
            ---------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
         		from users
         		where age is not null
        		group by age
         		order by age
         		limit 4"""
         
		df3 = spark.sql(qry)


     3. Write/save the DF to some external destination (such as a JSON file/Oracle DB/Hive)
	
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

   Save Modes
   ----------
	-> By default, writing to an existing directory returns an exception. 

	-> ignore
	-> append
	-> overwrite

	df3.write.json(outputPath, mode="overwrite")
	df3.write.mode("overwrite").json(outputPath)


   LocalTempViews & GlobalTempViews
   --------------------------------

	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


   DF Transformations
   -------------------
  
   1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(
                 col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 column("DEST_COUNTRY_NAME").alias("destination"), 
                 expr("count"),
                 expr("count+10 as newCount"),
                 expr("count > 365 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

   2. where / filter

	df3 = df2.where("domestic = false and count > 200")
	df3 = df2.filter("domestic = false and count > 200")

	df3 = df2.where( col("count") > 200 )
	df3 = df2.filter( col("count") > 200 )

   3. orderBy / sort 

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))

	df3 = df2.sort("count", "origin")

   4. groupBy  -> returns a 'GroupedData' object on which we apply some aggregation method to retunr a DF

	df3 = df2.groupBy("domestic", "highFrequency").count()
	df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	df3 = df2.groupBy("domestic", "highFrequency").max("count")
	df3 = df2.groupBy("domestic", "highFrequency").avg("count")

	df3 = df2.groupBy("domestic", "highFrequency") \
        	.agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		avg("count").alias("avg"),
              		max("count").alias("max")
            	    )

   5. limit
	df2 = df1.limit(10)

   6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin", 
                 "DEST_COUNTRY_NAME as destination", 
                 "count",
                 "count+10 as newCount",
                 "count > 365 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

   7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", col("count") > 365) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int") ) \
        	.withColumn("country", lit("India"))


	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", col("count") > 365) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int") ) \
        	.withColumn("frequencyGroup", when(col("count") <= 100, 1)
                                        	.when(col("count") <= 500, 2)
                                        	.when(col("count") <= 1000, 3)
                                        	.otherwise(4))

   8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	 .withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

   
   9. udf  (user-defined-function)
	
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"    

	getAgeGroupUDF = udf(getAgeGroup, StringType())

	userDf2= userDf.withColumn("ageGroup", getAgeGroupUDF(userDf["age"]))    
	userDf2.show()
	-----------------------------------------------------------
	@udf(returnType=StringType()) 
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior" 

	userDf2= userDf.withColumn("ageGroup", getAgeGroup(userDf["age"]))    
	userDf2.show()
	-----------------------------------------------------------
	spark.udf.register("getAgeGroupUDF", getAgeGroup, StringType())

	spark.catalog.listFunctions()

	qry = "select id, name, age, getAgeGroupUDF(age) as ageGroup from users"
	spark.sql(qry).show(truncate=False)


   10. drop  -> drops the columns

	df3 = df2.drop("newCount", "highFrequency")


   11. dropDuplicates  -> drops duplicate rows

	listUsers = [(1, "Raju", 5),
             (1, "Raju", 5),
             (3, "Raju", 5),
             (4, "Raghu", 35),
             (4, "Raghu", 35),
             (6, "Raghu", 35),
             (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	df3 = userDf.dropDuplicates()  
	df3 = userDf.dropDuplicates(["name", "age"]) 

   12. dropna

	usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
	usersDf.show()

	df3 = usersDf.dropna()
	df3.show()

	df3 = usersDf.dropna(subset=['phone', 'age'])
	df3.show()

   13. sample
		
	df1.sample(True, 0.5).count()
	df1.sample(True, 0.5, 567).count()
	df1.sample(True, 1.5, 567).count()    # fraction can be  > 1
	
	df1.sample(False, 0.7, 567).show()
	df1.sample(False, 1.7, 567).show()    # Error: fraction must be in the range [0,1]


   14. randomSplit

	df1, df2 = df1.randomSplit([1.0, 2.0], 575)
	print(df1.count(), df2.count())


   15. union, intersect, subtract

	df4 = df2.union(df3)
	df4.count()   # 14 + 1
	df4.show()

	df5 = df4.intersect(df3)
	df5.show()
	df5.rdd.getNumPartitions()

	df6 = df4.subtract(df3)
	df6.show()
	df6.rdd.getNumPartitions()


   16. distinct

	df1.distinct().count()
	df1.select("ORIGIN_COUNTRY_NAME").distinct().count()


   17. repartition

	df2 = df1.repartition(4)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(2)
	df3.rdd.getNumPartitions()

	df4 = df1.repartition(4, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()

	df5 = df1.repartition(col("DEST_COUNTRY_NAME"))
	df5.rdd.getNumPartitions()
		
	   => Here the number of shuffle partitions of the output DF is determined based on the
	      value od "spark.sql.shuffle.partitions" connfig property.

          => We can set it to a custom value as follows:
		spark.conf.set("spark.sql.shuffle.partitions", 5)

   18. coalesce

	df3 = df2.coalesce(2)
	df3.rdd.getNumPartitions()	

   19. joins   => discussed separatly


  Joins
  -------

   Supported Joins => inner, left_outer, right_outer, full_outer, left_semi, left_anti

   left_semi join
   --------------
	=> Is like inner join, but the data comes only from left side table. 
	=> Equivalent to a sub-query:
		select * from emp where deptid IN (select id from dept)

   left-anti join
   --------------
	=> Equivalent to a sub-query:
		select * from emp where deptid NOT IN (select id from dept)

   Using SQL Method
   ----------------
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	spark.catalog.listTables()

	qry = """select emp.*
         from emp left anti join dept on
         emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)
	joinedDf.show()

    
   Using DF API approach
   ---------------------
 	Supported Joins => inner, left_outer, right_outer, full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")
	joinedDf.show()

  ===================================================================================

  show() command
  --------------

	df1.show()
	df1.show(10)
	df1.show(10, False)    		# truncate: False
	df1.show(10, False, True)	# verticle: True


  Working with different file formats
  -----------------------------------
    -> Structured File Formats: Parquet (default), ORC, JSON, CSV (delimited text)


    JSON
    -----
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.json(outputPath)

    Parquet
    -------
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.parquet(outputPath)

   ORC
   ---
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.orc(outputPath)

   CSV (delimited text file)
   -------------------------
	read
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)

		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|")
		
  
   Creating an RDD from DF
   -----------------------
	rdd1 = df1.rdd
	rdd1.collect()


   Creating a DF from programmatic data
   -------------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Creating a DF from RDD
   ----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1.show()


   Creating a  DF using custom schema (programmatic schema inference)
   ------------------------------------------------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)
	df1.show()
        --------------------------------------------------
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)
 

  Use-Case
  --------

   Datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

   From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average user-rating 
  	-> Consider only those movies that are rated by atleast 30 users.
  	-> Data: movieId, title, totalRatings, averageRating
  	-> Arrange the data in the DESC order of averageRating
  	-> Save the output as a single pipe-serated CSV file with header.

	=> Try it yourself. 


  JDBC Format - Working with MySQL
  --------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


   Working with Hive
   -----------------

    Is a Data Warehousing plaform built on top og Hadoop.
    -> Hive is a abstraction built on top of MapReduce.

	warehouse => Is a diretory where hive stores all its managed tables
	metastore => Is an RDBMS service where hive stores all its meta-data. 

     import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()

=====================================
    Spark Streaming
=====================================
  
   Two libraries
	1. Spark Streaming
	2. Structured Streaming (this is preferred)


  Spark Streaming
  ----------------

     => microbatch based processing
     => Provides "seconds" scale latency.  (near-real-time processing)
     => does not support event-time processing


      StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch









 



















