
 Agenda (PySpark)
 ----------------
  Spark - Basics & Architecture
  Spark Core API (Low Level API)
	-> RDD - Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> Working with DataFrames
  Spark Streaming
	-> Spark Streaming (DStreams API)
	-> Structured Streaming (Based on DataFrames)
  Introduction to Spark MLlib


  Materials
  ---------
     -> PDF presentations
     -> Code Modules
     -> Class Notes
     => Github: 
	
  ===================================================

   Spark
   -----

     Apache Spark is an open-source distributed computing engine/framework to process data using in-memory
     computation on a cluster using simple programming constructs.  

     -> Spark is written in 'Scala' programming language

     Cluster : Is a unified entity consisting of many nodes whose cumulative resources can be used to  
               distribute your processing or storage.

     Distributed Computing : distributing the computations on many containers (executors) and perform the 
	       computations parallelly across many machines. 

     In-Memory : Abilty to save the intermediate results of task in the RAM and performing subsequent tasks on
		these persisted results. 

     => Spark is a unified computing engine. 

        Spark Unified Stack : 
        --------------------

	   -> Spark provides a consistent set of APIs for processing different analytics workloads using 
	      the same execution engine. 	

		-> Batch Analytics of unstructured data	   : Spark Core API (RDD API)
		-> Batch Analytics of structured data	   : Spark SQL
		-> Streaming Analytics (real-time)	   : Spark Streaming, Structured Streaming
		-> Predictive Analytics (machine learning) : Spark MLlib
		-> Graph Parallel Computations             : Spark GraphX

     => Spark is a polyglot
	-> Spark programs can be written in Scala, Java, Python and R.


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Spark application are submitted to a CM.
		-> Responsible for allocating resources (executors) for the spark application. 
		-> Supports multiple CMs : local, Spark Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process that manages the entire execution
		-> Manages the user-code and maintains the metadata of the application
		-> Can be launched in two deploye modes:
			client (default) : Driver runs on the client machine
			cluster : Driver runs on one of the nodes on the cluster
		-> Analyzes the user code and send the tasks to run on the executors		

	3. Executors
		-> Will receive tasks from the driver
		-> Tasks are executed and status is reported back to driver.

	4. SparkContext
		-> Runs the driver process
		-> Is the starting point of execution. 
		-> Is an application context
		-> Is a link between the driver and several tasks that are running on the cluster


    Getting started with Spark
    --------------------------     
     1. Working in your vLab:
	=> You connect to a Windows server.
	=> Double-Click on the "CentOS 7" icon on the desktop (of the window server)
		-> refer to "README.txt" file username & password
        => This will connect to CentOS lab

	1.1 Connect to PySpark shell:	
        	-> Open a Terminal 
		-> type : "pyspark"   
		-> this launches pyspark shell.

		-> Open a FireFox browser
		-> type "localhost:4040" in the address bar
		-> this launches the WebUI of PySpark Shell

	1.2 Connect to Jupyter Notebooks
		-> Open a terminal
		-> type:   "jupyter notebook --allow-root"
		-> This launches JN envirionment.

    2. Setting up your own PySpark development environment on your personal machine.

	-> Make sure that you have Anaconda Navigator installed
		https://www.anaconda.com/products/individual

	-> Setup "PySpark" with Jupyter Notebooks / Spyder
		Follow the steps mentioned in the document
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (free account) 

	   URL: https://databricks.com/try-databricks
	   Login URL: https://community.cloud.databricks.com/login.html


  RDD (Resilient distributed dataset)
  ------------------------------------	
  
    -> Fundamental in-memory data abstraction in Spark
	
    -> Is a collection of distributed in-memory partitions
	-> each partition is a collection of objects.

    -> RDDs follow 'Lazy Evaluation' 

    -> RDDs are immutable
	-> partition data con not be changed.


   How to create RDDs?
   -------------------

	Three ways:

	1. Create an RDD from some external file

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data
	
		rdd1 = sc.parallelize( [4,5,1,3,2,0,3,4,5,6,7,8,0,8,7,6,3,1,4,2,6,4,5,7,8,9,0], 2)

	3. By applyign transformations on existing RDD 

		rdd2 = rdd1.map( lambda x: x*2 )               


   What can we do with RDDs?
   ------------------------
	Two things:

	1. Transformation
		-> Returns an RDD
		-> Create the Lineage DAG of the RDD (at the driver)
		-> Does not cause execution.

	2. Actions
		-> Causes execution
		-> Converts the logical plan to pysical execution plan and launches a job on the cluster.

   RDD Lineage
   -----------
    -> RDD lineage is a 'logical plan' on how to create the RDD
    -> RDD lineage tracks all the dependent RDDs (hierarchy) all the way from the very firts RDD.
	
    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)     
       rddFile lineage => (4) rddFile -> sc.textFile
	
    rdd2 = rddFile.flatMap(lambda x: x.split(" "))
	rdd2 lineage => rdd2 -> rddFile.flatMap -> sc.textFile
  
    rdd3 = rdd2.map(lambda x: x.upper())
	rdd3 lineage => rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4 = rdd3.filter(lambda x: len(x) > 3)
	rdd4 lineage => rdd4 -> rdd3.filter -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4.collect()


  Types of Transformations
  ------------------------
	
     	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

  Executor's memory structure
  ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

  RDD Persistence
  ---------------
	
	rdd1 = sc.textFile( ..., 4 )
	rdd2 = rdd1.t2(....)	
	rdd3 = rdd1.t3(....)	
	rdd4 = rdd3.t4(....)	
	rdd5 = rdd3.t5(....)	
	rdd6 = rdd5.t6(....)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )    --------> instrauction to spark to persist the rdd partitions.	
	rdd7 = rdd6.t7(....)

	rdd6.collect()	
		rdd6 lineage => (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile, t3, t5, t6 => rdd6

        rdd7.collect()
		rdd7 lineage => (4) rdd7 -> rdd6.t7
		t7 => rdd7

	rdd6.unpersist()
	
	Storage Levels
        ---------------
	1. MEMORY_ONLY	    	-> default, Memory Serialized 1x Replication
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replication
	3. DISK_ONLY		-> Disk Serialized 1x Replication
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replication
	5. MEMORY_AND_DISK_2    -> Disk Memory Serialized 2x Replication

	Commands
        ---------
		rdd1.persist()
		rdd1.persist( StorageLevel.MEMORY_AND_DISK ) 
		rdd1.cache()   -> memory-only persistence

		rdd1.unpersist()
		
  
  RDD Transformations
  --------------------
   
   => All transformations return an RDD
   => Only created the RDD lineage DAG
   => Does not cause execution. 

  
  1. map		P: U -> V
			object to object transformations
			Transforms the objects of the input RDD to an object in the output RDD by applying the function.
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x)).collect()


  2. filter		P: U -> Boolean
			Only the objects for which the function return True will be in the output RDDs
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: x[-1] == "d").collect()

  3. glom		P: None
			Returns one list object per partition with all the elements of the partition
	
		rdd1			rdd2 = rdd1.glom()
		P0: 3,1,2,4,2,5 -> glom -> P0: [3,1,2,4,2,5]
		P1: 7,8,9,0,3,6 -> glom -> P1: [7,8,9,0,3,6]
		P2: 4,6,5,8,9,0 -> glom -> P2: [4,6,5,8,9,0]
		
		rdd1.count(): 18 (int)	rdd2.count(): 3 (list)

  4. flatMap		P: U -> Iterable[V] 
			flatMap flattens the iterables to unwarp the object from the iterables.
			input RDD: N objects, output RDD: >= N objects

	 rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation			
		
		rdd1	  rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )

		P0: 3,1,2,4,2,5 -> mapPartitions -> P0: 17
		P1: 7,8,9,0,3,6 -> mapPartitions -> P1: 33
		P2: 4,6,5,8,9,0 -> mapPartitions -> P2: 32

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()
		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get partition-index as additional parameter.  

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x*10), p)).collect()


  7. distinct			P: None, Optional: ?
				Returns an RDD with distinct objects
		















   
   

