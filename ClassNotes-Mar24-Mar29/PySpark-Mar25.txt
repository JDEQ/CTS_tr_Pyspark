
 Agenda (PySpark)
 ----------------
  Spark - Basics & Architecture
  Spark Core API (Low Level API)
	-> RDD - Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> Working with DataFrames
  Spark Streaming
	-> Spark Streaming (DStreams API)
	-> Structured Streaming (Based on DataFrames)
  Introduction to Spark MLlib


  Materials
  ---------
     -> PDF presentations
     -> Code Modules
     -> Class Notes
     => Github: 
	
  ===================================================

   Spark
   -----

     Apache Spark is an open-source distributed computing engine/framework to process data using in-memory
     computation on a cluster using simple programming constructs.  

     -> Spark is written in 'Scala' programming language

     Cluster : Is a unified entity consisting of many nodes whose cumulative resources can be used to  
               distribute your processing or storage.

     Distributed Computing : distributing the computations on many containers (executors) and perform the 
	       computations parallelly across many machines. 

     In-Memory : Abilty to save the intermediate results of task in the RAM and performing subsequent tasks on
		these persisted results. 

     => Spark is a unified computing engine. 

        Spark Unified Stack : 
        --------------------

	   -> Spark provides a consistent set of APIs for processing different analytics workloads using 
	      the same execution engine. 	

		-> Batch Analytics of unstructured data	   : Spark Core API (RDD API)
		-> Batch Analytics of structured data	   : Spark SQL
		-> Streaming Analytics (real-time)	   : Spark Streaming, Structured Streaming
		-> Predictive Analytics (machine learning) : Spark MLlib
		-> Graph Parallel Computations             : Spark GraphX

     => Spark is a polyglot
	-> Spark programs can be written in Scala, Java, Python and R.


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Spark application are submitted to a CM.
		-> Responsible for allocating resources (executors) for the spark application. 
		-> Supports multiple CMs : local, Spark Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process that manages the entire execution
		-> Manages the user-code and maintains the metadata of the application
		-> Can be launched in two deploye modes:
			client (default) : Driver runs on the client machine
			cluster : Driver runs on one of the nodes on the cluster
		-> Analyzes the user code and send the tasks to run on the executors		

	3. Executors
		-> Will receive tasks from the driver
		-> Tasks are executed and status is reported back to driver.

	4. SparkContext
		-> Runs the driver process
		-> Is the starting point of execution. 
		-> Is an application context
		-> Is a link between the driver and several tasks that are running on the cluster


    Getting started with Spark
    --------------------------     
     1. Working in your vLab:
	=> You connect to a Windows server.
	=> Double-Click on the "CentOS 7" icon on the desktop (of the window server)
		-> refer to "README.txt" file username & password
        => This will connect to CentOS lab

	1.1 Connect to PySpark shell:	
        	-> Open a Terminal 
		-> type : "pyspark"   
		-> this launches pyspark shell.

		-> Open a FireFox browser
		-> type "localhost:4040" in the address bar
		-> this launches the WebUI of PySpark Shell

	1.2 Connect to Jupyter Notebooks
		-> Open a terminal
		-> type:   "jupyter notebook --allow-root"
		-> This launches JN envirionment.

    2. Setting up your own PySpark development environment on your personal machine.

	-> Make sure that you have Anaconda Navigator installed
		https://www.anaconda.com/products/individual

	-> Setup "PySpark" with Jupyter Notebooks / Spyder
		Follow the steps mentioned in the document
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (free account) 

	   URL: https://databricks.com/try-databricks
	   Login URL: https://community.cloud.databricks.com/login.html


  RDD (Resilient distributed dataset)
  ------------------------------------	
  
    -> Fundamental in-memory data abstraction in Spark
	
    -> Is a collection of distributed in-memory partitions
	-> each partition is a collection of objects.

    -> RDDs follow 'Lazy Evaluation' 

    -> RDDs are immutable
	-> partition data con not be changed.


   How to create RDDs?
   -------------------

	Three ways:

	1. Create an RDD from some external file

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data
	
		rdd1 = sc.parallelize( [4,5,1,3,2,0,3,4,5,6,7,8,0,8,7,6,3,1,4,2,6,4,5,7,8,9,0], 2)

	3. By applyign transformations on existing RDD 

		rdd2 = rdd1.map( lambda x: x*2 )               


   What can we do with RDDs?
   ------------------------
	Two things:

	1. Transformation
		-> Returns an RDD
		-> Create the Lineage DAG of the RDD (at the driver)
		-> Does not cause execution.

	2. Actions
		-> Causes execution
		-> Converts the logical plan to pysical execution plan and launches a job on the cluster.

   RDD Lineage
   -----------
    -> RDD lineage is a 'logical plan' on how to create the RDD
    -> RDD lineage tracks all the dependent RDDs (hierarchy) all the way from the very firts RDD.
	
    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)     
       rddFile lineage => (4) rddFile -> sc.textFile
	
    rdd2 = rddFile.flatMap(lambda x: x.split(" "))
	rdd2 lineage => rdd2 -> rddFile.flatMap -> sc.textFile
  
    rdd3 = rdd2.map(lambda x: x.upper())
	rdd3 lineage => rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4 = rdd3.filter(lambda x: len(x) > 3)
	rdd4 lineage => rdd4 -> rdd3.filter -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4.collect()


  Types of Transformations
  ------------------------
	
     	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

  Executor's memory structure
  ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

  RDD Persistence
  ---------------
	
	rdd1 = sc.textFile( ..., 4 )
	rdd2 = rdd1.t2(....)	
	rdd3 = rdd1.t3(....)	
	rdd4 = rdd3.t4(....)	
	rdd5 = rdd3.t5(....)	
	rdd6 = rdd5.t6(....)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   ---> instrauction to spark to persist the rdd partitions.	
	rdd7 = rdd6.t7(....)

	rdd6.collect()	
		rdd6 lineage => (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile, t3, t5, t6 => rdd6

        rdd7.collect()
		rdd7 lineage => (4) rdd7 -> rdd6.t7
		t7 => rdd7

	rdd6.unpersist()
	
	Storage Levels
        ---------------
	1. MEMORY_ONLY	    	-> default, Memory Serialized 1x Replication
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replication
	3. DISK_ONLY		-> Disk Serialized 1x Replication
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replication
	5. MEMORY_AND_DISK_2    -> Disk Memory Serialized 2x Replication

	Commands
        ---------
		rdd1.persist()  -> memory-only persistence
		rdd1.persist( StorageLevel.MEMORY_AND_DISK ) 
		rdd1.cache()    -> memory-only persistence

		rdd1.unpersist()
		
  
  RDD Transformations
  --------------------
   
   => All transformations return an RDD
   => Only created the RDD lineage DAG
   => Does not cause execution. 

  
  1. map		P: U -> V
			object to object transformations
			Transforms the objects of the input RDD to an object in the output RDD by applying the function.
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x)).collect()


  2. filter		P: U -> Boolean
			Only the objects for which the function return True will be in the output RDDs
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: x[-1] == "d").collect()

  3. glom		P: None
			Returns one list object per partition with all the elements of the partition
	
		rdd1			rdd2 = rdd1.glom()
		P0: 3,1,2,4,2,5 -> glom -> P0: [3,1,2,4,2,5]
		P1: 7,8,9,0,3,6 -> glom -> P1: [7,8,9,0,3,6]
		P2: 4,6,5,8,9,0 -> glom -> P2: [4,6,5,8,9,0]
		
		rdd1.count(): 18 (int)	rdd2.count(): 3 (list)

  4. flatMap		P: U -> Iterable[V] 
			flatMap flattens the iterables to unwarp the object from the iterables.
			input RDD: N objects, output RDD: >= N objects

	 rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation			
		
		rdd1	  rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )

		P0: 3,1,2,4,2,5 -> mapPartitions -> P0: 17
		P1: 7,8,9,0,3,6 -> mapPartitions -> P1: 33
		P2: 4,6,5,8,9,0 -> mapPartitions -> P2: 32

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()
		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get partition-index as additional parameter.  

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p : map(lambda x: (i, x*10), p)).collect()


  7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects
				input RDD: N objects, output RDD: <= N objects

                  rdd1.distinct().collect()
		  rdd1.distinct(4).collect()

  Types of RDDs
  -------------
	=> Generic RDDs : RDD[U]
	=> Pair RDDs : RDD[(U, V)]


  8. mapValues			P: U -> V
				Applied only on Pair RDDs
				Transforms the 'value' part of the (K,V) pairs by applying the function.
				input RDD: N objects, output RDD:  N objects
  		

		rdd2.mapValues(lambda x : x*10).collect()
		rdd3.mapValues(list).collect()

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				The object of the RDD are sorted based on the function output.

		rddWords.sortBy(lambda x: x[-1]).collect()
		rdd2.sortBy(lambda x: x[1], False).collect()
		rdd2.sortBy(lambda x: x[1], False, 5).collect()

   10. groupBy			P: U -> V, Optional: numPartitions
				The elements of the RDD are grouped based on the function output
				Returns a pairs RDD where:
				    key: unique value of the function output
				    value: ResultIterable object containing all the object of the RDD that returned the key.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: List of ratios (e.g: [0.6, 0.4]). Optional: seed
				Split the RDD into multiple RDDs in the specified ratios. 

		rddList = rdd1.randomSplit( [0.4, 0.3, 0.3] )	
		rddList = rdd1.randomSplit( [0.5, 0.5], 46532 )

   12. repartition 		P: numPartition
				Is used to increase or decrease the number of partitions
				Causes global shuffle.

		rdd2 = rdd1.repartition(10)
		

   13. coalesce 		P: numPartitions
				Is used only to decrease the number of partitions
				Cause partition merging.

		rdd2 = rdd1.coalesce(10)

	Recommendations for good performence
        -------------------------------------
	-> The size of each partition should be around 128 MB
	-> The number of partitions should be a multiple of number of CPU core allocated.
	-> If the number of partitions is less than but close to 2000, bump it up to 2000 or more. 
	-> The number of cores in each executor should be 5


   14. partitionBy 		P: numPartitions  Optional: partition-function ( U -> Int )
				Applied on on Pair RDDs
				Is used to control which elements should go to which partition based on the 
				value of the key. 

	transactions = [
    	 {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    	 {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
   	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    	 {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
   	 {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   	 {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    	 {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions) \
         	.map(lambda d: (d['city'], d))
         
	rdd1.glom().collect()  

	def custom_partitioner(city): 
    		if (city == 'Chennai'):
        		return 0;
    		elif (city == 'Hyderabad'):
        		return 1;
    		elif (city == 'Vijayawada'):
        		return 1;
    		elif (city == 'Pune'):
        		return 2;
    		else:
        		return 3; 

	rdd2 = rdd1.partitionBy(4, custom_partitioner)


   15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command			number of partitions
	---------------------------------------------
	rdd1.union(rdd2)	   M + N partitions, narrow
	rdd1.intersection(rdd2)	   M + N partitions, wide
	rdd1.subtract(rdd2)	   M + N partitions, wide
	rdd1.cartesian(rdd2)	   M * N partitions, wide


   ..ByKey transformations
   -----------------------
     -> Are wide transformations
     -> Applies only to Pair RDDs


   16. sortByKey		P: None, Optional: ascending (True/False), numPartition
				Sorts the elements of the RDD based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 4).collect()

    17. groupByKey		P: None, Optional: numPartition
				Returns a Pair RDD where
				key: Unique-key of the input RDD
				value: grouped values with the same key (ResultIterable)

				NOTE: Avoid groupByKey if possible. 

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		 .flatMap(lambda x: x.split(" ")) \
        		 .map(lambda x: (x, 1)) \
        		 .groupByKey() \
        		 .mapValues(sum) \
        		 .sortBy(lambda x: x[1], False) \
        		 .coalesce(1)

    18. reduceByKey		P: U, U -> U, Optional: numPartition
				Reduce all the values of each unique key of the RDD by iterativly applying
				the function withih each partition and then across partitions.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y)

    11. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
                 
	student_rdd.collect()

	output = student_rdd.map(lambda t : (t[0], t[2])) \
            	.aggregateByKey( (0,0),
                            lambda z,v: (z[0] + v, z[1] + 1),
                            lambda x, y: (x[0] + y[0], x[1] + y[1])) \
            	.mapValues(lambda p: p[0]/p[1])


    12. joins    =>  join (inner-join), leftOuterJoin, rightOuterJoin, fullOuterJoin

		     RDD[(K, V)].join( RDD[(K, W)]) => RDD[(K, (V,W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)


    13. cogroup		-> Is used when you join RDDs with duplicate keys and you need unique-keys in the 
			   output. 
				=> groupByKey (on each RDD) -> fullOuterJoin	

	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
    	=> (key1, [5,17]) (key2, [4,7]) (key4, [17])

	cogrouped => (key1, ([10, 7], [5,17])) (key2, ([12, 6],[4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------

  1. collect	-> returns a list to the driver with all the objects of the RDD

  2. count

  3. saveAsTextFile 

  4. reduce			P: (U, U) -> U
				reduces an entire RDD to one value of the same type by iterativly applying the
				reduce function.
		rdd1
		P0: 9, 5, 6, 4, 2, 4 -> reduce -> -12 -> 41
		P1: 7, 8, 9, 0, 8, 9 -> reduce -> -27
		P2: 0, 5, 6, 7, 8, 0 -> reduce -> -26

		rdd1.reduce(lambda x, y: x if (x > y) else y )

   5. aggregate	
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

   6. take	
		rdd1.take(10)	

   7. takeOrdered
		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: x%2)

   8. takeSample
		rdd1.takeSample(True, 10)		# True: with-replacement-sampling
		rdd1.takeSample(True, 100)
		rdd1.takeSample(True, 100, 4565)

		rdd1.takeSample(False, 100, 4565)	# False: with-out-replacement-sampling

   9. countByValue
	-> returns a dict object with object counts

   10. countByKey

   11. foreach 		Takes some function as a parameter and runs that function on all the values
                        of the RDD, but does not return any value. 
    
   12. first

  ======================================================================

   Use-Case
   --------

   dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
   		
   From cars.tsv dataset, find the average weight of each make of 'American' orogin cars. 
   Arrange the data in the DESC order of average weight
   Save the output as a single text file. 
		
       => Try to do it yourself


















