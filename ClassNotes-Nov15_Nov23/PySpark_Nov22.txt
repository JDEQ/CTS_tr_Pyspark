
  Agenda
  ------	
   -> Spark - Basics & Architecture
   -> Spark Low Level API (Spark Core)
	-> RDDs - Transformations & Actions
	-> Shared Variables
   -> Spark SQL (DataFrames API)
   -> Machine Learning & Spark MLlib
   -> Introduction to Spark Streaming

  -----------------------------------
  Materials
   
	-> PDF Presentations  
	-> Code Modules
        -> Class Notes
   	-> Github: https://github.com/ykanakaraju/pyspark
 ---------------------------------

    Spark 
    ------	
	-> Spark is a unified in-memory distributed computing framework.

	-> Written in Scala programming language.

	-> Supports multiple languages (Spark is polyglot)
	   -> Scala, Python, Java, R    

    Cluster
    -------
	-> Is a unified entity containing a group of nodes whose cumulative resources 
           can be used to store or process our  data.


    Distributed Computing Framework
    --------------------------------
	-> A distributed computing framework runs several instances of the same task (tasks)
	   across several containers allocated in different nodes and processes the data
	   paralally. 

	-> Wih this we can process any amount in ashort time, by allocating more and more 
	   resources and more and more parallel tasks. 


   Spark Unified Framework
   -----------------------
	=> Spark provides a consistent set of APIs based on the same execution engine
	   to perform computations on different analytical workloads.

	Hadoop Ecosystem
        ----------------      
	Batch Analytics of Unstructured data	-> MapReduce
	Batch Analytics of Structured data	-> Hive, Drill, Impala
	Streaming Analytics (real-time)		-> Kafka, Storm, Samza
	Predictive Analytics (ML)		-> Mahout
	Graph Parallel Computations		-> Giraph

	Spark Framework
        ----------------      
	Batch Analytics of Unstructured data	-> Spark Core API (RDDs)
	Batch Analytics of Structured data	-> Spark SQL 
	Streaming Analytics (real-time)		-> Spark Streaming
	Predictive Analytics (ML)		-> Spark MLlib
	Graph Parallel Computations		-> Spark GraphX


   Spark Architecture
   ------------------

     1. Cluster Manager (CM)
	
	-> Applications are submitted to cluster manager
		-> Spark supports multiple CMs : Spark Standalone, YARN, Mesos, Kubernetes.

	-> Cluster Manager schedules the task and lauches a driver process 
	   (which sparkContext is created)

	-> CM allocates executors to the application

     2. Driver 	
	-> Master process
	-> Analyses the user-code
	-> Maintains all the meta-data of the application. 
	-> Sends tasks to the executors based on user-code

	Deploy Mode:
		- Client Mode (default): The driver process runs on the client machine
		- Cluster Mode : The driver runs on one of the nodes in the clsuter. 

     3. Executors	
	-> Executes the tasks sent by the driver
	-> All tasks does the same function, but on different partitions of data
	-> reports the status of the tasks to the driver

     4. SparkContext (or SparkSession)	
	-> Starting point of execution
	-> represents an application context
	-> Acts as a link between the driver and various tasks running on the cluster.


   Getting started with Spark
   --------------------------
    1. Installing Spark
	
	 URL: https://spark.apache.org/downloads.html

	 Download Spark binaries and unzip in a suitable location.

	 Setup environment variables:
	
		SPARK_HOME:  	E:\spark-3.0.0-bin-hadoop2.7
		HADOOP_HOME:	E:\spark-3.0.0-bin-hadoop2.7

		Add the following to the PATH env. variable:
			%SPARK_HOME%\bin
			%SPARK_HOME%\python
			%SPARK_HOME%\python\lib\py4j-0.10.9-src.zip

    		PYTHONPATH: %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%

         Open windows command terminal (or Anaconda prompt) and type "pyspark"


   2. Installing an IDE

	 Popular IDEs => Spyder, PyCharm
	 Install "Anaconda Navigator" from https://www.anaconda.com/products/individual

	 Follow the document shared with you to setup pyspark to work with Spyder (or Jupyter notebook)


   3. Create a free Databricks community edition account

	URL: https://databricks.com/try-databricks
	
	-> Signup to Databricks community edition
	-> Login
	-> Go thourgh the "Quickstart Tutorial"


	To Download a file from databricks:
	-----------------------------------

/FileStore/<FILEPATH>
https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

Example:
/FileStore/tables/wordcount-5.txt
https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


/FileStore/tables/WordcountExample1Output/part-00000

/FileStore/tables/demo_wordcount_out/part-00001

https://community.cloud.databricks.com/files/tables/demo_wordcount_out/part-00001?o=4949609693130439#tables/new/dbfs


  
   RDD  (Resilient distributed dataset)
   ------------------------------------

     -> Is the fundamental data abstraction of Spark. 

     -> Is a collection of in-memory distributed partitions
	   -> A partition is a collection of objects.

     -> RDDs are immutable (you can not change the content of an RDD)

     -> RDD has two things:		
		1. Lineage DAG - logical plan that describes how to create the RDD
		2. Data	       - The set of partitions.

     -> RDDs are lazily evaluated
		-> Only action commands cause execution of the RDDs.
		-> Transformations only cause creation of lineage DAg (logical plan)

     -> RDDs are resilient
		-> RDDs can recreate missing in-memory partitions at run-time.
	
   Creating RDDs
   -------------
	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.

		rdd1 = sc.parallelize( range(1, 101), 3 )

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


   What can we do with RDDs
   ------------------------

	Only Two things:

	1. Transformations
	    -> Transformations does not cause execution 
	    -> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
	    -> Triggers the actual execution of the RDDs and some output is generated
	    -> Causes the driver the convert the logical plan to a physical execution ad several
	       tasks are sent to the executor.

   RDD Lineage
   -----------

	-> Is a logical plan maintained by the driver which contains all the dependencies (parent RDDs)
           that caused the creation of this RDD all the way from the very first RDD.
  
	rddFile = sc.textFile( filePath, 4 )
	  lineage DAG: rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	  lineage DAG: rdd2 -> rddFile -> sc.textFile
	
	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	  lineage DAG: rdd3 -> rdd2 -> rddFile -> sc.textFile

	rdd4 = rdd3.map(lambda x: len(x))
	  lineage DAG: rdd4 -> rdd3 -> rdd2 -> rddFile -> sc.textFile

        rdd4.collect()   => triggers execution

	sc.textFile (rddFile) -> map (rdd2) -> flatMap (rdd3) -> map (rdd4) -> collect()
   

    $jupyter notebook   (might give an error. try the one below)
    $jupyter notebook --allow-root


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd4.t6(..)
	rdd6.persist( StorageLevel.MEMORY_ONLY )       ---> save the partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()	
	lineage of rdd6 => rdd6 -> rdd4.t6 -> rdd3.t4 -> rdd1.t3 -> sc.textFile

	sc.textFile (rdd1) ->  t3 (rdd3) -> t4 (rdd4) -> t6 (rdd6) -> collect()

	rdd7.collect()
	lineage of rdd7 => rdd7 -> rdd6.t7 

        rdd6.unpersist()

	
	Types of Persistence
	--------------------
		-> persisting as deserialized objects
		-> persisting as serialized objects
		-> persisting on disk


	Persistence Storage Levels
        --------------------------
	
	1. MEMORY_ONLY		=> RDD parsisted only in RAM (storage memory)

	2. MEMORY_AND_DISK	=> RDD partitions are persisted in-memory of available
				   otherwise, stored on the disk.

	3. DISK_ONLY

	4. MEMORY_ONLY_SER

	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2

	7. MEMORY_AND_DISK_2
	


  Executor Memory Structure
  -------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)
   
	
   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------

   1. map		P: U -> V 
			Transforms each element of the RDD by applying the function.
			Element to element transformation
			input RDD: N elements, output RDD: N elements    

   2. filter		P: U -> Boolean 
			Only objects that returns  True will be in the output. 
			input RDD: N elements, output RDD: <= N elements 
	
	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect() 


   3. flatMap		P: U -> Iterable[V]
			Flattens all the iterables returned by the function
			input RDD: N elements, output RDD: > N elements 

	rddWords.flatMap(lambda x: x.upper()).collect()
	rddWords.flatMap(lambda x: x.split(" ")).collect()

   4. glom			P: None			
	
	   rdd1			rdd2 = rdd1.glom()

	P0: 4,2,4,3,6,7 -> glom -> P0: [4,2,4,3,6,7]
	P1: 8,4,6,1,9,0 -> glom -> P1: [8,4,6,1,9,0]
	P2: 5,6,2,5,4,9 -> glom -> P2: [5,6,2,5,4,9]

   	rdd1.count = 18	(int)	 rdd2.count = 3 (array)


   5. mapPartitions		P: Iterator[U] -> Iterator[V]	
				Applies a function on the entire partition	

	rdd1	  rdd2 = rdd1.mapPartitions( lambda x: [sum(x)] )

	P0: 4,2,4,3,6,7 -> mapPartitions -> P0 : 26
	P1: 8,4,6,1,9,0 -> mapPartitions -> P1 : 28
	P2: 5,6,2,5,4,9 -> mapPartitions -> P2 : 31

	rdd1.mapPartitions(lambda x: [sum(x)]).collect()
	rdd1.mapPartitions(lambda x: map(lambda a: a + 10, x) ).collect()
   	

   6. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V]
				Applies a function on the entire partition. The function takes
				two input params: partition-index & partition-data.	

	rdd1.mapPartitionsWithIndex(lambda id, data: map(lambda a: (id, a + 10), data) ).collect()
	rdd1.mapPartitionsWithIndex(lambda id, data: map(lambda a: (id, a + 10), data) ).collect()


   7. distinct			P: None, Optional: numPartitions   
				Returns distinct elements of the RDD. (removes the duplicates)

		rdd1.distinct()
		rdd1.distinct(5)


   8. sortBy			P: U -> V, Optional: Ascending (True/False), numPartitions
				The objects of the RDD are sorted based on the function output.

		rddWords.sortBy(lambda x: len(x)).collect()
		rddPairs.sortBy(lambda x: x[1], False).collect()
		rddPairs.sortBy(lambda x: x[1], True, 6).collect()
		
   Types of RDDs
   -------------
	-> Generic RDDs : RDD[U]
	-> Pair RDDs	: RDD[(U, V)]


   9. mapValues			P: U -> V
				Applied only to Pair RDDs
				The value part of the (K, V) pairs are transformed using the function.
	
		rdd2.mapValues(lambda x: [x, x*x]).collect()


   10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD - (key, value) pairs, where:
				  key -> unique value of the function output
				  value -> ResultIterable[<all the objects of the RDD that produced the key>]

		rdd1.groupBy(lambda x: x%3).mapValues(list).collect()

		rdd1 = sc.textFile(file, 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.groupBy(lambda x: x) \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: An array of ratios (ex: [0.6, 0.4] )
				Returns an array of RDDs randomly split in the specified ratios.

		rddArr = rdd1.randomSplit([0.4, 0.3, 0.3])
		rddArr = rdd1.randomSplit([0.5, 0.5], 3423)   // here 3423 is a seed
		   -> Using a seed will create same output for muliptle executions.

   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions of output RDD
				Cause global shuffle.

		rdd2 = rdd1.repartition(4)   // rdd2 will have 4 partitions		

   13. coalesce 		P: numPartitions
				Is used to only decrease the number of partitions of output RDD
				Cause partition merging.

		rdd2 = rdd1.coalesce(2)   // rdd2 will have 2 partitions


    Recommendations & Best practices
    ---------------------------------
       1. Size of the partition - apprx. 128 MB
       2. Number of partitions can be 2x-3x the number of cores (minimum)
       3. Number of cores a in single executor should be 5 (or 4)
       4. If the number of partitions is close to 2000, bump it upto above 2000.


    14. partitionBy		P: numberOfPartitions, option: partitioning-function
				Applied to only pair RDDs.
				This is used to control which keys go to which partitions. 
        
	rdd3 = rdd2.partitionBy(2, lambda x: 0 if (x < 5) else 1)


    15. union, intersection, subtract, cartesian       P: RDD
				
	  Let us say rdd1 has M partitions and rdd2 has N partitions:

	  command			Number of partitions of output RDD
	  -----------------------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd1.cartesian(rdd2)		M * N, wide


    ...ByKey transformations	
	-> They are all wide transformations
	-> They are applied only to pair RDDs

	
    16. sortByKey		 P: None, Optional: Ascending (True/False), numPartitions
				 The elements are sorted based on the key.

		rdd100.sortByKey().glom().collect()		
		rdd100.sortByKey(False).glom().collect()
		rdd100.sortByKey(False, 4).glom().collect()
	

    17. groupByKey		 P: None, Optional: numPartitions
				 Returns a piar RDD with unique keys and aggregated values.
				 RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])] 
				 NOTE: Avoid groupByKey if possible. 	 

 		rdd1 = sc.textFile(file, 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda a: (a, 1)) \
         		.groupByKey() \
         		.mapValues(sum)

    18. reduceByKey		P: (U, U) -> U
				Reduces all the values of each unique-key by iterativly applying the 
				reduce function.

		rdd1 = sc.textFile(file, 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda a: (a, 1)) \
         		.reduceByKey(lambda x, y: x + y)

    19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.  


		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		rdd1 = student_rdd.map(lambda x: (x[0], x[2])) \
        		.aggregateByKey( (0,0),
                         	lambda z, v: (z[0] + v, z[1] + 1),
                         	lambda a, b: (a[0] + b[0], a[1] + b[1])) \
        		.mapValues(lambda x: x[0]/x[1])


     20. joins		-> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

			   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

			   join = names1.join(names2)   #inner Join
			   leftOuterJoin = names1.leftOuterJoin(names2)
			   rightOuterJoin = names1.rightOuterJoin(names2)
			   fullOuterJoin = names1.fullOuterJoin(names2)

     21. cogroup	  -> Is used to join RDDs that may have duplicate keys
			  -> groupByKey -> fullOuterJoin


	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]	
	-> (key1, [10, 7])  (key2, [12, 6])  (key3, [6])

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
	-> (key1, [5, 17])  (key2, [4, 7])  (key4, [17])

	=> (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4, 7])) (key3, ([6], []) )   (key4, ([], [17]))

     

   RDD Actions
   ------------
	1. collect

   	2. count

   	3. saveAsTextFile

	4. reduce			P: (U, U) -> U
					It will reduce the entire RDD into one value of the same type.
					The function will first reduce each partition to one value, and 
					then, these outputs are further reduced one final value by iterativly
					applying the reduce function. 

		P0: 8, 2, 4, 3, 1, 5, 4     -> -11
		P1: 6, 7, 9, 0, 3, 5, 1     -> -19 
		P2: 2, 3, 2, 5, 6, 7, 8, 9  -> -38		

		rdd1.reduce( lambda x, y: x - y )  => 46

	5. aggregate			P: Is used to aggregate the content of an RDD to a type that is
					different that the elements of the RDD.

		rdd1.aggregate( (0,0), 
				lambda z, v: (z[0] + v, z[1] + 1), 
				lambda x, y: (x[0] + y[0], x[1] + y[1]) )

	6. take(n)

	7. takeOrdered(n, [ordering fn])

		rddWords.takeOrdered(8, lambda x: len(x))

	8. takeSample(withReplacement, numElements)

		rdd1.takeSample(True, 7)         // withReplacement: True
		rdd1.takeSample(False, 7)	 // withReplacement: False
		rdd1.takeSample(False, 7, 56)    // 56 is a seed

	9. countByValue

	10. countByKey

	11. first

        12. foreach		P: a function that does not return anything.

     	13. saveAsSequenceFile	

		

   Use-Case
   --------
	From cars.tvs, get average weight of each make of American Origin and arrange them
	in the descending order of average weight and save the output into a single text file. 
  
	==> Please try to solve it
	    (the dataset is shared in the github - data_git folder)


  spark-submit command
  --------------------
	-> Is a single command used to submit any spark application (python, scala, java, R etc)
	   to any cluster manager.

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2

	spark-submit --master local 
		--deploy-mode cluster
		--executor-cores 5
		--executor-memory 10G
		--num-executor 20
		E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2

   Closure
   -------
	-> A closure referes to all the code that must be visible to an executor it perform
	   its computations

	-> A closure a serilaized and a copy is sent to every executor.


   Shared Variables
   ----------------
	Two types of Shared variables:

		1. Accumulator variables
		2. Broadcast variables


        Accumulator
        -----------
	The following code does not work:

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	counter = 0

	def isPrime( n ):
		returns 1 if n is prime
		else returns 0

	def f1( n ):
		global counter
		if ( isPrime(n) == 1 ) counter += 1
		return n * 10

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	println("num of primes = " + str(counter))   // the output will be 0


	=> Problem: Local variables CAN NOT be used to implement global counter
		-> Because anything that is part of a closure will be a local copy of an executor
		   that can not have a global state. 

	=> Solution: Accumulator


	counter = sc.accumulator(0)

	def isPrime( n ):
		returns 1 if n is prime
		else returns 0

	def f1( n ):
		global counter
		if ( isPrime(n) == 1 ) counter.add(1)
		return n * 10

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	println("num of primes = " + str(counter))   // the output will be 0

        --------------------------------------------------------

	Broadcast Variable
        ------------------
	-> A large immutable collection (such as a large list or dictionary) can be converted
	   into a broadcast variable.
	
	-> A broadcast variable is not a part of function closure. Only one copy of the variable
	   is stored in storage-memory and all task will lookup from this one copy.
                 -> This will save a lot of execution memory.		
	
	d = sc.broadcast({1: a, 2: b, 3: c, 4: d, ....})      // 100 MB 

	def f1( n ):
		global d;
		return d.value[n];

	rdd1 = sc.parallelize( [1,2,3,4,1,2,3,4,..])

	rdd2 = rdd1.map( f1 )

	rdd2.collect()   

   ===============================
      Spark SQL (pyspark.sql)
   ===============================

	-> Spark SQL is a hign-level API built on top of Spark Core.

	-> Is used for structured data processing.

	   => Structured File Format: Parquet (default), ORC, JSON, CSV (delimited text file)
	   => Hive (Hadoop's data warehousing infra )
	   => JDBC -> RDBMS Dbs, NoSQL Dbs
         
         -> SparkSession
		-> Is the starting point of execution in Spark SQL applications.
		-> Represents a user session within an application
		    -> A single application (represented by SparkContext) can have mulitiple 
		       SparkSessions.
		-> SparkSession is indroduced in Spark 2.0

	-> DataFrame (DF)
		-> Is the data abstraction of Spark SQL

		-> Is a collection of distributed, in-memory partitions that are immutable and 
		   lazily evaluated.

		-> DataFrame is a collection of "Row" objects. 
			-> Each Row is a collection of 'Column' objects
			-> Is column is stored in Spark SQL internal data types.

		-> DataFrame's components:

			Data   :   Collection of partitions of Row objects
			Schema :   Is a StructType object, describing the structure of the DF.

			StructType(
			     List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			     )
			)


     Steps in Spark SQL application
     -------------------------------

	1. Create a SparkSession

		spark = SparkSession \
        		.builder \
        		.appName("Dataframe Operations") \
        		.config("spark.master", "local[*]") \
        		.getOrCreate() 

	2. Read/Load data from an external data source or programmatic data into a DataFrame

		df1 = spark.read.format("json").load(inputFilePath)
		df1 = spark.read.json(inputFilePath)


	3. Apply transformations on the DF using DF API methods or by using SQL

		By using DF transformation methods:
		---------------------------------------

			df2 = df1.select("userid", "name", "age", "phone", "gender") \
         			.where("age is not null") \
         			.orderBy("age", "name") \
         			.groupBy("age").count() \
         			.limit(4)

		By Using SQL
		------------
			df1.createOrReplaceTempView("users")
			spark.catalog.listTables()

			qry = """select age, count(1) as count 
         			from users
         			where age is not null
         			group by age
         			order by age 
         			limit 4"""

			df3 = spark.sql(qry)


	4. Write/save the dataframe to a structured data file or to any structured data destination. 

		 df2.write.format("json").save(outputPath)
               
	
   LocalTempViews Vs GlobalTempViews
   ---------------------------------
	
   
   DataFrame Transformations
   -------------------------

    1. select

		df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

		df2 = df1.select(col("ORIGIN_COUNTRY_NAME"), 
                 		 column("DEST_COUNTRY_NAME"), 
                 		 expr("count"))

		df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 		column("DEST_COUNTRY_NAME").alias("destination"), 
                 		expr("count"),
                 		expr("count + 10 as newCount"),
                 		expr("count > 365 as highFrequency"),
                 		expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME as domestic"))

    2. where / filter
			
		df3 = df2.where("count > 100 and ORIGIN_COUNTRY_NAME = 'United States'")
		df3 = df2.filter("count > 100 and ORIGIN_COUNTRY_NAME = 'United States'")
		df3 = df2.where(col("count") > 100)

    3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.orderBy(col("count").asc(), col("origin").desc())
		df3 = df2.orderBy(asc("count"), desc("origin"))

		df3 = df2.sort(asc("count"), desc("origin"))		

    4. groupBy (with some aggregation function)
	  -> groupBy returns a GroupedData on which you should an aggregation method to get a DF.

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")
		df3 = df2.groupBy("domestic", "highFrequency").max("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
        		.agg( 	count("count").alias("count"),
              			sum("count").alias("sum"),
              			avg("count").alias("avg"),
              			max("count").alias("max")
            		     )

    5. limit     
		df2 = df1.limit(5)

    6. selectExpr

		df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"), 
                 	expr("DEST_COUNTRY_NAME as destination"), 
                 	expr("count"),
                 	expr("count + 10 as newCount"),
                 	expr("count > 365 as highFrequency"),
                 	expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME as domestic"))

		is same as:

		df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin", 
                 		"DEST_COUNTRY_NAME as destination", 
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 365 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME as domestic")

     7. withColumn & withColumnRenamed


		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", expr("count > 365")) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

    8. drop
		df4 = df3.drop("newCount", "highFrequency")

    9. union
		data = [("India", "Japan", 123),
       			("India", "Germany", 123),
       			("India", "Australia", 123),
       			("India", "France", 123)]

		df2 = spark.createDataFrame(data) \
           		.toDF("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

		df3 = df2.union(df1)
		df3.show()


    10. distinct
		df3.select("ORIGIN_COUNTRY_NAME").distinct().count()

    11. randomSplit
	
		df10, df11 = df1.randomSplit([0.6, 0.4], 35)
		df10.count()
		df11.count()

    12. sample
	
		df10 = df1.sample(True, 1.7)
		df10 = df1.sample(False, 0.7, 57)
		df10 = df1.sample(False, 1.7, 57)    // error : we can not use a fraction > 1

    13. repartition

		df2 = df1.repartition(4)
		df3 = df2.repartition(2)
		df4 = df1.repartition( col("DEST_COUNTRY_NAME") )
		df5 = df1.repartition( 8, col("DEST_COUNTRY_NAME") )

		df5.rdd.getNumPartitions()

    14. coalesce

		df6 = df5.coalesce(4)
		df6.rdd.getNumPartitions()   

    15. join
	
	   -> taken as a separated topic.


     Working with different file formats
     -----------------------------------
	
	JSON:

	      Reading:
	      		df1 = spark.read.format("json").load( <inputFilePath> )
			df1 = spark.read.json( <inputFilePath> )

	      Writing:
			df1.write.format("json").save(<outputDirPath>)
			df1.write.json(<outputDirPath>)

	Parquet
	
		Reading:
	      		df1 = spark.read.format("parquet").load( <inputFilePath> )
			df1 = spark.read.parquet( <inputFilePath> )

	      	Writing:
			df1.write.format("parquet").save(<outputDirPath>)
			df1.write.parquet(<outputDirPath>)

	ORC
		Reading:
	      		df1 = spark.read.format("orc").load( <inputFilePath> )
			df1 = spark.read.orc( <inputFilePath> )

	      	Writing:
			df1.write.format("orc").save(<outputDirPath>)
			df1.write.orc(<outputDirPath>)

	CSV
		Reading:
			df1 = spark.read.csv(inputFilePath, inferSchema=True)  // read from header-less file
			df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True)
			df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True, sep="|")

		Writing:
			df1.write.format("csv").save(<outputDirPath>, header=True)
			df1.write.csv(<outputDirPath>, header=True)
			df1.write.csv(<outputDirPath>, header=True, sep="|")

  SaveModes
  ----------

	-> defines the behaviour while writing to an existing directory.

	1. errorIfExists
	2. ignore
	3. append
	4. overwrite

	df2.write.mode("overwrite").json(outputPath)
        

    Converting an RDD from DataFrame
    --------------------------------
     => rdd1 = df1.rdd
   

   Creating DataFrames from programmatic data & from RDDs
   ------------------------------------------------------

    data
    -----
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Raghu", 25),
             (4, "Ravi", 35),
             (5, "Radhika", 45),
             (6, "Ramya", 35),
             (7, "Ramana", 25)]

    method 1
    ---------
	df3 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

    method 2
    --------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	df3 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

    method 3
    --------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rddRows = rdd1.map(lambda t: Row(t[0], t[1], t[2]))

	mySchema = StructType([
               StructField("id", IntegerType(), True),
               StructField("name", StringType(), True),
               StructField("age", IntegerType(), True)
           ])

	df3 = spark.createDataFrame(rddRows, mySchema)



  Applying Programmatic Schema on Source Data
  --------------------------------------------

	mySchema = StructType([
               StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
               StructField("DEST_COUNTRY_NAME", StringType(), True),
               StructField("count", LongType(), True)
           ])

	df1 = spark.read.schema(mySchema).csv(inputFilePath, header=True, inferSchema=True)
	df1.show()
	df1.printSchema()

	// this also works ...
	df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True, schema=mySchema)



   Joins
   ------

	Supported Joins:  inner, left_outer, right_outer, full_outer, left_semi, left_anti

	left_semi join
	--------------
	=> Like inner join, but the data comes ONLY from the left table.
	=> Output is as per the following subquery:
		select * from emp where deptid IN (select id from dept);

	left_anti join
	--------------
	=> Fetchs only those records from the left table ONLy whose join key is NOT there in the right table. 
	=> Output is as per the following subquery:
		select * from emp where deptid NOT IN (select id from dept);
	

	SQL Way of Joining DFs
        -----------------------
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*, dept.*
         	from emp LEFT OUTER JOIN dept on
         	emp.deptid = dept.id"""
         
	joinedDf = spark.sql(qry)

	joinedDf.show()

	
	DF API approach to Joining DFs
	------------------------------
	Supported Joins:  inner, left_outer, right_outer, full_outer, left_semi, left_anti
	
	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_outer")    // default join type: "inner"


	Enforcing Broadcast Joins (in case where both the DFs are big (i.e  > autoBroadcastJoinThreshold)
	-------------------------
	joinedDf = employee.join(broadcast(department), joinCol, "left_outer") 
	

   Use-Case: 
   ---------

	From movies.csv and ratings.csv get me the top 10 moves with highest average rating.

	-> Consider only those movies that are rated by atleast 30 users
	-> Data Required: movieId, title, totalRatings, avgRating
	-> Arrange in the DESC order of avgRating
	-> Save as a single pipe-separated values file. 
	
        => Please try to solve it...  (data files are available in the github)


   Working with MySQL (JDBC format)
   --------------------------------
       -> Please refer to the code from Github.


   Working with Hive
   -----------------
	
     Hive is a data warehousing platform built on top of Hadoop 

     Hive Warehouse:  is a directory where Hive stores all its managed data (databases, tables)
     Meta Store: is a service that Hive used to store all its metadata. 


	spark = SparkSession \
    		.builder \
    		.appName("Datasorces") \
    		.config("spark.master", "local") \
    		.config("spark.sql.warehouse.dir", warehouse_location) \
    		.enableHiveSupport() \
    		.getOrCreate()

  ================================================
        Machine Learning & Spark MLlib
  ================================================

   The goal of an ML project is to create an ML Model.

   ML Model => Is a learned entity
	       Learns from historic data and understand how label is generated from features. 
	       Algorithms process the training data to create a model.
                   -> An ML algorithm performs iterative computation on training data to create a model.
		
	       Algorithm performs an iterative computation on the training with a goal to minimize
	       a loss function.

	model = algorithm( < training-data> )

     	x	y	z	prediction    error
	--------------------------------------------
	100	200	390	395		5			
	200	50	470	455		15			
	400	100	895	895		0			
	150	100	410	395		15			
	250	50	550 	.. 		 
	--------------------------------------------
				 Loss:	      ~9.5
	
	iteration 1:  z = 2x + y + 0	->  Loss: 11.25
	iteration 2:  z = 2x + y - 5    ->  Loss: 9.5	

   Terminology
   -----------
	1. Features 	: input variables, dimesions, independent variable
	2. Label 	: Output, dependent variable
	3. ML Algorithm	: Mathematical computation that estabished a relattion between Labl & Features	
	4. Model	: Is the output of an ML Algorithm that can predict output from inputs with minimal error.
	5. Error	: Difference between an actual data point and prediction
	6. Loss		: Cumulative/Aggregated Error

  
  Steps in an ML Project
  ----------------------
    
     1. Data Collection   - collect the raw

     2. Data Preparation   ( > 60% of total project time)

	=> The goal of data preparation is to make raw data suitable for given as input 
	   to an algorithm.

	=> The goal here is to create a "Feature Vector"

	   -> EDA (Exploratory Data Analysis)
	   -> FE (Feature Engineering)

	=> All data should be in numerical format
	-> There should be no empty values, nulls etc.

     3. Train a model using ML algorithm.
	-> Output of this step is a "trained model"

	-> Split the training data into two splits => training set (70%), validation set(30%)
	-> Traing your data with training set (70%)

     4. Evaluate the model
	-> Take the predictions using the model for the validation set(30%)
	-> By comparing the predictions with the actual label with can get the accuracy of the model.

     5. Deploy the model 


    Types of Machine Learning
    -------------------------
	
	1. Supervised Learning
	    -> Input: Labelled Data; Input data contains label and features.

	    1.1 Classification
		-> Ouput is classified (one of few fixed values)
		-> Output: [0,1] (binomial), [1,2,3,4,5] (multinomial)
		-> Ex: Survival Prediction, Email Spam predicition etc. 

	    1.2 Regression
		-> Output is a continuous value
		-> Ex: House Price Prediction.

	2. Unsupervised Learning

	    -> Input: Unlabelled data. Input data contains only features & no label.
	    -> The training involves understanding the patterns in the data.

	    2.1 Clustering  
		-> Dividing the data into multiple clusters based on patterns

	    2.2 Dimesionality Reduction		

	3. Reinforcement Learning
	    -> Semi-supervised Learning.


   
   Titanic Survival Prediction Model (Titanic - Machine Learning from Disaster)
   ----------------------------------------------------------------------------
    URL: https://www.kaggle.com/c/titanic






