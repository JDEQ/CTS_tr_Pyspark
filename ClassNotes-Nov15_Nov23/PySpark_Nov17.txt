
  Agenda
  ------	
   -> Spark - Basics & Architecture
   -> Spark Low Level API (Spark Core)
	-> RDDs - Transformations & Actions
	-> Shared Variables
   -> Spark SQL (DataFrames API)
   -> Machine Learning & Spark MLlib
   -> Introduction to Spark Streaming

  -----------------------------------
  Materials
   
	-> PDF Presentations  
	-> Code Modules
        -> Class Notes
   	-> Github: https://github.com/ykanakaraju/pyspark
 ---------------------------------

    Spark 
    ------	
	-> Spark is a unified in-memory distributed computing framework.

	-> Written in Scala programming language.

	-> Supports multiple languages (Spark is polyglot)
	   -> Scala, Python, Java, R    

    Cluster
    -------
	-> Is a unified entity containing a group of nodes whose cumulative resources 
           can be used to store or process our  data.


    Distributed Computing Framework
    --------------------------------
	-> A distributed computing framework runs several instances of the same task (tasks)
	   across several containers allocated in different nodes and processes the data
	   paralally. 

	-> Wih this we can process any amount in ashort time, by allocating more and more 
	   resources and more and more parallel tasks. 


   Spark Unified Framework
   -----------------------
	=> Spark provides a consistent set of APIs based on the same execution engine
	   to perform computations on different analytical workloads.

	Hadoop Ecosystem
        ----------------      
	Batch Analytics of Unstructured data	-> MapReduce
	Batch Analytics of Structured data	-> Hive, Drill, Impala
	Streaming Analytics (real-time)		-> Kafka, Storm, Samza
	Predictive Analytics (ML)		-> Mahout
	Graph Parallel Computations		-> Giraph

	Spark Framework
        ----------------      
	Batch Analytics of Unstructured data	-> Spark Core API (RDDs)
	Batch Analytics of Structured data	-> Spark SQL 
	Streaming Analytics (real-time)		-> Spark Streaming
	Predictive Analytics (ML)		-> Spark MLlib
	Graph Parallel Computations		-> Spark GraphX


   Spark Architecture
   ------------------

     1. Cluster Manager (CM)
	
	-> Applications are submitted to cluster manager
		-> Spark supports multiple CMs : Spark Standalone, YARN, Mesos, Kubernetes.

	-> Cluster Manager schedules the task and lauches a driver process 
	   (which sparkContext is created)

	-> CM allocates executors to the application

     2. Driver 	
	-> Master process
	-> Analyses the user-code
	-> Maintains all the meta-data of the application. 
	-> Sends tasks to the executors based on user-code

	Deploy Mode:
		- Client Mode (default): The driver process runs on the client machine
		- Cluster Mode : The driver runs on one of the nodes in the clsuter. 

     3. Executors	
	-> Executes the tasks sent by the driver
	-> All tasks does the same function, but on different partitions of data
	-> reports the status of the tasks to the driver

     4. SparkContext (or SparkSession)	
	-> Starting point of execution
	-> represents an application context
	-> Acts as a link between the driver and various tasks running on the cluster.


   Getting started with Spark
   --------------------------
    1. Installing Spark
	
	 URL: https://spark.apache.org/downloads.html

	 Download Spark binaries and unzip in a suitable location.

	 Setup environment variables:
	
		SPARK_HOME:  	E:\spark-3.0.0-bin-hadoop2.7
		HADOOP_HOME:	E:\spark-3.0.0-bin-hadoop2.7

		Add the following to the PATH env. variable:
			%SPARK_HOME%\bin
			%SPARK_HOME%\python
			%SPARK_HOME%\python\lib\py4j-0.10.9-src.zip

    		PYTHONPATH: %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%

         Open windows command terminal (or Anaconda prompt) and type "pyspark"


   2. Installing an IDE

	 Popular IDEs => Spyder, PyCharm
	 Install "Anaconda Navigator" from https://www.anaconda.com/products/individual

	 Follow the document shared with you to setup pyspark to work with Spyder (or Jupyter notebook)


   3. Create a free Databricks community edition account

	URL: https://databricks.com/try-databricks
	
	-> Signup to Databricks community edition
	-> Login
	-> Go thourgh the "Quickstart Tutorial"

  
   RDD  (Resilient distributed dataset)
   ------------------------------------

     -> Is the fundamental data abstraction of Spark. 

     -> Is a collection of in-memory distributed partitions
	   -> A partition is a collection of objects.

     -> RDDs are immutable (you can not change the content of an RDD)

     -> RDD has two things:		
		1. Lineage DAG - logical plan that describes how to create the RDD
		2. Data	       - The set of partitions.

     -> RDDs are lazily evaluated
		-> Only action commands cause execution of the RDDs.
		-> Transformations only cause creation of lineage DAg (logical plan)

     -> RDDs are resilient
		-> RDDs can recreate missing in-memory partitions at run-time.
	
   Creating RDDs
   -------------
	Three ways:

	1. Create an RDD from some external text file

		rdd1 = sc.textFile( <filePath> )

		-> The default number of partitions is decided by "sc.defaultMinPartitions", whose
		   value is 2 if you have atleast two cores.

		rddFile = sc.textFile( file, 4 )   // 4 partitions are created.

	2.  Create an RDD from programmatic data. 
	
		rdd1 = sc.parallelize( range(1, 101) )

		-> The default number of partitions is decided by "sc.defaultParallelism", whose
		   value is equal to the number of cores allocated to your application.

		rdd1 = sc.parallelize( range(1, 101), 3 )

	3. By applying transformation on existing RDDs we can create new RDD.

		rdd2 = rdd1.map( ... )


   What can we do with RDDs
   ------------------------

	Only Two things:

	1. Transformations
	    -> Transformations does not cause execution 
	    -> Transformations cause only the lineage DAG to be created at the driver side.

	2. Actions
	    -> Triggers the actual execution of the RDDs and some output is generated
	    -> Causes the driver the convert the logical plan to a physical execution ad several
	       tasks are sent to the executor.

   RDD Lineage
   -----------

	-> Is a logical plan maintained by the driver which contains all the dependencies (parent RDDs)
           that caused the creation of this RDD all the way from the very first RDD.
  
	rddFile = sc.textFile( filePath, 4 )
	  lineage DAG: rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	  lineage DAG: rdd2 -> rddFile -> sc.textFile
	
	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	  lineage DAG: rdd3 -> rdd2 -> rddFile -> sc.textFile

	rdd4 = rdd3.map(lambda x: len(x))
	  lineage DAG: rdd4 -> rdd3 -> rdd2 -> rddFile -> sc.textFile

        rdd4.collect()   => triggers execution

	sc.textFile (rddFile) -> map (rdd2) -> flatMap (rdd3) -> map (rdd4) -> collect()
   

    $jupyter notebook   (might give an error. try the one below)
    $jupyter notebook --allow-root


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd4.t6(..)
	rdd6.persist( StorageLevel.MEMORY_ONLY )       ---> save the partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()	
	lineage of rdd6 => rdd6 -> rdd4.t6 -> rdd3.t4 -> rdd1.t3 -> sc.textFile

	sc.textFile (rdd1) ->  t3 (rdd3) -> t4 (rdd4) -> t6 (rdd6) -> collect()

	rdd7.collect()
	lineage of rdd7 => rdd7 -> rdd6.t7 

        rdd6.unpersist()

	
	Types of Persistence
	--------------------
		-> persisting as deserialized objects
		-> persisting as serialized objects
		-> persisting on disk


	Persistence Storage Levels
        --------------------------
	
	1. MEMORY_ONLY		=> RDD parsisted only in RAM (storage memory)

	2. MEMORY_AND_DISK	=> RDD partitions are persisted in-memory of available
				   otherwise, stored on the disk.

	3. DISK_ONLY

	4. MEMORY_ONLY_SER

	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2

	7. MEMORY_AND_DISK_2
	


  Executor Memory Structure
  -------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)
   
	
   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------

   1. map		P: U -> V 
			Transforms each element of the RDD by applying the function.
			Element to element transformation
			input RDD: N elements, output RDD: N elements    

   2. filter		P: U -> Boolean 
			Only objects that returns  True will be in the output. 
			input RDD: N elements, output RDD: <= N elements 
	
	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect() 


   3. flatMap		P: U -> Iterable[V]
			Flattens all the iterables returned by the function
			input RDD: N elements, output RDD: > N elements 

	rddWords.flatMap(lambda x: x.upper()).collect()
	rddWords.flatMap(lambda x: x.split(" ")).collect()

   4. glom			P: None			
	
	   rdd1			rdd2 = rdd1.glom()

	P0: 4,2,4,3,6,7 -> glom -> P0: [4,2,4,3,6,7]
	P1: 8,4,6,1,9,0 -> glom -> P1: [8,4,6,1,9,0]
	P2: 5,6,2,5,4,9 -> glom -> P2: [5,6,2,5,4,9]

   	rdd1.count = 18	(int)	 rdd2.count = 3 (array)


   5. mapPartitions		P: Iterator[U] -> Iterator[V]	
				Applies a function on the entire partition	

	rdd1	  rdd2 = rdd1.mapPartitions( lambda x: [sum(x)] )

	P0: 4,2,4,3,6,7 -> mapPartitions -> P0 : 26
	P1: 8,4,6,1,9,0 -> mapPartitions -> P1 : 28
	P2: 5,6,2,5,4,9 -> mapPartitions -> P2 : 31

	rdd1.mapPartitions(lambda x: [sum(x)]).collect()
	rdd1.mapPartitions(lambda x: map(lambda a: a + 10, x) ).collect()
   	

   6. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V]
				Applies a function on the entire partition. The function takes
				two input params: partition-index & partition-data.	

	rdd1.mapPartitionsWithIndex(lambda id, data: map(lambda a: (id, a + 10), data) ).collect()
	rdd1.mapPartitionsWithIndex(lambda id, data: map(lambda a: (id, a + 10), data) ).collect()


   7. distinct			P: None, Optional: numPartitions   
				Returns distinct elements of the RDD. (removes the duplicates)

		rdd1.distinct()
		rdd1.distinct(5)


   8. sortBy			P: U -> V, Optional: Ascending (True/False), numPartitions
				The objects of the RDD are sorted based on the function output.

		rddWords.sortBy(lambda x: len(x)).collect()
		rddPairs.sortBy(lambda x: x[1], False).collect()
		rddPairs.sortBy(lambda x: x[1], True, 6).collect()
		
   Types of RDDs
   -------------
	-> Generic RDDs : RDD[U]
	-> Pair RDDs	: RDD[(U, V)]


   9. mapValues			P: U -> V
				Applied only to Pair RDDs
				The value part of the (K, V) pairs are transformed using the function.
	
		rdd2.mapValues(lambda x: [x, x*x]).collect()


   10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD - (key, value) pairs, where:
				  key -> unique value of the function output
				  value -> ResultIterable[<all the objects of the RDD that produced the key>]

		rdd1.groupBy(lambda x: x%3).mapValues(list).collect()

		rdd1 = sc.textFile(file, 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.groupBy(lambda x: x) \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: An array of ratios (ex: [0.6, 0.4] )
				Returns an array of RDDs randomly split in the specified ratios.

		rddArr = rdd1.randomSplit([0.4, 0.3, 0.3])
		rddArr = rdd1.randomSplit([0.5, 0.5], 3423)   // here 3423 is a seed
		   -> Using a seed will create same output for muliptle executions.

   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions of output RDD
				Cause global shuffle.

		rdd2 = rdd1.repartition(4)   // rdd2 will have 4 partitions		

   13. coalesce 		P: numPartitions
				Is used to only decrease the number of partitions of output RDD
				Cause partition merging.

		rdd2 = rdd1.coalesce(2)   // rdd2 will have 2 partitions


    Recommendations & Best practices
    ---------------------------------
       1. Size of the partition - apprx. 128 MB
       2. Number of partitions can be 2x-3x the number of cores (minimum)
       3. Number of cores a in single executor should be 5 (or 4)
       4. If the number of partitions is close to 2000, bump it upto above 2000.


    14. partitionBy		P: numberOfPartitions, option: partitioning-function
				Applied to only pair RDDs.
				This is used to control which keys go to which partitions. 
        
	rdd3 = rdd2.partitionBy(2, lambda x: 0 if (x < 5) else 1)


    15. union, intersection, subtract, cartesian       P: RDD
				
	  Let us say rdd1 has M partitions and rdd2 has N partitions:

	  command			Number of partitions of output RDD
	  -----------------------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd1.cartesian(rdd2)		M * N, wide


    ...ByKey transformations	
	-> They are all wide transformations
	-> They are applied only to pair RDDs

	
    16. sortByKey		 P: None, Optional: Ascending (True/False), numPartitions
				 The elements are sorted based on the key.

		rdd100.sortByKey().glom().collect()		
		rdd100.sortByKey(False).glom().collect()
		rdd100.sortByKey(False, 4).glom().collect()
	

    17. groupByKey		 P: None, Optional: numPartitions
				 Returns a piar RDD with unique keys and aggregated values.
				 RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])] 
				 NOTE: Avoid groupByKey if possible. 	 

 		rdd1 = sc.textFile(file, 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda a: (a, 1)) \
         		.groupByKey() \
         		.mapValues(sum)

    18. reduceByKey		P: (U, U) -> U
				Reduces all the values of each unique-key by iterativly applying the 
				reduce function.

		rdd1 = sc.textFile(file, 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda a: (a, 1)) \
         		.reduceByKey(lambda x, y: x + y)





   RDD Actions
   ------------
	1. collect

   	2. count

   	3. saveAsTextFile

	4. reduce			P: (U, U) -> U
					It will reduce the entire RDD into one value of the same type.
					The function will first reduce each partition to one value, and 
					then, these outputs are further reduced one final value by iterativly
					applying the reduce function. 

		P0: 8, 2, 4, 3, 1, 5, 4     -> -11
		P1: 6, 7, 9, 0, 3, 5, 1     -> -19 
		P2: 2, 3, 2, 5, 6, 7, 8, 9  -> -38		

		rdd1.reduce( lambda x, y: x - y )  => 46

		




