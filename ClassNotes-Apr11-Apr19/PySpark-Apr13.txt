
  Agenda (PySpark)
  ----------------
  Spark - Basics & Architecture
  Spark Core API
	-> RDD : Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> DataFrame Oprtations
  Spark Streaming
	-> DStreams API
	-> Strcutured Structured
  Introduction to Spark MLLib


  Materials
  ----------
     -> PDF Presentations
     -> Code Modules
     -> Class Notes
     -> Github: https://github.com/ykanakaraju/pyspark


  Spark
  ------
      -> Spark is a unified in-memory distributed computing framework 
      -> Spark is a BigData analytics framework.      

      -> Spark is written in 'SCALA' programming language  

      -> Spark is a polyglot
	  -> Supports Scala, Java, Python, R       

      Cluster => Is a unified entity comprising of many nodes whose combined resources can be used
	         to distribute the processing. 

      In-Memory => Spark can persist intermediate results of tasks in the RAM and can launch subsequent
		   task on these in-memory results. 

	     -> In-memory computations are subjected to availability of RAM
	     -> Spark is 100 times faster than MapReduce if you use 100% in-memory computatition
	     -> Spark is 6 to 7 times faster than MapReduce even of disk-based computions is used.
    
 
      Spark unified stack
      --------------------
	-> Spark provides a consistent set of APIs for multiple analytical workloads based on the same
           execution engine.

           Batch anlytics of unstructured data	   : Spark Core API (RDD API)
	   Batch anlytics of structured data	   : Spark SQL 
	   Streaming analytics (real-time)	   : Spark Streaming, Structued Streaming
           Predictive analytics (machine-learning) : Spark MLlib
	   Graph parallel computations	           : Spark GraphX


	=> For additional community-built API for spark, visit : https://spark-packages.org/
		
	
     Spark Architecture
     ------------------ 

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
	


    Getting started with PySpark
    -----------------------------

	1. Using your vLab
	      -> PySpark shell
	      -> Jupyter Notebooks

        2. Setting up PySpark environement on your personal machine.
	      -> Download and install 'Anaconda Navigator'
		    URL: https://www.anaconda.com/products/distribution#windows
		         https://docs.anaconda.com/anaconda/install/windows/

	      -> Follow the instruction mentioned in the shared document.
		 https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

        3. Signup to Databricks Community Edition acccount (free)		
		-> Signup: https://databricks.com/try-databricks
 		-> Login: https://community.cloud.databricks.com/login.html

		NOTE: Read the "Quick Start" tutorial.


    RDD (Resilient Distributed Dataset)
    ------------------------------------
	-> Fundamental data abstraction of Spark. 

	-> RDD is a collection of distributed in-memory partitions
		-> each partition is a collection of objects of some type.

	-> RDDs are immutable. Partition data can not be changed.

	-> RDDs are lazily evaluated 
		-> Transformations only cause RDD lineage DAG to be created. Does not cause executions
		-> Action commands trigger execution.  

	-> RDDs are resilient
		-> RDDs can recreate the missing partitions at run-time.	


    How to create RDDs ?
    --------------------

	Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
		rdd1.getNumPartitions()

	2. Create an RDD from some programmatic data

		rdd2 = sc.parallelize(range(1, 100), 3)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())


    What can you do with an RDD ?
    -----------------------------

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


    RDD Lineage
    -----------
	RDD Lineage is a logical plan containing the hierarchy of dependencies all the way from
        the very first RDD.

	RDD Lineage is maintained by driver and is created when you perform a transformation (or data loading)

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
	   Lineage DAG =>  rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   Lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    Lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile



   RDD Persistence
   ---------------

	rdd1 = sc.textFile(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist()       -> instruction to Spark to persist the rdd6 partitions.  
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	  lineage of rdd6 : rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		transformations: (sc.textFile, t3, t5, t6)

	rdd7.collect()
	  lineage of rdd7: rdd7 -> rdd6.t7
		transformations: (t7)

	rdd6.unpersist()

	StorageLevel
        -------------
	1. MEMORY_ONLY	      => default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK    => Disk Memory Serialized 1x Replicated
	3. DISK_ONLY	      => Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2      => Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2  => Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()     -> in-memory persistence
	rdd1.persist()	 
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


   RDD Transformations
   -------------------
    => The output of a Transformation is an RDD
    => Transformation only cause RDD lineage graph to be created. Does not cause execution. 
    => The default number of output partitions is equal to the number of input partitions. 


   1. map		P: U -> V
			object to object transformation
			input RDD: N objects, output RDD: N object

	rddFile.map(lambda x: x.split(" ")).collect()


   2. filter		P: U -> Boolean
			only those objects of input RDD for which the function returns True will be in the
			output RDD. 
			input RDD: N objects, output RDD: <= N object

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()


  3. glom		P: None
			Returns one list object per partition with all the objects of the partition.

		rdd1		   rdd2 = rdd1.glom()

		P0: 4,2,5,4,6,7,8,9 -> glom -> P0: [4,2,5,4,6,7,8,9]
		P1: 5,4,2,9,0,7,8,5 -> glom -> P1: [5,4,2,9,0,7,8,5]
		P2: 9,0,8,7,9,6,0,3 -> glom -> P2: [9,0,8,7,9,6,0,3]
		
		rdd1.count() = 24 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables generated by the function.
			input RDD: N objects, output RDD: >= N object

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transform
			Transforms and entire partition into output partition by applying the function.

		rdd1        rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )    

		P0: 4,2,5,4,6 -> mapPartitions -> P0: 21
		P1: 5,4,2,9,0 -> mapPartitions -> P1: 20
		P2: 9,0,8,7,9 -> mapPartitions -> P2: 33

		rdd1.mapPartitions(lambda x: [sum(x)]).collect()
		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we can partition-index as an additional function parameter. 

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))])
			.filter(lambda x: (x[0]%2 == 0))
			.flatMap(lambda x: x[1])
			.collect()

   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects from the input RDD.
				Output RDD is hash partitioned.
				
  		rdd1.distinct().glom().collect()
		rdd1.distinct(4).glom().collect()

    Types of RDDs
    --------------

	=> Generic RDDs :  RDD[U]
	=> Pair RDD	:  RDD[(U, V)]


   8. mapValues			P: U -> V
				Applied to only Pair RDDs
				Transforms only the value part of the (k, v) pairs by applying a function
  
		rdd2.map( lambda x: x*10 ).collect()
		rdd3.map( lambda x: list(x[1]) ).collect()
		-> Here rdd2 & rdd3 must be a pair RDDs. 


   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the elements of the output RDD based on the function output.	
		
		rdd2.sortBy(lambda x: x[1]).collect()
		rdd2.sortBy(lambda x: x[1], False).collect()
		rdd1.sortBy(lambda x: x%3, False, 2).glom().collect()

   10. groupBy			P: U -> V  Optional: numPartitions
				Returns a Pair RDD where 
				   key: the unique value of the function output
			           value: ResultIterable with all the elements of the RDD that produced the key.
				** Avoid groupBy if possible. 

		wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len) \
              			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of weights (e.g. [0.4, 0.3, 0.3])  Optional: seed
				Returns a list of RDDs split randomly the specified weights from the input RDD.

		rddList = rdd1.randomSplit([0.6, 0.4])
		rddList = rdd1.randomSplit([0.6, 0.4], 3454)


   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions of the output RDD
				Results in global shuffle.

    
   13. coalesce			P: numPartitions
				Is used only to decrease the number of partitions of the output RDD
				Results in partition merging
	
	Recommendations
	---------------
	-> The size of the RDD should be nearly 128 MB
	-> The number of partitions should be a multiple of number of CPU cores
	-> If the number of partitions is close bu less than 2000, bump it up to 2000 or more.
	-> The number of cores in each executor should be 5


   14. union, intersection, subtract, cartesian

	let us say rdd1 has M partitions and rdd2 has N partitions

	 command			output partitions
         --------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd1.cartesian(rdd2)		M * N, wide


   15. partitionBy		P: numPartitions, Optional: partitioning-function (default: hash)
				Applied only on Pair RDD
				Is used to control which keys go to which partition based on some partition
				function. 		
			
		rdd3 = rdd1.map(lambda x: (x, 0)).partitionBy(2).map(lambda x: x[0])

   transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;      

rdd1 = sc.parallelize(transactions, 3) \
        .map(lambda d: (d['city'], d)) \
        .partitionBy(3, custom_partitioner) \
        .map(lambda x: x[1])

rdd1.glom().collect()

 
   ..ByKey Transformations
   -----------------------
    -> Are all wide transformations
    -> Applied only on pair RDDs.
	

   16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the elements of the output RDD based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(False, 1).collect()

   17. groupByKey		P: None, Optional: numPartitions
				Returns a Pair RDD with unique keys and grouped values.
				** Avoid groupByKey if possible. 
	
		wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.map(lambda x: (x, 1)) \
              			.groupByKey(1) \
              			.mapValues(len)

   18. reduceByKey		P; U, U -> U   Optional: numPartitions
				Reduces all the values of each unique-key to one vlue of the same type by
				iterativly applying the function.

		wordcount = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.map(lambda x: (x, 1)) \
              			.reduceByKey(lambda x, y: x + y)

   19. aggregateByKey		P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])


   20. joins		=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

			   RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup			Is used when you want to join RDDs with duplicate keys and want unique keys
				in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  ------------

   1. collect 

   2. count

   3. saveAsTextFile

   4. reduce			P: U, U -> U
				Reduces the entire RDD to one final value of the same type by iterativly
				applying the reduce function on each partition and then further reduces the
				values produced at each partition to one final value. 

		rdd1

		P0: 6, 3, 4, 5, 2, 8, 7, 8, 9, 0	-> reduce -> 52   => 153
		P1: 7, 8, 3, 2, 1, 2, 1, 2, 4, 3	-> reduce -> 33
		P2: 5, 4, 6, 7, 8, 9, 6, 4, 2, 9, 0, 8  -> reduce -> 68	
	
		rdd1.reduce(lambda x, y: x + y)  

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )

   5. aggregate

	Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. take
		rdd1.take(10)

   7. takeOrdered

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)

   8. takeSample

		rdd1.takeSample(True, 20)
		rdd1.takeSample(True, 20, 4564)

		rdd1.takeSample(False, 20)

   9. countByValue
		rddWords.countByValue()

   10. countByKey
		rddPairs.countByKey()

   11. saveAsSequenceFile

   12. foreach		P: Some function that is applied on all the objects of the RDD, but does not return anything.


   Use-case
   --------
	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	Find the average weight of all the makes of cars with 'American' origin.
	Arrange the data in the DESC order of average-weight
	Save the output as single text file.

	=> Try it yourself...


   Closure
   -------
      => A closure constitutes all the code (variables and methods) that must be visible within a executor for the 
	 tasks to perform their computatitions on the RDDs.

      => Closures are serialized (by the Driver) and a separate copy is sent (by the driver) to all executors. 


	c = 0  	

	def isPrime(n):
		returns True if n is Prime
		else False		

	def f1(n):
		global c
		if (isPrime(n)) c += 1
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )
	rdd2.collect()

	print(c)     # c = 0

	Limitation: We can not use local variables that are part of the closure to implement global counters.
        Solution: Use Accumulator variable.
      
   
   Shared Variables
   ----------------

    1. Accumulator variable

	-> Maintained by the driver
	-> Not part of closure (not a local copy)
	-> All tasks can add to this accumulator. 
	-> All tasks share one copy of the variable maintained at the driver side.
	-> Used to implement global counter
	
	c = sc.accumulator(0) 	

	def isPrime(n):
		returns True if n is Prime
		else False		

	def f1(n):
		global c
		if (isPrime(n)) c.add(1)
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     # c = 80


   2. Broadcast Variable


	d = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, 6: f, 7: g, .. })   # 100 MB 

	def f1(n):
		global d
		return d.value[n]

 	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

   ========================================================
      spark-submit command
   ========================================================
   
     spark-submit is a single command that is used to submit any spark application (Scala, Java, Python or R)
     to any cluster manager (local, standalone, YARN, Mesos, Kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G
		--driver-cores 4
		--executor-memory 10G
		--executor-cores 5
		--num-executors 10
		E:\\PySpark\\wordcount.py [app arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py
  	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 1


   =================================================
       Spark SQL  (pyspark.sql)
   =================================================

     -> Is a high-level API built on top of Spark Core

     -> Spark's structured data processing API
	  => Structured data files : Parquet (default), ORC, JSON, CSV (delimited text file)
	  => JDBC Format : RDBMS, NoSQL
	  => Hive

    -> SparkSession
	-> Starting point of execution for Spark SQL application
	-> Represents a user-session with its own consifuration running within an application. 
	-> We can have multiple sessions (SparkSession) running within an application (SparkContext)
	-> Introduced from Spark 2.0 onwards (before that we has sqlContext)

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()  


    -> DataFrame (DF)

	-> The data abstraction of Spark SQL	
	-> DF is a collection of distributed in-memory partitions which are immutable and lazily evaluated. 
	-> DF is a collection of "Row" objects
 
	-> DF has two components:

		1. Data   : Collection of Rows
		2. Schema : StructType object

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)
  

    Basic Steps in a Spark SQL program
    ----------------------------------

    1. Read/load data from some data source into a DataFrame

		inputPath = "E:\\PySpark\\data\\users.json"

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()

    2. Transform the DF using DF Transformation methods or using SQL

	   Using DF Transformation methods:
	   --------------------------------

		df2 = df1.select("userid", "name", "gender", "age") \
         		.where("age is not null") \
         		.sort("age", "name") \
         		.groupBy("age").count() \
         		.limit(4)

	   Using SQL:
           ----------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
         		from users
         		where age is not null
         		group by age
         		order by age
         		limit 4"""

		df3 = spark.sql(qry)
		df3.show()


    3. Write/save the DF to some structured destination. 

		outputPath = "E:\\PySPark\\output\\json"
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

		df2.write.save(outputPath)


  LocaltempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


  Save Modes
  -----------
	By default, df output can not written to an existing directory (errorIfExists)
     
     	-> ignore
	-> append
	-> overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)
	

  DF Transformations
  ------------------

   1. select

		df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

		df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 		column("DEST_COUNTRY_NAME").alias("destination"), 
                 		expr("count").cast("int"),
                 		expr("count + 10 as newCount"),
                 		expr("count > 200 as highFrequency"),
                 		expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))
		

   2. where / filter

		df3 = df2.where("domestic = false and count > 200")
		df3 = df2.filter("domestic = false and count > 200")

		df3 = df2.where( col("count") > 200 )


   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")

		df3 = df2.orderBy(desc("count"), asc("origin"))
		df3 = df2.sort(desc("count"), asc("origin"))


   4. groupBy  => returns  a "pyspark.sql.group.GroupedData" object
		  Apply some aggregation method to return a DF

		df3 = df2.groupBy("highFrequency", "domestic").count()
		df3 = df2.groupBy("highFrequency", "domestic").avg("count")
		df3 = df2.groupBy("highFrequency", "domestic").max("count")
		df3 = df2.groupBy("highFrequency", "domestic").sum("count")

		df3 = df2.groupBy("highFrequency", "domestic") \
         			.agg( count("count").alias("count"),
               				sum("count").alias("sum"),
               				max("count").alias("max"),
               				avg("count").alias("avg"))

   5. limit  
		df2 = df1.limit(10)

   6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin", 
                 "DEST_COUNTRY_NAME as destination", 
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

	is same as:

	df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"), 
                 expr("DEST_COUNTRY_NAME as destination"), 
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


    7. withColumn

		df3 = df1.withColumn("newCount", col("count") + 10) \
         		.withColumn("highFrequency", expr("count > 200")) \
         		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
         		.withColumn("count", col("count").cast("int"))

		Example : Adding an "ageGroup" column
		-------------------------------------

		listUsers = [(1, "Raju", 5),
             		(2, "Ramesh", 15),
             		(3, "Rajesh", 18),
             		(4, "Raghu", 35),
             		(5, "Ramya", 25),
            		(6, "Radhika", 35),
             		(7, "Ravi", 70)]


		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])

		userDf.show()

		df11 = userDf.withColumn("ageGroup", when(userDf["age"] <= 12, "child")
                           .when(userDf["age"] < 20, "teenager")
                           .when(userDf["age"] < 60, "adult")
                           .otherwise("senior"))

    8. withColumnRenamed

		df3 = df1.withColumn("newCount", col("count") + 10) \
         		.withColumn("highFrequency", expr("count > 200")) \
         		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
         		.withColumn("count", col("count").cast("int")) \
         		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


    9. udf  (user-defined-function) 

		def getAgeGroup( age ):
    			if (age <= 12):
        			return "child"
    			elif (age >= 13 and age <= 19):
        			return "teenager"
    			elif (age >= 20 and age < 60):
        			return "adult"
    			else:
        			return "senior"
    
		getAgeGroupUdf = udf(getAgeGroup, StringType())    

		df11 = userDf.withColumn("ageGroup", getAgeGroupUdf(userDf.age) )

		---------------------------------------------------------------

		@udf(returnType=StringType())
		def getAgeGroup( age ):
    			if (age <= 12):
        			return "child"
    			elif (age >= 13 and age <= 19):
        			return "teenager"
    			elif (age >= 20 and age < 60):
        			return "adult"
    			else:
        			return "senior"  

		df11 = userDf.withColumn("ageGroup", getAgeGroup(userDf.age) )
		------------------------------------------------------------------

		spark.udf.register("get_age_group", getAgeGroup, StringType())

		userDf.createOrReplaceTempView("users")

		qry = "select id, name, age, get_age_group(age) as ageGroup from users"
		spark.sql(qry).show(truncate=False)

     10. drop   => is used to exclude the specified columns in the output dataframe.

		df4 = df3.drop("newCount", "highFrequency")

     11. dropna  => drops the rows with nulls in any column or specified columns.

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df5 = usersDf.dropna()
		df5 = usersDf.dropna(subset=["age", "phone"])

     12. dropDuplicates => drops the duplicate rows. 
	
		df4 = userDf.dropDuplicates()   # rows are considered as dups if all columns have same values
		df4 = userDf.dropDuplicates(["id", "age"]) # rows are considered as dups if specified columns have same values
		

     13. sample

		df2 = df1.sample(True, 0.5)
		df2 = df1.sample(True, 1.5, 4564)    # fraction 1.5 is allowed

		df2 = df1.sample(False, 0.5, 4564)
		df2 = df1.sample(False, 1.5, 4564)    # ERROR: fraction 1.5 is NOT allowed. fraction should be in the range [0,1]


     14. union, intersect, subtract
		=> Performed on two input DFs of the same schema. 

		df4 = df2.union(df3)
		df4.count()
		df4.show()

		df5 = df4.intersect(df3)   # common ( df2+df3   & df3)
		df5.show()
		df5.rdd.getNumPartitions()

		df6 = df4.subtract(df3)  # df2+df3 - df3
		df6.show()

     15. distinct

		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

     16. randomSplit

		df10, df11 = df1.randomSplit([0.6, 0.4], 464)

		df10.count()
		df11.count()

    17. repartition

		df2 = df1.repartition(8)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(4)
		df3.rdd.getNumPartitions()

		df3 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

    18. coalesce

		df5 = df2.coalesce(10)
		df5.rdd.getNumPartitions()

     19. join






   Working with different file formats
   -----------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")
	
  
   Creating an RDD from DF
   -----------------------
	rdd1 = df1.rdd

   
   Creating a DF from programmatic data
   ------------------------------------
   	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

   	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
   	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Creating a DF from RDD
   ----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

   
   Creating a DF with programmatic schema
   -------------------------------------- 
	mySchema = StructType([
              StructField("id", IntegerType(), True),
              StructField("name", StringType(), True),
              StructField("age", IntegerType(), True) 
            ])

	df1 = spark.createDataFrame(rdd1, schema = mySchema)
     
       ------------------------------------------------------

	mySchema = StructType([
              StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
              StructField("DEST_COUNTRY_NAME", StringType(), True),
              StructField("count", IntegerType(), True) 
            ])

	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	df2 = spark.read.json(inputPath, schema=mySchema)
	df2 = spark.read.schema(mySchema).json(inputPath)

























