
  Agenda (PySpark)
  ----------------
  Spark - Basics & Architecture
  Spark Core API
	-> RDD : Transformations & Actions
	-> Shared Variables
  Spark SQL
	-> DataFrame Oprtations
  Spark Streaming
	-> DStreams API
	-> Strcutured Structured
  Introduction to Spark MLLib


  Materials
  ----------
     -> PDF Presentations
     -> Code Modules
     -> Class Notes
     -> Github: https://github.com/ykanakaraju/pyspark


  Spark
  ------
      -> Spark is a unified in-memory distributed computing framework 
      -> Spark is a BigData analytics framework.      

      -> Spark is written in 'SCALA' programming language  

      -> Spark is a polyglot
	  -> Supports Scala, Java, Python, R       

      Cluster => Is a unified entity comprising of many nodes whose combined resources can be used
	         to distribute the processing. 

      In-Memory => Spark can persist intermediate results of tasks in the RAM and can launch subsequent
		   task on these in-memory results. 

	     -> In-memory computations are subjected to availability of RAM
	     -> Spark is 100 times faster than MapReduce if you use 100% in-memory computatition
	     -> Spark is 6 to 7 times faster than MapReduce even of disk-based computions is used.
    
 
      Spark unified stack
      --------------------
	-> Spark provides a consistent set of APIs for multiple analytical workloads based on the same
           execution engine.

           Batch anlytics of unstructured data	   : Spark Core API (RDD API)
	   Batch anlytics of structured data	   : Spark SQL 
	   Streaming analytics (real-time)	   : Spark Streaming, Structued Streaming
           Predictive analytics (machine-learning) : Spark MLlib
	   Graph parallel computations	           : Spark GraphX
		
	
     Spark Architecture
     ------------------ 

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.
	


    Getting started with PySpark
    -----------------------------

	1. Using your vLab
	      -> PySpark shell
	      -> Jupyter Notebooks

        2. Setting up PySpark environement on your personal machine.
	      -> Download and install 'Anaconda Navigator'
		    URL: https://www.anaconda.com/products/distribution#windows
		         https://docs.anaconda.com/anaconda/install/windows/

	      -> Follow the instruction mentioned in the shared document.
		 https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

        3. Signup to Databricks Community Edition acccount (free)		
		-> Signup: https://databricks.com/try-databricks
 		-> Login: https://community.cloud.databricks.com/login.html

		NOTE: Read the "Quick Start" tutorial.


    RDD (Resilient Distributed Dataset)
    ------------------------------------
	-> Fundamental data abstraction of Spark. 

	-> RDD is a collection of distributed in-memory partitions
		-> each partition is a collection of objects of some type.

	-> RDDs are immutable. Partition data can not be changed.

	-> RDDs are lazily evaluated 
		-> Transformations only cause RDD lineage DAG to be created. Does not cause executions
		-> Action commands trigger execution.  
		


    How to create RDDs ?
    --------------------

	Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
		rdd1.getNumPartitions()

	2. Create an RDD from some programmatic data

		rdd2 = sc.parallelize(range(1, 100), 3)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())


    What can you do with an RDD ?
    -----------------------------

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


    RDD Lineage
    -----------
	RDD Lineage is a logical plan containing the hierarchy of dependencies all the way from
        the very first RDD.

	RDD Lineage is maintained by driver and is created when you perform a transformation (or data loading)

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4)
	   Lineage DAG =>  rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	   Lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	    Lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile



   RDD Persistence
   ---------------

	rdd1 = sc.textFile(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist()       -> instruction to Spark to persist the rdd6 partitions.  
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	  lineage of rdd6 : rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		transformations: (sc.textFile, t3, t5, t6)

	rdd7.collect()
	  lineage of rdd7: rdd7 -> rdd6.t7
		transformations: (t7)

	rdd6.unpersist()

	StorageLevel
        -------------
	1. MEMORY_ONLY	      => default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK    => Disk Memory Serialized 1x Replicated
	3. DISK_ONLY	      => Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2      => Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2  => Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()     -> in-memory persistence
	rdd1.persist()	 
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD
 


   RDD Transformations
   -------------------
    => The output of a Transformation is an RDD
    => Transformation only cause RDD lineage graph to be created. Does not cause execution. 
    => The default number of output partitions is equal to the number of input partitions. 


   1. map		P: U -> V
			object to object transformation
			input RDD: N objects, output RDD: N object

	rddFile.map(lambda x: x.split(" ")).collect()


   2. filter		P: U -> Boolean
			only those objects of input RDD for which the function returns True will be in the
			output RDD. 
			input RDD: N objects, output RDD: <= N object

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()


  3. glom		P: None
			Returns one list object per partition with all the objects of the partition.

		rdd1		   rdd2 = rdd1.glom()

		P0: 4,2,5,4,6,7,8,9 -> glom -> P0: [4,2,5,4,6,7,8,9]
		P1: 5,4,2,9,0,7,8,5 -> glom -> P1: [5,4,2,9,0,7,8,5]
		P2: 9,0,8,7,9,6,0,3 -> glom -> P2: [9,0,8,7,9,6,0,3]
		
		rdd1.count() = 24 (int)    rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables generated by the function.
			input RDD: N objects, output RDD: >= N object

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transform
			Transforms and entire partition into output partition by applying the function.

		rdd1        rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )    

		P0: 4,2,5,4,6 -> mapPartitions -> P0: 21
		P1: 5,4,2,9,0 -> mapPartitions -> P1: 20
		P2: 9,0,8,7,9 -> mapPartitions -> P2: 33

		rdd1.mapPartitions(lambda x: [sum(x)]).collect()
		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we can partition-index as an additional function parameter. 

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))])
			.filter(lambda x: (x[0]%2 == 0))
			.flatMap(lambda x: x[1])
			.collect()

   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects from the input RDD.
				Output RDD is hash partitioned.
				
  		rdd1.distinct().glom().collect()
		rdd1.distinct(4).glom().collect()

    Types of RDDs
    --------------

	=> Generic RDDs :  RDD[U]
	=> Pair RDD	:  RDD[(U, V)]


   8. mapValues			P: U -> V
				Applied to only Pair RDDs
				Transforms only the value part of the (k, v) pairs by applying a function
  
		rdd2.map( lambda x: x*10 ).collect()
		rdd3.map( lambda x: list(x[1]) ).collect()
		-> Here rdd2 & rdd3 must be a pair RDDs. 


   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the elements of the output RDD based on the function output. 

	rdd1.sortBy(lambda x: x%3, False, 2).glom().collect()
	rdd2.sortBy(lambda x: x[1], False).collect()
	rdd2.sortBy(lambda x: x[1]).collect()

   






















