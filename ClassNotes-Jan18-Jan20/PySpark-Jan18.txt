
  Agenda  (9:30 AM to 6:30 PM)
  ----------------------------
	-> Spark - Basics & Architecture
	-> Spark Core API
		-> RDD Transformations & Actions
		-> Shared Variables
	-> Spark SQL
	-> Spark MLlib & Machine Learning
	-> Introduction to Spark Streaming


  Materials
  ---------
	1. PDF presentations
	2. Code Modules
	3. Class Notes
	-> https://github.com/ykanakaraju/pyspark


   Spark
   -----

    => Spark is a unified in-memory distributed computing framework  

	distributed computing => being able to distribute your computations across many
				 machines and perform computations in parallel. 

	in-memory => The intermediate results of computations can be persisted in-memory and
		     subsequent tasks can perform computations on them. 

	unified => Spark provides a consistent set of APIs for performing different analytical workloads
		   based on the same execution. 

		Batch analytics of unstructured data 	: Spark Core API (low-level api)
		Batch analytics of structured data	: Spark SQL
		Streaming analytics (real-time)		: Spark Streaming, Structured Streaming
		Predictive analytics (ML)		: Spark MLlib
		Graph parallel computitions		: Spark GraphX


    => Spark is written in SCALA language

    => Spark is a polyglot - supports multiple languages
		-> Scala, Java, Python & R

		PySpark -> Spark library for writing applications using Python. 	

    => Spark applications an run on multiple 'cluster managers'
	   -> local, spark standalone scheduler, YARN, Mesos and Kubernetes 


   
   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.

   
    Getting started with Spark 
    ---------------------------

      1. Working in your vLab
		-> Click on the CentOS-7 icon on the desktop of windows server.
		-> Enter the username and password (README.txt file on the desktop)	
		
		-> Launch the pyspark shell.
			-> Open a Terminal
			-> Type:  "pyspark" at the command prompt
			-> This will launch thr PySpark Shell application
			-> Open a FireFox browser and type "http://localhost:4040/jobs/" to launch the Web UI.

	        -> Launch the Jupyter Notebooks
			-> Open a terminal
			-> Type: "jupyter notebook --allow-root" at the command prompt
			-> This will launch the Jupyter notebook environment.

	2. Setting up PySpark environment on your local machine.

		-> Download and install Anaconda Navigator
			https://www.anaconda.com/products/individual#windows

		-> To setup PySpark with Jupyter/Spyder, follow the steps mentioned in 
		   the document shared in the github. 
	
			https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

        3. Try Databricks community edition
		-> https://databricks.com/try-databricks



   RDD (Resilient Distributed Dataset)
   ------------------------------------

      -> Is the fundamental in-memory data-abstraction of Spark.

      -> Collection of distributed in-memory partitions.
		-> A partition is a collection of objects.

      -> RDDs are immutable

      -> RDDs are lazily evaluation
		-> Transformation does not cause execution
		-> Only actions cause execution

      -> RDDs are resilient
		-> RDDs can recreate the missing partitions at runtime by reapplying
		   the transformations. 

      -> RDD has two components

		1. RDD Lineage DAG  -> Logical plan on how to create an RDD.

		2. RDD Data  	    -> Collection of in-memory partitions



   How to create RDDs
   -------------------  

	3 ways:

	1. Create an RDD form external files.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. RDDs can be created from programmatic data

		rdd1 = sc.parallelize( range(1, 100), 3 )
		rdd1 = sc.parallelize( [3,2,1,2,3,4,5,6,7,8,9,0,6,4,3,2,1,4,5,6,7,8,9,0], 3 )

	3. By applying transformations on existing RDDs
		
		rdd2 = rdd1.map(lambda x: x*2)


		
   What can we do on an RDD
   -------------------------
	
	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.
   Lineage DAG
   ------------

       -> A lineage DAG is a logical plan on how to create the partitions of the RDD
       -> Tracks all dependencies all the way from the very first RDD.

       => An RDD object can be thought of as a set of instructions on how to create the partitions
          of that RDD called "Lineage DAG" (which is a logical plan)


	 rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	 Lineage:   (4) rddFile -> sc.textFile

	 rdd2 = rddFile.flatMap(lambda x: x.split(" "))
	 Lineage:   (4) rdd2 -> rddFile.flatMap -> sc.textFile

	 rdd3 = rdd2.map(lambda x: x.upper())
	 Lineage:   (4) rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile

	 rdd4 = rdd3.filter(lambda x: len(x) > 3)
	 Lineage:   (4) rdd4 -> rdd3.filter -> rdd2.map -> rddFile.flatMap -> sc.textFile

   	rdd4.collect()  -> triggers the execution.


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist(StorageLevel.MEMORY_ONLY)    --> instruction to spark to persist RDD partitions in memory
	rdd7 = rdd6.t7(..)

        rdd6.collect()
	lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	
	sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> collected to the driver

	rdd7.collect()
 	lineage of rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	
	rdd6 -> t7 (rdd7) collected to the driver

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		-> default, memory serialized 1x replica
	2. MEMORY_AND_DISK	-> disk memory serialized 1x replica
	3. DISK_ONLY		-> disk only 1x replica
	4. MEMORY_ONLY_2	-> memory serialized 2x replica
	5. MEMORY_AND_DISK_2	-> disk memory serialized 2x replica

	Persistence Commands
  	--------------------
		rdd1.persist()   // memory serialized 1x replica
		rdd1.persist(StorageLevel.DISK_ONLY)
		rdd1.cache()
		
		rdd1.unpersist()


   RDD Transformations
   -------------------

    1. map		P: U -> V
			Object to Object transformations
   			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: len(x)).collect()

   2. filter		P: U -> Boolean
			Filters the objects based on the function
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()


   3. glom		P: None
			Returns a list object with all the elements of each partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 2,3,1,4,5,6 -> glom -> P0: [2,3,1,4,5,6]
		P1: 9,0,3,6,9,0 -> glom -> P1: [9,0,3,6,9,0]
		P2: 2,1,4,3,5,4	-> glom -> P2: [2,1,4,3,5,4]

		rdd1.count = 18 (int)	 rdd2.count = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			Flattens the elements of the iterables produced by the function.
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies a function on the entire partition.
				Partition to Partition transformation. 

		rdd1	     rdd2 = rdd1.mapPartitions(lambda p : sum(p) )

		P0: 2,3,1,4,5,6 -> mapPartitions
		P1: 9,0,3,6,9,0 -> mapPartitions
		P2: 2,1,4,3,5,4	-> mapPartitions

		rdd1.mapPartitions(lambda p:  map(lambda x: x*10, p)).collect()
		rdd1.mapPartitions(lambda p: [sum(p)]).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]	
				Similar to mapPartitions, but here we get partition-index as an additional
				function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()
     		rdd1.mapPartitionsWithIndex(lambda i, p:  map(lambda x: (i, x*10), p)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct elements of the input RDD. 

		rddWords.distinct().collect()
		rddWords.distinct(3).collect()

   
   8. sortBy			P: U -> V, Optional:  ascending (True/False), numPartitions
				Sorts the elements of the RDD based on the function output.

		rddWords.sortBy(lambda x: x[-1]).glom().collect()
		rddWords.sortBy(lambda x: x[-1], False).glom().collect()
		rddWords.sortBy(lambda x: x[-1], True, 3).glom().collect()

  
   Types of RDDs
   -------------
	1. Generic RDDs :  RDD[U]
	2. Pair RDDs:	   RDD[(K, V)]


   9. mapValues			P: U -> V
				Applied only to pair RDDs
				Transforms only the value part of the (K, V) pairs

		rdd3.mapValues(lambda x: x*10).collect()
	

   10. groupBy			P: U -> V, Optional: numPartitions
				Groups the elements of the RDD based on the function output


		rddWords.groupBy(len).mapValues(list).collect()
		rddWords.groupBy(lambda x: x[0]).mapValues(list).collect()
		rddWords.groupBy(lambda x: x[0], 2).mapValues(list).collect()


		Wordcount Program
		-----------------
		  rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len) \
              			.sortBy(lambda a: a[1], False, 1)


   11. randomSplit		P: List of ratios (ex: [0.6, 0.4]). Optional: seed
				Splits the rdd randomly into multiple RDDs in the specified ratio. 

		rddList = rdd1.randomSplit([0.3, 0.3, 0.4])
		rddList = rdd1.randomSplit([0.3, 0.3, 0.4], 564)    # 564 is the seed


   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of output RDD.
				Performs global shuffle.


   13. coalesce			P: numPartitions
				Used to only decrease the number of partitions of output RDD.
				Performs partition-merging.

	Recommendations
        ---------------
	-> The size of each partition should be around 128MB  (100 -150 MB)  
	-> The number of partitions should be a multiple of number of cores
	-> The number of cores in each executor should be 5

	
   14. partitionBy		P: numPartitions, Optional: Partitioning function : U -> Int

				-> Applied only to Pair RDD and partioning happend based on the keys.
				-> Used to control which keys go to which partitions
		                -> default partitioning fun: hash of the key

			transactions = [
    			{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    			{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    			{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

			def custom_partitioner(city): 
    				if (city == 'Chennai'):
        				return 0;
    				elif (city == 'Hyderabad'):
        				return 1;
    				elif (city == 'Vijayawada'):
        				return 1;
    				elif (city == 'Pune'):
        				return 2;
    				else:
        				return 3;      

			rdd1 = sc.parallelize(transactions, 3).map(lambda d: (d['city'], d))
			rdd1.glom().collect()

			rdd2 = rdd1.partitionBy(3, custom_partitioner)
			rdd2.glom().collect()
				

   15. union, intersection, subtract, cartesian

	Let us say, rdd1 has M partitions and rdd2 has N partitions

	command				# output partitions
        ---------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


   ..ByKey transformations
   -----------------------
	=> Are all wide transformations
	=> Applied to only Pair RDDs

    16. groupByKey		P: None,  Optional: numPartitions
				Groupsthe elements of the RDD based on the keys
				The output RDD is a Pair RDD with unique keys and grouped Values (ResultIterable) 

	rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              .flatMap(lambda x: x.split(" ")) \
              .map(lambda x: (x,1)) \
              .groupByKey() \
              .mapValues(len) \
              .sortBy(lambda a: a[1], False, 1)

	rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              .flatMap(lambda x: x.split(" ")) \
              .map(lambda x: (x,1)) \
              .groupByKey(1) \
              .mapValues(len) \
              .sortBy(lambda a: a[1], False)


    17. sortByKey		P: None, Optional: ascending (True/False), numPartitions		
				Sorts the RDD based on the key
		
		rddwc.sortByKey().collect()
		rddwc.sortByKey(False).collect()
		rddwc.sortByKey(True, 4).collect()

   
    18. reduceByKey	        P: (U, U) -> U, Optional: numPartitions		
				Reduces the different values of each unique key within the partition first and 
				across partitions by iterativly applying the reduce function.
				
		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.map(lambda x: (x,1)) \
              			.reduceByKey(lambda x, y: x + y) \
              			.sortBy(lambda a: a[1], False, 1)
	

   RDD actions
   ------------

    1. collect

    2. count
   
    3. saveAsTextFile

    4. reduce		    => P:  (U, U) -> U
			       Reduces the entire RDD into one value of the same type by iterativly
				applying the reduce on the objects of each partition first and then 
				reduces the reduced values of each partition.

		rdd1
		-------------------------
		P0: 3, 2, 1, 2, 3, 4, 5, 6 -> reduce -> -20 -> 43
		P1: 7, 8, 9, 0, 6, 4, 3, 2 -> reduce -> -25
		P2: 1, 4, 5, 6, 7, 8, 9, 0 -> reduce -> -38

		rdd1.reduce(lambda x, y: x - y)

		






	

	












