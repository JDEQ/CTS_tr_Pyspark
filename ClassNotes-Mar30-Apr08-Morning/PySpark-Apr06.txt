
 Agenda (8 sessions of 4 hours each) - PySpark
 ----------------------------------------------
   -> Spark : Basics & Architecture
   -> Spark Core (RDD API)
	-> RDD Transformations & Actions
	-> Shared Variables
   -> Spark SQL 
	-> DataFrames API
   -> Spark Streaming (Real-time analytics)
	-> Spark Streaming (DStreams API)
	-> Structured Streaming 
   -> Introduction to Spark MLLib

  Materials
  ----------
    -> PDF Presentations
    -> Code Modules 
    -> Class Notes
    -> Github: https://github.com/ykanakaraju/pyspark

  Introductions
  --------------
     -> Total experience - x years of experience
     -> Programming background (Python, Java, C#, ...)
     -> Spark: New/Beginner/Practitioner

  Spark
  -----
    
     -> Spark is a unified in-memory distributed computing engine/framework. 
     -> Is a cluster computing framework 
     -> Spark is used for Big Data Analytics. 

     -> Spark is written in 'Scala'.
   
     -> Spark is a polyglot
	 -> Spark apps can be written in Scala, Java, Python, R 



     Cluster => Is unified entity containing several nodes whose combined resources can be used to
                distribute your storage or processing. 

     In-memory => Intermediate results of tasks can be persisted in-memory and subsequent tasks can
		process these in-memory results (of previous tasks).

		-> Spark is upto 100x faster than MR (MapReduce) if 100% in-memory computation is used. 
		-> Spark is 6 to 7x faster than MR even if disk-based computations are used.    

     Unified Computing Engine
     ------------------------
     -> Provides a consistent set of API for processing different analytical workloads using the
	same engine using simple programming constructs. 

       	Batch Analytics of Unstructured Data	=> Spark Core API
	Batch Analytics of Structured Data	=> Spark SQL
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics (ML)		=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX


   Spark Architecture
   ------------------
     
     1. Cluster Manager

	-> Application are submitted to CM
	-> Schedules the job
	-> Allocates resources (RAM & CPU cores) to the application

	=> Supports Spark Standalone, YARN, Mesos, Kubernetes

    2.  Driver
	
	-> Master process
	-> Contains a "SparkContext" (which is an application context)
	-> Manages the user-code and sends the tasks to the executors

	Deploy-Modes
	  -> Client mode : default, Driver runs on the client machine.
	  -> Cluster mode : Driver runs on one of the node in the cluster.

    3. Executors
	
	-> Are allocated by Cluster Manager
	-> Tasks are executed in the executors
	-> Send the status to the driver.

    4. SparkContext

	-> Is an application context
	-> Is the starting point of executor
	-> Is the link between the driver and the various tasks on the cluster.


  Getting started with Spark
  ---------------------------

    1. Working in your vLab

	-> Lauching PySpark shell
		-> Open a terminal window and type "pyspark" command.
		-> this launches a pyspark shell.
	
        -> Launch 'Jupyter Notebook'           
		-> Open a terminal window and type "jupyter notebook"  or 
		   "jupyter notebook --allow-root" command.

    2. Setting up PySpark environment on your personal machine

	-> Make sure you have "Anaconda" installed.  
		url: https://www.anaconda.com/products/distribution

	-> Setting up pyspark with Jupyter Notebook or Spyder
		-> Follow the instruction given in the shared document.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (free account)

   	-> Signup: https://databricks.com/try-databricks
	-> Login: https://community.cloud.databricks.com/login.html


   Spark Core API & RDD
   ---------------------

    -> RDD :  Resilient Distributed Dataset
    -> Is the fundamental data abstraction of Spark Core API.
   
    -> Is a collection of distributed in-memory partition. 

   -> RDDs are lazily evaluation.	


    How to create RDDs ?
    ------------------   
    Three ways:

	1. From some external data file

	     	rdd1 = sc.textFile( <filePath>, [numPartitions] )
		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

        2. From programmatic data

		rdd1 = sc.parallelize( range(1, 100), 2)
		rdd1 = sc.parallelize( ["hello", "how" "are", "you"], 2)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*2)
	

    What can you do with an RDD ?
    ----------------------------

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.

    RDD Lineage DAG
    ----------------
    RDD Lineage DAG is a logical execution plan that tracks the operations to be performed to create the
    partitions of the RDD

    This tracks the heirarchy of dependencies all the way from the very first RDD. 


     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)      
	lineage DAG : rddFile -> sc.textFile

    rddWords = rddFile.flatMap(lambda x: x.split(" "))
	lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

    rddPairs = rddWords.map(lambda x: (x,1))
	lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile
    
    rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	
    

   RDD persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )     -> instruction to spark to persist rdd6 partitions. 
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	    => sc.textFile, t3, t5, t6 -> rdd6

        rdd7.collect()
	rdd7 -> rdd6.t7 
	    => t7 -> rdd7

        rdd6.unpersist()


       Storage Levels
       --------------
	1. MEMORY_ONLY		: default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		: Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.persist()   => memory-only persistence
	rdd1.cache()     => memory-only persistence

	rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	rdd1.unpersist()

  Executor's memory structure
  ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD
   
   RDD Transformations
   -------------------
	=> Transformations return an RDD
	=> Transformations do not cause execution
	=> They only cause creation of RDD Lineage Graphs. 


   1. map		P: U -> V
			object to object transformation
			transforms each object of the input RDD into an object of the output RDD by applying a function.
			input RDD: N objects, output RDD: N objects

		rddLists = rddFile.map(lambda x: x.split(" "))

   2. filter		P: U -> Boolean
			Only those object for which the function returns True will be in the output RDD
			input RDD: N objects, output RDD: <= N objects
		
		rddFile.filter(lambda x: len(x) > 51).collect()

   3. glom		P: None 
			Returns a list for each partition with all the elements of the partition
			input RDD: N objects, output RDD: = number of partitions

		rdd1		   rdd2 = rdd1.glom()
		P0: 3,2,4,6,5,7,8,9 -> glom -> P0: [3,2,4,6,5,7,8,9]
		P1: 5,4,8,9,0,5,7,0 -> glom -> P1: [5,4,8,9,0,5,7,0]
		P2: 6,2,4,3,8,6,9,9 -> glom -> P2: [6,2,4,3,8,6,9,9]

		rdd1.count() = 24 (int)	      rdd2.glom() = 3 (list)


   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: > N objects

  		 rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation.
			Applied a function on the entire partition to create the output partition

		rdd1       rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 8, 4, 1, 0, 4, 9, 7    -> mapPartitions -> P0: 80, 40, 10, 0, 40, 90, 70
		P1: 6, 4, 3, 1, 8, 7, 6    -> mapPartitions -> P1: 60, 40, 30, 10, 80, 70, 60
		P2: 0, 4, 0, 2, 3, 1, 2, 2 -> mapPartitions -> P2: 0, 40, 0, 20, 30, 10, 20, 20
		
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Is similar to mapPartition, but we have partition-index as addititonal parameter. 
				
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects

		rdd2.distinct().collect()
		rdd2.distinct(4).collect()
   
   Types of RDDs
   -------------
	Generic RDDs 	=> RDD[U]
	Pair RDD	=> RDD[(U, V)]

   8. mapValues			P: U -> V
				Applied only on Pair RDD
				Transforms only the value part of the pair RDD by applying the function

		rdd3 = rdd2.mapValues(lambda x: (x, x+5))

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Elements are sorted based on the function output. 
		
		rdd1.sortBy(lambda x: x > 5).glom().collect()
		rdd1.sortBy(lambda x: x > 5, False).glom().collect()
		rdd1.sortBy(lambda x: x > 5, True, 2).glom().collect()			


   10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD where the
				 key: unique values of the function output
				 value: ResultIterable object with all the RDD elements that returned the key.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

   		rddWords.groupBy(len, 1).mapValues(len).glom().collect()
    
   11. randomSplit		P: list of ratios (e.g: [0.5, 0.3, 0.2] )  Optional: seed
				Returns a list of RDDs ramdomly split from the RDD in the specified ratios.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 45435)  # 45435 is the seed. 

   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions of output RDD
				Cause global shuffle.

		rdd2 = rdd1.repartition(5)


   13. coalesce			P: numPartitions
				Is used to only decrease the number of partitions of output RDD	
				Cause partition merging.

		rdd2 = rdd1.coalesce(5)	


	Recommendations 
        ----------------
	-> The size of the partition should be around 128 MB
	-> The number of partitions should be a multiple of the number of cores.
	-> If the number of partitions is close to but less than 2000, bump it up to 2000
	-> The number of cores per executor should be 5


  14. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions, rdd2 has N partitions

	command				numPartitions
        ---------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


  15. partitionBy		P: numPartitions   Optional: partition-function (default: hash)
				Applied only on Pair RDDs
				USed to control which 'keys' go to which partition


transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;    

rdd1 = sc.parallelize(transactions, 3) \
        .map(lambda d: (d['city'], d)) \
        .partitionBy(4, custom_partitioner)


  ...ByKey Transformations
  ------------------------
        -> Are all wide transformations
	-> Applied only on pair PDDs
	

   16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the RDD based on the key

		 rddPairs.sortByKey().collect()
		 rddPairs.sortByKey(False).collect()
		 rddPairs.sortByKey(False, 5).collect()

   17. groupByKey		P: None, Optional: numPartitions
				Returns an RDD with unique-keys and grouped values.
				groupBy performs global-shuffle, hense very inefficient. Try to avoid it. 

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey(1) \
        		.mapValues(sum) \
        		.coalesce(1)

   18. reduceByKey		P: (U, U) -> U,  Optional: numPartitions
				Reduces all the values of each unique-key within each partition and then 
				across partitions by iterativly applying the function.

	     rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 3) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.coalesce(1)

    19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins		=> join (inner), leftOuterJoin, rightOuterJoin, fullOuterJoin
			   Operates on two pair RDD of same key-type.

			   RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

    21. cogroup 	  => Is used when you want to join pair RDDs with duplicate key and you want
			     unique-keys (with grouped values) in the output. 

				=> groupByKey (on both RDDs) -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
			=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
			=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) -> U
			Reduces the entire RDD into one final value of the same type by iterativly applying
			the function on each partition (narrow-op) and then across partitions (wide-op)
		rdd1
		P0: 9, 6, 4, 3, 2, 1 -> reduce -> -7  => 2
		P1: 9, 7, 0, 3, 1, 2 -> reduce -> -4
		P2: 8, 6, 0, 4, 2, 1 -> reduce -> -5

		rdd1.reduce(lambda x, y : x - y)  

   5. aggregate	
	Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

   6. take
		rdd1.take(15)

   7. takeOrdered 
		rddWc.takeOrdered(10)
		rddWc.takeOrdered(10, lambda x: len(x[0]))

   8. takeSample
		rdd1.takeSample(True, 10)
		rdd1.takeSample(True, 100)    	# where number of sample can be  > size of rdd
		rdd1.takeSample(True, 10, 5646) # 5646 is a seed

		rdd1.takeSample(False, 10)	: False: with out replacement  
		 
  9. countByValue

  10. countByKey

  11. first

  12. foreach      => P: function that does not return anything
		      Applies the function on all objects of the RDD.

  13. saveAsSequenceFile

 
   Use-Case
   --------	
    From cars.tsv dataset, find the average weight of models from makes of American origin only.
    -> Arrange the data in the DESC order pf average weight
    -> Save the output as a single text file. 
   
	 => Try to solve this use-case

    
   Closure
   -------

	A closure is all the code (variables and functions) that must be visible inside an executor for the
	tasks to perform their computations on the RDD. 

	-> The closure is serialized and separate copy is sent to every executor (local copy).

	c = 0

	def isPrime( n ):
		returns True  if n is prime
		returns False if n is not prime

	def f1( n ):
		global c
		if (isPrime(n)) c += 1
		return n*2

       	rdd1 = sc.parallelize( range(1, 4001), 4)
	
	rdd2 = rdd1.map( f1 )

       	rdd2.collect()
	
	print(c)      // c = 0

    
        Limitation: Local variables can not be used in a closure to implement global counter.



    Shared Variables
    ----------------

    1. Accumulator variable

		-> Maintained by the driver
		-> Not part of closure 9hnnse not a local copy)
		-> All tasks can add to this accumulator. 
		-> All tasks share one copy of the variable maintained at the driver side.
		-> Used to implement global counter


	c = sc.accumulator(0)

	def isPrime( n ):
		returns True  if n is prime
		returns False if n is not prime

	def f1( n ):
		global c
		if (isPrime(n)) c.add(1)
		return n*2

       	rdd1 = sc.parallelize( range(1, 4001), 4)
	
	rdd2 = rdd1.map( f1 )

       	rdd2.collect()
	
	print(c)      // c = 80


  2. Broadcast variable
		
		d = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, 6: f, ....})      # 100 MB

		def f1( n ):
			global d
			return d.value[n]

		rdd1 = sc.parallelize([1,2,3,4,5,...], 3)
		rdd2 = rdd1.map( f1 )
		rdd2.collect()              # a,b,c,d
    
  -------------------------------------------------

    spark-submit command
    --------------------

     Is a single command to submit any spark application (python, scala, java, R) to any
     cluster manager (local, standalone, yarn, mesos, kebernetes)

     spark-submit [options] <app jar | python file | R file> [app arguments]

     spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 4
		--executor-memory 5G
		--executor-cores 5
		--num-executors 10   
		E:\pyspark\wordcount.py [app args]   


     spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py
     spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt wcout 1        

  ================================================      
     Spark SQL  (pyspark.sql)
  ================================================
    
    => Is a high-level API built on top og Spark Core
    
    => Is Spark's 'stuctured data processing' API

	  Structured file formats : parquet (default), ORC, JSON, CSV (delimited text file)
	  JDBC Format : RDBMS, NoSQL
	  Hive : data warehousing platform on Hadoop

    => SparkSession
	  -> Represents a user-session within an application
	  -> An application (rpresented by SparkContext) can have multiple sessions (SparkSession)
	  -> Introduced in Spark 2.0

	spark  = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()  


    => DataFrame (DF)

	-> Data abstraction of Spark SQL

	-> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	
	-> Collection of 'Row' objects 
		-> Each row has a group of columns
		-> Columns are processed using Spark SQL internel type representations

	-> DF has two components
		1. Data		=> Collection of Row
		2. Schema	=> StructType object

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)


    Steps in a Spark SQL program
    -----------------------------

      1.  Read/load data from some data source into a DataFrame.

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

    
      2. Apply transformation of the DF using DF Transformation methods or using SQL

		Using DF API methods
                --------------------
		df2 = df1.select("userid", "name", "age", "gender") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)

		Using SQL
		---------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
         		from users
         		where age is not null
         		group by age
         		order by age
         		limit 4"""
         
		df3 = spark.sql(qry)
		df3.show()
 
      3. Write/Save the DF's data to some external destination. 

		df2.write.format("json").save(outputPath)
		df2.write.mode("overwrite").format("json").save(outputPath)

		df2.write.json(outputPath, mode="overwrite")
		df2.write.mode("overwrite").json(outputPath)


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


  Save Modes
  ----------

	By default, we get an Exception if we try to save an DF to an existing directory.

	-> ignore
	-> append
	-> overwrite

	df2.write.format("json").save(outputPath, mode="overwrite")
	df2.write.mode("overwrite").format("json").save(outputPath)

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)


  Working with different file formats
  -----------------------------------
     JSON
     ----
	read

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.json(outputPath)
		df2.write.mode("overwrite").json(outputPath)

     Parquet
     -------
	read

		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.parquet(outputPath)
		df2.write.mode("overwrite").parquet(outputPath)

     ORC
     ----
	read

		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.orc(outputPath)
		df2.write.mode("overwrite").orc(outputPath)

    CSV (delimited text file)
    -------------------------
	read
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)

		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df2.write.csv(outputPath, mode="overwrite", header=True)
		df2.write.csv(outputPath, mode="overwrite", header=True, sep="|")
  
     
   DF Transformations
   ------------------

   1. select
		#select with string parameters
		df2 = df1.select("ORIGIN_COUNTRY_NAME",	"DEST_COUNTRY_NAME", "count")

		#select with column parameters
		df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 	column("DEST_COUNTRY_NAME").alias("destination"),
                 	expr("count").cast("int"),
                 	expr("count + 10 as newCount"),
                 	expr("count > 200 as highFrequency"),
                	expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                     )

   2. where / filter

		df3 = df2.where("domestic = false and count > 500")
		df3 = df2.filter("domestic = false and count > 500")

		df3 = df2.filter( col("count") > 200 )

   3. orderBy / sort
		df3 = df2.orderBy("count", "destination")
		df3 = df2.orderBy(col("count").asc(), col("destination").desc())
		df3 = df2.orderBy(asc("count"), desc("destination"))

		df3 = df2.sort(asc("count"), desc("destination"))

   4. groupBy (with aggregation methods) => Returns a 'pyspark.sql.group.GroupedData' object

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").max("count")
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
         		.agg(   count("count").alias("count"),
               			max("count").alias("max"),
               			sum("count").alias("sum"),
               			avg("count").alias("avg") )
	

   5. limit
	
		df2 = df1.limit(10)

   6. selectExpr

		df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
                 		expr("DEST_COUNTRY_NAME as destination"),
                 		expr("count"),
                 		expr("count + 10 as newCount"),
                 		expr("count > 200 as highFrequency"),
                 		expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                 		)

		is same as:

		df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 		"DEST_COUNTRY_NAME as destination",
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 200 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

    7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int"))


	df6 = df5.withColumn("ageGroup", when( col("age") < 12, "child") \
                                 .when( col("age") < 20, "teenager") \
                                 .when( col("age") < 60, "adult")
                                 .otherwise("senior") )


    8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

   9. udf (user-defined-function)

	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"

	getAgeGroupUDF = udf(getAgeGroup, StringType())
	df6 = df5.withColumn("ageGroup", getAgeGroupUDF(col("age")) )
	----------------------------------------
        @udf(returnType=StringType()) 
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"


	df6 = df5.withColumn("ageGroup", getAgeGroup(col("age")) )
       ------------------------------------------
	spark.udf.register("get_age_group_udf", getAgeGroup, StringType())

	df5.createOrReplaceTempView("users")

	qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"
	spark.sql(qry).show()
	------------------------------------------


    10. drop	-> excludes the 'specified columns' in the output DF

	 	df3 = df2.drop("newCount", "highFrequency")


    11. dropna

		df3 = usersDf.dropna()
		df3 = usersDf.dropna(subset=["age", "phone"])

    12. dropDuplicates

		df3 = userDf.dropDuplicates()
		df3.show()

		df3 = userDf.dropDuplicates(["name", "age"])
		df3.show()

    13. distinct
		df1.distinct().count()
		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

    14. union, intersect, subtract

		df5 = df3.union(df3) 
		df6 = df5.subtract(df4)  
		df6 = df5.intersect(df4)  
		df6.show()

    15. sample

		df2 = df1.sample(True, 0.5)		# True : with-replacement sampling
		df2 = df1.sample(True, 1.5)		# fraction can be  > 1 for with-replacement sampling
		df2 = df1.sample(True, 0.5, 56757)	# 56757 is a seed

		df2 = df1.sample(False, 0.5, 56757)
		df2 = df1.sample(False, 1.5, 56757)	# ERROR: fraction must be in th range [0,1]
		

    16. randomSplit

		df10, df11, df12 = df1.randomSplit([1.0,2.0,3.0], 46)

		df10.count()
		df11.count()
		df12.count()

    17. repartition

		df2 = df1.repartition(8)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(4)
		df3.rdd.getNumPartitions()

		df3 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

    18. coalesce

		df5 = df2.coalesce(10)
		df5.rdd.getNumPartitions()

    19. joins => is discussed separatly



   Creating an RDD from DF
   ----------------------
	rdd1 = df1.rdd


   Creating a DF from programming data
   ------------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Creating a DF from RDD
   ----------------------

	rdd1 = spark.sparkContext.parallelize(listUsers)
	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])


   Creating a DF with programmatic schema
   --------------------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)

       ---------------------------------------------------
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)])


	df1 = spark.read.json(inputPath, schema=mySchema)  
	df1 = spark.read.schema(mySchema).json(inputPath)  


    Joins
    -----

     Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti
	
	
      left-semi join
      --------------
	-> Similar to inner join, but data comes only from left side table

	-> Equivalent to the following sub-query:
		select * from emp where deptid IN (select id from dept)


      left-anti join
      ---------------
	-> Equivalent to the following sub-query:
		select * from emp where deptid NOT IN (select id from dept)


employee = spark.createDataFrame([
    (1, "Raju", 25, 101),
    (2, "Ramesh", 26, 101),
    (3, "Amrita", 30, 102),
    (4, "Madhu", 32, 102),
    (5, "Aditya", 28, 102),
    (6, "Pranav", 28, 100)])\
  .toDF("id", "name", "age", "deptid")
  
employee.printSchema()
employee.show()  
  
department = spark.createDataFrame([
    (101, "IT", 1),
    (102, "ITES", 1),
    (103, "Opearation", 1),
    (104, "HRD", 2)])\
  .toDF("id", "deptname", "locationid")
  
department.show()  
department.printSchema()

# Using SQL method
employee.createOrReplaceTempView("emp")
department.createOrReplaceTempView("dept")

spark.catalog.listTables()

qry = """select emp.*
         from emp left anti join dept
         on emp.deptid = dept.id"""

joinedDf = spark.sql(qry)

joinedDf.show()


# Using DF API methods

joinCol = employee["deptid"] == department["id"]
joinedDf = employee.join(department, joinCol, "left_anti")


  Join Types:  Supported Joins: 
	inner, 
	left/left_outer, 
	right/right_outer, 
	full/full_outer, 
	left_semi, 
	left_anti
  
    Use-Case
    --------
     Use DF API methods: 

     From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average user-rating.
     -> Consider only those movies that are rated by atleast 30 users.
     -> Data: movieId, title, numberOfRatings, averageRating
     -> Arrange the data in the DESC order of averageRating
     -> Save the output as a single pipe-separated CSV file with header.

  










