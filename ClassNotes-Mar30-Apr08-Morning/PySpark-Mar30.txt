
 Agenda (8 sessions of 4 hours each) - PySpark
 ----------------------------------------------
   -> Spark : Basics & Architecture
   -> Spark Core (RDD API)
	-> RDD Transformations & Actions
	-> Shared Variable
   -> Spark SQL 
	-> DataFrames API
   -> Spark Streaming (Real-time analytics)
	-> Spark Streaming (DStreams API)
	-> Structured Streaming 
   -> Introduction to Spark MLLib

  Materials
  ----------
    -> PDF Presentations
    -> Code Modules 
    -> Class Notes
    -> Github: https://github.com/ykanakaraju/pyspark

  Introductions
  --------------
     -> Total experience - x years of experience
     -> Programming background (Python, Java, C#, ...)
     -> Spark: New/Beginner/Practitioner

  Spark
  -----
    
     -> Spark is a unified in-memory distributed computing engine/framework. 
     -> Is a cluster computing framework 
     -> Spark is used for Big Data Analytics. 

     -> Spark is written in 'Scala'.
   
     -> Spark is a polyglot
	 -> Spark apps can be written in Scala, Java, Python, R 



     Cluster => Is unified entity containing several nodes whose combined resources can be used to
                distribute your storage or processing. 

     In-memory => Intermediate results of tasks can be persisted in-memory and subsequent tasks can
		process these in-memory results (of previous tasks).

		-> Spark is upto 100x faster than MR (MapReduce) if 100% in-memory computation is used. 
		-> Spark is 6 to 7x faster than MR even if disk-based computations are used.    

     Unified Computing Engine
     ------------------------
     -> Provides a consistent set of API for processing different analytical workloads using the
	same engine using simple programming constructs. 

       	Batch Analytics of Unstructured Data	=> Spark Core API
	Batch Analytics of Structured Data	=> Spark SQL
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics (ML)		=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX


   Spark Architecture
   ------------------
     
     1. Cluster Manager

	-> Application are submitted to CM
	-> Schedules the job
	-> Allocates resources (RAM & CPU cores) to the application

	=> Supports Spark Standalone, YARN, Mesos, Kubernetes

    2.  Driver
	
	-> Master process
	-> Contains a "SparkContext" (which is an application context)
	-> Manages the user-code and sends the tasks to the executors

	Deploy-Modes
	  -> Client mode : default, Driver runs on the client machine.
	  -> Cluster mode : Driver runs on one of the node in the cluster.

    3. Executors
	
	-> Are allocated by Cluster Manager
	-> Tasks are executed in the executors
	-> Send the status to the driver.

    4. SparkContext

	-> Is an application context
	-> Is the starting point of executor
	-> Is the link between the driver and the various tasks on the cluster.


  Getting started with Spark
  ---------------------------

    1. Working in your vLab

	-> Lauching PySpark shell
		-> Open a terminal window and type "pyspark" command.
		-> this launches a pyspark shell.
	
        -> Launch 'Jupyter Notebook'           
		-> Open a terminal window and type "jupyter notebook"  or 
		   "jupyter notebook --allow-root" command.

    2. Setting up PySpark environment on your personal machine

	-> Make sure you have "Anaconda" installed.  
		url: https://www.anaconda.com/products/distribution

	-> Setting up pyspark with Jupyter Notebook or Spyder
		-> Follow the instruction given in the shared document.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (free account)

   	-> Signup: https://databricks.com/try-databricks
	-> Login: https://community.cloud.databricks.com/login.html


   Spark Core API & RDD
   ---------------------

    -> RDD :  Resilient Distributed Dataset
    -> Is the fundamental data abstraction of Spark Core API.
   
    -> Is a collection of distributed in-memory partition. 	


    How to create RDDs
    ------------------   
    Three ways:

	1. From some external data file

	     rdd1 = sc.textFile( <filePath>, [numPartitions] )

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )


        2. From programmatic data

		rdd1 = sc.parallelize( range(1, 100), 2)
		rdd1 = sc.parallelize( ["hello", "how" "are", "you"], 2)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*2)
	

    What can you do with an RDD
    ----------------------------


    RDD Lineage DAG
    ----------------



  

    

    
   


  



  














