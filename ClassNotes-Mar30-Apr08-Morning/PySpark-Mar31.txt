
 Agenda (8 sessions of 4 hours each) - PySpark
 ----------------------------------------------
   -> Spark : Basics & Architecture
   -> Spark Core (RDD API)
	-> RDD Transformations & Actions
	-> Shared Variable
   -> Spark SQL 
	-> DataFrames API
   -> Spark Streaming (Real-time analytics)
	-> Spark Streaming (DStreams API)
	-> Structured Streaming 
   -> Introduction to Spark MLLib

  Materials
  ----------
    -> PDF Presentations
    -> Code Modules 
    -> Class Notes
    -> Github: https://github.com/ykanakaraju/pyspark

  Introductions
  --------------
     -> Total experience - x years of experience
     -> Programming background (Python, Java, C#, ...)
     -> Spark: New/Beginner/Practitioner

  Spark
  -----
    
     -> Spark is a unified in-memory distributed computing engine/framework. 
     -> Is a cluster computing framework 
     -> Spark is used for Big Data Analytics. 

     -> Spark is written in 'Scala'.
   
     -> Spark is a polyglot
	 -> Spark apps can be written in Scala, Java, Python, R 



     Cluster => Is unified entity containing several nodes whose combined resources can be used to
                distribute your storage or processing. 

     In-memory => Intermediate results of tasks can be persisted in-memory and subsequent tasks can
		process these in-memory results (of previous tasks).

		-> Spark is upto 100x faster than MR (MapReduce) if 100% in-memory computation is used. 
		-> Spark is 6 to 7x faster than MR even if disk-based computations are used.    

     Unified Computing Engine
     ------------------------
     -> Provides a consistent set of API for processing different analytical workloads using the
	same engine using simple programming constructs. 

       	Batch Analytics of Unstructured Data	=> Spark Core API
	Batch Analytics of Structured Data	=> Spark SQL
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics (ML)		=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX


   Spark Architecture
   ------------------
     
     1. Cluster Manager

	-> Application are submitted to CM
	-> Schedules the job
	-> Allocates resources (RAM & CPU cores) to the application

	=> Supports Spark Standalone, YARN, Mesos, Kubernetes

    2.  Driver
	
	-> Master process
	-> Contains a "SparkContext" (which is an application context)
	-> Manages the user-code and sends the tasks to the executors

	Deploy-Modes
	  -> Client mode : default, Driver runs on the client machine.
	  -> Cluster mode : Driver runs on one of the node in the cluster.

    3. Executors
	
	-> Are allocated by Cluster Manager
	-> Tasks are executed in the executors
	-> Send the status to the driver.

    4. SparkContext

	-> Is an application context
	-> Is the starting point of executor
	-> Is the link between the driver and the various tasks on the cluster.


  Getting started with Spark
  ---------------------------

    1. Working in your vLab

	-> Lauching PySpark shell
		-> Open a terminal window and type "pyspark" command.
		-> this launches a pyspark shell.
	
        -> Launch 'Jupyter Notebook'           
		-> Open a terminal window and type "jupyter notebook"  or 
		   "jupyter notebook --allow-root" command.

    2. Setting up PySpark environment on your personal machine

	-> Make sure you have "Anaconda" installed.  
		url: https://www.anaconda.com/products/distribution

	-> Setting up pyspark with Jupyter Notebook or Spyder
		-> Follow the instruction given in the shared document.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (free account)

   	-> Signup: https://databricks.com/try-databricks
	-> Login: https://community.cloud.databricks.com/login.html


   Spark Core API & RDD
   ---------------------

    -> RDD :  Resilient Distributed Dataset
    -> Is the fundamental data abstraction of Spark Core API.
   
    -> Is a collection of distributed in-memory partition. 

   -> RDDs are lazily evaluation.	


    How to create RDDs ?
    ------------------   
    Three ways:

	1. From some external data file

	     	rdd1 = sc.textFile( <filePath>, [numPartitions] )
		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

        2. From programmatic data

		rdd1 = sc.parallelize( range(1, 100), 2)
		rdd1 = sc.parallelize( ["hello", "how" "are", "you"], 2)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*2)
	

    What can you do with an RDD ?
    ----------------------------

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.

    RDD Lineage DAG
    ----------------
    RDD Lineage DAG is a logical execution plan that tracks the operations to be performed to create the
    partitions of the RDD

    This tracks the heirarchy of dependencies all the way from the very first RDD. 


     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)      
	lineage DAG : rddFile -> sc.textFile

    rddWords = rddFile.flatMap(lambda x: x.split(" "))
	lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

    rddPairs = rddWords.map(lambda x: (x,1))
	lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile
    
    rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	
    

   RDD persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )        --> instruction to spark to persist rdd6 partitions. 
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	    => sc.textFile, t3, t5, t6 -> rdd6

        rdd7.collect()
	rdd7 -> rdd6.t7 
	    => t7 -> rdd7

       rdd6.unpersist()


       Storage Levels
       --------------
	1. MEMORY_ONLY		: default, Memory Serialized 1x Replication
	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replication
	3. DISK_ONLY		: Disk Serialized 1x Replication
	4. MEMORY_ONLY_2	: Memory Serialized 2x Replication
	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replication


	Commands
	--------
	rdd1.persist()   => memory-only persistence
	rdd1.cache()     => memory-only persistence

	rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	rdd1.unpersist()	



  Executor's memory structure
  ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)



   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   
   RDD Transformations
   -------------------
	=> Transformations return an RDD
	=> Transformations do not cause execution
	=> They only cause creation of RDD Lineage Graphs. 


   1. map		P: U -> V
			object to object transformation
			transforms each object of the input RDD into an object of the output RDD by applying a function.
			input RDD: N objects, output RDD: N objects

		rddLists = rddFile.map(lambda x: x.split(" "))

   2. filter		P: U -> Boolean
			Only those object for which the function returns True will be in the output RDD
			input RDD: N objects, output RDD: <= N objects
		
		rddFile.filter(lambda x: len(x) > 51).collect()

   3. glom		P: None 
			Returns a list for each partition with all the elements of the partition
			input RDD: N objects, output RDD: = number of partitions

		rdd1		   rdd2 = rdd1.glom()
		P0: 3,2,4,6,5,7,8,9 -> glom -> P0: [3,2,4,6,5,7,8,9]
		P1: 5,4,8,9,0,5,7,0 -> glom -> P1: [5,4,8,9,0,5,7,0]
		P2: 6,2,4,3,8,6,9,9 -> glom -> P2: [6,2,4,3,8,6,9,9]

		rdd1.count() = 24 (int)	      rdd2.glom() = 3 (list)


   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: > N objects

  		 rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation.
			Applied a function on the entire partition to create the output partition

		rdd1       rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 8, 4, 1, 0, 4, 9, 7    -> mapPartitions -> P0: 80, 40, 10, 0, 40, 90, 70
		P1: 6, 4, 3, 1, 8, 7, 6    -> mapPartitions -> P1: 60, 40, 30, 10, 80, 70, 60
		P2: 0, 4, 0, 2, 3, 1, 2, 2 -> mapPartitions -> P2: 0, 40, 0, 20, 30, 10, 20, 20
		
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Is similar to mapPartition, but we have partition-index as addititonal parameter. 
				
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects

		rdd2.distinct().collect()
		rdd2.distinct(4).collect()

   




 




