
  PySpark Curriculum - 7 sessions
  --------------------------------

  	Spark - Understanding & Architecture
	Spark Core API - RDD API
	   -> RDD Transformations & Actions
	Spark SQL
	Machine Learning & Spark MLlib
	Spark Streaming - Intro.

 ------------------------------------------------------------------------

  Big Data -> Is 'some of kind data' because of that the storage and processing of the data
	      becomes impossible/very inefficient for systems that are based on single-server
	      architecture.


    Data Properties:
	-> Volume   ( 100GB's .. )
        -> Velocity ( 20% data growth, streaming analytics )
	-> Variety  ( 80 - 90% of all data is unstructured/semi-structured)
 	     -> log-files, voice-files, image processing, sensor data processing

  -> 'Big Data' is a problem statement. 


   What is a Cluster?
   ------------------
     A set of nodes whose cumulative resources can be used to distribute storage and 
     processing. This is a unified system managed by some cluster manager tools.

   
   Big Data Solution - Hadoop
   ---------------------------

	-> Solution to big-data is to use cluster architectures, where you take the resources
           from many nodes.

	-> The 'hadoop' framework provides a solution to storage and processing of big data
           using  a cluster of commodity hardware. 
	
		-> HDFS : Distributed Storage Framework
			-> Splits the file into blocks of 128 MB each and distributes them
			   across many machines

		-> MapReduce: Distributed Processing Framework
			      (Cluster Computing Framework)
			-> Distributing computating solution.
    
   What is a Framework ?
   ---------------------
    	-> Set of libraries that solve a specific problem.
	-> Developer use these frameworks to write their programs in a specific way 
           defined by the framework. 
	-> Framework can take care of how to execute that code.	
	

   Disk based computation Vs im-memory computation
   ------------------------------------------------

	Disk-based computation the intermediate results of each tasks are stored on the disk.
	Subsequent tasks will pick up the data from disk into memory, process that and again
        write the output on disk.

	In-memory computation stores the results of all intermediate tasks in-memory (RAM) and 
        on those in-memory partitions it can launch subsequent tasks. 

   
   What is Spark ?
   ---------------
     -> Spark is a unified in-memory distributed computing framework.
	
	-> Spark is written in 'Scala'
	-> Spark is a polyglot. Spark programs can be written Scala, Java, Python & R


         (Bench marked by Databricks)
	 -> Spark is 100x faster than MR (on TB scale data) when 100% in-memory computation is used.
	 -> Spark is 6 to 7x faster than MR when disk-based computation is used.

	
   Spark's unified framework
   -------------------------
	
     -> Spark provides a consistent set of APIs to perform different analytics workloads.

	Batch processing of unstructured data 	=> Spark Low Level API (RDDs)
	Batch processing of Structured data	=> Spark SQL
	Streaming data processing		=> Spark Streaming, Spark Structured Streaming
	Predictive Analytics/Machine Learning	=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX


   Spark Building Blocks & Architecture
   ------------------------------------

     1. Driver 	
	    -> master process which manages all the tasks
	    -> maintains the user-code
	        -> maintains all the meta-data (RDD lineage DAGs)
	    -> contains "SparkContext" object (or 'SparkSession' object in case SparkSQL)


	    Deploy-Modes
	    ------------
	    1. client mode: (default) the driver process runs on the client machine
	    2. Cluster mode: the driver process runs on one of the nodes in the cluster	

	 	
     2. Executor Processes
	   -> The tasks (such as map, filter, ...) are executed in the executor processes
	   -> Every task does the same work on one partition of the data.


     3. Cluster Manager (CM)
	   -> Spark applications are submitted to CM (such as YARN)
	   -> Spark supports 4 different CMs
		-> Spark Standalone Scheduler, YARN, Mesos, Kubernites
	   -> Allocates some containers for the job to which the driver can send tasks
	      to executed.

     4. SparkContext
	   -> is created inside the driver process
	   -> represents an application context and cluster connection with specified configurations.
	   -> is the link between driver and exectutor processes
		
        
    Spark Layered Model
    -------------------
       Programming Lang      => Scala, Python, Java, R, SQL	
       Spark High Level API  =>	Spark SQL, Spark MLlib, Spark Streaming, Spark GraphX
       Spark Core API        => Low Level API (RDDs)
       Cluster Manager       => Standalone, YARN, Mesos, Kubernetes
       Storage Level         => Linux, Hadoop, AWS S3, Kafka, MongoDB



   RDD (Resilient Distributed Dataset)
   -----------------------------------

     -> Main data abstraction in Spark Core API

     -> RDD is a collection of distributed partitions.
	 -> Each partition is a collection of some type of objects (int, string, order, employee)

     -> RDDs are immutable
	
     -> RDDs are lazily evaluated.  
	  -> Actual execution will be differed until the last moment when the output is actually
	     request using an action command.
	  -> Transformations does not cause execution, Only Actions cause execution.

     -> RDDs are resilent
	   -> RDDs are resilient to missing or un-available in-memory partitions.
	   -> If a required partition is not found, it will launch the required tasks
	      to compute the partitions (using lineage DAGs)

     -> RDD
	  -> Metadata: Lineage DAG 
		 -> maintained by driver
	         -> created when a transformation is seen.

	  -> Data: Partitions of RDDs
		-> Created by launched tasks in reponse to action command.

  
   Different ways of working with PySpark
   --------------------------------------

    -> Using PySpark Shell.

	-> Simply install Anaconda distribution for Python, install Spark and setup some environment
	   variables.

	Environment Variables:
        ----------------------
	PYTHONPATH  -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%
	SPARK_HOME  -> C:\Apps\spark-3.0.0-bin-hadoop2.7
	HADOOP_HOME -> C:\Apps\spark-3.0.0-bin-hadoop2.7

        Add this to PATH environment variable:

	   -> C:\Apps\spark-3.0.0-bin-hadoop2.7\bin

    -> Use an IDE such as Spyder or Jupyter Notebooks environment

    -> Signup to Databrick's Community Edition 	(Free Account)

   
   How to create RDDs
   ------------------

   3 ways:

	1. From some external files. 
		rdd1 = sc.textFile(<file>, [num of partitions])

		default number of partitions is decided by  "sc.defaultMinPartitions"

	2. From programmatic data.
		rdd2 = sc.parallelize([2,3,2,4,,5,6,7,8,9,0,0,3,4,5,6,7,8], [num of partitions])

		default number of partitions is decided by  "sc.defaultParallelism"
	

	3. By applying transformation on an existing RDD 
		rdd3 = rdd2.map(lambda x: x*2)
 

   
   What can we do to an RDD ?
   --------------------------
 
    Two things:

	1. RDD Transformations
	    -> Creates an other RDD
	    -> Transformations does not cause immediate execution.
	    -> Transformations only produce meta-data (RDD lineage DAGs)

	2. RDD Actions
	    -> Action trigger execution of the tasks (and hense cause the creation of partitions)
	    -> Actions produce some output.

   NOTE:
   -----

        <rdd>.getNumPartitions()   => give u number of partitions of the RDD


   RDD Lineage DAG
   ---------------

      -> The driver process maintains the lineage of all RDDs that are created. 
	 -> the lineage tracks the heirarchy of RDDs that created this this RDD all the
	    way from the very first RDD


	rddFile = sc.textFile(file)	
		DAG: rddFile -> sc.textFile
	
   	rdd2 = rddFile.map(lambda x: x.upper())
		DAG: rdd2 -> rddFile -> sc.textFile

	rdd3 = rdd2.map(lambda x: len(x))
		DAG: rdd3 -> rdd2 -> rddFile -> sc.textFile

	rdd3.collect()
	
	  -> rdd3 -> rdd2.map -> rddFile.map -> sc.textFile
		tasks: sc.textFile (rddFile) -> map (rdd2) -> map (rdd3) ---> collect()

	
    DAG Visualization:
    ------------------
	-> DAG can be visualized form the Web UI
	-> <rdd>.toDebugString()



    Types of Transformations
    ------------------------

	1. Narrow
		-> Partition to Partition transformations
		-> There is no shuffling of data
		-> Output RDD will have same number of partition as that of input RDD
		-> Ex: map, filter, flatmap

	2. Wide
		-> Each partition requires data from multiple input partitions
		-> Shuffling of data is involved
		-> Output RDD can have different number of partition than that of input RDD
   		-> Ex: distinct, sortBy, ..ByKey 


    RDD Transformations
    -------------------   
        --> output of every transformation is an RDD 

	
     1. map			F: U -> V 
				Element to element transformation 
				Input RDD: N elements,  Output RDD: N elements
				

     2. glom			Parameters: None
				Creates an array object with all elements of the partition.
				Input RDD: N elements,  Output RDD: # of partitions
				


		rdd1		   rdd2 = rdd1.glom()
		P0: 1,2,3,4   -> glom ->  P0: [1,2,3,4]
		P1: 3,2,4,1   -> glom ->  P1: [3,2,4,1]
		P2: 5,7,2,9   -> glom ->  P2: [5,7,2,9]
    

    3. filter			F: U -> Boolean
				The ouput partition contains only those elements for which
				the function returns True.
				Input RDD: N elements,  Output RDD: <= N elements


    4. distinct			Parameter: None, optionally takes the number of partitions of the output RDD
				Input RDD: N elements,  Output RDD: <= N elements
		

    5. flatMap			F: U -> Iterable[V]   (iterable means some collection)
				Flattens all the elements of the output iterable of the function.

    6. mapPartitions		F: Iterator[U] -> Iterator[V]
				The function input takes all the elements of each partition.


    7. mapPartitionsWithIndex   F: index, Iterator[U] -> Iterator[V]
				The function input takes two params:
				  1. partition-index (int)
				  2. all the elements of each partition (iterable).
	
		rdd10.mapPartitionsWithIndex(lambda index, data: [(index,  sum(data))] )


    8. sortBy    		F: U -> V, where V is the key based on which elements are
				sorted. Optionally you can provide Sorting Order (True/False) as
                                second param, and number of partitions as third parameter. 

				rdd2 = rdd1.sortBy(lambda x: x, True, 4)

				Input RDD: N elements,  Output RDD: N elements



    => From Usage perspective RDD can be two types:
	 -> Generic RDDs       	ex:  RDD[U]
	 -> Pairs RDDs		ex: RDD[(U, V)]


    9. groupBy			F: U -> V, where V is the key based on which elements are aggregated
				as (Key, Value) pairs, where Key is the output of the function, and
				aggregated values are the values that produced that key.

				rdd2 = rdd1.groupBy(U -> V)  
				=>  rdd1: RDD[U], rdd2: RDD[(V, Iterable[U])]

			       Input RDD: N elements,  Output RDD: Number of unique keys produced by the function


    10. mapValues		-> Used only pair RDDs.
				The function is applied only to the values part of the (K, V) pairs.


    11. randomSplit		-> Splits the RDD into an array of RDDs in the specified ratios.
				   Optionally, use a seed to produce the same output across multiple 
				   executions.

				  rddArray = rdd1.randomSplit([0.5, 0.5])

    12. repartition(n)		-> Used to increase (or decrease) the number of partitions of the output RDD. 
				   This is usful to agjust the partition size in case of narrow tramsformations.
				   Cause global shuffle, hense very expensive

    13. coalesce(n)		-> Used to decrease the number of partitions of the output RDD. 
				   Causes partition merging, hense less shuffling.

    14. partitionBy		-> Applied only on Pair RDDs
				   It will create the partitions by applying the partitioning logic based 
				   on the values of the key.

				Take two parameters:

				1. number of partitions
				2. partitioning function (optional)
					-> if no function is provided, the default hash function is used.
					
    15. union, intersection, subtract, cartesian
				-> Paramater: rdd
 				
	  Let us say, we have rdd1 with M partitions, & rdd2 with N partitions

	   command				number of ouput partitions
	------------------------		---------------------------
	  rdd1.union(rdd2)			     	M + N				
	  rdd1.intersection(rdd2)			M + N
	  rdd1.subtract(rdd2)				M + N
	  rdd1.cartesian(rdd2)				M * N

 
     ....ByKey Transformations

	-> They operate only on Pair RDDs
	-> The apply some functionality of the values of each unique key.
	-> They are all wide transformations
	-> They all optionally take the number of partitions (of output RDD) as parameter.


   16.  groupByKey		-> F: None	
				   Aggregates all the values of each unique key.	
				   RDD[(U,V)].groupByKey() => RDD[(U, Iterable[V])]

				   Results in global shuffle and is very inefficient.
				   AVOID IT if you can..		
		   

    17. sortByKey	      -> Sorts the elements based on the key.

	output = sc.textFile(data_path) \
            		.flatMap(lambda x: x.split(" ")) \
            		.map(lambda x: (x, 1)) \
            		.groupByKey() \
            		.mapValues(sum) \
			.sortByKey()

    18. reduceByKey

			

    

   Hash Function:

	-> Takes the hashcode of the 'key' of the (k, v) pairs
	-> The modulus of that hashcode with the number of partitions decides which partition that (K,V) pair goes to.



    RDD Persistence
    ---------------

       RDD persistence refers to an active instruction to spark to save the partitions of
       the RDD 'in-memory' or 'on-disk' so that they are not subjected to eviction. 

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t1()
	rdd3 = rdd2.t2()
	rdd4 = rdd2.t3()
	rdd5 = rdd4.t4()
	rdd6 = rdd4.t5()
	rdd7 = rdd6.t6()

	rdd7.persist(<Storage-Level>) 

	rdd8 = rdd7.t7()       

	rdd8.collect()

	DAG of rdd8: rdd8 -> rdd7.t7 -> rdd6.t6 -> rdd4.t5 -> rdd2.t3 -> rdd1.t1 -> sc.textFile
  
        execution:  sc.textFile (rdd1) -> t1 (rdd2) -> t3 (rdd4) -> t5 (rdd6) -> t6 (rdd7) -> t7 (rdd8)
               -> sc.textFile, t1, t3, t5, t6, t7  (in multiple stages)
  

        rdd7.collect()  -> rdd7 -> rdd6.t6 -> rdd4.t5 -> rdd2.t3 -> rdd1.t1 -> sc.textFile
	       -> sc.textFile, t1, t3, t5, t6
  

         Types of Persistence
         --------------------
		1. In-Memory Deserialized Format
		2. In-Memory Serialized Format
		3. On Disk


         RDD Perssitence Commands
         ------------------------
	   ->   <rdd>.persist( StorageLevel.MEMORY_ONLY )
		<rdd>.cache() -> same as persist( StorageLevel.MEMORY_ONLY )


         RDD Persistence Storage Level
         -----------------------------
 		<rdd>.persist(pyspark.storage.StorageLevel.<STORAGE-LEVEL>)


	  -> MEMORY_ONLY	: RDD partitions are stored in RAM, if enough RAM is available.
				  If not, RDD may not bepersisted at all or partially persisted.
				   -> Deserialized. 
				   -> Default 

	  -> MEMORY_AND_DISK	: RDD partitions are stored in RAM, if enough RAM is available.
				  If not, RDD is persisted in Disk.
				   -> Deserialized.

	  -> DISK_ONLY		: RDD partitions are stored on Disk

	  -> MEMORY_ONLY_SER	: RDD partitions are stored in RAM, if enough RAM is available.
				  If not, RDD may not be or partially persisted
				   -> Serialized. 


	  -> MEMORY_AND_DISK_SER : RDD partitions are stored in RAM, if enough RAM is available.
				   If not, RDD is persisted in Disk.
				   -> Serialized.
	
	  -> MEMORY_ONLY_2	: Same as MEMORY_ONLY, except that 2 copies are stored on two
				  different executor nodes. 

	  -> MEMORY_AND_DISK_2  : Same as MEMORY_AND_DISK, except that 2 copies are stored on two
				  different executor nodes. 



      Executor Memory Structure
      -------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html


	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)




    Guidelines to decide on partitions
    ----------------------------------
  
	-> Don’t have too big partitions – your job will fail due to 2 GB limit 
	   on shuffle blocks. 

	-> Don’t have too few partitions – your job will be slow, not using 
	   parallelism 

	-> Rule of thumb: ~ 128 MB per partition is a good size 

	-> If number of partitions closer to but less than 2000, bump them 
	   up to more than 2000 (Spark uses different data structures if 
	   partitions are more than 2000) 

	-> Have around 2-3x more partitions than the avaialble cores.
















