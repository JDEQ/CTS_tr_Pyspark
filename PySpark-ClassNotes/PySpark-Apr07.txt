
  PySpark Curriculum - 7 sessions
  --------------------------------

  	Spark - Understanding & Architecture
	Spark Core API - RDD API
	   -> RDD Transformation & Action
	Spark SQL
	Machine Learning & Spark MLlib
	Spark Streaming - Intro.

 ------------------------------------------------------------------------

  Big Data -> Is 'some of kind data' because of that the storage and processing of the data
	      becomes impossible/very inefficient for systems that are based on single-server
	      architecture.


    Data Properties:
	-> Volume   ( 100GB's .. )
        -> Velocity ( 20% data growth, streaming analytics )
	-> Variety  ( 80 - 90% of all data is unstructured/semi-structured)
 	     -> log-files, voice-files, image processing, sensor data processing

  -> 'Big Data' is a problem statement. 


   What is a Cluster?
   ------------------
     A set of nodes whose cumulative resources can be used to distribute storage and 
     processing. This is a unified system managed by some cluster manager tools.

   
   Big Data Solution - Hadoop
   ---------------------------

	-> Solution to big-data is to use cluster architectures, where you take the resources
           from many nodes.

	-> The 'hadoop' framework provides a solution to storage and processing of big data
           using  a cluster of commodity hardware. 
	
		-> HDFS : Distributed Storage Framework
			-> Splits the file into blocks of 128 MB each and distributes them
			   across many machines

		-> MapReduce: Distributed Processing Framework
			      (Cluster Computing Framework)
			-> Distributing computating solution.
    
   What is a Framework ?
   ---------------------
    	-> Set of libraries that solve a specific problem.
	-> Developer use these frameworks to write their programs in a specific way 
           defined by the framework. 
	-> Framework can take care of how to execute that code.	
	

   Disk based computation Vs im-memory computation
   ------------------------------------------------

	Disk-based computation the intermediate results of each tasks are stored on the disk.
	Subsequent tasks will pick up the data from disk into memory, process that and again
        write the output on disk.

	In-memory computation stores the results of all intermediate tasks in-memory (RAM) and 
        on those in-memory partitions it can launch subsequent tasks. 

   
   What is Spark ?
   ---------------
     -> Spark is a unified in-memory distributed computing framework.
	
	-> Spark is written in 'Scala'
	-> Spark is a polyglot. Spark programs can be written Scala, Java, Python & R


         (Bench marked by Databricks)
	 -> Spark is 100x faster than MR (on TB scale data) when 100% in-memory computation is used.
	 -> Spark is 6 to 7x faster than MR when disk-based computation is used.

	
   Spark's unified framework
   -------------------------
	
     -> Spark provides a consistent set of APIs to perform different analytics workloads.

	Batch processing of unstructured data 	=> Spark Low Level API (RDDs)
	Batch processing of Structured data	=> Spark SQL
	Streaming data processing		=> Spark Streaming, Spark Structured Streaming
	Predictive Analytics/Machine Learning	=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX


   Spark Building Blocks & Architecture
   ------------------------------------

     1. Driver 	
	    -> master process which manages all the tasks
	    -> maintains the user-code
	        -> maintains all the meta-data (RDD lineage DAGs)
	    -> contains "SparkContext" object (or 'SparkSession' object in case SparkSQL)


	    Deploy-Modes
	    ------------
	    1. client mode: (default) the driver process runs on the client machine
	    2. Cluster mode: the driver process runs on one of the nodes in the cluster	

	 	
     2. Executor Processes
	   -> The tasks (such as map, filter, ...) are executed in the executor processes
	   -> Every task does the same work on one partition of the data.


     3. Cluster Manager (CM)
	   -> Spark applications are submitted to CM (such as YARN)
	   -> Spark supports 4 different CMs
		-> Spark Standalone Scheduler, YARN, Mesos, Kubernites
	   -> Allocates some containers for the job to which the driver can send tasks
	      to executed.

     4. SparkContext
	   -> is created inside the driver process
	   -> represents an application context and cluster connection with specified configurations.
	   -> is the link between driver and exectutor processes
		
        
    Spark Layered Model
    -------------------
       Programming Lang      => Scala, Python, Java, R, SQL	
       Spark High Level API  =>	Spark SQL, Spark MLlib, Spark Streaming, Spark GraphX
       Spark Core API        => Low Level API (RDDs)
       Cluster Manager       => Standalone, YARN, Mesos, Kubernetes
       Storage Level         => Linux, Hadoop, AWS S3, Kafka, MongoDB



   RDD (Resilient Distributed Dataset)
   -----------------------------------

     -> Main data abstraction in Spark Core API

     -> RDD is a collection of distributed partitions.
	 -> Each partition is a collection of some type of objects (int, string, order, employee)

     -> RDDs are immutable
	
     -> RDDs are lazily evaluated.  (TO BE DISCUSSED)



   
   How to create RDDs
   ------------------

   3 ways:

	1. From some external files. 
		rdd1 = sc.textFile(<file>, [num of partitions])

	2. From programmatic data.
		rdd2 = sc.parallelize([2,3,2,4,,5,6,7,8,9,0,0,3,4,5,6,7,8], [num of partitions])

	3. By applying transformation on an existing RDD 
		rdd3 = rdd2.map(lambda x: x*2)
 







  
  