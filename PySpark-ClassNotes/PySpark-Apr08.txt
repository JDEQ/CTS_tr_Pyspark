
  PySpark Curriculum - 7 sessions
  --------------------------------

  	Spark - Understanding & Architecture
	Spark Core API - RDD API
	   -> RDD Transformations & Actions
	Spark SQL
	Machine Learning & Spark MLlib
	Spark Streaming - Intro.

 ------------------------------------------------------------------------

  Big Data -> Is 'some of kind data' because of that the storage and processing of the data
	      becomes impossible/very inefficient for systems that are based on single-server
	      architecture.


    Data Properties:
	-> Volume   ( 100GB's .. )
        -> Velocity ( 20% data growth, streaming analytics )
	-> Variety  ( 80 - 90% of all data is unstructured/semi-structured)
 	     -> log-files, voice-files, image processing, sensor data processing

  -> 'Big Data' is a problem statement. 


   What is a Cluster?
   ------------------
     A set of nodes whose cumulative resources can be used to distribute storage and 
     processing. This is a unified system managed by some cluster manager tools.

   
   Big Data Solution - Hadoop
   ---------------------------

	-> Solution to big-data is to use cluster architectures, where you take the resources
           from many nodes.

	-> The 'hadoop' framework provides a solution to storage and processing of big data
           using  a cluster of commodity hardware. 
	
		-> HDFS : Distributed Storage Framework
			-> Splits the file into blocks of 128 MB each and distributes them
			   across many machines

		-> MapReduce: Distributed Processing Framework
			      (Cluster Computing Framework)
			-> Distributing computating solution.
    
   What is a Framework ?
   ---------------------
    	-> Set of libraries that solve a specific problem.
	-> Developer use these frameworks to write their programs in a specific way 
           defined by the framework. 
	-> Framework can take care of how to execute that code.	
	

   Disk based computation Vs im-memory computation
   ------------------------------------------------

	Disk-based computation the intermediate results of each tasks are stored on the disk.
	Subsequent tasks will pick up the data from disk into memory, process that and again
        write the output on disk.

	In-memory computation stores the results of all intermediate tasks in-memory (RAM) and 
        on those in-memory partitions it can launch subsequent tasks. 

   
   What is Spark ?
   ---------------
     -> Spark is a unified in-memory distributed computing framework.
	
	-> Spark is written in 'Scala'
	-> Spark is a polyglot. Spark programs can be written Scala, Java, Python & R


         (Bench marked by Databricks)
	 -> Spark is 100x faster than MR (on TB scale data) when 100% in-memory computation is used.
	 -> Spark is 6 to 7x faster than MR when disk-based computation is used.

	
   Spark's unified framework
   -------------------------
	
     -> Spark provides a consistent set of APIs to perform different analytics workloads.

	Batch processing of unstructured data 	=> Spark Low Level API (RDDs)
	Batch processing of Structured data	=> Spark SQL
	Streaming data processing		=> Spark Streaming, Spark Structured Streaming
	Predictive Analytics/Machine Learning	=> Spark MLlib
	Graph Parallel Computations		=> Spark GraphX


   Spark Building Blocks & Architecture
   ------------------------------------

     1. Driver 	
	    -> master process which manages all the tasks
	    -> maintains the user-code
	        -> maintains all the meta-data (RDD lineage DAGs)
	    -> contains "SparkContext" object (or 'SparkSession' object in case SparkSQL)


	    Deploy-Modes
	    ------------
	    1. client mode: (default) the driver process runs on the client machine
	    2. Cluster mode: the driver process runs on one of the nodes in the cluster	

	 	
     2. Executor Processes
	   -> The tasks (such as map, filter, ...) are executed in the executor processes
	   -> Every task does the same work on one partition of the data.


     3. Cluster Manager (CM)
	   -> Spark applications are submitted to CM (such as YARN)
	   -> Spark supports 4 different CMs
		-> Spark Standalone Scheduler, YARN, Mesos, Kubernites
	   -> Allocates some containers for the job to which the driver can send tasks
	      to executed.

     4. SparkContext
	   -> is created inside the driver process
	   -> represents an application context and cluster connection with specified configurations.
	   -> is the link between driver and exectutor processes
		
        
    Spark Layered Model
    -------------------
       Programming Lang      => Scala, Python, Java, R, SQL	
       Spark High Level API  =>	Spark SQL, Spark MLlib, Spark Streaming, Spark GraphX
       Spark Core API        => Low Level API (RDDs)
       Cluster Manager       => Standalone, YARN, Mesos, Kubernetes
       Storage Level         => Linux, Hadoop, AWS S3, Kafka, MongoDB



   RDD (Resilient Distributed Dataset)
   -----------------------------------

     -> Main data abstraction in Spark Core API

     -> RDD is a collection of distributed partitions.
	 -> Each partition is a collection of some type of objects (int, string, order, employee)

     -> RDDs are immutable
	
     -> RDDs are lazily evaluated.  
	  -> Actual execution will be differed until the last moment when the output is actually
	     request using an action command.
	  -> Transformations does not cause execution, Only Actions cause execution.

     -> RDDs are resilent
	   -> RDDs are resilient to missing or un-available in-memory partitions.
	   -> If a required partition is not found, it will launch the required tasks
	      to compute the partitions (using lineage DAGs)

     -> RDD
	  -> Metadata: Lineage DAG 
		 -> maintained by driver
	         -> created when a transformation is seen.

	  -> Data: Partitions of RDDs
		-> Created by launched tasks in reponse to action command.

  
   Different ways of working with PySpark
   --------------------------------------

    -> Using PySpark Shell.

	-> Simply install Anaconda distribution for Python, install Spark and setup some environment
	   variables.

	Environment Variables:
        ----------------------
	PYTHONPATH  -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%
	SPARK_HOME  -> C:\Apps\spark-3.0.0-bin-hadoop2.7
	HADOOP_HOME -> C:\Apps\spark-3.0.0-bin-hadoop2.7

        Add this to PATH environment variable:

	   -> C:\Apps\spark-3.0.0-bin-hadoop2.7\bin

    -> Use an IDE such as Spyder or Jupyter Notebooks environment

    -> Signup to Databrick's Community Edition 	(Free Account)

   
   How to create RDDs
   ------------------

   3 ways:

	1. From some external files. 
		rdd1 = sc.textFile(<file>, [num of partitions])

		default number of partitions is decided by  "sc.defaultMinPartitions"

	2. From programmatic data.
		rdd2 = sc.parallelize([2,3,2,4,,5,6,7,8,9,0,0,3,4,5,6,7,8], [num of partitions])

		default number of partitions is decided by  "sc.defaultParallelism"
	

	3. By applying transformation on an existing RDD 
		rdd3 = rdd2.map(lambda x: x*2)
 

   
   What can we do to an RDD ?
   --------------------------
 
    Two things:

	1. RDD Transformations
	    -> Creates an other RDD
	    -> Transformations does not cause immediate execution.
	    -> Transformations only produce meta-data (RDD lineage DAGs)

	2. RDD Actions
	    -> Action trigger execution of the tasks (and hense cause the creation of partitions)
	    -> Actions produce some output.

   NOTE:
   -----

        <rdd>.getNumPartitions()   => give u number of partitions of the RDD


   RDD Lineage DAG
   ---------------

      -> The driver process maintains the lineage of all RDDs that are created. 
	 -> the lineage tracks the heirarchy of RDDs that created this this RDD all the
	    way from the very first RDD


	rddFile = sc.textFile(file)	
		DAG: rddFile -> sc.textFile
	
   	rdd2 = rddFile.map(lambda x: x.upper())
		DAG: rdd2 -> rddFile -> sc.textFile

	rdd3 = rdd2.map(lambda x: len(x))
		DAG: rdd3 -> rdd2 -> rddFile -> sc.textFile

	rdd3.collect()
	
	  -> rdd3 -> rdd2.map -> rddFile.map -> sc.textFile
		tasks: sc.textFile (rddFile) -> map (rdd2) -> map (rdd3) ---> collect()

	
    DAG Visualization:
    ------------------
	-> DAG can be visualized form the Web UI
	-> <rdd>.toDebugString()



    Types of Transformations
    ------------------------

	1. Narrow
		-> Partition to Partition transformations
		-> There is no shuffling of data
		-> Output RDD will have same number of partition as that of input RDD
		-> Ex: map, filter, flatmap

	2. Wide
		-> Each partition requires data from multiple input partitions
		-> Shuffling of data is involved
		-> Output RDD can have different number of partition than that of input RDD
   		-> Ex: distinct, sortBy, ..ByKey 


    RDD Transformations
    -------------------   
        --> output of every transformation is an RDD 

	
     1. map			F: U -> V 
				Element to element transformation 
				Input RDD: N elements,  Output RDD: N elements
				

     2. glom			Parameters: None
				Creates an array object with all elements of the partition.
				Input RDD: N elements,  Output RDD: # of partitions
				


		rdd1		   rdd2 = rdd1.glom()
		P0: 1,2,3,4   -> glom ->  P0: [1,2,3,4]
		P1: 3,2,4,1   -> glom ->  P1: [3,2,4,1]
		P2: 5,7,2,9   -> glom ->  P2: [5,7,2,9]
    

    3. filter			F: U -> Boolean
				The ouput partition contains only those elements for which
				the function returns True.
				Input RDD: N elements,  Output RDD: <= N elements


    4. distinct			Parameter: None, optionally takes the number of partitions of the output RDD
				Input RDD: N elements,  Output RDD: <= N elements
		

    5. flatMap			F: U -> Iterable[V]   (iterable means some collection)
				Flattens all the elements of the output iterable of the function.

    6. mapPartitions		F: Iterator[U] -> Iterator[V]
				The function input takes all the elements of each partition.


    7. mapPartitionsWithIndex   F: index, Iterator[U] -> Iterator[V]
				The function input takes two params:
				  1. partition-index (int)
				  2. all the elements of each partition (iterable).
	
		rdd10.mapPartitionsWithIndex(lambda index, data: [(index,  sum(data))] )


    8. sortBy    -> to be discussed.




    RDD Persistence
    ---------------

       RDD persistence refers to an active instruction to spark to save the partitions of
       the RDD 'in-memory' or 'on-disk' so that they are not subjected to eviction. 

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t1()
	rdd3 = rdd2.t2()
	rdd4 = rdd2.t3()
	rdd5 = rdd4.t4()
	rdd6 = rdd4.t5()
	rdd7 = rdd6.t6()

	rdd7.persist(<Storage-Level>) 

	rdd8 = rdd7.t7()       

	rdd8.collect()

	DAG of rdd8: rdd8 -> rdd7.t7 -> rdd6.t6 -> rdd4.t5 -> rdd2.t3 -> rdd1.t1 -> sc.textFile
  
        execution:  sc.textFile (rdd1) -> t1 (rdd2) -> t3 (rdd4) -> t5 (rdd6) -> t6 (rdd7) -> t7 (rdd8)
               -> sc.textFile, t1, t3, t5, t6, t7  (in multiple stages)
  

        rdd7.collect()  -> rdd7 -> rdd6.t6 -> rdd4.t5 -> rdd2.t3 -> rdd1.t1 -> sc.textFile
	       -> sc.textFile, t1, t3, t5, t6
  

         RDD Persistence Storage Level
         -----------------------------
 		<rdd>.persist(pyspark.storage.StorageLevel.<STORAGE-LEVEL>)


	  -> MEMORY_ONLY

	  -> MEMORY_AND_DISK

	  -> DISK_ONLY

	  -> MEMORY_ONLY_SER

	  -> MEMORY_AND_DISK_SER
	
	  -> MEMORY_ONLY_2

	  -> MEMORY_AND_DISK_2















             

