
   Agenda ( 7 sessions of 4 hours each )
   -------------------------------------
	Big Data
	Spark - Basics & Architecture
	Spark Core API
	RDD - Transformations and Actions
	Spark SQL  (DataFrames)
	Machine Learning & Spark MLlib
	Introduction to Spark Streaming

 
     Big Data
     --------
 
	-> The kind of data that is so huge and complex that on on-hand data management systems
	   can not reasonbly handle (store & process). 

	-> Volume, Velocity, Variety

    	-> The solutions offered to solve big-data issues are architectures that run on computing cluster
	
	
    Computing Cluster
    -----------------
	
      A unified entity comprising of a group of nodes whose combined resources can be used to store	
      and process data. 

   
    Hadoop
    ------
	-> Is big data a platform that provides two things:
	
	    1. HDFS: 		Hadoop Distributed File System   
		     		Distribution Storage Framework 

	    2. MapReduce:       Distributed parallel processing framework
		
	    3. YARN:		Cluster manager for Hadoop
				Manages resources in the cluster
				Allocates resources (RAM & CPU Cores) to jobs across nodes

    MapReduce is not good:
    -----------------------
	-> Not good with lot of small files
	-> Not good for real-time / ad-hoc query processing 
	-> Not good for iterative computations (as MR used disk based computations)


    What is Spark ?
    --------------

	-> Spark is a unified in-memory distributed computing framework.

	-> Written in SCALA programming language.

	-> Spark is polyglot
		-> Supports Scala, Python, Java, & R			

	In-memory computing:
	   -> The results of a task can be persisted in-memory (RAM) and subsequent tasks 
              can process the results of the previous tasks.

        Spark Unified Stack:
	   -> Spark provides a set of consistent APIs using the same execution engine to handle
	      different analaytical workloads. 
	
	      Batch analytics of unstructured data	=> Spark Core API (RDDs)
	      Batch analytics of structured data	=> Spark SQL
	      Predictive analytics using ML		=> Spark MLlib
              Streaming analytics of unstructured data  => Spark Streaming (RDDs)
	      Streaming analytics of structured data	=> Spark Structured Streaming (DataFrames)
	      Graph parallel computation		=> Spark GraphX


     Spark Architecture
     ------------------

	1. Cluster Manager
		-> Accepts job submission and schedules them to run on the cluster
		-> Allocates executors (container) to run the tasks of the job
		-> Spark supports multiple cluster managers
			-> local, spark standalone, yarn, mesos, kubernetes

	2. Driver process
		-> Is the master process
		-> maintains the user code
		-> creates a physical execution plan based on the used-code
		-> Sends tasks to the executors
		-> Creates a SparkContext object
		
		deploy-modes:

		1. client-mode (default) : the driver runs on the client machine
		2. cluster-mode: the driver runs in one of the containers in the cluster

      3. Executors
		-> Tasks are sent to the executors by the driver
		-> All the executors run the same set of tasks, but on different partitions
		-> Executors communicate the status os tasks to the driver process.

       4. Spark Context
		-> Is the starting point and the first thing that gets created in a driver process.
		-> Is an application context
		-> Is a link between the driver process and various tasks running on the cluster.


    Getting started with PySpark    
    -----------------------------

	1. Download Spark and lauch PySpark shell
		https://spark.apache.org/downloads.html

		SPARK_HOME : E:\spark-3.0.0-bin-hadoop2.7
		Add %SPARK_HOME%\bin to the PATH environment variable
		Then go to cmd promt and type "pyspark" to lauch pyspark shell

	2. Using an IDE (such as Spyder or PyCharm)
		-> Install Anaconda distribution for python
		-> Follow the document shared with you, to setup Syder/Jupyter with PySpark

	3. Using Databricks Community Edition	
	  	https://databricks.com/try-databricks	


    RDD (Resilent Distributed Dataset)
    ----------------------------------	
	
	-> Is the fundamental data abstraction of spark core API.
	-> RDD is a collection of distributed, in-memory partitions.
		-> A partition is a collection of objects

	-> RDDs are immutable

	-> RDD
		data 	  -> partitions
		meta-data -> lineage DAG (maintained by the driver)

	-> RDDs are lazily evaluated
		-> RDD transformations does not cause executions
		-> RDD actions trigger execution
	
	
   How to create RDDs
   -------------------
     
	There are three ways:

	1. We can create RDDs from external data (such as data files)

		rdd1 = sc.textFile( <filepath>, [ <numPartitions> ])
	
		-> default number of partitions is determined the value of sc.defaultMInPartitions

	2. We can create an RDD from programmatic data
		
		rdd2 = sc.parallelize( <anyCollection>, [ <numPartitions> ])

		-> default number of partitions is determined the value of sc.defaultParallelism	

	3. By applying transformations on existing RDDs
	
		rdd3 = rdd1.map(.....)
		
			
  NOTE:  <rdd>.getNumPartitions()  --> returns the number of partitions of the RDD.


   What can we do with an RDD ?
   ---------------------------

	1. Transformations
		-> produces an RDD, does not produce any output
		-> Does not cause execution, but is only RDD lineage DAG to be created
	

	2. Actions
		-> Produces some output (such returning some data to the driver, saving to a dir etc)
		-> Triggeres the execution


   RDD Lineage DAG
   ---------------

	RDD Lineage is a dependency graph that tracks all the dependencies that causes the creation
	of the RDD all the way from the very first RDD. 

  
    	rdd1 = sc.textFile( filePath )
	Lineage:  rdd1 -> sc.textFile	

    	rdd2 = rdd1.map( lambda x: x.upper() )	
	Lineage:  rdd2 -> rdd1.map -> sc.textFile

    	rdd3 = rdd2.map( lambda a: len(a))
	Lineage:  rdd3 -> rdd2.map -> rdd1.map -> sc.textFile
	
    	rdd3.collect()
	=> sc.textFile (rdd1) -> map (rdd2) -> map (rdd3) -> collect  (physical plan)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
		-> Does not cause shuffling of the data from one partition to other partitions
		-> Partition to partition transformations
		-> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
		-> Causes shuffling of the data
		-> One output partition may need data from multiple input partitions
		-> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( filePath, 100 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd7 = rdd6.t7(...)
	rdd8 = rdd6.t8(...) 

	rdd8.persist( StorageLevel.MEMOEY_AND_DISK ) 

	rdd9 = rdd8.t9(...)
	rdd8.collect()
	Lineage: sc.textFile -> rdd1.t3 -> rdd3.t5 -> rdd5.t6 -> rdd6.t8 -> rdd8
	Physical Plan => [sc.textFile -> t3 -> t5 -> t6 -> t8]

        rdd9.collect()
	Lineage: rdd8.t9 -> rdd9


	StorageLevel
        ------------
	-> What kind of persistence you want to do is defined by StorageLevel.

	Three types of persistence:  Persist as deserialized objects
				     Persist as Serialized files
				     Persist on the disk.

	1. MEMORY_ONLY (default) -> deserialized format 
				-> Stored ONLY in memory
				-> The RDD may be fully persisted, or partially persisted
				   or not persisted at all.
				-> Even the persisted partitions are prone to eviction.

	2. MEMORY_AND_DISK	-> deserialized format 
				-> Stored in RAM if available, and disk if RAM is not available.
				
	3. DISK_ONLY		-> Stored on disk only (serialized)

	4. MEMORY_ONLY_SER	-> stored serialized format

	5. MEMORY_AND_DISK_SER	-> stored serialized format

	6. MEMORY_ONLY_2	-> 2 different copies of the partitions are stored on
				   different executors nodes. 

	7. MEMORY_AND_DISK_2	-> 2 different copies of the partitions are stored on
				   different executors nodes. 

	
	Persistence Commands:
	---------------------
		rdd.persist(StorageLevel.MEMORY_ONLY)
		rdd.cache()  // also memory_only
		rdd.unpersist()
	

   Executor Memory Structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------
     
       	-> RDD transformation are applied on an RDD.
       	-> RDD transformations create output RDD
 	-> Transformations does not cause executions. They only create lineage DAGs
  

     1. map		P: U -> V  
			Performs an element to element transformation
			input RDD: N objects,  output RDD: N objects


     2. filter		P: U -> Boolean
			Only those input elements for which the function returns true will be in the
			output RDD
			input RDD: N objects,  output RDD: <= N objects


     3. glom		P: None
			Creates an array with all the elements of the RDD partition
			input RDD: N objects,  output RDD: = number of partitions


		rdd1			rdd2 = rdd1.glom()

		P0: 1,2,3,1,3,4,5 -> glom -> [1,2,3,1,3,4,5]
		P1: 2,3,4,5,4,5,1 -> glom -> [2,3,4,5,4,5,1]
		P2: 3,4,5,6,7,1,5 -> glom -> [3,4,5,6,7,1,5]

  		rdd1.count() -> 21	  rdd2.count() -> 3

 
     4. flatMap		P: U -> Iterable[V]   (iterable = some collection)
			Flattens all the iterables produced by the function.
			input RDD: N objects,  output RDD: >= N objects


     5. distinct	P: None, Optional: # of partitions
			Output RDD will have distinct/unique elements of the input RDD.
			input RDD: N objects,  output RDD: <= N objects

		rdd1.distinct().collect()
		rdd1.distinct(5).collect()   // where 5 is number of output partitions.

    6. mapPartitions	P: Iterator[U] -> Iterator[V]

		rdd1		rdd2 = rdd1.mapPartitions( fn )

		P0: 1,2,3,1,3,4,5 -> mapPartitions(fn) -> ..
		P1: 2,3,4,5,4,5,1 -> mapPartitions(fn) -> ..
		P2: 3,4,5,6,7,1,5 -> mapPartitions(fn) -> ..

  		rdd1.count() -> 21	  rdd2.count() -> 3

		rdd1.mapPartitions(lambda x:  [sum(x), sum(x)] ).collect()


    7. mapPartitionsWithIndex
			P:  (Int, Iterator[U]) -> Iterator[V]

		Two function params are: partition-index & partition-data

		rdd1.mapPartitionsWithIndex(lambda index, data: [(index, sum(data))]).collect()


    8. sortBy		P: U -> V
			The RDD elements are sorted based the value of the function output (V)


		rddWords.sortBy(lambda x: x[-1]).collect()   		// sort by last letter of the word
		rddWords.sortBy(lambda x: len(x), False).collect()  	// False for desc sort

		rdd1.sortBy(lambda x: x%2, True, 15).glom().collect()


    9. groupBy		P: U -> V
			The RDD elements are grouped based on function output (V)
			RDD[U].groupBy(f: U -> V) => RDD[(V, Iterable[U])]

        -- wordcount
	rddWords.groupBy(lambda x: x).map(lambda x: (x[0], len(x[1]))).collect()
	rddWords.groupBy(lambda x: x).mapValues(len).collect()

	-- specifying number of output partitions. 
	rdd1.groupBy(lambda x: x > 5, 2).mapValues(list).glom().collect()

	
    NOTE:

    From usage perspective, RDD can two types:
	-> Generic RDDs:  RDD[U]
	-> Pair RDD:      RDD[(U, V)]
   	
   
    10. mapValues	P: U -> V
			Is applied only on Pair RDDs
			Applied the function on only the "value" part of the pair RDD.

    11. randomSplit	P: An Array of ratios
			Return an array of RDDs split randomly (approx.) in the specified ratios 

		rddArr = rdd1.randomSplit([0.3, 0.3, 0.4])

		rddArr = rdd1.randomSplit([0.6, 0.4], 456)    // here 456 is seed
		rddArr[0].collect()
		rddArr[1].collect()

	
    12. repartition	P: Number of partitions
			Used to increase or decrease the number of partitions of the output RDD.
			Results in global shuffle.


    13. coalesce	P: Number of partitions
			Used to ONLY decrease the number of partitions of the output RDD.
			Results in partition merging.



    Recommendattion for better performance
    --------------------------------------

	-> The optimal size of a partition is 128 MB 
	-> Having too many small partitions, or too few very partitions is not recommended.
	-> Each executor should have 5 cores (or 4 cores)
     
         Example: Assuming you need 100 CPU cores: 

         100 executors with 1 core each    ---> thin executors (BAD)
	 5 executors with 20 cores each	   ---> fat executors (BAD)
	 20 executirs with 5 cores each	   ---> recommended
	 25 executors with 4 cores each	   ---> is also good

	-> The number of partitions can be 2 to 3x than the available cores. 
     	-> If your number of partitions is closer to but less than 2000, bump it up to 2000. 
	
	
   14.  partitionBy	P: Number of Partitions, optionally, a partition function.
			Works ONLY on pair RDDs.
			Controls how objects are partitioned (based on a custom function)
			Partitioning is applied based on the key.		
			

		def city_partitioner(city): 
    			return len(city)

		rdd3 = sc.parallelize(transactions, num_partitions) \
         	 	 .map(lambda e: (e['city'], e))

		rdd4 = rdd3.partitionBy(4, city_partitioner)


   15. union, intersection, subtract, cartesian		P: RDD		
	
	 Let us assume that rdd1 has M partitions & rdd2 has N partitions

	  transformation			# of output partitions
         -------------------------------------------------------------
	  rdd1.union(rdd2)			M + N
	  rdd1.intersection(rdd2)		M + N
	  rdd1.subtract(rdd2)			M + N
	  rdd1.cartesian(rdd2)			M * N

   
    ..ByKey Transformations

	-> They operate only on pair RDDs
	-> They are all wide transformations
	-> We have optional parameter to control number of output partitions.

	
   16. sortByKey	P: None, optional: Ascending: True/False, Number of Partitions

		rdd5 = rdd4.sortByKey()
		rdd5 = rdd4.sortByKey(False)
		rdd5 = rdd4.sortByKey(False, 10)
		rdd5 = rdd4.sortByKey(True, 10)


   17. groupByKey	P: None, optional: Number of Partitions
			RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]	
			Output will have unique keys and aggregated values.
			-> Do not use groupByKey if possible
			
	sc.textFile(<file>).flatMap(lambda x: x.split(" "))
			.map(lambda a: (a, 1))
			.groupByKey()
			.mapValues(sum)
			.collect()

		rddPairs..groupByKey().mapValues(sum).collect()


   18. reduceByKey	P: Reduce Function, Optional: # of partitions
			reduces all the values of each unique-key within each partitions and then 
			across the partition by iterativly applying the reduce function to all the
			values of each unique-key.
			
		rddPairs.reduceByKey(lambda x, y: x + y).collect()


    19. aggregateByKey	   -> Is used to aggregate the values of each unique key to a value
			      which is different than the type of the value element.

		Three mandatory parameters:

		1. zero-value:  Intiali value of the type of type pf output you want to
				produce for for each unique key  [ in this ex: (0, 0) ]

		2. Sequence-Function : Is iterativly applied to each partition (narrow) to 
				merge all the values of each unique key with the zero-value

		3. Combine-Function : Reduces all the aggregated values (produced by seq-fn) for
			       each unique-key across partitions		

		

P0: ('a', 10), ('b', 20), ('a', 30), ('b', 40), ('c', 30), ('c', 10), ('a', 10)
	=> (a, (50, 3)) (b, (60, 2)) (c, (40, 2))


P1: ('c', 30), ('a', 30), ('b', 10), ('b', 40), ('c', 60), ('b', 30), ('a', 40)
	=> (a, (70, 2)) (b, (80, 3)) (c, (90, 2))
	

P2: ('c', 40), ('a', 40), ('b', 20), ('b', 50), ('c', 50), ('b', 10), ('a', 20)
	=> (a, (60, 2)) (b, (80, 3)) (c, (90, 2))


  ZV:      (0, 0)
  Seq Fn:  lambda zv, e :  ( zv[0] + e, z[1] + 1 )
  Comb Fn: lambda a, b: (a[0] + b[0], a[1] + b[1])

 
	rdd11 = rdd10.aggregateByKey( (0,0), 
				      lambda z, e: (z[0] + e, z[1] + 1), 
                                      lambda a, b: (a[0] + b[0], a[1] + b[1]) )

	rdd11.mapValues(lambda a: a[0]/a[1]).collect()  => returns avg for each unique key


   
    20. join transformations	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
				  RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)


    21. cogroup			=> Is used when you have duplicate keys in the RDDs
				   groupByKey on each RDD -> fullOuterJoin


rdd1 => ("key1", 10), ("key2", 12), ("key1", 7), ("key2", 6), ("key3", 6)
	  -> (key1, [10, 7]) (key2, [12,6]) (key3, [6])

rdd2 => ("key1", 5), ("key2", 4), ("key2", 7), ("key1", 17), ("key4", 17)
	 -> (key1, [5, 17]) (key2, [4, 7]) (key4, [17])


rdd1.cogroup(rdd2).collect()
 => (key1, ([10, 7], [5, 17])) (key2, ([12,6], [4, 7])), (key3. ([6], [])), (Key4, ([], [17]))



    RDD Actions
    ------------

	1. collect

	2. count

	3. saveAsTextFile

	4. saveAsSequenceFile

	5. reduce	=> reduces an entire RDD into one single output of the same type as RDD
			   elements by iterativly applying a reduce function ( (U, U) -> U ) in 
			   each partition and then across partitions.

		rdd1 -> [1,2,1,4,6,4,8,9,7,5]

		P0: 1, 2, 1, 4, 6 -> 6
		P1: 4, 8, 9, 7, 5 -> 9

		rdd1.reduce(lambda x, y: x if (x > y) else y) 

	6. take

	7. takeOrdered 	=> rdd1.takeOrdered(5)
			   rdd1.takeOrdered(5, lambda x: x%2)

	8. takeSample	=> P: withReplacement (boolean), # of samples

			rdd1.takeSample(False, 10)  // withReplacement: False
			rdd1.takeSample(True, 20)   // withReplacement: True
			rdd1.takeSample(True, 20, 2454)  // where 2454 is the seed


	9. countByValue

	10. countByKey		-> applied to Pair RDDs
				-> retuns a map with each key's count

	11. foreach

	12. first


   
  Distributed Shared Variables
  ----------------------------

   Closure:  All the code that must be visible for performing a computation in an
	     executor.

	    The closure is serialized and a separate copy is sent to every executor.

	np = 0

	def isPrime( n ) :
		return 0 if not prime
		return 1 if prime

	def f1( n ) :
		global np
		np += isPrime( n )
		return n*2
		
	rdd1 = sc.parallelize(range(1, 40001), 4)

	rdd2 = rdd1.map(f1)
	
        print(np)   --> This is not going to get the total count
        
	

       Closure Problem: A local variable can not be used to capture the global state
			of an entire RDD which distributed across multiple executors.

	To solve this, Spark provides a shared variable called "Accumulator"


	Accumulator:
        ------------
	np = sc.accumulator(0)

	def isPrime( n ) :
		return 0 if not prime
		return 1 if prime

	def f1( n ) :
		global np
		if (isPrime( n ) == 1) :
		    np.add(1)
		return n*2
		
	rdd1 = sc.parallelize(range(1, 40001), 4)

	rdd2 = rdd1.map(f1)
	
        print( np ) 
	

      Broadcast Variable
      ------------------
	lookup = sc.broadcast({1: 'a', 2:'e', 3:'i', 4:'o', 5:'u'}) 
	result = sc.parallelize([2, 1, 3]).map(lambda x: lookup.value[x]) 
	print( result.collect() )


     Spark-submit  command
     =====================
	-> A single command to submit any spark application to any cluster manager.


	spark-submit --master yarn://host:port
		     --deploy-mode cluster
		     --executor-memory 2G
		     --driver-memory 1500M
		     --num-executors 20
		     --executor-cores 5
		      wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


    Use-case
    --------	
	From cars.tsv file, find out the average weight of each american make. Arange the output
	in the descending oorder of average weight and save in one output file as text file.
 
	==> Try to solve this


 =============================================
     Spark SQL   (pyspark.sql)
 =============================================
     
   -> Is structured data processing API

	  Structured Files: Parquet (default), ORC, JSON, CSV (delimited text file)
	  Hive
	  JDBC Format: RDBMS, NoSQL

   -> SparkSession
	 -> Starting point of Spark SQL API
	 -> represents a user-session which itself is inside an application (sparkContext)    
   

	spark = SparkSession \
        	.builder \
        	.appName("Basic Dataframe Operations") \
        	.config("spark.master", "local") \
        	.getOrCreate() 

    -> DataFrame
	-> Is an in-memory partitioned, distributed, immutable, lazily evaluted dataset.
	-> Contains "Row" objects
		-> Row is a StructType object
		-> Row contains a collection of columns

	-> DataFrame contains two things
                      Data     : Collection of Rows
		      MetaData : Schema 

		DataFrame  => Comparable to RDD[Rows] + schema
			   => Executed using Spark SQL execution engine which uses optimizers like
			      catalyst optimizer, tungstun.
			    => Must more efficient than RDDs


     Steps in Spark SQL application
     ------------------------------	
			   
	1. Read/Load the data into a dataframe

		inputDataPath = "E:\\PySpark\\data\\output\\csv\\*6-c000.csv"        // matched files
		inputDataPath = "E:\\PySpark\\data\\output\\csv"		     // all files in dir
		inputDataPath = "E:\\PySpark\\data\\output\\csv\\2015-summary.csv"   // single file

		df1 = spark.read.format("json").load(inputDataPath)
		df1 = spark.read.json(inputDataPath)


	2. Apply transformations on the DataFrame

		Two ways:

		1. Using DataFrame API transformations

			df2 = df1.select("userid", "name", "age", "phone", "gender") \
         			.where("age is not null") \
         			.orderBy("age", "name") \
         			.groupBy("age").count() \
         			.limit(4)

		2. Using SQL

			df1.createOrReplaceTempView("users")
			spark.catalog.listTables()

			qry = """select age, count(*) as count from users
         			where age is not null 
         			group by age
         			order by age
         			limit 4"""

			df3 = spark.sql(qry)
			df3.show()


	3. Write/Save the dataframe to structured-file/hive/rdbms etc.		
	
    		df2.write.format("json").save(outputDir)
		df2.write.json(outputDir)

    
    DataFrame Transformations
    ------------------------

	1. select

		df2 = df1.select(col("DEST_COUNTRY_NAME").alias("destination"), 
                 		column("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 		expr("count"),
                 		expr("count > 100 as highFrequency"),
                 		expr("count + 10 as newCount"),
                 		expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic") 
                 		)

	2. where / filter

		df2 = df1.where("count > 100 and ORIGIN_COUNTRY_NAME = 'United States'")
		df2 = df1.where(col("count") > 100)

	3. orderBy / sort

		df2 = df1.orderBy("ORIGIN_COUNTRY_NAME", "count")
		df2 = df1.orderBy(col("ORIGIN_COUNTRY_NAME").desc(), col("count").asc() )
		df2 = df1.sort(desc("ORIGIN_COUNTRY_NAME"), asc("count") )

	4. groupBy (with aggregation)

		df3 = df2.groupBy("domestic", "highFrequency").avg("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
         		.agg(	count("count").alias("count"), 
              			sum("count").alias("sum"), 
              			avg("count").alias("avg")
			    )

	5. limit

		df2.limit(10)

	6. selectExpr
		
		df2 = df1.select(expr("DEST_COUNTRY_NAME as destination"), 
                 		expr("ORIGIN_COUNTRY_NAME as origin"), 
                 		expr("count"),
                 		expr("count > 100 as highFrequency"),
                 		expr("count + 10 as newCount"),
                 		expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic") 
                 )

		IS SAME AS

		df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination", 
                 			"ORIGIN_COUNTRY_NAME as origin", 
                 			"count",
                 			"count > 100 as highFrequency",
                 			"count + 10 as newCount",
                 			"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic" 
                 		     )

	7. withColumn

	8. withColumnRenamed

		df3 = df1.withColumn("highFrequency", expr("count + 10")) \
        		.withColumn("newCount", col("count") + 10) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	9. drop

		df4 = df3.drop("newCount", "highFrequency")
		df4.show()

	10. distinct

		df3 = df2.select("destination").distinct()

	11. union

		df3 = df1.union(df2)
		  -> where df1 & df2 should have same schema.

	12. randomSplit
		dfArr = df1.randomSplit([0.3, 0.3, 0.4], 788)

		dfArr[0].count()
		dfArr[1].count()
		dfArr[2].count()


	13. repartition & coalesce

		df2 = df1.repartition(4)
		df3 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df3 = df2.repartition(10, col("DEST_COUNTRY_NAME"))

		NOTE: When the DF results in shuffling, spark by default, creates predefined
		number of partitions based on "spark.sql.shuffle.partitions" config whose
		default value is 200. But this config can be set for a sparksession.
		
			=> spark.conf.set("spark.sql.shuffle.partitions", "5")

		df4 = df3.coalesce(5)    // only to decrease the number of partitions.
		df4.rdd.getNumPartitions()


   NOTES:
   ------
	
   -> Creating an RDD from Dataframe

	rdd1 = df1.rdd    -> df1 is a DataFrame & rdd1 is an RDD[Row]


   -> Schema  (rdd1.schema)

	StructType(
		List(StructField(age,LongType,true),
		     StructField(gender,StringType,true),
                     StructField(name,StringType,true),
                     StructField(phone,StringType,true),
                     StructField(userid,LongType,true)
		)
	)  
	

    Working with different data formats
   ------------------------------------

        JSON
	-----
		read:
		-----
	    	df1 = spark.read.format("json").load(inputFilePath) 
		df1 = spark.read.json(inputFilePath) 

		write:
		-----
		df2.write.format("json").save(outputDir)
		df2.write.mode("overwrite").json(outputDir)

	CSV
	---
		read
		-----
		df1 = spark.read \
        		.format("csv") \
        		.option("header", "true") \
        		.option("inferSchema", "true") \
        		.load(inputFilePath)

		df1 = spark.read \
        		.option("header", "true") \
        		.option("inferSchema", "true") \
        		.csv(inputFilePath)

		df1 = spark.read \
        		.option("header", "true") \
        		.option("inferSchema", "true") \
        		.option("sep", "|") \       # optiobnally sepcify the separator
        		.csv(inputFilePath)

		write
		-----
		   df2.write \
    			.format("csv") \
    			.mode("overwrite") \
    			.option("header", "true") \
    			.option("sep", "|") \
    			.save(outputDir)

	Parquet
	-------

		read
		----
		df1 = spark.read.format("parquet").load(inputFilePath)
		df1 = spark.read.parquet(inputFilePath)

		write
		-----
		df2.write.mode("overwrite").format("parquet").save(outputDir)
		df2.write.mode("overwrite").parquet(outputDir)

	ORC
	---
		read
		----
		df1 = spark.read.format("orc").load(inputFilePath)
		df1 = spark.read.orc(inputFilePath)

		write
		-----
		df2.write.mode("overwrite").orc(outputDir)
	
	
    LocalTempViews & GlobalTempViews
   ---------------------------------

	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.


    Applying programmatic schema
    ----------------------------

	my_schema  = StructType(
                [StructField('ORIGIN_COUNTRY_NAME', StringType(), True), 
                 StructField('DEST_COUNTRY_NAME', StringType(), True),
                 StructField('count', IntegerType(), True)]
            )

	df1 = spark.read.schema(my_schema).json(inputFilePath) 


    Creating DataFrames using programmatic data
    -------------------------------------------

	method 1
        --------

	list = [(101, "Raju", 45),
       		(102, "Komala", 39),
       		(103, "Aditya", 25),
       		(104, "Amrita", 23)]

	df1 = spark.createDataFrame(list).toDF("id", "name", "age")


	method 2
        --------

	list = [(101, "Raju", 45), (102, "Komala", 39), (103, "Aditya", 25), (104, "Amrita", 23)]

	rdd = spark.sparkContext.parallelize(list)
	rddRows = rdd.map(lambda t: Row(t[0], t[1], t[2]) )

	mySchema = StructType([
            StructField("id", IntegerType(), False),
            StructField("name", StringType(), False),
            StructField("age", IntegerType(), False)
        ])

	df2 = spark.createDataFrame(rddRows, mySchema)

	df2.show()


    Save Modes
    ----------
	=> Controls the behaviour when you are writing a DF to an existing directory.

	errorifexists  -> default
	ignore
	append
	overwrite

		     df2.write \
    			.format("csv") \
    			.mode("overwrite") \
    			.option("header", "true") \
    			.save(outputDir)



   Programmatic Schema
   -------------------
    
    mySchema = StructType([
            StructField("DEST_COUNTRY_NAME", StringType(), False),
            StructField("ORIGIN_COUNTRY_NAME", StringType(), False),
            StructField("count", LongType(), False)    
        ])

    df1 = spark.read.schema(mySchema).csv(inputDataPath, header=True, inferSchema=True)



   Joins
   -----

     -> inner (default), left_outer, right_outer, full_outer, left_semi, left_anti



employee = spark.createDataFrame([
    (1, "Raju", 25, 101),
    (2, "Ramesh", 26, 101),
    (3, "Amrita", 30, 102),
    (4, "Madhu", 32, 102),
    (5, "Aditya", 28, 102),
    (6, "Pranav", 28, 10000)]).toDF("id", "name", "age", "deptid")
  
department = spark.createDataFrame([
    (101, "IT", 1),
    (102, "ITES", 1),
    (103, "Opearation", 1),
    (104, "HRD", 2)]).toDF("id", "deptname", "locationid")
  

employee.createOrReplaceTempView("emp")
department.createOrReplaceTempView("dept")

qry = """select emp.*, dept.id, dept.deptname, dept.locationid
         from emp full outer join dept
         on emp.deptid = dept.id"""

joinedDF = spark.sql(qry)

joinedDF.show()
	

  
  Left-Semi Join
  --------------

      -> Is similar to inner-join, but the data comes only from the left side table
      -> select * from emp where deptid IN ( select id from dept )
	 
	
  Left-Anti Join
  --------------

      -> Is similar to inner-join, but the data comes only from the left side table for the
	 join column for values are NOT there in the seconf table. 

      -> select * from emp where deptid NOT IN ( select id from dept )


  Joins using DataFrame API
  -------------------------
     -> inner (default), left_outer, right_outer, full_outer, left_semi, left_anti


	joinCol = employee["deptid"] == department["id"]

	oinedDF = employee.join(department, joinCol)      // inner join
	joinedDF.show()

	joinedDF = employee.join(department, joinCol, "left_outer")
	joinedDF.show()

	joinedDF = employee.join(department, joinCol, "left_anti")
	joinedDF.show()


     Enforcing Broadcast Join
     -------------------------
	joinedDF = employee.join(broadcast(department), joinCol, "inner")

   
     Working with Hive
     -----------------
	=> Hive is a data warehousing platform built on top of HDFS on Hadoop
	
	=> Warehouse
		-> Is a directory path where Hive stores all its managed tables' data file. 

	=> Metastore
		-> An RDBMS database such as MySQL where Hive stores all its metastore.


warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")
spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


 Working with MySQL
 ------------------

   Use-Case
   --------
   From movies.csv & ratings.csv datasets, get the top 10 movies with highest average rating.

   -> Consider only movies that are rated by atleast 50 users
   -> Data need: movieId, title, totalRatings, averageRating  
   -> Show the data in the desc order of averageRating  
   -> Save the output as a single JSON file   

   
spark = SparkSession \
        .builder \
        .appName("Basic Dataframe Operations") \
        .config("spark.master", "local") \
        .getOrCreate()  
    
spark.conf.set("spark.sql.shuffle.partitions", "5")

moviesFile = "E:\\PySpark\\data\\movielens\\movies.csv"
ratingsFile = "E:\\PySpark\\data\\movielens\\ratings.csv"

moviesDF = spark.read.csv(moviesFile, header=True, inferSchema=True)
ratingsDF = spark.read.csv(ratingsFile, header=True, inferSchema=True)

moviesDF.show(5)
ratingsDF.show(5)

df2 = ratingsDF.groupBy("movieId") \
        .agg(count("rating").alias("totalRatings"), 
             avg("rating").alias("averageRating")) \
        .where("totalRatings > 50") \
        .orderBy( desc("averageRating") ) \
        .limit(10)

df2.show(40)

joinCol = df2["movieId"] == moviesDF["movieId"]

outputDF = df2.join(moviesDF, joinCol) \
              .drop(moviesDF["movieId"]) \
              .select("movieId", "title", "totalRatings", "averageRating") \
              .orderBy( desc("averageRating") )

outputDF.show()

outputDF.rdd.getNumPartitions()

outputDir = "E:\\PySpark\\data\\movielens\\output"

outputDF.coalesce(1).write.mode("overwrite").json(outputDir)

 =======================================================================

  Machine Learning & Spark MLLib
  ------------------------------ 

      The goal of an ML project is to create an ML Model.

      ML Model  => Is a learned entity
		=> It learns from pre-existing/historic data
		=> It learns about:
			-> how output is derived from inputs
			-> hidden patterns in the data 
		=> Using that learning a model can:
			-> predict the output for unseen data.
			-> catgorize data into different clusters
			-> forecast the futuristic trends. 
			-> give recommentations (netflix, amazon etc) 

    Terminology
    ------------
	1. Features	: inputs, independent variables, dimensions
	2. Label	: output
	3. Algorithm	: a mathematic iterative computation that creates an association between
		          the label and features. 
	4. Model	: Is the output of the algorithmic training (iterative computation)
	5. Error	: The difference between actual and predicted values w.r.t a single data point
	6. Loss		: Combined errors of all data points (many loss functions are there)


    The steps in an ML project
     --------------------------

	1. Data Collection

	2. Data Prepartition   (60% to 65% time)
		-> Process of making raw-data usable by an algorithm.
                -> Creating of "Feature Vector" is the goal of Data Preparation step.

		-> Data must be numeric (double data type)
		-> Data must not have any nulls, empty strings an so on.

		2.1  EDA - exploratory data analysis
		2.2  Feature Engineering

	3. Train the model
		-> Using one or more models
		-> Train using only 70% of the prepared data and keep 30% aside for validation

	4. Evaluate the model.
		-> Get the prodictions from the model for 30% validation dataset
		-> By comparing the predictions with the actual labels, we can compute 
		   accuracy/loss metrics.

        5. Deploy the model.	


   Types of Machine Learning
   -------------------------

	1. Supervised Learning
		=> Data: Features + Label  (labelled data)
		
		1.1 Classification
			-> Output belongs to few fixed values.
			-> Ex: 1/0, True/False, [1,2,3,4,5]

		1.2 Regression
			-> Output is a continuous values
			-> House Price Prediction

	2. Unsupervised Learning
		=> Data: does not contain labels

		2.1 Clustering
			-> grouping the outputs into different clusters based on patterns

		2.2 Dimensionality Reduction
			-> Used as intermediate model to reduce the number of dimensions into
			   fewer dimension (by projecting them into their principle component)		


	3. Reinforcement Learning
		-> Semi supervized learning


    Popular ML libraries in the industry
    ------------------------------------

	Mechine Learning :  Spark MLLib, SciKitLearn, SAS, PyTorch
	Deep Learning	 :  TensorFlow, Keras, PyTorch

     
    Spark MLLib
    -----------
	Two libraries are there:

		1. pyspark.mllib   => Based on RDDs
		2. pyspark.ml	   => Based on DataFrames  (current)


	1. Feature Tools : Feature Extractors, Feature Transformers, Feature Selectors
	2. Algorithms	 : Classification, Regression, Clustering, Collaborative Filtering
	3. Pipeline	 : defines the flow of transformations applied on the input data
	4. Model Selection Tools & Tuning : TrainValidationSplit, CrossValidation
   	5. Ulitities	 : LinAlg, Statistics


   Spark MLLib Building Blocks
   ---------------------------

	1. Feature Vector

		-> Is an object of vector type which hold all the features used for training a model
		-> Two Types:

		1. Dense Vector  :  Vectors.dense(0,4,0,0,0,0,0,6,2,7,0,0,0,0,0,0,4,8,9,0)
		2. Sparse Vector :  Vectors.sparse(20, [1,7,8,9,16,17,18], [4,6,2,7,4,8,9])

	
	2. Estimator
		input: DataFrame, output: model
		method: fit
		
		model = <estimator>.fit( <dataFrame> )
		
		Ex: All algorithms, A lot of Feature selectors and transformers


	3. Transformer
		input: DataFrame, output: DataFrame
		method: transform

		dfOutput = <transformer>.transform( dfInput )

		Ex: model, lot of feature transformers.


	4. Pipeline

		pl = Pipeline().setStages( [ T1, T2, E1, E2 ] )
	
		plModel = pl.fit( df )			
		df5 = plModel.transform( df )
	
		df -> T1 -> df2 -> T2 -> df3 -> E1 -> df4 -> E2 -> PlModel


	5. Model Selection technique

		-> TrainValidationSplit

			-> Estimator: 			Pipeline
			-> setEstimatorParamMaps :      ParameterGrid
			-> setTrainRatio:		Data Fraction for training the model
			-> setEvaluator:		BinaryClassificationEvaluator 

			Will evaluate multiple model and returns the best model. 


     Mini Project:
     ------------
	kaggle.com  -> 	https://www.kaggle.com/c/titanic


PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,"Braund, Mr. Owen Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs. John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S


   Label:  Survived

   Features: Pclass,Age,SibSp,Parch,Fare  
		-> numerical features
	     Sex,Embarked  
		-> categorical features
		-> convert these into onehotencoded format



df => [ genderIndexer, embarkIndexer, genderEncoder, embarkEncoder, assembler, rf ]


 |-- Survived: float (nullable = true)
 |-- Sex: string (nullable = true)
 |-- Embarked: string (nullable = true)
 |-- Pclass: float (nullable = true)
 |-- Age: float (nullable = true)
 |-- SibSp: float (nullable = true)
 |-- Parch: float (nullable = true)
 |-- Fare: float (nullable = true)
  -> indexedSex
  -> indexedEmbarked
  -> sexVec
  -> embarkedVec

Pclass, Age, SibSp, Parch, sexVec, embarkedVec

["Pclass","sexVec","Age","SibSp","Parch","Fare","embarkedVec"]  -> features

 ====================================================
    Introduction to Spark Streaming
 ====================================================
  
    Batch Data Processing	
	-> Data 	: Bounded  (10 GB)
	-> Processing	: Bounded  (starts and ends)


    Streaming Data Processing
	-> Data		: Unbounded (real time and continuous data)
	-> Processing	: The processing is real-time and continuous.
	
    Stream Processing APIs:

	-> Spark Streaming 	: Based on RDDs API (low level api)
	-> Structured Streaming : Based on Spark SQL (DataFrames)

    Spark Streaming : (DStream API)
		
	   => Starting point: streamingContext which is defined with a window.
	   => Created micro-batches from streaming data and each micro-batch is represented
		   by an RDD
	   => This create a continuous flow of RDDs. This is "Descritized Stream" (DStream)
	   => This provides a "near-real-time" processing (i.e provides Seconds scale latencies)

  
   Streaming data sources that are nativly supported are:

	-> Socket Stream
	-> File Stream




   



   
