
  Agenda (PySpark)
  ----------------
    -> Spark - Basics & Architecture
    -> Spark Core API
	 -> RDD : Transformations & Actions
	 -> Shared Variables
    -> Spark SQL
	 -> DataFrame Operations
    -> Spark Streaming
	 -> Spark Structured Streaming
    -> Introduction to Spark MLlib


   Materials
   ---------
	-> PDF Presentations 
        -> Code Modules
	-> Class Notes
	=> GitHub: https://github.com/ykanakaraju/pyspark
	
   Spark
   ------
      => Open Source unified in-memory distributed computing engine/framework.
      => For big data analysis using in-memory computation on a cluster using simple programming constructs. 
      => Spark is a cluster computing framework. 

      => Spark is written in "SCALA" programming language
  
      		Cluster : Is a unified entity comprising of many nodes whose combined resources can be used to distribute
                	  storage and processing.

                => Each cluster is managed by a 'Cluster Manager'

       => Spark is a polyglot
		=> Supports Scala, Java, Python, R

       => Spark supports multiple cluster managers.
		-> local, spark standalone, YARN, Mesos, Kubernetes. 
	
      
   Spark unified stack
   --------------------
	-> Spark provides a consistent set of API for processing different analytics work loads
	   based on the same execution engine.

	 	Batch Analytics of Unstructured Data	: Spark Core (RDD API)
		Batch Analytics of Structured Data	: Spark SQL
		Streaming Analytics (real-time)		: Spark Streaming, Structured Streaming
		Predictive Analytics (ML)		: Spark MLLib
		Graph parallel computations		: Spark GraphX
	
	   => Open Source Community built APIs: spark-packages.org


   Spark Architecture
   ------------------

       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


    Getting Started with Spark
    --------------------------
	
    1. Working in your vLab		
	-> PySpark Shell
	-> Jupyter Notebook.

    2. Settting up pyspark environment on your personal machine.	
	-> Make sure that you install "Anaconda Navigator" 
	   URL: https://www.anaconda.com/products/distribution#windows

	-> To setup PySpark to work with Jupyter Notebook or Spyder follow the
	   instrcutions given in the shared document. 

	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Account (free account)
	-> Sign up URL: https://databricks.com/try-databricks
	-> Login :  https://community.cloud.databricks.com/login.html
		
	Read >> Guide: Quickstart tutorial


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
      => Fundamental data abstarction of Spark Core API.

      => RDD is a collection of distributed in-memory partitions.
	  -> Partition is a colection of (in-memory) objects. 
  
      => RDDs are lazily evaluated.
	    -> Transformations does not cause execution
	    -> Action commands cause execution.

      => RDDs are immutable.

      => RDDs are resilent
	   -> RDDs can create missing in-memory partitions at run-time.
	   -> Fault-tolerant to missing in-memory partitions. 


   How to create RDD ?
   -------------------
	3 ways:

	1. Create an RDD from some external data file.
	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data
	    rdd1 = sc.parallelize( range(1, 1000), 3 )

	3. By applying transformations o existing RDDs
	    rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with an RDD ?
   -----------------------------
	Two things:
	
	1. Transformations
		-> Create RDD Lineage DAGs
		-> Does not cause execution.

	2. Actions
		-> Trigger execution of the RDD by converting the logical plan to physical plan
		-> Produces some output.

   RDD Lineage DAG
   ---------------
    RDD Lineage DAG is a logical plan maintained by Driver for every RDD.
    RDD Lineage tracks the heirarchy of all dependent RDD all the way from the very first RDD.
    RDD Lineage DAG is created an RDD object is created by transformation or data loading commands. 

     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	 => Lineage DAG of rddFile: (4)  rddFile -> textFile

     rddWords = rddFile.flatMap(lambda x: x.split(" "))
	 => Lineage DAG of rddWords: (4) rddWords -> rddFile.flatMap -> textFile

     rddPairs = rddWords.map(lambda x: (x, 1))
 	=> Lineage DAG of rddPairs: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile

     rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	=> Lineage DAG of rddWc: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile
   	

   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK ) ---> instruction to Spark to save the rdd6 partitions.
	rdd7 = rdd6.t7(..)

	l1 = rdd6.collect()
	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		transformations: [sc.textFile, t3, t5, t6]

        l2 = rdd7.collect()

	lineage of rdd6: rdd7 -> rdd6.t7
		transformations: [t7]


   	Storage Levels
   	--------------	
	1. MEMORY_ONLY		: default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		: Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated (2 copies on two 'different' executors)
	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated	


	Commands
	--------
		rdd.persist()
		rdd.persist( StorageLevel.DISK_ONLY)
		rdd.cache()

		rdd.unpersist()


   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 

	


   RDD Transformations
   -------------------
	
     1. map		P: U -> V
			object to object transformation
			Transformas each input object to output object by applying the function.
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: len(x)).collect()	


    2. filter		P: U -> Boolean
			Only those objects for which the function return True will be in the output
			input RDD: N objects, output RDD: <= N objects

		rdd1.filter(lambda x: x%3 == 0).collect()

   
    3. glom		P: None
			Returns one list object per partition with all the elements of the partition
			input RDD: N objects, output RDD: = number of partitions

		
		rdd1		    rdd2 = rdd1.glom()

		P0: 7,6,8,9,0,8 -> glom -> P0: [7,6,8,9,0,8]
		P1: 5,4,3,2,1,7 -> glom -> P1: [5,4,3,2,1,7]
		P2: 6,3,0,3,5,1	-> glom -> P2: [6,3,0,3,5,1]

		rdd1.count() = 18 (int)	  rdd2.count() = 3 (list)

		rdd1.glom().map(sum).collect()


    4. flatMap		P: U -> Iterable[V]
			Flattens the elements of the iterables produced by the function.
			input RDD: N objects, output RDD : >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


    5. mapPartitions	P: Iterable[U] -> Iterable[V]
			transforms each partition by applying the function into output partition

		rdd1          rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )  

		P0: 7,6,8,9,0,8 -> mapPartitions -> P0: 38
		P1: 5,4,3,2,1,7 -> mapPartitions -> P1: 21
		P2: 6,3,0,3,5,1	-> mapPartitions -> P2: 18

		rdd1.mapPartitions(lambda p : [ sum(p) ]).collect()
		rdd1.mapPartitions(lambda p:  map(lambda x: (x, x*10), p) ).collect()			


    6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V] 
			Similar to mapPartitions, but you get the partition-index as additional function parameter.


		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, list(p))]) \
	    		.filter(lambda x: x[0] == 0) \
            		.flatMap(lambda x: x[1]).collect()


    7. distinct			P: None, Optional: numPartitions
				Returns the distinct objects of the input RDD.
				input RDD: N objects, output RDD : <= N objects

		rddWords.distinct().collect()
		rddWords.distinct(4).collect()

    Types of RDDs
    -------------
	1. Generic RDD: RDD[U]
	2. Pair RDD:	RDD[(U, V)]   


    8. mapValues	P: U -> V
			Applied only on Pair RDDs
			Transform the value part of the (k,v) by applying the function.

		rdd4.mapValues(lambda x: x[1]).collect()


    9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Returns a sorted RDD where elements of the RDD are sorted based on the function output.	


		rddWords.sortBy(len).collect()				# ASC with same # of partitions
		rddWords.sortBy(len, False).collect()			# DESC with same # of partitions
		rdd1.sortBy(lambda x: x%5, True, 2).glom().collect()	# ASC with 2 partitions

    10. groupBy






 

















