
  Agenda 
  ------

   	-> Spark - Basics & Architecture
	-> Spark Core API (Low level API)
		-> RDD Transformations & Actions
		-> Shared Variables
	-> Spark SQL
	-> Spark MLlib  & Machine Learning
	-> Introduction to Spark Streaming


   Materials
   ---------	

	-> PDF presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark

   =======================================================

   Cluster
   -------
	-> Is a unified entity comprising of many nodes whose cumulative resources can be used
	   to store and process big data. 

   Spark
   -----
      	-> Spark is a unified, in-memory distributed computing framework. 

	-> Spark is written in Scala programing language.

	-> Spark is a polyglot
	     -> Scala, Java, Python, R

	-> Spark applications can be submitted to multiple cluster managers
	     -> local, spark standlone, yarn, mesos, kubernetes.

  	Distributed computing 
	    -> ability to distribute the processing across many nodes and perform the coputations in parallel

	In-memory
	    -> the intermediate results of computations can be persisted in-memory and subsequent tasks can 
	       direct run on these persisted results. 

	Unified Framework
	    -> Spark provides a consistent set of APIs for processing different analytical workloads 
	       based on the same execution engine. 	    

		Batch analytics of unstructured data	: Spark Core API (RDD)
		Batch analytics of structured data	: Spark SQL
		Streaming analytics 			: Spark Streaming, Structured Streaming
		Predictive analytics (machine learning) : Spark MLlib
		Graph parallel computations		: Spark GraphX

		
   Spark Architecture
   ------------------

        1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Creates a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.



    How to get started with Spark
    ------------------------------

     1. Working in your vLab

	-> Click on the link shared with you, follow the step and connect to the lab

	-> Click on the CentOS 7 icon (in Windows Server Desktop) and enter your user name and password
		  -> Refer to README.txt file (found on the desktop)
	
	-> Connect to PySpark shell
		-> Open a Terminal
		-> type "pyspark" at the terminal prompt.

	-> Connect to Jupyter Notebooks
		-> Open a terminal
		-> Type "jupyter notebook --allow-root"
		-> This launches Jupyter Notebook environement.

    2. Setting up PySpark on your local machine.

	-> Download and install 'Anaconda Navigator'
		URL:  https://www.anaconda.com/products/individual#windows
	-> Follow the steps mentioned in the shared document.
		-> https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition Account

	-> URL: https://databricks.com/try-databricks


   RDD ( Resilient Distributed Dataset )
   -------------------------------------
     
     => The fundamental data-abstraction of Spark

     => Collection of distributed in-memory partitions.
            -> A partition is a collection of objects (of any type)

     => RDDs are immutable

     => RDDs are lazily evaluated	
	  -> Transformations does not cause execution. They only create lineage DAG.
	  -> The actual execution is triggered by 'action' commands.
	
	
   How to create RDDs ?
   --------------------
	Three ways:

	1. From an external data file.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic data (such as python collections)

		rdd1 = sc.parallelize( range(1, 101), 3 )
		rdd1 = sc.parallelize( [3,2,1,4,3,5,6,7,8,9,4,5,6,7,8,9,0,,3,2,4,2,5,4,6,7,8,9], 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   What can you do with an RDD ?
   -----------------------------

	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.


   RDD Lineage DAGs
   ----------------
	
     -> RDD Lineage DAG is a logical plan on how to create the RDD partitions.
     -> Lineage graph maintains all dependencies of the RDD all the way from the very first RDD.
	

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )	
	Lineage DAG : rddFile -> sc.textFile

	rdd2 = rddFile.flatMap(lambda x: x.split(" "))
   	Lineage DAG : rdd2 -> rddFile.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: x.upper())
	Lineage DAG: rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile
	
	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: rdd4 -> rdd3.filter -> rdd2.map -> rddFile.flatMap -> sc.textFile


	rdd4.collect()
	   -> sc.textFile -> flatMap -> map -> filter -> rdd4 ==> collected
	

   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ...., 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )       ---> instruction to spark to persist rdd6 partitions (not subject them to GC)
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	
	rdd6 =>  (4)  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	     => sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect() 

	rdd7.collect()

	rdd7 =>  (4) rdd7 -> rdd6.t7
	     => rdd6 -> t7 (rdd7) -> collect() 
		
	rdd6.unpersist()	

	Storage Levels
	---------------	
	1. MEMORY_ONLY 	      -> default, memory serialized 1x replication
	2. MEMORY_AND_DISK    -> disk memory serialized 1x replication	
	3. DISK_ONLY	      -> disk serialized 1x replication	
	4. MEMORY_ONLY_2      -> memory serialized 2x replication
	5. MEMORY_AND_DISK_2  -> disk memory serialized 2x replication	

	Commands
	--------
		-> rdd1.persist()    				// memory-only
		-> rdd1.persist( StorageLevel.DISK_ONLY )
		-> rdd1.cache()					// memory-only
	

   Types of Transformations
   -------------------------
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------
  
    1. map		P: U -> V
			Object to object transformation
			input RDD: N objects,  output RDD: N objects

		rdd1.map(lambda x: x > 7).collect()

   2. filter		P: U -> Boolean
			The output RDD will have only those objects for which the function returns true.	
   
		rdd1.filter(lambda x: x%2 == 0).collect()

   3. glom		P: None
			Returns a list object per partition with all the elements of a partition.


		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,4,3,5,6 -> glom -> P0: [3,2,1,4,3,5,6]
		P1: 4,3,2,4,3,5,6 -> glom -> P1: [4,3,2,4,3,5,6]
		P2: 9,0,7,8,9,7,3 -> glom -> P2: [9,0,7,8,9,7,3]

		rdd1.count() = 21 (int)	     rdd2.count() = 3 (list)

		rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap		P: U -> Iterable[V]
			Flatmap flattens the iterables produced by the function.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Partition to partition transformation

		rdd1	   rdd2 = rdd1.mapPartitions(lambda x : [sum(x)] )

		P0: 3,2,1,4,3,5,6 -> mapPartitions -> P0: 24
		P1: 4,3,2,4,3,5,6 -> mapPartitions -> P1: 25
		P2: 9,0,7,8,9,7,3 -> mapPartitions -> P2: 43

		rdd1.mapPartitions(lambda x : [sum(x)] )
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).collect()

   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Same as mapPartitions, but we have partition-index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, x : [(i, sum(x))]).collect()
		rdd1.mapPartitionsWithIndex(lambda index, data : map(lambda a: (index, a*10), data)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects of the input RDD. 

		rddWords.distinct().collect()
	






















