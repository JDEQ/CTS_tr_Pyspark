
  Agenda 
  ------

   	-> Spark - Basics & Architecture
	-> Spark Core API (Low level API)
		-> RDD Transformations & Actions
		-> Shared Variables
	-> Spark SQL
	-> Spark MLlib  & Machine Learning
	-> Introduction to Spark Streaming


   Materials
   ---------	

	-> PDF presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark

   =======================================================

   Cluster
   -------
	-> Is a unified entity comprising of many nodes whose cumulative resources can be used
	   to store and process big data. 

   Spark
   -----
      	-> Spark is a unified, in-memory distributed computing framework. 

	-> Spark is written in Scala programing language.

	-> Spark is a polyglot
	     -> Scala, Java, Python, R

	-> Spark applications can be submitted to multiple cluster managers
	     -> local, spark standlone, yarn, mesos, kubernetes.

  	Distributed computing 
	    -> ability to distribute the processing across many nodes and perform the coputations in parallel

	In-memory
	    -> the intermediate results of computations can be persisted in-memory and subsequent tasks can 
	       direct run on these persisted results. 

	Unified Framework
	    -> Spark provides a consistent set of APIs for processing different analytical workloads 
	       based on the same execution engine. 	    

		Batch analytics of unstructured data	: Spark Core API (RDD)
		Batch analytics of structured data	: Spark SQL
		Streaming analytics 			: Spark Streaming, Structured Streaming
		Predictive analytics (machine learning) : Spark MLlib
		Graph parallel computations		: Spark GraphX

		
   Spark Architecture
   ------------------

        1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Creates a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.



    How to get started with Spark
    ------------------------------

     1. Working in your vLab

	-> Click on the link shared with you, follow the step and connect to the lab

	-> Click on the CentOS 7 icon (in Windows Server Desktop) and enter your user name and password
		  -> Refer to README.txt file (found on the desktop)
	
	-> Connect to PySpark shell
		-> Open a Terminal
		-> type "pyspark" at the terminal prompt.

	-> Connect to Jupyter Notebooks
		-> Open a terminal
		-> Type "jupyter notebook --allow-root"
		-> This launches Jupyter Notebook environement.

    2. Setting up PySpark on your local machine.

	-> Download and install 'Anaconda Navigator'
		URL:  https://www.anaconda.com/products/individual#windows
	-> Follow the steps mentioned in the shared document.
		-> https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition Account

	-> URL: https://databricks.com/try-databricks


   RDD ( Resilient Distributed Dataset )
   -------------------------------------
     
     => The fundamental data-abstraction of Spark

     => Collection of distributed in-memory partitions.
            -> A partition is a collection of objects (of any type)

     => RDDs are immutable

     => RDDs are lazily evaluated	
	  -> Transformations does not cause execution. They only create lineage DAG.
	  -> The actual execution is triggered by 'action' commands.
	
	
   How to create RDDs ?
   --------------------
	Three ways:

	1. From an external data file.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic data (such as python collections)

		rdd1 = sc.parallelize( range(1, 101), 3 )
		rdd1 = sc.parallelize( [3,2,1,4,3,5,6,7,8,9,4,5,6,7,8,9,0,7,3,2,4,2,5,4,6,7,8,9], 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   What can you do with an RDD ?
   -----------------------------

	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.


   RDD Lineage DAGs
   ----------------
	
     -> RDD Lineage DAG is a logical plan on how to create the RDD partitions.
     -> Lineage graph maintains all dependencies of the RDD all the way from the very first RDD.
	

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )	
	Lineage DAG : rddFile -> sc.textFile

	rdd2 = rddFile.flatMap(lambda x: x.split(" "))
   	Lineage DAG : rdd2 -> rddFile.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: x.upper())
	Lineage DAG: rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile
	
	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: rdd4 -> rdd3.filter -> rdd2.map -> rddFile.flatMap -> sc.textFile


	rdd4.collect()
	   -> sc.textFile -> flatMap -> map -> filter -> rdd4 ==> collected
	

   RDD Persistence
   ---------------
	rdd1 = sc.textFile( ...., 4 )
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )       ---> instruction to spark to persist rdd6 partitions (not subject them to GC)
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	
	rdd6 =>  (4)  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	     => sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect() 

	rdd7.collect()

	rdd7 =>  (4) rdd7 -> rdd6.t7
	     => rdd6 -> t7 (rdd7) -> collect() 
		
	rdd6.unpersist()	

	Storage Levels
	---------------	
	1. MEMORY_ONLY 	      -> default, memory serialized 1x replication
	2. MEMORY_AND_DISK    -> disk memory serialized 1x replication	
	3. DISK_ONLY	      -> disk serialized 1x replication	
	4. MEMORY_ONLY_2      -> memory serialized 2x replication
	5. MEMORY_AND_DISK_2  -> disk memory serialized 2x replication	

	Commands
	--------
		-> rdd1.persist()    				// memory-only
		-> rdd1.persist( StorageLevel.DISK_ONLY )
		-> rdd1.cache()					// memory-only
	

   Types of Transformations
   -------------------------
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------
  
    1. map		P: U -> V
			Object to object transformation
			input RDD: N objects,  output RDD: N objects

		rdd1.map(lambda x: x > 7).collect()

   2. filter		P: U -> Boolean
			The output RDD will have only those objects for which the function returns true.	
   
		rdd1.filter(lambda x: x%2 == 0).collect()

   3. glom		P: None
			Returns a list object per partition with all the elements of a partition.


		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,4,3,5,6 -> glom -> P0: [3,2,1,4,3,5,6]
		P1: 4,3,2,4,3,5,6 -> glom -> P1: [4,3,2,4,3,5,6]
		P2: 9,0,7,8,9,7,3 -> glom -> P2: [9,0,7,8,9,7,3]

		rdd1.count() = 21 (int)	     rdd2.count() = 3 (list)

		rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap		P: U -> Iterable[V]
			Flatmap flattens the iterables produced by the function.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Partition to partition transformation

		rdd1	   rdd2 = rdd1.mapPartitions(lambda x : [sum(x)] )

		P0: 3,2,1,4,3,5,6 -> mapPartitions -> P0: 24
		P1: 4,3,2,4,3,5,6 -> mapPartitions -> P1: 25
		P2: 9,0,7,8,9,7,3 -> mapPartitions -> P2: 43

		rdd1.mapPartitions(lambda x : [sum(x)] )
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).collect()

   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Same as mapPartitions, but we have partition-index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, x : [(i, sum(x))]).collect()
		rdd1.mapPartitionsWithIndex(lambda index, data : map(lambda a: (index, a*10), data)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct objects of the input RDD. 

		rddWords.distinct().collect()

   Types of RDDs
   --------------
	-> Generic RDDs : RDD[U]
	-> Pair RDDs	: RDD[(U, V)]



   8. mapValues			P: U -> V
				Applied only on Pair RDDs
				Applies the function on only value part of the (K, V) pairs. 

	 	rdd3.mapValues(lambda x : (x,x)).collect()


   9. sortBy			P: U -> V. Optional: ascending (True/False), numPartitions
				Objects of the output RDD are sorted based on the value of the function
				output.

		rdd3.sortBy(lambda x: x[1]).glom().collect()
		rdd3.sortBy(lambda x: x[1], False).glom().collect()
		rdd3.sortBy(lambda x: x[1], False, 6).glom().collect()


   10. groupBy			P: U -> V, Optional: numPartitions
				The elements of the RDD are grouped based on function on output.
				Returns a Pair RDD where the key is the function output and value is the grouped
				objects that produced the function output.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         			.flatMap(lambda x: x.split(" ")) \
         			.groupBy(lambda x: x) \
         			.mapValues(len) \
         			.sortBy(lambda x: x[1], False, 1)

   11. ramdomSplit		P: list of ratios
				Splits the RDD randomly as multiple RDDs approximatly in the specified ratios. 

		rddList = rdd1.randomSplit([0.4, 0.6])
		rddList = rdd1.randomSplit([0.4, 0.6], 456667)
		

   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions
				Results in global shuffle.	

		rdd2 = rdd1.repartition(6)

   13. coalesce			P: numPartitions
				Is used to only decrease the number of partitions
				Results in partition merging

		rdd2 = rdd1.coalesce(3)

   14. partitionBy		P: numPartitions, Optional: partitioning function
				Applied only on Pair RDDs	
				Is used to control which keys for to which partitions
				

   15. union, intersection, subtract & cartesian


   ...ByKey Transformations
        => Are all wide transformation	
	=> Applied only to pair RDDs

    16. sortByKey		P: None, Optional: ascending(True/False), numPartitions
				Sorts based on the key

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 6).collect()


   17. groupByKey		P: None, Optional: numPartitions
				Groups the elements of the RDD based on the key, so that the output
				is a pair RDD with unique keys and grouped values.
				Results in global shuffle

				NOTE: avoid groupByKey if possible.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda x: (x, 1)) \
         		.groupByKey() \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)

   18. reduceByKey		P: (U, U) -> U   optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the 
				reduce function on all the values of each unique key.

		rddPairs.reduceByKey(lambda x, y: x + y)
		rddPairs.reduceByKey(lambda x, y: x + y, 2)		

  		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         			.flatMap(lambda x: x.split(" ")) \
         			.map(lambda x: (x, 1)) \
         			.reduceByKey(lambda x, y: x + y) \
         			.sortBy(lambda x: x[1], False) \
         			.coalesce(1)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 


	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
	student_rdd.collect()

	output = student_rdd.map(lambda t: (t[0], t[2])) \
            	.aggregateByKey( (0,0),
                            lambda z, v: (z[0] + v, z[1] + 1),
                            lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            	.mapValues(lambda x: x[0]/x[1]) \
            	.sortBy(lambda x: x[1], False)


    20. joins	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			
			RDD[(U, V)].join( RDD[(U, W)]) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)
	


    21. cogroup 	=> Is performed when you want to join RDDs with duplicate keys. 
				-> groupByKey -> fullOuterJoin

		rdd1 => [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
			-> (key1, [10,7]) (key2, [12,6]) (key3, [6])	

		rdd2 => [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
			-> (key1, [5,17]) (key2, [4,7]) (key4, [17])	

			=> (key1, ([10,7], [5,17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))

		rdd3 = rdd1.cogroup(rdd2)

  RDD Actions
  ----------

   1. collect

   2. count

   3. saveAsTextFile 

   4. reduce			P: U, U -> U
				reduces the entire RDD into one final value of the same type as the elements
				of the RDD by iterativly applying the reduce function first within partitions and
				the across partitions the outputs of partitions.
		rdd1		
		P0 : 2, 3, 1, 4, 2, 5, 6 -> reduce -> -19 -> 44
		P1 : 7, 8, 9, 0, 9, 8, 7 -> reduce -> -34
		P2 : 6, 5, 4, 5, 6, 7, 8 -> reduce -> -29

		rdd1.reduce( lambda x, y: x - y )

   5. aggregate		-> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.

		
			      rdd1.aggregate( (0,0), 
			        lambda z,v : (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1])
			      )	

   6. take
		rddWords.take(50) 

   7. takeOrdered

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, len)

   8. takeSample

		rdd1.takeSample(True, 15)
		rdd1.takeSample(False, 15)
		rdd1.takeSample(False, 15, 534)

   9. countByValue

   10. countByKey

   11. first

   12. foreach    => Takes a function as parameter and applies the function on all the values of the RDD
		     Does not return anything. 

   
   Use-Case : 	
	From cars.tsv dataset, find out the average weight of make of American origin
	Arrange the data in DESC order of average weight
	Save the output as a single text file.

	=> Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	Try it yourself
   

  ---------------------- 
   spark-submit command
  ----------------------  

    Is single command to submit any spark application (scala, java, python, R) to any 
    cluster manager (local, spark standalone, yarn, mesos, kubernetes). 

	spark-submit [options] <app jar | python file | R file> [app arguments]

        spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		wordcount.py   [application arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput 1

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


   Closure
   --------
	-> A closure constitutes all the code (variables and methods) that must be visible inside an 
	   executor for performing computations on the RDD.

	-> The closure is a serialized a separate local copy is sent to every executor.

		c = 0		# counter

		def isPrime( a ):
			if (a is Prime) return 1
			else return 0
		
		def f1( n ):
			global c
			if (isPrime(n) == 1) c += 1
			return n *2

		rdd1 = sc.parallelize( range(1, 4001), 4 )

		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(c)     # 0

		
	We can not use "local variables" which are part of closures to implement global counter. 
	We have use "Accumulator" variables in this case.


   Shared Variables
   ----------------

	1. Accumulator

		-> Is a shared variable that can be added to by multiple distributed tasks
		-> Mainitained by the driver.
		-> Not part of function closure and hence is not a local copy. 


		c = sc.accumulator(0)		# accumulator for global counter

		def isPrime( a ):
			if (a is Prime) return 1
			else return 0
		
		def f1( n ):
			global c
			if (isPrime(n) == 1) c.add(1)
			return n *2

		rdd1 = sc.parallelize( range(1, 4001), 4 )

		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(c)     # 0


     2. Broadcast

	-> You can convert large immutable collections into broadcast variables
	-> Is not part of the function closure and hence is not a local variable
	-> One copy of the variable is sent to every executor
	-> All tasks within that executor can read from that single copy (per executor)	


		d = {1: a, 2: b, 3: c, 4: d, 5: e, 6: f, ......}    // 100 MB
		bcDict = sc.broadcast( d )	

		def f1( n ) :
			global bcDict 
			return bcDict.value[n]

		rdd1 = sc.parallelize( [1,2,3,,4,5,6, ...], 4 )
		rdd2 = rdd1.map( f1 )

		rdd2.collect()    

   =============================================
       Spark SQL
   =============================================
   
     => Spark Structured data processing API

		Structured data files:  Parquet (default), ORC, JSON, CSV (delimited text)
		JDBC format : RDBMS database, NoSQL databases.
		Hive

     => Spark Session
	 -> Starting point of execution in Spark SQL
 	 -> Represents a user-session (with its own configuration) within an application

	   spark = SparkSession \
        	  .builder \
        	  .appName("Dataframe Operations") \
        	  .config("spark.master", "local[*]") \
        	  .getOrCreate()  

	
		











