
  Agenda
  -------

   Pre-requisite: Python

   Spark - Basics & Architecture
   Spark Core API
	-> RDD Transformations & Actions
	-> Shared variables
   Spark SQL
	-> DataFrame Operation
   Spark MLlib & Machine Learning
   Introduction to Spark Streaming 	


  Materials
  ---------
     -> PDF Presentations
     -> Code Modules
     -> Class Notes
     -> GitHub: https://github.com/ykanakaraju/pyspark	


  Spark
  -----
    -> Spark is written in 'Scala' programming language
    -> Spark is a unified in-memory distributed computing framework for big data analytics.

    -> Spark is a polyglot
	-> Spark programs can be written in Scala, Java, Python and R

    -> Spark programs can be submitted to multiple cluster managers
	 -> Spark Standalone Scheduler, YARN, Mesos, Kubernetes.   

  Cluster
  -------
    -> Is a unified entity consisting of many nodes whose cumulative resources can be used to
       distributed your storage and processing.

   -> in-memory distributed computing : The intermediate results can be persisted in RAM and sebsequent tasks
      can further process these in-memory results. 

   Spark Unified Framework
   -----------------------
      Spark provides a consistent set of APIs for processing various analytics workloads based
      on the same execution engine.

	-> Batch Analytics of unstructured data	: Spark Core
	-> Batch Analytics of structured data	: Spark SQL
	-> Streaming Analytics			: Spark Streaming, Structured Streaming
	-> Predictive Analytics (ML)		: Spark MLlib
	-> Graph Parallel Computations		: Spark GraphX


   Spark Architecture
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


    Getting started with Spark
    --------------------------

    1. Working in your vLab

	-> Connect to Windows server
	-> On the desktop, click on CentOS7 and enter your username and password
		(refer to README.txt file on the desktop for username and password)
	-> This is your lab

	-> Open a terminal and connect to pyspark  (type 'pyspark' at the prompt)
		-> pyspark

      	-> Open a terminal and connect to Jupyter Notebook
		-> jupyter notebook --allow-root

    2. Setting up PySpark environment on your local machine.
	
	 -> Download and install 'Anaconda Navigator'
		-> URL: https://www.anaconda.com/products/individual

	 -> You can try pip install
		-> Open Anaconda Prompt
		-> Type "pip install pyspark"
       
         -> If pip install is not work, then try setting up pyspark as per the instructions
	    given in the shared document. 

		-> https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Comminity Edition account.
	
	URL: https://databricks.com/try-databricks
	
	-> Signup with a valid email address
	-> Create an account by following the steps mentioned in the email. 
	-> Login to Databricks using userid and password. 
	-> Go though "Quickstart Tutorial"
		
     			
   RDD (Resilient distributed dataset)
   -----------------------------------
	
      -> Fundamental data abstraction of Spark
 
      -> Is a collection of distributed in-memory partitions. 
	  -> A partition is a collection of objects of some type.
	
      -> RDDs are immutable

      -> RDDs have two components:
		-> Meta Data : Lineage DAG (logical plan) maintained by the driver
		-> Data	: in-memory partitions

       -> RDDs are lazilty evaluated.


   How to create RDDs ?
   --------------------
   Three ways:

	1. Creating RDDs from external data files

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The default numPartitions is given by the value of sc.defaultMinPartitions property.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating RDDs from programmatic data
	
		rdd10 = sc.parallelize( [1,2,3,5,3,4,6,7,8,9,0,3,4,5,7,6,8,9,0,7,8] )
		-> The default numPartitions is given by the value of sc.defaultParallelism property
		   (whose value is equal to the number CPU cores allocated to the application)

		rdd10 = sc.parallelize( [1,2,3,5,3,4,6,7,8,9,0,3,4,5,7,6,8,9,0,7,8], 3 )


	3. By applying tranformations on existing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   What can you do with RDDs ?
   ---------------------------	
	Two things

	1. Transformations
		-> Only rdds are 'created'
		-> That means, a logical plan on how to create the RDD partitions (called Lineage DAG)
		   is created and maintained by driver.
		-> Actual executions of the tasks is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan to physical plan and the driver sends the required tasks
		   to the executors to compute the partitions of the RDD.


   RDD Lineage DAGs
   -----------------

    RDD Lineage DAG refers to a logical plan maintained by driver which contains all the dependencies that
    caused the creation of this RDD all the way from the very first RDD.

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rdd1 =>  (4) rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
	Lineage DAG of rdd2 =>  (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: x.upper())
	Lineage DAG of rdd3 =>  (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG of rdd4 =>  (4) rdd4 -> rdd3.filter -> rdd2.map -> rdd1.flatMap -> sc.textFile


   RDD Persistence
   ---------------	
	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )       -> instruction (in the DAG) to persist the rdd partitions.
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	lineage DAG of rdd6 : rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	sc.textFile -> t3 -> t5 -> t6 -> rdd6 ==> collected to the driver	

	rdd7.collect()
	lineage DAG of rdd7 : rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	t7 -> rdd7 ==> collected to the driver

        rdd6.unpersist()

	
	Storage Levels
        --------------
	Note: PySpark does not support 'deserialized persistence'

	1. MEMORY_ONLY		:  default, memory serialized 1x replicated
	2. MEMORY_AND_DISK	:  disk memory serialized 1x replicated
	3. DISK_ONLY		:  disk serialized 1x replicated
	4. MEMORY_ONLY_2	:  memory serialized 2x replicated
	5. MEMORY_AND_DISK_2	:  disk memory serialized 2x replicated

	Commands
	--------
		-> rdd1.persist()       			# memory serialized 1x replication
		-> rdd1.persist( StorageLevel.MEMORY_AND_DISK ) 
		-> rdd1.cache()
	
		-> rdd1.unpersist()


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Transformations
   -------------------

    => All transformations return an RDD as an output.

   
   1. map			P: U -> V
				Object to Object transformation
				input RDD: N objects, output RDD: N objects

	  	rddFile.map(lambda x: len(x.split(" "))).collect()


   2. filter			P: U -> Boolean
				Output RDD will have only those objects for which the function returns True.
				input RDD: N objects, output RDD: <= N objects		

		rdd2.filter(lambda x: x[1] % 2 == 0).collect()


   3. glom			 P: None
				 Creates one list object per partition with all the elements of the partition.

		rdd1			rdd2 = rdd1.glom()

		P0 : 4,1,2,5,3,4  -> glom -> P0 : [4,1,2,5,3,4]
		P1 : 7,1,6,2,5,0  -> glom -> P1 : [7,1,6,2,5,0]
		P2 : 9,2,5,3,7,0  -> glom -> P2 : [9,2,5,3,7,0]
 		
	       rdd1.count() = 18 (int)	 rdd2.count() = 3 (list)

		rdd1.collect() 		=> [4,1,2,5,3,4,7,1,6,2,5,0,9,2,5,3,7,0]
		rdd1.glom().collect() 	=> [[4,1,2,5,3,4], [7,1,6,2,5,0], [9,2,5,3,7,0]]


   4. flatMap			 P: U -> Iterable[V]
				 flapMap flattens the iterables produced by the function.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions		 P: Iterable[U] -> Iterable[V]
				 Partition to Partition transformation
				 Applies a function on the entire input partition to create an output Partition.

		rdd1			rdd2 = rdd1.mapPartitions(lambda x: [sum(x)])

		P0 : 4,1,2,5,3,4  -> mapPartitions -> P0: 19
		P1 : 7,1,6,2,5,0  -> mapPartitions -> P1: 21
		P2 : 9,2,5,3,7,0  -> mapPartitions -> P2: 26		

		rdd1.mapPartitions(lambda x: [sum(x)]).collect()
		rdd1.mapPartitions(lambda x : map(lambda a: a*10, x)).collect()


   6. mapPatitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions, but we get partition-index as an additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, x: [(i, sum(x))]).collect()
		rdd1.mapPartitionsWithIndex(lambda index, data : map(lambda a: (index, a*10), data)).collect()

		rdd2 = rdd1.mapPartitionsWithIndex(lambda i, d : map(lambda a: (i, a), d)) \
		           .filter(lambda x: x[0] == 1) \
                           .map(lambda x: x[1])

		Here rdd2 will have only the elements of partition 1 of rdd1.
		(simpler solution: rdd2 = sc.parallelize(rdd1.glom().collect()[1]))


    7. distinct			P: None, Optional: numPartitions
				Returns only the distinct objects of the RDD.

		rdd2 = rdd1.distinct()
		rdd2 = rdd1.distinct(5)   # rdd2 will have 5 partitions

   
    8. sortBy			P: U -> V, Optional : ascending (True/False), numPartitions
				The objects of the output RDD are sorted based on the function output.

		rddWords.sortBy(lambda x: x[1]).collect()
		rddWords.sortBy(lambda x: x[1], False).collect()   # desc sort
		rdd1.sortBy(lambda x: x%6, True, 5).collect()	   # can specify numPartitions as third parameter

		Note: All objects with the same function output will be there in the same partition.


   Types of RDDs
   --------------     
	
	-> Generic RDD : RDD[U]
	-> Pair RDD :    RDD[(U, V)]


    9. mapValues		P: U -> V
				Applied to Pair RDDs only
				Transforms the value part of the (K,V) pairs by applying a function.

	   rdd3 = rdd2.mapValues(lambda x: (x, x+10))  # x represent the value part of the (K, V) pairs


    10. groupBy			P: U -> V, Optional: numPartitions
				Groups the elements of the RDD based on the function output.
				Returns a pair RDD where
				  key -> each unique value of the function output
				  value -> ResultIterable objects contain RDD objects that produced the key.

				 RDD[U].groupBy(U -> V) => RDD[(V, ResultIterable[U])]

		rdd1.groupBy(lambda x: x > 5).mapValues(list).collect()


		rdd1 = sc.textFile("E://Spark//wordcount.txt", 3) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: List of ratios (ex: [0.6, 0.4]). Optional: seed
				Returns a list of RDDs randomly split in the given ratios.

		rddlist = rdd1.randomSplit([0.4, 0.3, 0.3])
		rddlist = rdd1.randomSplit([0.4, 0.3, 0.3], 464435)  # here 464435 is a seed.

 
  12. repartition		P: numPartitions
				Is used to increase or decrease the number of output partitions. 
				Cause global shuffle of data

		rdd11 = rdd10.repartition(6)    # rdd11 will have 6 partitions
		rdd12 = rdd11.repartition(10)   # rdd11 will have 10 partitions

	
   13. coalesce			P: numPartitions
				Is used to only decrease the number of output partitions. 
				Causes partition-merging

		rdd11 = rdd10.coalesce(6)	# rdd11 will have 6 partitions


   14. partitionBy		P: numPartitions, Optional: partitioning function
				Is applied only to pair RDDs.
				Is used to control which objects goes to which partitions based on the key.

		rdd3 = rdd2.partitionBy(4, lambda x: x + 10)
		rdd3 = rdd2.partitionBy(4)  # hash is used for partitioning


   15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				    # of output partitions
	----------------------------------------------------------
	rdd1.union(rdd2)			M + N, narrow
	rdd1.intersection(rdd2)			M + N, wide
	rdd1.subtract(rdd2)			M + N, wide
	rdd1.cartesian(rdd2)			M * N, wide


   ..ByKey transformations
   -----------------------
      	-> Wide transformations
	-> Applied to only pair RDDs
   	
     16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the RDD based on the key.

		rddPairs.sortByKey().glom().collect()
		rddPairs.sortByKey(False).glom().collect()
		rddPairs.sortByKey(False, 6).glom().collect()


    17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD with unique keys and grouped values (ResultIterable)
				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]

				Note: Avoid groupByKey, very inefficient
				
		rdd1 = sc.textFile("E://Spark//wordcount.txt", 3) \
        			.flatMap(lambda x: x.split(" ")) \
        			.map(lambda x: (x, 1)) \
        			.groupByKey() \
        			.mapValues(len) \
        			.sortBy(lambda x: x[1], False, 1)


     18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				reduces all the values of each unique key to one value of the same type
				by iterativly applying the function.
	
			rdd1 = sc.textFile("E://Spark//wordcount.txt", 3) \
        			.flatMap(lambda x: x.split(" ")) \
        			.map(lambda x: (x, 1)) \
        			.reduceByKey(lambda x, y: x + y, 1) \
        			.sortBy(lambda x: x[1], False, 1)


     19. aggregateByKey		
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce.
		2. Sequence function: 
			-> merges all the values of each unique key in each partition with the zero-value
			-> We get one aggreated value per unique-key in each partition. 			        
		3. Combine function: 
			-> reduces all the aggregates values of each unique key per partition produced 
			   by seq.fn into one final value.

student_rdd = sc.parallelize([
  ("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  ("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  ("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  ("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  ("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  ("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  ("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)

avg_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
            .aggregateByKey((0,0),
                            lambda z,v: (z[0] + v, z[1] + 1),
                            lambda a,b: (a[0] + b[0], a[1] + b[1])) \
            .mapValues(lambda x : x[0]/x[1]) \
            .sortBy(lambda x: x[1], False)


	
      20. Joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			
			RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]
			
		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)


     21. cogroup	-> Is used to join RDDs with duplicate keys
			-> groupByKey -> fullOuterJoin

	rdd1 -> [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		groupBykey:  [(key1, [10, 7]) (key2, [12,6]) (key3, [6])]
	
	rdd2 -> [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		groupBykey:  [(key1, [5, 17]) (key2, [4,7]) (key4, [17])]

	rdd1.cogroup(rdd2) 
	=> (key1, ([10, 7], [5, 17])), (key2,([12,6],[4,7])), (key3,([6], [])), (key4, ([], [17])) 

   
   Recommendations
   ---------------
	-> The size of the should be around 128 MB
	-> The number of partitions should be a multiple of number of cores allocated.
	-> If the number of partitions is less than but close to 2000, bump it up to 2000. 
	-> The number of cores in each executor should be 5
	
  
   RDD Actions
   ------------

   1. collect

   2. count

   3. saveAsTextFile	=> rdd1.saveAsTextFile("E:\\PySpark\\output\\rdd1")

   4. reduce		=> P: (U, U) -> U
 			   Reduces the entire RDD to one final value of the same type by iterativly applying
			   the reduce first within every partition (narrow) and then across partition (wide)				
		
		rdd1
		P0: 9, 3, 5, 4, 6, 7, 0 -> reduce -> 34  => 100
		P1: 8, 9, 2, 4, 3, 6, 8 -> reduce -> 40
		P2: 0, 0, 8, 7, 6, 5, 0 -> reduce -> 26

		rdd1.reduce(lambda x, y: x + y)
		9,3,5,4,6,7,0 => 12,5,4,6,7,0 => 17,4,6,7,0 => 21,6,7,0 => 27,7,0 => 34,0 => 34
		34, 40, 26 => 74, 26 => 100

   5. aggregate

		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

		rdd1.aggregate( (0,0), 
                                lambda z,v : (z[0]+v, z[1]+1), 
                                lambda a,b: (a[0]+b[0], a[1]+b[1]) )	

   
   6. take		=> take(n) returns a list with first n objects  ex: rdd1.take(10)

   7. takeOrdered

		rdd1.takeOrdered(20)
		rdd1.takeOrdered(20, lambda x: x%3)

   8. takeSample
		rdd1.takeSample(True, 10)        # with replacement sampling
		rdd1.takeSample(True, 100)   	 # sampling count can be > rdd.count()     
		rdd1.takeSample(True, 10, 3453)  # 3453 is a seed

		rdd1.takeSample(False, 10)	 # without replacement sampling
		rdd1.takeSample(False, 10, 3453)

   9. countByValue

   10. countByKey

   11. foreach		-> applies a function on all objects of the RDD
			   does not return any value.

   12. first

		
   Use-Case
   --------
   From cars.tsv dataset, find out the average-weight of each make from among American cars.
   Arrange the data in DESC order of average-weight
   Save the output as a single text file. 

	=> Try it yourself
	=> dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv


   Closure
   -------
      -> A closure contains all the code (variables and methods) that must be visible in an executor
	 for performing its computations on the RDD. 

      -> The closure is serialized and copy is sent to every executor by the driver.

	c = 0

	def isPrime(a):
		returns 1 if n is prime
		returns 0 if n is not prime

	def f1 (n) :
		global c
		if (isPrime(n) == 1) c = c + 1
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()
	
	print(c)      # returns 0 only - this closure issue.


	Limitations of Closures
        -----------------------
	-> The local variables in the closure can not be used to implement global counter.
	-> Hence you can not use local variable for implementing global counters.


   Shared Variables
   ----------------

    1. Accumulator variable

	=> Is a shared varible (shared by multiple tasks)
	=> Maintained by the driver
	=> Not part of function closure
	=> All tasks can add to this variable (using 'add' method)
	=> Is used to implement 'global counter'
	

	c = sc.accumulator(0)

	def isPrime(a):
		returns 1 if n is prime
		returns 0 if n is not prime

	def f1 (n) :
		global c
		if (isPrime(n) == 1) c.add(1)
		return n*2

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
	rdd2.collect()	
	print( c.value )  



   2. Broadcast variable

	-> Is a single variable that is broadcasted to all executors by the driver.
	-> Is not part of function closure, and hense not a local variable inside every task.
	-> All tasks in that executor can read from that variable.

	-> Use broadcast variable when you have large immutable collection (such as a lookup table)
	   that fits in the executor memory. 


	d1 = sc.broadcast({1:a, 2:b, 3:c, 4:d, 5:e, 6:f, ...})   // 100 MB

	def f1( k ):
		global d1
		return d1.value[k]

	rdd1 = sc.parallelize([1,2,3,4,5,6, ...])

	rdd2 = rdd1.map( f1 )

	rdd2.collect()   

	
	example
	-------
	lookup = sc.broadcast({1:'a', 2:'e', 3:'i', 4:'o', 5:'u'}) 
	result = sc.parallelize([2, 1, 3, 4, 5]).map(lambda x: lookup.value[x]) 
	print( result.collect() )

	

    Spark-Submit
    ------------

	-> Is a single command used to submit any spark application (scala, java, python, R)
	   to any cluster manager (local, spark standalone scheduler, YARN, Mesos, k8s)

	   spark-submit [options] <app jar | python file | R file> [app arguments]

	   spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5		
		--num-executors 20
		wordcount.py [application arguments]

	
	  spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount.py
	  spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput 1
	

  ======================================
      Spark SQL
  ======================================
         
       
      















