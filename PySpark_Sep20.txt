
   Agenda  (7 sessions of 4 hours each)
   ------------------------------------

   	- Spark - Basics & architecture
	- Spark Core API (RDD API) - Low Level API
	    - RDD Transformations & Actions	
	- Spark SQL - DataFrames API
	- Machine Learning & Spark MLlib
	- Introduction Spark Streaming

  ---------------------------------------------------

   What is Big Data ?

	-> Any data that is so big and complex that it become hard to store and process
	   using onhand data management systems and traditional application models.

   
   Computing Cluster
   	-> A unified entity containing a lot of nodes whose cumulative resources can be used
	   distribute storage and process.
   
   Hadoop      
       	-> is an opensource framework for storage and processing of big data.
	-> runs on a cluster made of commodity hardware.

	-> Provides two frameworks for big data

	1. HDFS (Hadoop distributed file system)
		=> Distrinuted Storage Solutions
		-> Splits the file into blocks of 128 MB each.
		-> Spread the bloks of the file across many nodes in the cluster	
		-> Each block is replicated in three different nodes for fail-safety.

	2. MapReduce 
		-> Distributed Processing Solutions
		-> Distributes the processing across many machines (Map phase) and then
		   aggregates the results produced by all these mapper instances (reduce phase) 		    
  
   MapReduce is not good at few use-cases:

	-> Not good with lot of small files
	-> Not good with ad-hoc querying or random-access of data
	-> Not good with iterative computations.

	
   What is Spark ?
   ---------------

	-> Spark is a unified in-memory distributed processing framework
	-> Spark is written in Scala
	-> Spark supports multiple languages: Scala, Java, Python & R
	-> Spark applications can run on mulitple cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes. 
	
	In-memory framework
	--------------------
	   -> The inetermediate partitions can be persisted in memory and further tasks
	      can be launched on these in-memory persisted partitions.

	Unified framework
	------------------

	   -> Spark provides a cosnistenet set of APIs for different analytics workloads
	      running on the same execution engine. 


		Hadoop Ecosystem
                -----------------
		Batch Analytics of unstructured data  	: MapReduce
		Batch Analytics of Structured data	: Hive, Impala, Drill, HBase
		Streaming (real time) Analytics		: Kafka, Storm, Samza
		Predictive Analytics using ML		: Mahout
		Graph parallel computations		: Giraph

		Spark Framework	
		---------------
		Batch Analytics of unstructured data  	: Spark Core API
		Batch Analytics of Structured data	: Spark SQL
		Streaming (real time) Analytics		: Spark Streaming, Structured Streaming
		Predictive Analytics using ML		: Spark MLlib
		Graph parallel computations		: Spark GraphX

	
	Spark Layered Structure
	-----------------------

	Programming Lang  : Scala, Java, Python, R
	Spark High Level  : Spark SQL, Spark Streaming, Sparl MLlib, Spark GraphX
	Spark Low Level	  : Spark Core API (RDDs)
	Cluster Managers  : Spark Standalone, YARN, Mesos, Kebernetes  
	Storage Layer     : HDFS, Linux, RDBMS, NoSQL, Kafka


   Getting started with Spark
   --------------------------
	Pre-requisite. 
		-> Install Anaconda navigator
			Download and install from the following URL
			https://www.anaconda.com/products/individual-d#windows

	1. Install Spark and run PySpark Shell
		https://spark.apache.org/downloads.html

		Download Spark xxx.tgz file and extract it to a suitable folder.

		Setup your environment varibles:
		-> Add SPARK_HOME & HADOOP_HOME env. variables and point them to Spark installation folder. 
		-> Add bin folder of the spark installation folder to the PATH environment var. 
		-> PYTHONPATH -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH% 
	 
	2. Using an IDE (such as Spyder or PyCharm)		
		-> Follow the steps mentioned in the document shared with you.

	3. Signup to free Databricks community edition account
		https://databricks.com/try-databricks
		-> Spend some time exploring the Quickstart tutorial


    Spark Architecture
    ------------------
  
     1. Cluster manager (CM)
	-> Jobs are submitted to CM
	-> CM schedules the job and allocates resources to the job

     2. Driver Process
	-> Driver process is the first process that gets created.
	-> Driver process manages the user-code and lauches tasks on the cluster.
	-> Driver contains a "SparkContext" object  (or "SparkSession" in the case of Spark SQL).
	
	Deploy-modes:
	
	1. Client Mode (default) -> the driver process runs on the client machine
	2. Cluster Mode		 -> the driver runs on one the node in the cluster

     3. Executor Processes

	-> Different tasks as per the programming logic are sent to be executed on various
	   executors allocated by the Cm to the application
	-> All tasks does the same process (logic) but of different partitions of data.
	-> report the status of the tasks to the driver

      4. SparkContext
	 -> represents an application contextand connection to the cluster.	
	 -> is the first objects that gets created (starting point of Spark core application).
	 -> is the link between driver and different tasks running on the cluster.



    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
		-> Each partition is a collection of objects	

	-> RDDs are immutable
		-> You can not change the content of a partition.

	-> RDD has two components
		1. RDD lineage DAG -> logical plan that describes how to compute the RDD partitions
		2. RDD DAG	   -> in-memory partitions

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands trigger execution

	-> RDDs are resilient
		-> RDDs are resilient to the missing in-memory partitions. RDDs can recreate
		   such missing partition on the fly and continue the tasks on them. 


   How to create RDDs?
   -------------------

	There are three ways:

	1. We can create RDDs from external data

		rdd1 = sc.textFile( <filePath> )

		-> default number of partitions is decided by a config property of SparkContext
		   called "sc.defaultMinPartitions" whose value is 2 if the number of cores 
		   allocated is atleast 2.

		rdd1 = sc.textFile( filePath, 4 )    # 4 partitions


	2. We can create RDD from programmatic data using parallelize

		rdd1 = sc.parallelize([2,3,2,4,5,6,7,8,9,6,6,7])

		-> default number of partitions is decided by a config property of SparkContext
		   called "sc.defaultParallelism" whose value is equal to the number of cores 
		   allocated to the application.


	3. By applying transformations on an existing RDD

		rdd2 = rdd1.map(lambda x: x.upper())


   What are RDD Lineage DAGs
   -------------------------
	-> Maintained by the driver
	-> Is a DAG of dependencies that caused the creation of the RDD all the way from the 
	   first RDD.

	rdd1 = sc.textFile( filePath )
	Lineage DAG : 	rdd1 -> sc.textFile	

	rdd2 = rdd1.map(lambda x: x.upper())
	Lineage DAG : 	rdd2 -> rdd1.map -> sc.textFile	   

	rdd3 = rdd2.filter(lambda a: len(a) > 50) 
	Lineage DAG : 	rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile	 
	
	rdd4 = rdd3.map(lambda x: len(x))
	Lineage DAG : 	rdd4 -> rdd3.map -> rdd2.filter -> rdd1.map -> sc.textFile
		
	rdd3.collect()  => sc.textFile -> map -> filter -> rdd3
	
   What can you do with an RDD ?
   -----------------------------

	Only two things:

	1. Transformations
		-> Does not cause execution
		-> They do not produce any output. They produce only other RDDs
		-> They cause the creation of lineage DAGs

	2. Actions
		-> Trigger execution
		-> Converts the lineage DAg (logical plan) into a physical execution plan

 

  Types of Transformations
  ------------------------

	Two types:

	1. Narrow transformations		
		-> Data shuffling does not happen
		-> The computation of each partition is dependent on only its input partition
		-> partition to partition tranformations
		-> map, filter, flatmap, glom, mapPartitions, ...

	2. Wide transformations
		-> Data shuffling happens
		-> The computation of each partition dependends on multiple input partitions
		-> output RDD may have different number of partitions than input RDD
		-> distinct, sortBy, groupBy, ...ByKey 


  RDD Persistence
  ---------------
      	rdd1 = sc.textFile( file )
	rdd2 = rdd1.t2(..)	
	rdd3 = rdd1.t3(..)	
	rdd4 = rdd3.t4(..)	
	rdd5 = rdd3.t5(..)	
	rdd6 = rdd5.t6(..)	
	rdd7 = rdd6.t7(..)
	rdd7.persist()         ---> instruction to spark to not GC rdd7 automatically. 	
	rdd8 = rdd7.t8(..)

	rdd7.collect()
	
	Lineage rdd7 => rdd7 -> rdd6.t7	-> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> t7 (rdd7) ---> collect()
	
        rdd8.collect()

	Lineage of rdd8 => rdd8 -> rdd7.t8 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		rdd7.t8 -> rdd8 -> collect()

	
	Persistence   -->  in-memory deserialized
			   in-memory serialized
			   on-disk
	
    Commands
    ---------
	rdd.persist()
	rdd.persist( StorageLevel.MEMORY_AND_DISK )
	rdd.cache()
	rdd.unpersist()  // delete the persisted partitions

   Storage Levels
   --------------
	MEMORY_ONLY (default)   -> stores in RAM as deserialized object
				
	MEMORY_AND_DISK		-> stores in RAM if available, or stores on disk.

	DISK_ONLY

	MEMORY_ONLY_SER		-> stores in RAM in serialized format.

	MEMORY_AND_DISK_SER

	MEMORY_ONLY_2

	MEMORY_AND_DISK_2	
      

  Executor Memory Structure
  --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Transformations
  -------------------
	-> Transformations does not cause execution.
	-> The output of every transformation is 

   1. map  		=> P: U -> V 
			   Element to element transformation
			   input RDD: N elements, output RDD: N elements 

   2. filter		=> P: U -> Boolean
			   Only those elements for which the function returns True will be
			   in the output partition.
			   input RDD: N elements, output RDD: <= N elements 

   3. glom		=> P: None
			   Transform all the elements of each partition into one array object
			   in the output RDD

	    rdd1		rdd2 = rdd1.glom()
	P0: 1,2,4,3,5,7  --> glom  --> P0: [1,2,4,3,5,7]
	P1: 1,2,8,9,4,7  --> glom  --> P1: [1,2,8,9,4,7]
	P2: 8,8,1,1,3,3  --> glom  --> P2: [8,8,1,1,3,3]		
	rdd1.count = 18			rdd2.count = 3

   4. flatMap		=> P: U -> Iterable[V]
			 Flattens all the elements of the iterables produced by the function.
			 input RDD: N elements, output RDD: >= N elements 

   5. distinct		=> P: None, optional: number of output partitions.
			Return distinct elements from the RDD (removes the dups)
			   
		
   6. mapPartitions	=> P: Iterator[U] -> Iterator[V]
			   The function is applied to the entire partition.

		rdd1		rdd2 = rdd1.mapPartitions( lambda x: ... )

	P0: 1,2,4,3,5,7  --> mapPartitions  --> P0:  2,4,8,6,10,14
	P1: 1,2,8,9,4,7  --> mapPartitions  --> P1: 
	P2: 8,8,1,1,3,3  --> mapPartitions  --> P2: 		
	rdd1.count = 18			


   7. mapPartitionsWithIndex	=> p: Int, Iterator[U] -> Iterator[V]
				The function is applied to the entire partition.

   Two types of RDDs:
	1. Generic RDD: RDD[U]
	2. Pair RDD:	RDD[(U, V)]

	
   8. mapValues		=> P: U -> V
			   Applied only to pair RDDs
			   Transforms the 'value' part of the (K,V) pairs by applying the function.

	rddPairs2 = rddWords.map(lambda x: (x, (x, len(x)))  
		-> where x (fn input) represents the 'value' of the (K, V) pairs


   9. sortBy		=> P: U -> V
			  The objects of the RDD are sorted according the value of the function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
	        rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 10).glom().collect()
		rdd1.sortBy(lambda x: x%3, numPartitions = 10).glom().collect()

   10. groupBy		=> P: U -> V
			   Returns a pair RDD, where the 'key' is the unique value of the function output
			   and 'value' is an iterable (ResultIterable) object containing all RDD objects
			   that produced the same key.

			   RDD[U].groupBy( U -> V, [numPartitions] ) => RDD[(V, ResultIterable[U])]
			
	wordcount solution:
	
	rdd1 = sc.textFile(inputFile) \
        	.flatMap(lambda x: x.split(" ")) \
        	.groupBy(lambda x: x) \
        	.mapValues(len) \
        	.sortBy(lambda x: x[1], False) \		
		.saveAsTextFile("E:\\PySpark\\output\\wc")


   11. randomSplit	=> P: Array of ratios
			   returns an array of RDDs approximatly split randomly in the sepecified ratios.
		
	rddArr = rdd1.randomSplit([0.5, 0.5])
	rddArr = rdd1.randomSplit([0.5, 0.5], 4645 )    // here 4645 is a seed.


   12. repartition	=> P: numberOfPartitions
			  Used to increase or decrease the number of partitions of the output RDD
			  This is important in case of 'narrow' transformation.
			  Results in global shuffle
			  Output partitions are approximatly equally sized.

   13. coalesce		=> P: numberOfPartitions
			  Used to only "decrease" the number of partitions
			  Causes partition-merging (no global shuffle)

   14. partitionBy	=> P: numOfPartitions, optional: partitioningfunction
			  Applied ONLY to pair RDD
			  Is used to control partition arrangement. We can programmatically control
			  which elements go to which partition based the key of the (k,v) pairs.


   15. union, intersection, subtract, cartesian
			=> P: RDD[U].union(RDD[U])
	
  	-> Let us say we have rdd1 with M partitions and rdd2 with N partitions

	command				numPartitions
     	-------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow	
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


   ..ByKey Transformations
	-> Are wide transformations
	-> Applied ONLY to Pair RDD
	-> They operate on the values of each unique key
		
	
   16. sortByKey		P: ascending, optional: numPartitions
				Sorts the elements of the RDD based on the key
				The the elements with the same key will always be in the same partition.

		rddPairs.sortByKey().glom().collect()   	// asc sort
		rddPairs.sortByKey(false).glom().collect()   	// desc sort
		rddPairs.sortByKey(True, 6).glom().collect() 	// asc with 6 partitions
		
   17. groupByKey		P: None, optional: numPartitions
				Groups the elements based on the key.
				Results in global shuffle and is very inefficient.
				Note: Try to avoid if possible..
  
		rdd1 = sc.textFile(inputFile) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)

   18. reduceByKey		P: U, U -> U	
				reduces all the values of each unique key by iterativly applying the 
				reduce function within each partition first, and then across partitions.

		rdd1 = sc.textFile(inputFile) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)

   19. aggregateByKey

   20. join transformations      => join (inner join), leftOuterJoin, rightOuterJoin, fullOuterJoin
			
				    RDD[(U, V)].join(RDD[(U, W)]) => RDD[(U, (V, W))]		

             		--> rdd1.join(rdd2).join(rdd3).join(rdd4)

   21. cogroup			=> Is used group RDDs that have duplicate keys
				   groupByKey on each RDD -> fullOuterJoin

	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	-> [(key1, [10, 7]), (key2, [12, 6]), (key3, [6])]

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
	-> [(key1, [5, 17]), (key2, [4,7]), (key4, [17])]

    	[(key1, ([10, 7], [5, 17])), (key2, ([12, 6], [4,7])), (key3, ([6], [])), (key4, ([], [17]))]
		

   Recommendations
   ---------------
	1. Size of the partition : around 128 MB  (100MB to 150MB)
	2. Number of partitions can be upto 2 to 3x the available cores.
	3. If your number of partitions is close to 2000, bump it up to more than 2000

	
  Use-Case (RDD API)
  -------------------
	From cars.tsv dataset give me the average weight of all 'American' makes in the descending order
        of average weight and save the output into a directory as in one output file.

	inputFile = "E:\\PySpark\\data\\cars.tsv"

	def seq_fun(zv, el) :
    		return (zv[0] + el, zv[1] + 1)

	def comb_fun(a, b):
    		return (a[0] + b[0], a[1] + b[1])

	rddOut = sc.textFile(inputFile) \
           .map(lambda s: s.split("\t")) \
           .map(lambda arr: (arr[0], arr[6], arr[9])) \
           .filter(lambda t: t[2] == "American") \
           .map(lambda t: (t[0], int(t[1]))) \
           .aggregateByKey( (0,0), seq_fun, comb_fun) \
           .mapValues(lambda x: x[0]/x[1]) \
           .sortBy(lambda x: x[1], False, 1)

 
  RDD Actions
  ------------
	1. collect	  -> returns an array with all the elements of the RDD
	2. count
	3. saveAsTextFile
	   saveAsSequenceFile -> Only pair RDDs can be saved as sequence files.
	4. take		 -> returns an array with the first N the elements of the RDD
	5. takeOrdered

		rdd1.takeOrdered(4)
		rdd1.takeOrdered(4, lambda x: x%2)

	6. takeSample
	7. countByValue
	8. countByKey	 -> (generally) applied on pair RDDs
	9. first
	10. foreach      -> Takes a function and applies it on all the elements
			    Does not return anything.
	11. reduce		p: U, U -> U
				Reduces the entire RDD into one single value of the same type by iterativly
				applying the reduce function.
				Reduces each partition first (narrow), and then reduces across partitions
	12. aggregate		

		rdd1: 
  		P0: 2, 3, 4, 5   	   -> (14, 4)
  		P1: 6, 7, 3, 4		   -> (20, 4)
		P2: 5, 6, 7, 8		   -> (26, 4)
		P3: 9, 5, 6, 4, 6, 7, 8    -> (45, 7)     
	
	  1. zero-value: ""   (final value is always of the type of zero-value)
	  2. sequence-function: merges all the values within each partition to the zero value.
	  3. combine-function: reduces the values produce for each partition by seq-fn to a 
			       final value of the same type

	 rdd1.aggregate( (0, 0), lambda z, v: (z[0]+v, z[1]+1), lambda a, b : ( a[0] + b[0], a[1] + b[1] ) )



   Closure
   -------
	-> A closure constitute all the code (inclusing external variables and functions) that must
	   be visible for an executor to perform its tasks. 
	-> This closure is serialized and a local copy is sent to every executor.


   Shared Variables
   ----------------

      -> Accumulator
      -> Broadcast

      The problem with local variables:

	count = 0     // count of prime numbers

	def isPrime( n ) :
	   return 1 of n if prime number
	   else return 0

	def f1(n):
	   if (isPrime(n) == 1) count = count + 1
	   return n * 10

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

	print(count)   			
  
	=> Because closures are local copies, the local variable that are part of the closures
	can not be used to implement counter.


    Accumulators:
    -------------

	-> Provide a way to implement global counter
	-> Accumulators are maintained by the driver process.
	-> Accumulators are NOT part of function closure (hense they are not local copies)


	count = sc.accumulator(0)     // count of prime numbers

	def isPrime( n ) :
	   return 1 of n if prime number
	   else return 0

	def f1(n):
	   global count
	   if (isPrime(n) == 1) count.add(1)
	   return n * 10

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

	print(count)   	


    Broadcast variable
    ------------------
	A copy of a broadcast variables are sent to every executor node and all tasks running in that
	node can refer to the same variable. This saves a lot of execution memory.

	   bcDict1 = sc.broadcast({ 1: a, 2: b, 3: c  })  // 100 MB

	   def f1 (key) :
		global bcDict1 
		dict1 = dcDict1.value
		return dict1[key]

	   rdd1 = sc.parallelize([1,2,3,1,2,3,1,2,3,3,3,2,2], 2)
	
	   rdd2 = rdd1.map(f1)      --> rdd2: [a,b,c,a,b,c,a,b,c,c,c,c,b,b] 


   ==========================
     Spark-submit command
   ==========================

     -> Is a single command to submit any spark application (scala, python, java, R) to 
	any cluster manager (YARN, Mesos, Spark Standaline, local etc..)

	spark-submit --master yarn
		     --deploy-mode cluster
		     --executor-memory 10G
		     --driver-memory 5G
		     --executor-core 5
		      --num-executor 100
		       wordcount.py  <commandline-parameters>

	spark-submit --master local[*] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout
  

 ========================================
    Spark SQL   (pyspark.sql)
 ========================================

   -> High level API built on top of Spark Core. 

   -> Structured data processing API
	
	1. Structured data files :  Parquet (default), ORC, JSON, CSV (delimited text)
	2. Hive  (data warehouseing platform built on top of Hadoop)
	3. JDBC format : RDBMS, NoSQL databases.

   -> SparkSession
	-> Starting point of any Spark SQL application
	-> Represents a user session within an application (sc)
        -> A single application (sparkContext) can have multiple SparkSessions

	spark = SparkSession \
        	.builder \
        	.appName("Basic Dataframe Operations") \
        	.config("spark.master", "local") \
        	.getOrCreate() 

	spark.conf.set("sparl.sql.shuffle.partitions", "5")


   -> DataFrame (DF)
	-> All data in Spark SQL (pyspark lib) is expressed as DataFrames
   	-> Note: Dataset is not supported in PySpark.

	-> DF is a collection of immutable, lazily evaluated, in-memory partitions.

	-> DF is collection of "Row" objects (pyspark.sql.Row) with Schema
	        -> DF Meta Data: Schema
		-> DF Data: Partitions containing Row objects
	
		-> DF is similar to "RDD[Row] + schema".
   -> Row
	-> A group of named columns with a datatype
	-> The data types of these columns are Spark SQL's internal types (not python or scala objects)	
	-> Row is a object of type "StructType" (which represents a schema)

	-> schema:

		StructType(
		  List(
		     StructField(age,LongType,true),
		     StructField(gender,StringType,true),
		     StructField(name,StringType,true),
		     StructField(phone,StringType,true),
		     StructField(userid,LongType,true)
		  )
		)

	
  The steps in a Spark SQL applications
  -------------------------------------
    Prerequisite: You need to create a SparkSession object

    1. read/load data from some external source into a dataframe.

		inputFile = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputFile)
		df1 = spark.read.json(inputFile)


    2. process the dataframe using DF transformations API or by applying SQL	

	 Using dataframe API:
		df2 = df1.select("userid", "name", "age", "phone", "gender") \
         		.where("phone is not null") \
         		.groupBy("age").count() \
         		.orderBy("age") \
         		.limit(4)

	 Using SQL
	        spark.catalog.listTables()
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count 
         		from users
         		where age is not null
         		group by age
         		order by age
         		limit 4"""         

		df3 = spark.sql(qry)
		df3.show()
		df3.printSchema()

    3. write/save the DF to a structure data file or Hive or RDBMS database etc.

		outputDir = "E:\\PySpark\\output\\df3_2"
  		df3.write.format("json").save(outputDir)
		df3.write.json(outputDir)
  

  LocalTempView & GlobalTempView
  ------------------------------


  DataFrame Transformations
  --------------------------

   1. select 
	
	 df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	 df2 = df1.select(
		 col("ORIGIN_COUNTRY_NAME"),
                 column("DEST_COUNTRY_NAME"),
                 expr("count"))

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"),
		 lit("India").alias("country")
                )


   2. where / filter
		
	df3 = df2.where("count > 100")
	df3 = df2.filter("count > 100")

	df3 = df2.where(col("count") > 100)
	df3 = df2.filter(col("count") > 100)

 
   3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")

	df3 = df2.orderBy(col("count"), col("origin"))
	df3 = df2.orderBy(col("count").asc(), col("origin").desc())
	df3 = df2.orderBy(asc("count"), desc("origin"))


   4. groupBy with aggregation functions

	 -> groupBy returns a "GroupedData" object (not a dataframe)
	 -> apply aggregation functions to return a dataframe from "GroupedData"

	df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	df3 = df2.groupBy("domestic", "highFrequency").avg("count")
	df3 = df2.groupBy("domestic", "highFrequency").count()

	df3 = df2.groupBy("domestic", "highFrequency") \
         	.agg( count("count").alias("countCount"), 
               	      sum("count").alias("sumCount"),
               	      avg("count").alias("avgCount")
             	   )

   5. limit
		
	df3 = df2.limit(10)

   6. selectExpr

	df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
                 expr("DEST_COUNTRY_NAME as destination"),
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                )

	is same as 

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 "DEST_COUNTRY_NAME as destination",
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"
                )

    7. withColumn

	   df2 = df1.withColumn("newCount", col("count") + 10) \
         	    .withColumn("highFrequency", col("count") > 200) \
         	    .withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))

    8. withColumnRenamed

	   df3 = df2.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	    .withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

    9. drop

	  df4 = df3.drop("newCount", "highFrequency")

   10. union
	
	data = [("India", "Germany", 23),
        	("India", "France", 23),
        	("India", "Italy", 23)]

	df3 = spark.createDataFrame(data) \
           	.toDF("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

	df3.show()
	df3.printSchema()

	df4 = df3.union(df1)


   11. randomSplit

	dfArr = df1.randomSplit([0.4, 0.3, 0.3])
	dfArr = df1.randomSplit([0.4, 0.3, 0.3], 345)  // where 345 is a seed

	dfArr[0].count()
	dfArr[1].count()
	dfArr[2].count()

   12. sample
		
	  df3 = df1.sample(False, 0.05, 45)

	   parameters: 
		1. withReplacement -> True/False
		2. fraction -> decimal number
		3. optional, seed.

   13. distinct
	df3 = df1.select("ORIGIN_COUNTRY_NAME").distinct()


   14. repartition & coalesce

	df1.rdd.getNumPartitions()

	df2 = df1.repartition(4)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(2)
	df3.rdd.getNumPartitions()

	df4 = df1.repartition( 6, col("DEST_COUNTRY_NAME") )
	df4.rdd.getNumPartitions()

	df5 = df4.coalesce(3)
	df5.rdd.getNumPartitions()   


  SaveModes
  ---------
   -> SaveModes control the behavious when the output directory already exists.

	default: errorIfExists
	-> ignore
	-> append
	-> overwrite


  Applying Programmatic Schema
  -----------------------------
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), False),
            StructField("DEST_COUNTRY_NAME", StringType(), False),
            StructField("count", IntegerType(), False)
        ])

	inputFile = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	df1 = spark.read.schema(mySchema).json(inputFile)

 
  Creating DataFrames from Programmatic data
  -------------------------------------------
	data = [(10, 'Raju', 45), 
        	(11, 'Ramesh', 35), 
        	(12, 'Mahesh', 25), 
        	(13, 'Suresh', 15), 
        	(14, 'Ganesh', 25), 
        	(15, 'Rajesh', 45)]
     method 1:
     --------
	df2 = spark.createDataFrame(data).toDF("id", "name", "age")

	df2.show()
	df2.printSchema()

     method 2:
     --------
	rdd = spark.sparkContext.parallelize(data)
	rddRows = rdd.map(lambda x: Row(x[0], x[1], x[2]))
	rddRows.collect()

	mySchema = StructType([
            StructField("id", IntegerType(), False),
            StructField("name", StringType(), False),
            StructField("age", IntegerType(), False)
        ])

	df3 = spark.createDataFrame(rddRows, mySchema)
	df3.show()
	df3.printSchema()


   Joins
   ------
    
     -> inner, left_outer, right_outer, full_outer, left_semi, left_anti

    -> left-semi join
	-> is similar to inner join, but data is fetched from left side table only. 
	-> equivalent to a sub-query:
              select * from emp where deptid in (select deptid from dept)
	
    -> left-anti join	
	-> select * from emp where deptid not in (select deptid from dept)


   Using SQL approach
   -------------------

	spark.catalog.listTables()

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*
         	from emp left anti join dept
         	on emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)

	joinedDf.show()


   Using DF API approach
   ---------------------- 
	supported joins:
	-> inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

   	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(broadcast(department), joinCol, "inner")
	joinedDf.show()


   Join Optimizations
   ------------------

	1. Shuffle Joins   (Big Table to Big Table)
		-> Shuffle Hash Join
		-> Sort Merge Join

	2. Broadcast Joins (Big Table to Small Table)
		-> automatically performed if one of the tables/DFs is small

	
	Config:  spark.sql.autoBroadcastJoinThreshold = 10 MB


   Window Functions
   -----------------

	id	dept	salary	sum("salary")
	------------------------------------	
	7	IT	40000	40000	
	1	IT	50000	90000		
	11	IT	50000	140000		
	2	IT	60000	200000		

	3	Sales	45000	45000		
	8	Sales	45000	90000		
	4	Sales	50000	140000		
	9	Sales	50000	190000		
	10	Sales	55000	245000			

	5	HR	40000	40000		
	12 	HR	40000	80000		
	6	HR	60000	140000		

  window = Window.partitionBy("dept")
		.orderBy("salary")
		.rowsBetween(Window.unboundedPreceding, Window.currentRow)
	
  --> sum("salary").over(window)



  Working with different file formats
  -----------------------------------
    -> JSON, CSV, Parquet, ORC

   JSON

	inputFile = "E:\\PySpark\\data\\flight-data\\json\\*.json"

	df1 = spark.read.json(inputFile)
	
	df2 = df1.where("count> 200")
	df2.write.mode("overwrite").json(outputDir)

   CSV

	df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputFile)
	df1 = spark.read.csv(inputFile, header=True, inferSchema=True)
	df1 = spark.read.csv(inputFile, header=True, inferSchema=True, sep="|")

	df2.write.mode("overwrite").csv(outputDir, header=True)
	df2.write.mode("overwrite").csv(outputDir, header=True, sep="|")

   Parquet (default)

	inputFile =  "E:\\PySpark\\data\\flight-data\\output\\parquet"
	df1 = spark.read.parquet(inputFile)

 	df2.write.mode("overwrite").parquet(outputDir)

   ORC

	inputFile =  "E:\\PySpark\\data\\flight-data\\output\\orc"
	df1 = spark.read.orc(inputFile)

 	df2.write.mode("overwrite").orc(outputDir)


   Working with Hive
   -----------------
	
      ==> Hive is a "data warehousing" platform on Hadoop
	
	warehouse: is an HDFS directory in which Hive stores all its data files
	metastore: an external service (such as MySQL) where hive stores all its meta-data

    	1. create a "Hive enabled" SparkSession
	2. Such a SparkSession automatically treata Hive warehouse as it catalog.
	3. You can HQL statements using the sparkSession.

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 


   Working with MySQL
   ------------------
     ...

   Use-case
   --------
    From movies & ratings datasets, get me the top 10 movies with highest average rating. 
    	-> Consider only those movies that are rated by atleast 30 users
	-> data: movieId, title, totalRatings, avgRating.	
	-> Arrange the data in the DESC order of average rating.
    	-> Save the output as a single CSV file with "|" delimiter. 		

moviesFile = "E:\\PySpark\\data\\movielens\\movies.csv"
ratingsFile = "E:\\PySpark\\data\\movielens\\ratings.csv"

moviesDF = spark.read.csv(moviesFile, header=True, inferSchema=True)
ratingsDF = spark.read.csv(ratingsFile, header=True, inferSchema=True)

moviesDF.show()
ratingsDF.show()

df1 = ratingsDF.groupBy("movieId") \
        .agg(count("rating").alias("totalRatings"), 
             avg("rating").alias("avgRating")) \
        .where("totalRatings >= 30") \
        .orderBy(desc("avgRating")) \
        .limit(10)
        
joinCol = df1["movieId"] == moviesDF["movieId"]

joinedDf = df1.join(moviesDF, joinCol) \
            .drop(moviesDF["movieId"]) \
            .select("movieId", "title", "totalRatings", "avgRating") \
            .orderBy(desc("avgRating")) \
            .coalesce(1)

joinedDf.show()

joinedDf.rdd.getNumPartitions()

outputDir = "E:\\PySpark\\data\\movielens\\output"
joinedDf.write.mode("overwrite").csv(outputDir, header=True, sep="|")


  -------------------------------
  Machine Learning & Spark MLlib 
  ------------------------------
 
    -> The goal of ML is create a trained ML model which learns from historic data and based
       on that learning it can predict futurisic output with out explicit programming.
 
	-> Give you predictions
	-> Projection, forecasting, predictions etc
	-> Can recommend products (Amazon eCom), recommend movies (NetFlix)
	
    Terminology
    -----------
    1. Features    -> inputs (from which a model learns)
    2. Label	   -> output (about which the model is learning)
    3. Algorithm   -> is a iterative mathematic computation on the historical data with a goal	
		      establish a relation/function between the label and feature with minimal
		      possible loss.
    4. Model	   -> Is the output of the algorithmic computation.
    5. Error	   -> variation between redicted and actual for a given data point
    6. Loss Function -> 
    7. Loss	   -> Cumulative error computed using a loss functions. 

   
   Steps in ML Project
   ------------------------

    1. Data Collection
	=> Collection if raw data

    2. Data Prepartion	(60 to 65% of time is spent here)
	=> Is the process of converting the raw data into a format that can be used
	   as training data for an algorithm. 

	=> output: prepared data  (Feature Vector)

	Rules:	All the data must be numeric 
		There should be no null/empty-values.

    3. Training the model	
	=> The prepared data is fit to an algorithm
	=> Algorithm return an ML model

    4. Evaluate the model
	=> Split the training data into two sets: 70 % (training) & 30 % (validation)
	=> Train the model using "training" dataset
	=> Get the preditions using the model for "validation" dataset.   
	=> By comparing the predictions with the labels, you can evaluate the model.

    5. Deploy the model.


  Types of Machine Learning
  -------------------------

	1. Supervised Learning
		Data : Has both "label" and "features" (labeled data)
		Training: Create a relation between label and features.
		Model: Predicts the label given unseen features. 

		1.1  Classification
			-> The label belongs to few fixed values.
			-> Label:  1/0, True/False, Yes/No, [1,2,3,4,5] 
			-> Ex: Email Spam, Dignostics, Survival Prediction

		1.2  Regression		
			-> The label is a continuous value
			-> House Price pridiction

	2. Unsupervised Learning
		Data: Has only features. no label
		Training: Understanding patterns in the data
		Model: Groups the data into mulitple categories

		2.1   Clustering
	
		2.2   Dimensionality Reduction

 
	3. Reinforcement Learning
		=> Semi-supervised learning	
	

   Spark MLlib
   -----------

       Popular Libraries:
	  => Machine Learning: Spark MLlib, SAS, SciKit Learn, PyTorch
	  => Deep Learning: TensorFlow (Google), Keras, PyTorch

       Two libraries:
	 1. pyspark.mllib   => Based on RDDs (not preferred)
	 2. pyspak.ml	    => Based on DataFrames (current one)	


   What do we have in Spark MLlib
   -------------------------------

	1. Tools to work with Features:
		-> Feature Selector
		-> Feature Transformers
		-> Feature Extractors

	2. Algorithms
	
	3. Pipeline:    
		-> Approach to ML in Spark MLlib 

	4. Model Selection and Tuning
		-> TrainValidation Split
		-> CrossValidation

	5. Utility Packages:  Statistics, LinearAlgebra.






 


