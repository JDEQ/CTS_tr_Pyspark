
  Agenda
  ------
  
   Prerequite: Python Programming Language

   Spark - Basics & Architecture
   Spark Core API (Low-level API)	
	-> RDD Transformations & Actions
   Spark SQL
	-> DataFrame Operations
   Machine Learning & Spark MLlib
   Introduction to Spark Streaming
  
   Materials
   ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark
   
   Spark
   -----
     -> Spark is written in 'Scala' programming language. 

     -> Spark is a unified in-memory distributed computing framework for big data analytics.	

     -> Spark is a polyglot
	-> Spark apps can be written in Scala, Java, Python, R  

     -> Spark programs can run on multiple cluster managers 
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes


   Cluster
   -------
	-> Is a unified entity consisting of a group of nodes who cumulative resources can be used
	   to store and process/compute data.  
		
	-> Distributed Storage : Splits the data that is to be stored on the disk into small chunks
	   called blocks and these blocks are distributed across lot of nodes in the cluster. 

       -> Distributed Computing: Distributing the computations across many machine and running your tasks
	  by using the resources (RAM & CPU cores) of many machine parallelly.

       -> In-memory computing -> in-memory distributing computing refers to the ability of the framework to
	  store the intermediate results in the RAM and subsequent tasks can read these results from memory 
	  and continue the processing. 

   Spark Unified Framework
   -----------------------
     Spark provides a consist set of API for processing different analytical workloads using the
     same execution engine and common data abstractions. 

	-> Batch Analytics of Unstructured data 	: Spark Core API (RDD API) 
	-> Batch Analytics of Structured data 		: Spark SQL
	-> Streaming Analytics (real-time)		: Spark Streaming, Structured Streaming
	-> Predictive Analytics (machine learning)	: Spark MLlib
	-> Graph parallel computations			: Spark GraphX

     
   Getting started with Spark 
   --------------------------
  
      1. Using your vLab
	
	 -> Login into your Lab (Windows Server)
	 -> Click on 'CentOS 7' icon and enter username and password (given in README.txt)
	 -> Now youur inside your lab

	 -> Open a terminal and connect to pyspark shell
		-> type "pyspark"

	 -> Open a terminal and connect to "Jupyter Notebook"
		-> jupyter notebook    (if this is not working, try the other command)
		-> jupyter notebook --allow-root


      2. Setting up your own PySpark environment 
	   
	   -> Install "Anaconda Navigator"   (URL: https://www.anaconda.com/products/individual)
	   -> You have two tools -> Jupyter Notebook & Spyder
	   -> To setup PySpark on Jupyter Notebook & Spyder follow the steps in the document shared in github.
	 

      3. Signup to Databricks community edition
		
             URL: https://databricks.com/try-databricks

		-> Signup using the above url
		-> Login with your userid and password.
		-> Go through "Guide: Quickstart tutorial"

     
   Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


   RDD (Resilient Distributed Dataset)
   ----------------------------------- 

	-> Is the fundamental data abstraction of spark

	-> Is a collection in distributed in-memory partitions
		-> Each partition is a collection of objects (of some type)

	-> RDDs are immutable

	-> RDD has two components
		-> Meta Data : RDD Lineage DAG
		-> Data      : A set of partitions

	-> RDDs are lazily evaluated.

	
    How to create RDDs ?
    --------------------
	
     Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile( <filePath>, [numPartitions] )
		
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> default number of partitions is given by the value of sc.defaultMinPartitions
		   sc.defaultMinPartitions has a value of 2 if your app has atleast 2 cores allocated.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize( [3,3,2,5,7,8,9,0,0,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7] )
		-> default number of partitions is given by the value of sc.defaultParallelism
		  sc.defaultParallelism has a value equal to number of cores allocated.
		
		rdd1 = sc.parallelize( [3,3,2,5,7,8,9,0,0,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7], 3 )


	3. By applying transformations on existing RDDs.

		rdd3 = rdd2.map(lambda x: x.upper())

   
    What can you do on an RDD ?
    ---------------------------

	Two things

	1. Transformations
		-> Only rdds are 'created'
		-> That means, a logical plan on how to create the RDD partitions (called Lineage DAG)
		   is created and maintained by driver.
		-> Actual executions of the tasks is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan to physical plan and the driver sends the required tasks
		   to the executors to compute the partitions of the RDD.

 
    RDD Lineage DAG
    ---------------	
    Lineage DAG is created when a transformation command is executed. 
    Lineage DAG of an RDD is a logical plan that tracks all the dependencies that causes the creation of
    this RDD all the way from the very first RDD.


	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rdd1 : (4) rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.upper())
	Lineage DAG of rdd2 : (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: x.split(" "))
	Lineage DAG of rdd3 : (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG of rdd3 : (4) rdd4 -> rdd3.filter ->  rdd2.map -> rdd1.flatMap  -> sc.textFile

  
   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...) 
	rdd3 = rdd1.t3(...) 
	rdd4 = rdd3.t4(...) 
	rdd5 = rdd3.t5(...) 
	rdd6 = rdd5.t6(...) 
	rdd6.persist( StorageLevel.DISK_ONLY ) --> instruction to spark to persist the rdd6 partitions.
	rdd7 = rdd6.t7(...) 

	rdd6.collect()
	lineage rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	  
	sc.textFile -> t3 -> t5 -> t6 -> rdd6 ==> collected...

	rdd7.collect()
	lineage rdd7 => rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	  
	t7 -> rdd7 ==> collected.

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY   	-> (default) Serialized format, only in RAM
	2. MEMORY_AND_DISK	-> Serialized format, RAM if available, else DISK
	3. DISK_ONLY		-> Serialized on DISK
	4. MEMORY_ONLY_2	-> Serialized, memory with 2x replication
	5. MEMORY_AND_DISK_2
	
        Commands
        ---------
		rdd1.persist()     // memory serialized 1x replica
		rdd1.persist(StorageLevel.MEMORY_AND_DISK)
		rdd1.cache()

		rdd1.unpersist()


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  -------------------
   => Every transformation returns RDD

	 
   1. map 		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects

   2. filter		P: U -> Boolean
			Filter the objects of the input RDD based on the filter expression.
			input RDD: N objects, output RDD: >= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, output RDD: = numPartitions


		rdd1			rdd2 = rdd1.glom()
	
		P0 : 3,2,1,4,3,5 -> glom -> P0: [3,2,1,4,3,5]
		P1 : 5,3,2,4,3,6 -> glom -> P1: [5,3,2,4,3,6]
		P2 : 5,3,2,6,0,9 -> glom -> P2: [5,3,2,6,0,9]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)	

		rdd1.collect() => [3,2,1,4,3,5,5,3,2,4,3,6,5,3,2,6,0,9]   
		rdd1.glom().collect() => [[3,2,1,4,3,5], [5,3,2,4,3,6], [5,3,2,6,0,9]]
		rdd1.glom().map(len).collect() => [6,6,6]

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: > N objects
   
   		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions		P: Itarable[U] -> Iterable[V]
				Entire partition is taken as function input and function transforms the input
				into another output iterable object

				Partition to partition transformation

		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).collect()



   6. mapPartitionsWithIndex	P: Int, Itarable[U] -> Iterable[V]
				Same as mapPartitions, but we partition-index is an additonal function
				parameter.
		
		rdd1.mapPartitionsWithIndex(lambda i, d: [(i, sum(d))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, x: map(lambda a: (i, a*10), x)).collect()


   7. distinct			P: None,  Optional: numPartitions
				Returns an RDD with distinct elements
		
		rdd2 = rdd1.distinct()
  	





















  




