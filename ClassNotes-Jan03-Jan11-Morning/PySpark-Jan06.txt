
  Agenda
  ------
  
   Prerequite: Python Programming Language

   Spark - Basics & Architecture
   Spark Core API (Low-level API)	
	-> RDD Transformations & Actions
   Spark SQL
	-> DataFrame Operations
   Machine Learning & Spark MLlib
   Introduction to Spark Streaming
  
   Materials
   ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark
   
   Spark
   -----
     -> Spark is written in 'Scala' programming language. 

     -> Spark is a unified in-memory distributed computing framework for big data analytics.	

     -> Spark is a polyglot
	-> Spark apps can be written in Scala, Java, Python, R  

     -> Spark programs can run on multiple cluster managers 
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes


   Cluster
   -------
	-> Is a unified entity consisting of a group of nodes who cumulative resources can be used
	   to store and process/compute data.  
		
	-> Distributed Storage : Splits the data that is to be stored on the disk into small chunks
	   called blocks and these blocks are distributed across lot of nodes in the cluster. 

       -> Distributed Computing: Distributing the computations across many machine and running your tasks
	  by using the resources (RAM & CPU cores) of many machine parallelly.

       -> In-memory computing -> in-memory distributing computing refers to the ability of the framework to
	  store the intermediate results in the RAM and subsequent tasks can read these results from memory 
	  and continue the processing. 

   Spark Unified Framework
   -----------------------
     Spark provides a consist set of API for processing different analytical workloads using the
     same execution engine and common data abstractions. 

	-> Batch Analytics of Unstructured data 	: Spark Core API (RDD API) 
	-> Batch Analytics of Structured data 		: Spark SQL
	-> Streaming Analytics (real-time)		: Spark Streaming, Structured Streaming
	-> Predictive Analytics (machine learning)	: Spark MLlib
	-> Graph parallel computations			: Spark GraphX

     
   Getting started with Spark 
   --------------------------
  
      1. Using your vLab
	
	 -> Login into your Lab (Windows Server)
	 -> Click on 'CentOS 7' icon and enter username and password (given in README.txt)
	 -> Now youur inside your lab

	 -> Open a terminal and connect to pyspark shell
		-> type "pyspark"

	 -> Open a terminal and connect to "Jupyter Notebook"
		-> jupyter notebook    (if this is not working, try the other command)
		-> jupyter notebook --allow-root


      2. Setting up your own PySpark environment 
	   
	   -> Install "Anaconda Navigator"   (URL: https://www.anaconda.com/products/individual)
	   -> You have two tools -> Jupyter Notebook & Spyder
	   -> To setup PySpark on Jupyter Notebook & Spyder follow the steps in the document shared in github.
	 
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


      3. Signup to Databricks community edition
		
             URL: https://databricks.com/try-databricks

		-> Signup using the above url
		-> Login with your userid and password.
		-> Go through "Guide: Quickstart tutorial"

     
   Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


   RDD (Resilient Distributed Dataset)
   ----------------------------------- 

	-> Is the fundamental data abstraction of spark

	-> Is a collection in distributed in-memory partitions
		-> Each partition is a collection of objects (of some type)

	-> RDDs are immutable

	-> RDD has two components
		-> Meta Data : RDD Lineage DAG
		-> Data      : A set of partitions

	-> RDDs are lazily evaluated.

	
    How to create RDDs ?
    --------------------
	
     Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile( <filePath>, [numPartitions] )
		
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> default number of partitions is given by the value of sc.defaultMinPartitions
		   sc.defaultMinPartitions has a value of 2 if your app has atleast 2 cores allocated.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize( [3,3,2,5,7,8,9,0,0,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7] )
		-> default number of partitions is given by the value of sc.defaultParallelism
		  sc.defaultParallelism has a value equal to number of cores allocated.
		
		rdd1 = sc.parallelize( [3,3,2,5,7,8,9,0,0,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7], 3 )


	3. By applying transformations on existing RDDs.

		rdd3 = rdd2.map(lambda x: x.upper())

   
    What can you do on an RDD ?
    ---------------------------

	Two things

	1. Transformations
		-> Only rdds are 'created'
		-> That means, a logical plan on how to create the RDD partitions (called Lineage DAG)
		   is created and maintained by driver.
		-> Actual executions of the tasks is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan to physical plan and the driver sends the required tasks
		   to the executors to compute the partitions of the RDD.

 
    RDD Lineage DAG
    ---------------	
    Lineage DAG is created when a transformation command is executed. 
    Lineage DAG of an RDD is a logical plan that tracks all the dependencies that causes the creation of
    this RDD all the way from the very first RDD.


	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rdd1 : (4) rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.upper())
	Lineage DAG of rdd2 : (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: x.split(" "))
	Lineage DAG of rdd3 : (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG of rdd3 : (4) rdd4 -> rdd3.filter ->  rdd2.map -> rdd1.flatMap  -> sc.textFile

  
   RDD Persistence
   ---------------
	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...) 
	rdd3 = rdd1.t3(...) 
	rdd4 = rdd3.t4(...) 
	rdd5 = rdd3.t5(...) 
	rdd6 = rdd5.t6(...) 
	rdd6.persist( StorageLevel.DISK_ONLY ) --> instruction to spark to persist the rdd6 partitions.
	rdd7 = rdd6.t7(...) 

	rdd6.collect()
	lineage rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	  
	sc.textFile -> t3 -> t5 -> t6 -> rdd6 ==> collected...

	rdd7.collect()
	lineage rdd7 => rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	  
	t7 -> rdd7 ==> collected.

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY   	-> (default) Serialized format, only in RAM
	2. MEMORY_AND_DISK	-> Serialized format, RAM if available, else DISK
	3. DISK_ONLY		-> Serialized on DISK
	4. MEMORY_ONLY_2	-> Serialized, memory with 2x replication
	5. MEMORY_AND_DISK_2
	
        Commands
        ---------
		rdd1.persist()     // memory serialized 1x replica
		rdd1.persist(StorageLevel.MEMORY_AND_DISK)
		rdd1.cache()

		rdd1.unpersist()


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  -------------------
   => Every transformation returns RDD

	 
   1. map 		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects

   2. filter		P: U -> Boolean
			Filter the objects of the input RDD based on the filter expression.
			input RDD: N objects, output RDD: >= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, output RDD: = numPartitions


		rdd1			rdd2 = rdd1.glom()
	
		P0 : 3,2,1,4,3,5 -> glom -> P0: [3,2,1,4,3,5]
		P1 : 5,3,2,4,3,6 -> glom -> P1: [5,3,2,4,3,6]
		P2 : 5,3,2,6,0,9 -> glom -> P2: [5,3,2,6,0,9]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)	

		rdd1.collect() => [3,2,1,4,3,5,5,3,2,4,3,6,5,3,2,6,0,9]   
		rdd1.glom().collect() => [[3,2,1,4,3,5], [5,3,2,4,3,6], [5,3,2,6,0,9]]
		rdd1.glom().map(len).collect() => [6,6,6]

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: > N objects
   
   		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions		P: Itarable[U] -> Iterable[V]
				Entire partition is taken as function input and function transforms the input
				into another output iterable object

				Partition to partition transformation

		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).collect()



   6. mapPartitionsWithIndex	P: Int, Itarable[U] -> Iterable[V]
				Same as mapPartitions, but we partition-index is an additonal function
				parameter.
		
		rdd1.mapPartitionsWithIndex(lambda i, d: [(i, sum(d))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, x: map(lambda a: (i, a*10), x)).collect()


   7. distinct			P: None,  Optional: numPartitions
				Returns an RDD with distinct elements
		
		rdd2 = rdd1.distinct()
  	

   8. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the elements of the RDD based on function output

		rddWords.sortBy(len).collect()
		rddWords.sortBy(lambda x: x[0], False).collect()      # desc sort
		rdd1.sortBy(lambda x: x%6, True, 5).glom().collect()  # output RDD has 5 partitions
  
   Types of RDDs
	-> Generic RDDs:  RDD[U]
	-> Pair RDDs:     RDD[(U,V)]

   9. mapValues			P: U -> V
				Applied to Pair RDDs only
				Transforms the value part only by applying the function. Does not change
				the key.

	 rdd3.mapValues(lambda x : [x, x])	=> here x represents the 'value' part of (k,v) pairs

   10. groupBy			P: U -> V. Optional: numPartitions
				Returns a Pair RDD where the:
				key: the unique value of the function output
				value: ResultIterable of the elements of the RDD that produced the key.


		outputrdd = sc.textFile("E://Spark//wordcount.txt", 3) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len) \
              			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of ratios (ex: [0.6, 0.4]), Optional: seed
				Returns a list of RDDs

		rddlist = rdd1.randomSplit([0.5, 0.5])
		rddlist = rdd1.randomSplit([0.5, 0.5], 7567)   $ here 7567 is the seed.


   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions
				Cause global shuffle.

		rdd11 = rdd10.repartition(6)     # rdd11 will have 6 partitions
		rdd12 = rdd11.repartition(3)     # rdd12 will have 3 partitions


   13. coalesce			P: numPartitions
				Is used to decrease the number of partitions.
				Cause partition merging

		rdd11 = rdd10.coalesce(3)


   14. partitionBy		P: numPartitions, Optional: partitioning-function
				Applied only on PairRDDs and partitioning happens based on the 'key'

		rdd4 = rdd3.partitionBy(3)    # hash is used as the default partitioning function.
		rdd4 = rdd3.partitionBy(3, lambda x: x + 5)

     
   15. union, intersection, subtract & cartesian

	  Let us say rdd1 has M partitions & rdd2 has N partitions

	  command				# of output partitions
	 --------------------------------------------------------------
	 rdd1.union(rdd2)			M + N, narrow
	 rdd1.intersection(rdd2)		M + N, wide
	 rdd1.subtract(rdd2)			M + N, wide
	 rdd1.cartesian(rdd2)			M * N, wide


   ..ByKey Transformations
   -----------------------
     	-> Are wide transformations
	-> Applied only to pair RDD


    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the RDD based on the key

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 4).collect()


   17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD with unique-keys and grouped values.
				NOTE: Avoid groupByKey if possible. Not efficient.

	         outputrdd = sc.textFile("E://Spark//wordcount.txt", 3) \
              			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
              			.groupByKey() \
              			.mapValues(sum) \
              			.sortBy(lambda x: x[1], False, 1)


   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the reduce 
				function on different values of each unique key.

	outputrdd = sc.textFile("E://Spark//wordcount.txt", 3) \
              			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
              			.reduceByKey(lambda x, y: x + y) \
              			.sortBy(lambda x: x[1], False, 1)   

    19. aggregateByKey	
	
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce.
		2. Sequence function: 
			-> merges all the values of each unique key in each partition with the zero-value
			-> We get one aggreated value per unique-key in each partition. 			        
		3. Combine function: 
			-> reduces all the aggregates values of each unique key per partition produced 
			   by seq.fn into one final value.

student_rdd = sc.parallelize([
  ("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  ("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  ("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  ("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  ("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  ("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  ("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

avg_rdd = student_rdd.map(lambda a: (a[0], a[2])) \
            .aggregateByKey((0,0), 
                            lambda z,v: (z[0]+v, z[1]+1), 
                            lambda a,b:(a[0]+b[0], a[1]+b[1])) \
            .mapValues(lambda x: x[0]/x[1])



     20. joins	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

			RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

			join = names1.join(names2)   #inner Join
			leftOuterJoin = names1.leftOuterJoin(names2)
			rightOuterJoin = names1.rightOuterJoin(names2)
			fullOuterJoin = names1.fullOuterJoin(names2)

     21. cogroup	=> Is used to join RDDs with duplicate keys
			   -> groupByKey -> fullOuterJoin

	rdd1 = [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10,7]) (key2, [12,6]) (key3, [6])
	
        rdd2 = [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

	rdd1.cogroup(rdd2) =>
		(key1, ([10,7], [5, 17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))

	
    Recommendations
    ---------------
	-> The size of each partition should be approx. 128 MB 
	-> The number of partitions should be a multiple of number of cores.
	-> Each executor should have 5 cores
	-> If the number of partitions is less than but close to 2000, then bump it up to more than 2000. 


   RDD Actions
   ------------
	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		P: (U, U) -> U
				Reduces the entire RDD into one final value of the same type by iterativly
				applying the reduce function parallelly on every partition first and then reduce
				the output of all the partitions.
		rdd1				

		P0:  9, 7, 5, 3, 1         -> reduce  ->  -7 => 31
		P1:  0, 9, 5, 1, 2	   -> reduce  -> -17
		P2:  3, 8, 6, 4, 3, 2, 1   -> reduce  -> -21

		rdd1.reduce( lambda x, y: x - y ) = 31 

		
	5. aggregate

		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

		rdd1.aggregate( (0,0), lambda z,v : (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )	


	6. take			=> take(n)  ex: take(10)

        7. takeOrdered		=> rdd2.takeOrdered(20)
				   rdd2.takeOrdered(20, lambda x: x%4)

        8. takeSample
				rdd2.takeSample(True, 20)	  // withReplacement sampling
				rdd2.takeSample(True, 20, 465)    // 465 is a seed
				rdd2.takeSample(False, 20)	  // withoutReplacement sampling
				rdd2.takeSample(False, 20, 798)	  // 798 is a seed

       9. countByValue		rddWords.countByValue()
				defaultdict(<class 'int'>, 
				{'hadoop': 25, 'spark': 40, 'scala': 28, 'flatmap': 12, 'map': 6, 'groupby': 6, 'flume': 6, 'oozie': 6, 'sqoop': 6, 'hive': 19, 'mapreduce': 6, 'hdfs': 6, 'rdd': 43, 'transformations': 10, 'actions': 10, 'asd': 5, 'sadas': 1, 'das': 6, 'd': 1})

       10. countByKey		rddPairs.countByKey()

       11. foreach    		-> Executes a function on all objects of the RDD
			 	   Does not return anything

       12. first


   Use-Case
   --------
   From cars.tsv dataset, find out the average weight of each make of cars of American origin
   Arrange the data in the DESC order of average weight
   Save the output as a single text file. 
   
	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	=> Try to solve it.


   Closures
   --------
	A closure represents all the variables and methods that must be visible to an executor
	to perform its computations on an RDD. 

	The 'closure' is serialized and a copy of it is sent to every executor


	 c = 0   # counter

	 def isPrime(a) :
		return 1 if a is prime
		return 0 if n is not prime

	 def f1 (n) :
		global c
		if ( isPrime(n) == 1 ) c = c + 1
		return n*2	 

	 rdd1 = sc.parallelize( range(1, 4001), 4 )

	 rdd2 = rdd1.map( f1 )

	 output = rdd2.collect()

	 print(c)     // 0 


	Limitation:
	----------
	-> Local variables that are part of function closures can not be used to implement global
	   counters.
		-> Beacuse these are local copies with in every tasks and changes to these variables
		   can not be propagated back to driver.

	-> You CAN NOT USE local variables to impliment global counters. Use 'accumulator' for this purpose. 
	
   Shared Variables		
   -----------------

	Accumulator
	-----------
	Is a shared variable that is maintained by driver and can be added to by all the tasks
	Accumulator is not part of closure, hense it is not a local copy.
	

	 c = sc.accumulator(0)

	 def isPrime(a) :
		return 1 if a is prime
		return 0 if n is not prime

	 def f1 (n) :
		global c
		if ( isPrime(n) == 1 ) c.add(1)
		return n*2	 

	 rdd1 = sc.parallelize( range(1, 4001), 4 )

	 rdd2 = rdd1.map( f1 )

	 output = rdd2.collect()

	 print(c)     // 0 


	Broadcast
        ---------
	=> A broadcast variable is a single copy sent (broadcasted) to every executor node and all the
	   tasks running in that executor can access this single copy. 

	=> By converting a large immutable collection into a broadcast variable you can save a lot of 
	   execution memory.


	dict = {1:'a', 2:'b', 3:'c', 4:'d', 5:'e', 6:'f', .......}   // 100MB
        bcDict = sc.broadcast( dict )

	def f1( n ) :
	    global bcDict 
	    return bcDict.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,...], 4)

	rdd2 = rdd1.map( f1 )
	
	rdd2.collect()


  
	Example
	-------
	lookup = sc.broadcast({1: 'a', 2:'e', 3:'i', 4:'o', 5:'u'}) 
	result = sc.parallelize([2, 1, 3, 4, 5]).map(lambda x: lookup.value[x]) 
	print( result.collect() )











