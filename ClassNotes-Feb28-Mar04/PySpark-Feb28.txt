
  Agenda - PySpark
  ----------------
   Spark - Basics & Architecture
   Spark Low-level API
	RDDs - Transformations and Actions
	Shared Variables
   Spark SQL 
   Spark Streaming
   Intro to Spark MLLib


   Materials
   ---------
     	* PDF Presentations
	* Code Modules
	* Class Notes
	* GitHub: https://github.com/ykanakaraju/pyspark

  Spark
  -----
	=> Spark is a unified in-memory distributed computing framework.
	=> Spark is data analytics framework.
	=> Spark is written in Scala Programming Language
	=> Spark is NOT a programming language, NOT a database, NOT a tool, NOT an OS

	=> Cluster:  Is unified entity comprising of many nodes whose cumulative resources can be
		     used to store and process your data. 

	=> in-memory computation : We can persist the results on computation in-memory

	=> Spark is a polyglot
		-> Scala, Java, Python and R

	Spark Unified Framework:
	------------------------
	Spark provides a consitent set of libraries for performing analytics on different
	analytical workloads using the same execution engine. 

	Batch Analytics of Unstructured Data	=> Spark Core API (RDD)
	Batch Analytics of Structured Data	=> Spark SQL (DataFrames)
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics			=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX

	=> Spark application can run on multiple cluster managers
		local, spark standalone, YARN, Mesos, Kubernetes
	

  Spark Architecture
  ------------------
	1. Cluster Manager
		-> Spark application are submitted to a CM.
		-> Responsible for allocating resources (executors) for the spark application. 
		-> Supports multiple CMs : local, Spark Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process that manages the entire execution
		-> Manages the user-code and maintains the metadata of the application
		-> Can be launched in two deploye modes:
			client (default) : Driver runs on the client machine
			cluster : Driver runs on one of the nodes on the cluster
		-> Analyzes the user code and send the tasks to run on the executors		

	3. Executors
		-> Will receive tasks from the driver
		-> Tasks are executed and status is reported back to driver.

	4. SparkContext
		-> Runs the driver process
		-> Is the starting point of execution. 
		-> Is an application context
		-> Is a link between the driver and several tasks that are running on the cluster

   
   Getting started with Spark
   --------------------------

    1. Working with your vLabs:
		
	-> Check out your emails for instructions and connect to "BigData Enbl vLab"	
		-> You will be connected to a Windows Server
	-> Click on the "CentOS 7" icon on the desktop
	-> Enter your username and password (checkout README.txt for username and password)
	-> You will then be connected to your VLab (CentOS lab)

	Connect to PySpark Shell
	------------------------
	-> Open a terminal
	-> Type "pyspark". 
	-> This launches PySpark shell

	Connect to Jupyter Notebooks
	-----------------------------
	-> Open a terminal
	-> type "jupyter notebook --allow-root"
		$ jupyter notebook --allow-root
        -> Check out PySparkTest.ipynb for quick reference


    2. Setting up PySpark on your personal machine.

	-> Install Anaconda Navigator
	   URL: https://www.anaconda.com/products/individual
	-> Follow the steps mentioned in the shared document. (in the gitgub)
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition account (free account)
	URL: https://databricks.com/try-databricks

	Go through "Guide: Quickstart tutorial"

	
   RDD (Resilient Distributed Dataset)
   ------------------------------------
	
   => Core data abstraction of Spark Core API (Low-level API)
	
   => Is a datastructure that represents a collection of distributed in-memory partitions
	-> A partition is a collection of objects.

   => RDDs are lazily evaluted.
	-> RDD Transformations cause lineage DAGs
	-> Execution is triggered by 'Action' commands

   => RDDs are immutable	


   How to create RDDs?
   --------------------
	
     Three ways:

	1. From some external data files

	   rdd1 = sc.textFile( <filePath>, [numPartitions] )

	2. From programmatic data

	   rdd1 = sc.parallelize( [2,3,1,2,4,3,5,6,7,8,9,7,8,9,0,5], 3)

	3. By applying transformations on existing RDDs

	   rdd2 = rdd1.map(lambda x: x.upper())


   What can we do with RDDs
   ------------------------

	1. Transformations		
		Output: RDD
		Transformations does not cause execution of the RDD
		They only create Lineage DAG of the RDD
		Does not produce any output.

	2. Actions
		Produce output
		Cause the pysical execution plan to be created from RDD lineage (logical plan)
		and required tasks to be executed are sent to the cluster.  

   RDD Lineage
   ------------
    RDD Lineage is a logical plan on what tasks to launch to create the RDD partititons.

    Lineage DAG contains all dependencies (heirarchy of RDDs) that caused the creation of this RDD
    all the way from the very RDD.
    
	
	rddFile = sc.textFile( file, 4 )	
	Lineage : (4) rddFile -> sc.textFile	
	
	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile

 

  Types of transfortmations
  --------------------------
	
	1. Narrow Transformations

		

	2. Wide Transformations



  RDD Transformations
  ------------------	  
   Transformations return RDD.
	

   1. map		P: U -> V
			Object to Object transformations		
			input RDD: N objects, Output RDD: N objects

		rdd1.map(lambda x: x%3).collect()

   2. filter		P: U -> Boolean	
			Only those objects for which the function returns True will be there in the output
			input RDD: N objects, Output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, Output RDD: = number of partitions		
	
		rdd1			rdd2 = rdd1.glom()

		P0: 2,1,3,2,5,4,6 -> glom -> P0: [2,1,3,2,5,4,6]
		P1: 4,2,3,1,5,4,7 -> glom -> P1: [4,2,3,1,5,4,7]
		P2: 9,0,7,0,5,8,1 -> glom -> P2: [9,0,7,0,5,8,1]

		rdd1.count() = 21 (int)	   rdd2.count() = 3 (list)			

   		rdd1.glom().map(len).collect()

   4. flatMap		P: U -> Iterable[U]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, Output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation.	


		rdd1	 rdd2 = rdd1.mapPartitions( lambda p :  [sum(p)] )					

		P0: 2,1,3,2,5,4,6 -> mapPartitions -> P0: 27
		P1: 4,2,3,1,5,4,7 -> mapPartitions -> P1: 26
		P2: 9,0,7,0,5,8,1 -> mapPartitions -> P2: 30

		rdd1.mapPartitions(lambda p : [sum(p)]).collect()

		NOTE: Any operation we do with map can be done with mapPartitions also
		      Some operation that we can perform with mapPartitions can not be performed using map

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V] 
				Similar to mapPartitions but we get partition-index as additional function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD (dedups the RDD)
				input RDD: N objects, Output RDD: <= N objects

		rddWords.distinct().collect()
		rddWords.distinct(4).collect()

   


				

   












