
  Agenda - PySpark
  ----------------
   Spark - Basics & Architecture
   Spark Low-level API
	RDDs - Transformations and Actions
	Shared Variables
   Spark SQL 
   Spark Streaming
   Intro to Spark MLLib


   Materials
   ---------
     	* PDF Presentations
	* Code Modules
	* Class Notes
	* GitHub: https://github.com/ykanakaraju/pyspark

  Spark
  -----
	=> Spark is a unified in-memory distributed computing framework.
	=> Spark is data analytics framework.
	=> Spark is written in Scala Programming Language
	=> Spark is NOT a programming language, NOT a database, NOT a tool, NOT an OS

	=> Cluster:  Is unified entity comprising of many nodes whose cumulative resources can be
		     used to store and process your data. 

	=> in-memory computation : We can persist the results on computation in-memory

	=> Spark is a polyglot
		-> Scala, Java, Python and R

	Spark Unified Framework:
	------------------------
	Spark provides a consitent set of libraries for performing analytics on different
	analytical workloads using the same execution engine. 

	Batch Analytics of Unstructured Data	=> Spark Core API (RDD)
	Batch Analytics of Structured Data	=> Spark SQL (DataFrames)
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics			=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX

	=> Spark application can run on multiple cluster managers
		local, spark standalone, YARN, Mesos, Kubernetes
	

  Spark Architecture
  ------------------
	1. Cluster Manager
		-> Spark application are submitted to a CM.
		-> Responsible for allocating resources (executors) for the spark application. 
		-> Supports multiple CMs : local, Spark Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process that manages the entire execution
		-> Manages the user-code and maintains the metadata of the application
		-> Can be launched in two deploye modes:
			client (default) : Driver runs on the client machine
			cluster : Driver runs on one of the nodes on the cluster
		-> Analyzes the user code and send the tasks to run on the executors		

	3. Executors
		-> Will receive tasks from the driver
		-> Tasks are executed and status is reported back to driver.

	4. SparkContext
		-> Runs the driver process
		-> Is the starting point of execution. 
		-> Is an application context
		-> Is a link between the driver and several tasks that are running on the cluster

   
   Getting started with Spark
   --------------------------

    1. Working with your vLabs:
		
	-> Check out your emails for instructions and connect to "BigData Enbl vLab"	
		-> You will be connected to a Windows Server
	-> Click on the "CentOS 7" icon on the desktop
	-> Enter your username and password (checkout README.txt for username and password)
	-> You will then be connected to your VLab (CentOS lab)

	Connect to PySpark Shell
	------------------------
	-> Open a terminal
	-> Type "pyspark". 
	-> This launches PySpark shell

	Connect to Jupyter Notebooks
	-----------------------------
	-> Open a terminal
	-> type "jupyter notebook --allow-root"
		$ jupyter notebook --allow-root
        -> Check out PySparkTest.ipynb for quick reference


    2. Setting up PySpark on your personal machine.

	-> Install Anaconda Navigator
	   URL: https://www.anaconda.com/products/individual
	-> Follow the steps mentioned in the shared document. (in the gitgub)
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition account (free account)
	URL: https://databricks.com/try-databricks

	Go through "Guide: Quickstart tutorial"

	
   RDD (Resilient Distributed Dataset)
   ------------------------------------
	
   => Core data abstraction of Spark Core API (Low-level API)
	
   => Is a datastructure that represents a collection of distributed in-memory partitions
	-> A partition is a collection of objects.

   => RDDs are lazily evaluted.
	-> RDD Transformations cause lineage DAGs
	-> Execution is triggered by 'Action' commands

   => RDDs are immutable	


   How to create RDDs?
   --------------------
	
     Three ways:

	1. From some external data files

	   rdd1 = sc.textFile( <filePath>, [numPartitions] )

	2. From programmatic data

	   rdd1 = sc.parallelize( [2,3,1,2,4,3,5,6,7,8,9,7,8,9,0,5], 3)

	3. By applying transformations on existing RDDs

	   rdd2 = rdd1.map(lambda x: x.upper())


   What can we do with RDDs
   ------------------------

	1. Transformations		
		Output: RDD
		Transformations does not cause execution of the RDD
		They only create Lineage DAG of the RDD
		Does not produce any output.

	2. Actions
		Produce output
		Cause the pysical execution plan to be created from RDD lineage (logical plan)
		and required tasks to be executed are sent to the cluster.  

   RDD Lineage
   ------------
    RDD Lineage is a logical plan on what tasks to launch to create the RDD partititons.

    Lineage DAG contains all dependencies (heirarchy of RDDs) that caused the creation of this RDD
    all the way from the very RDD.
    
	
	rddFile = sc.textFile( file, 4 )	
	Lineage : (4) rddFile -> sc.textFile	
	
	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile
 

  Types of transfortmations
  --------------------------
	
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Persistence
  ---------------
     	rdd1 = sc.textFile(....., 4)
	rdd2 = rdd1.t2(...)	
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)	
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   => is an instruction to persist the RDD partitions (not subjected to GC)
	rdd7 = rdd6.t7(...)	

	rdd6.collect()
	
	lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collected

	rdd7.collect()
	
	lineage of rdd7 =>  rdd7 -> rdd6.t7
		rdd6 -> t7 (rdd7) -> collected


	Storage-Level
        -------------
	1. MEMORY_ONLY	    	-> default, memory serialized with 1x replication
	2. MEMORY_AND_DISK  	-> disk memory serialized with 1x replication
	3. DISK_ONLY	    	-> disk serialized with 1x replication   
	4. MEMORY_ONLY_2	-> memory serialized with 2x replication (2 copies on 2 DIFFERENT executors)
	5. MEMORY_AND_DISK_2	-> disk memory serialized with 2x replication (2 copies on 2 DIFFERENT executors)

	Commands
	--------
	rdd1.persist()  # memory only persistence
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )
	rdd1.cache()    # memory only persistence

	rdd1.unpersist()   # remove the persisted partitions.




  RDD Transformations
  ------------------	  
   Transformations return RDD.
	

   1. map		P: U -> V
			Object to Object transformations		
			input RDD: N objects, Output RDD: N objects

		rdd1.map(lambda x: x%3).collect()

   2. filter		P: U -> Boolean	
			Only those objects for which the function returns True will be there in the output
			input RDD: N objects, Output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, Output RDD: = number of partitions		
	
		rdd1			rdd2 = rdd1.glom()

		P0: 2,1,3,2,5,4,6 -> glom -> P0: [2,1,3,2,5,4,6]
		P1: 4,2,3,1,5,4,7 -> glom -> P1: [4,2,3,1,5,4,7]
		P2: 9,0,7,0,5,8,1 -> glom -> P2: [9,0,7,0,5,8,1]

		rdd1.count() = 21 (int)	   rdd2.count() = 3 (list)			

   		rdd1.glom().map(len).collect()

   4. flatMap		P: U -> Iterable[U]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, Output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation.	


		rdd1	 rdd2 = rdd1.mapPartitions( lambda p :  [sum(p)] )					

		P0: 2,1,3,2,5,4,6 -> mapPartitions -> P0: 27
		P1: 4,2,3,1,5,4,7 -> mapPartitions -> P1: 26
		P2: 9,0,7,0,5,8,1 -> mapPartitions -> P2: 30

		rdd1.mapPartitions(lambda p : [sum(p)]).collect()

		NOTE: Any operation we do with map can be done with mapPartitions also
		      Some operation that we can perform with mapPartitions can not be performed using map

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V] 
				Similar to mapPartitions but we get partition-index as additional function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD (dedups the RDD)
				input RDD: N objects, Output RDD: <= N objects

		rddWords.distinct().collect()
		rddWords.distinct(4).collect()

   Types of RDDs

	-> generic RDDs : RDD[U]
	-> pair RDDs: RDD[(K, V)]
	
   8.  mapValues		P: U -> V
				Applied ONLY to Pair RDDs
				Transforms only the value part of the Pair RDD by applying the function.
				input RDD: N objects, Output RDD: N objects

		rdd3.mapValues(sum).collect()
		rdd3.mapValues(lambda x : (x[0], x[0] > x[1])).collect()  
                   -> here x is the 'value' part of the Pair RDD.
		

   9. sortBy			P: U -> V,  Optional: ascending (True/false), numPartitions
				The object of the RDD are sorted based on the function output
				input RDD: N objects, Output RDD: N objects

		rddWords.sortBy(lambda x: len(x)).collect()
		rddWords.sortBy(len, False).collect()
		rdd1.sortBy(lambda x: x%2, True, 2).glom().collect()
		

   10. groupBy			P: U -> V, Optional: numPartitions
				Group the objects of the RDD based on function output.
				Returns a pair RDD, where:
				   key: each unique value of the function output
				   value: ResultIterable containing all the grouped objects of the RDD.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.groupBy(lambda x: x) \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)  

   11. randomSplit		P: A list of ratios (e.g: [0.6, 0.4])
				Returns a list of RDDs split randomly in the specified ratios.

		rddList = rdd1.randomSplit([0.3, 0.4, 0.3])
		rddList = rdd1.randomSplit([0.5, 0.5], 456)

   12. repartition		P: numPartitions
				Used to increase or decrease the number of output partitions.
				results in global shuffle

   13. coalesce			P: numPartitions
				Used to decrease the number of output partitions.
				results in partition merging	
    Recommendations:
    ---------------
	-> The size of each partition should be around 128 MB	
	-> The number of partitions should be a mulitple of the number of CPU cores allocated.
	-> If the number of partitions is close to but less than 2000, bump it up to 2000 or more.
	-> The number of cores in each executors should be 5

   14. partitionBy		P: numPartitions, Options: partitioning-function (default: hash)
				Applied only to Pair RDDs
				Controls which keys goes to which partition
		

   15. union, intersection, subtract, cartesian

		
	Let us say we have rdd1 with M partitions and rdd2 with N partitions

	Command				Output Partitions
	------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide		


   ...ByKey transformations
   -------------------------
        -> Applied only on Pair RDDs
	-> Wide transformations


    16. sortByKey		P: None, Optional: ascending (True/false), numPartitions
				Sorts the objects of the RDD based on the 'key'

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()	
		rdd2.sortByKey(False, 6).glom().collect()


    17. groupByKey		P: None, Optional: numPartitions
				Groups the objects based on the key
				Returns a Pair RDD, where:
				   key: Unique key
				   values: ResultIterable with all the values with the same key.

				NOTE: Avoid groupByKey if you can

 		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda x: (x, 1)) \
         		.groupByKey() \
         		.mapValues(sum)


    18. reduceByKey		P: (U, U) -> U, Optional: numPartitions	
				Reduces all the values of each unique key with in partitions and then across
				partititions into one final value of the same type by iterativly applying the 
				reduce function.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda x: (x, 1)) \
         		.reduceByKey(lambda x, y: x + y)

    19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                        		lambda z, v: (z[0] + v, z[1] + 1),
                        		lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                			.mapValues(lambda x: x[0]/x[1])


    20. joins	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		    RDD[(U, V)].join( RDD[(U, W)]) => RDD[(U, (V,W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)	


    21. cogroup	=> Is used to join RDDs with duplicate keys and you want unique keys in the output
		  -> groupByKey -> fullOuterJoin

	rdd1 => [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

	rdd2 => [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4, 7]) (key4, [15])

	cogroup: (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4, 7])) (key3, ([6], [])) (key4, ([], [15]))


  RDD Actions
  ------------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce			P: (U, U) -> U
				reduces an entire RDD into one final value of the same type by iterativly
				applying the reduce function on each each RDD in the first stage and further
				reduces the outputs of all partitions into final value in the next stage.		
		rdd1				 
	
		P0: 9, 7, 6, 2, 1, 3, 5, 0  -> reduce -> -15 -> reduce -> 37
		P1: 7, 8, 6, 4, 6, 5, 2, 1  -> reduce -> -25
		P2: 4, 7, 9, 8, 0, 0, 3, 4  -> reduce -> -27

		rdd1.reduce(lambda x, y: x - y)

   5. aggregate		

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )


   6. first

   7. take

   8. takeOrdered

		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: x%2)

   9. takeSample

		rdd1.takeSample(True, 10)
		rdd1.takeSample(True, 10, 7567)
		rdd1.takeSample(True, 100, 3123)

		rdd1.takeSample(False, 10, 3534)

		
   10. countByValue

   11. countByKey

   12. foreach		=> P: function that is applied on the objects of the RDD but does not return things.

	rddWc.foreach(lambda x: print( "key: " + x[0] + ",value: " + str(x[1]) ))

   13. saveAsSequenceFile : Saves the RDD in Sequence file format

	rddWc.saveAsSequenceFile("E:\\PySpark\\output\\seq")


   Use-Case
   --------

   















