
  Agenda - PySpark
  ----------------
   Spark - Basics & Architecture
   Spark Low-level API
	RDDs - Transformations and Actions
	Shared Variables
   Spark SQL 
   Spark Streaming
   Intro to Spark MLLib


   Materials
   ---------
     	* PDF Presentations
	* Code Modules
	* Class Notes
	* GitHub: https://github.com/ykanakaraju/pyspark

  Spark
  -----
	=> Spark is a unified in-memory distributed computing framework.
	=> Spark is data analytics framework.
	=> Spark is written in Scala Programming Language
	=> Spark is NOT a programming language, NOT a database, NOT a tool, NOT an OS

	=> Cluster:  Is unified entity comprising of many nodes whose cumulative resources can be
		     used to store and process your data. 

	=> in-memory computation : We can persist the results on computation in-memory

	=> Spark is a polyglot
		-> Scala, Java, Python and R

	Spark Unified Framework:
	------------------------
	Spark provides a consitent set of libraries for performing analytics on different
	analytical workloads using the same execution engine. 

	Batch Analytics of Unstructured Data	=> Spark Core API (RDD)
	Batch Analytics of Structured Data	=> Spark SQL (DataFrames)
	Streaming Analytics			=> Spark Streaming, Structured Streaming
	Predictive Analytics			=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX

	=> Spark application can run on multiple cluster managers
		local, spark standalone, YARN, Mesos, Kubernetes
	

  Spark Architecture
  ------------------
	1. Cluster Manager
		-> Spark application are submitted to a CM.
		-> Responsible for allocating resources (executors) for the spark application. 
		-> Supports multiple CMs : local, Spark Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process that manages the entire execution
		-> Manages the user-code and maintains the metadata of the application
		-> Can be launched in two deploye modes:
			client (default) : Driver runs on the client machine
			cluster : Driver runs on one of the nodes on the cluster
		-> Analyzes the user code and send the tasks to run on the executors		

	3. Executors
		-> Will receive tasks from the driver
		-> Tasks are executed and status is reported back to driver.

	4. SparkContext
		-> Runs the driver process
		-> Is the starting point of execution. 
		-> Is an application context
		-> Is a link between the driver and several tasks that are running on the cluster

   
   Getting started with Spark
   --------------------------

    1. Working with your vLabs:
		
	-> Check out your emails for instructions and connect to "BigData Enbl vLab"	
		-> You will be connected to a Windows Server
	-> Click on the "CentOS 7" icon on the desktop
	-> Enter your username and password (checkout README.txt for username and password)
	-> You will then be connected to your VLab (CentOS lab)

	Connect to PySpark Shell
	------------------------
	-> Open a terminal
	-> Type "pyspark". 
	-> This launches PySpark shell

	Connect to Jupyter Notebooks
	-----------------------------
	-> Open a terminal
	-> type "jupyter notebook --allow-root"
		$ jupyter notebook --allow-root
        -> Check out PySparkTest.ipynb for quick reference


    2. Setting up PySpark on your personal machine.

	-> Install Anaconda Navigator
	   URL: https://www.anaconda.com/products/individual
	-> Follow the steps mentioned in the shared document. (in the gitgub)
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition account (free account)
	URL: https://databricks.com/try-databricks

	Go through "Guide: Quickstart tutorial"

	Downloading a file from DBFS (Databricks File System)
        -----------------------------------------------------
	/FileStore/<FILEPATH>
	https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs
	
	Ex:
	/FileStore/tables/movielens/part-00000-tid-3595762387243659169-2268f666-140a-4481-aa73-42547033f374-18-1-c000.csv
	https://community.cloud.databricks.com/files/tables/movielens/part-00000-tid-3595762387243659169-2268f666-140a-4481-aa73-42547033f374-18-1-c000.csv?o=4949609693130439#tables/new/dbfs

	
   RDD (Resilient Distributed Dataset)
   ------------------------------------
	
   => Core data abstraction of Spark Core API (Low-level API)
	
   => Is a datastructure that represents a collection of distributed in-memory partitions
	-> A partition is a collection of objects.

   => RDDs are lazily evaluted.
	-> RDD Transformations cause lineage DAGs
	-> Execution is triggered by 'Action' commands

   => RDDs are immutable	


   How to create RDDs?
   --------------------
	
     Three ways:

	1. From some external data files

	   rdd1 = sc.textFile( <filePath>, [numPartitions] )

	2. From programmatic data

	   rdd1 = sc.parallelize( [2,3,1,2,4,3,5,6,7,8,9,7,8,9,0,5], 3)

	3. By applying transformations on existing RDDs

	   rdd2 = rdd1.map(lambda x: x.upper())


   What can we do with RDDs
   ------------------------

	1. Transformations		
		Output: RDD
		Transformations does not cause execution of the RDD
		They only create Lineage DAG of the RDD
		Does not produce any output.

	2. Actions
		Produce output
		Cause the pysical execution plan to be created from RDD lineage (logical plan)
		and required tasks to be executed are sent to the cluster.  

   RDD Lineage
   ------------
    RDD Lineage is a logical plan on what tasks to launch to create the RDD partititons.

    Lineage DAG contains all dependencies (heirarchy of RDDs) that caused the creation of this RDD
    all the way from the very RDD.
    
	
	rddFile = sc.textFile( file, 4 )	
	Lineage : (4) rddFile -> sc.textFile	
	
	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile
 

  Types of transfortmations
  --------------------------
	
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Persistence
  ---------------
     	rdd1 = sc.textFile(....., 4)
	rdd2 = rdd1.t2(...)	
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)	
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   => is an instruction to persist the RDD partitions (not subjected to GC)
	rdd7 = rdd6.t7(...)	

	rdd6.collect()
	
	lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collected

	rdd7.collect()
	
	lineage of rdd7 =>  rdd7 -> rdd6.t7
		rdd6 -> t7 (rdd7) -> collected


	Storage-Level
        -------------
	1. MEMORY_ONLY	    	-> default, memory serialized with 1x replication
	2. MEMORY_AND_DISK  	-> disk memory serialized with 1x replication
	3. DISK_ONLY	    	-> disk serialized with 1x replication   
	4. MEMORY_ONLY_2	-> memory serialized with 2x replication (2 copies on 2 DIFFERENT executors)
	5. MEMORY_AND_DISK_2	-> disk memory serialized with 2x replication (2 copies on 2 DIFFERENT executors)

	Commands
	--------
	rdd1.persist()  # memory only persistence
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )
	rdd1.cache()    # memory only persistence

	rdd1.unpersist()   # remove the persisted partitions.




  RDD Transformations
  ------------------	  
   Transformations return RDD.
	

   1. map		P: U -> V
			Object to Object transformations		
			input RDD: N objects, Output RDD: N objects

		rdd1.map(lambda x: x%3).collect()

   2. filter		P: U -> Boolean	
			Only those objects for which the function returns True will be there in the output
			input RDD: N objects, Output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, Output RDD: = number of partitions		
	
		rdd1			rdd2 = rdd1.glom()

		P0: 2,1,3,2,5,4,6 -> glom -> P0: [2,1,3,2,5,4,6]
		P1: 4,2,3,1,5,4,7 -> glom -> P1: [4,2,3,1,5,4,7]
		P2: 9,0,7,0,5,8,1 -> glom -> P2: [9,0,7,0,5,8,1]

		rdd1.count() = 21 (int)	   rdd2.count() = 3 (list)			

   		rdd1.glom().map(len).collect()

   4. flatMap		P: U -> Iterable[U]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, Output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation.	


		rdd1	 rdd2 = rdd1.mapPartitions( lambda p :  [sum(p)] )					

		P0: 2,1,3,2,5,4,6 -> mapPartitions -> P0: 27
		P1: 4,2,3,1,5,4,7 -> mapPartitions -> P1: 26
		P2: 9,0,7,0,5,8,1 -> mapPartitions -> P2: 30

		rdd1.mapPartitions(lambda p : [sum(p)]).collect()

		NOTE: Any operation we do with map can be done with mapPartitions also
		      Some operation that we can perform with mapPartitions can not be performed using map

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V] 
				Similar to mapPartitions but we get partition-index as additional function parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD (dedups the RDD)
				input RDD: N objects, Output RDD: <= N objects

		rddWords.distinct().collect()
		rddWords.distinct(4).collect()

   Types of RDDs

	-> generic RDDs : RDD[U]
	-> pair RDDs: RDD[(K, V)]
	
   8.  mapValues		P: U -> V
				Applied ONLY to Pair RDDs
				Transforms only the value part of the Pair RDD by applying the function.
				input RDD: N objects, Output RDD: N objects

		rdd3.mapValues(sum).collect()
		rdd3.mapValues(lambda x : (x[0], x[0] > x[1])).collect()  
                   -> here x is the 'value' part of the Pair RDD.
		

   9. sortBy			P: U -> V,  Optional: ascending (True/false), numPartitions
				The object of the RDD are sorted based on the function output
				input RDD: N objects, Output RDD: N objects

		rddWords.sortBy(lambda x: len(x)).collect()
		rddWords.sortBy(len, False).collect()
		rdd1.sortBy(lambda x: x%2, True, 2).glom().collect()
		

   10. groupBy			P: U -> V, Optional: numPartitions
				Group the objects of the RDD based on function output.
				Returns a pair RDD, where:
				   key: each unique value of the function output
				   value: ResultIterable containing all the grouped objects of the RDD.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.groupBy(lambda x: x) \
         		.mapValues(len) \
         		.sortBy(lambda x: x[1], False, 1)  

   11. randomSplit		P: A list of ratios (e.g: [0.6, 0.4])
				Returns a list of RDDs split randomly in the specified ratios.

		rddList = rdd1.randomSplit([0.3, 0.4, 0.3])
		rddList = rdd1.randomSplit([0.5, 0.5], 456)

   12. repartition		P: numPartitions
				Used to increase or decrease the number of output partitions.
				results in global shuffle

   13. coalesce			P: numPartitions
				Used to decrease the number of output partitions.
				results in partition merging	
    Recommendations:
    ---------------
	-> The size of each partition should be around 128 MB	
	-> The number of partitions should be a mulitple of the number of CPU cores allocated.
	-> If the number of partitions is close to but less than 2000, bump it up to 2000 or more.
	-> The number of cores in each executors should be 5

   14. partitionBy		P: numPartitions, Options: partitioning-function (default: hash)
				Applied only to Pair RDDs
				Controls which keys goes to which partition
		

   15. union, intersection, subtract, cartesian

		
	Let us say we have rdd1 with M partitions and rdd2 with N partitions

	Command				Output Partitions
	------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide		


   ...ByKey transformations
   -------------------------
        -> Applied only on Pair RDDs
	-> Wide transformations


    16. sortByKey		P: None, Optional: ascending (True/false), numPartitions
				Sorts the objects of the RDD based on the 'key'

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()	
		rdd2.sortByKey(False, 6).glom().collect()


    17. groupByKey		P: None, Optional: numPartitions
				Groups the objects based on the key
				Returns a Pair RDD, where:
				   key: Unique key
				   values: ResultIterable with all the values with the same key.

				NOTE: Avoid groupByKey if you can

 		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda x: (x, 1)) \
         		.groupByKey() \
         		.mapValues(sum)


    18. reduceByKey		P: (U, U) -> U, Optional: numPartitions	
				Reduces all the values of each unique key with in partitions and then across
				partititions into one final value of the same type by iterativly applying the 
				reduce function.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
         		.flatMap(lambda x: x.split(" ")) \
         		.map(lambda x: (x, 1)) \
         		.reduceByKey(lambda x, y: x + y)

    19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                        		lambda z, v: (z[0] + v, z[1] + 1),
                        		lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                			.mapValues(lambda x: x[0]/x[1])


    20. joins	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		    RDD[(U, V)].join( RDD[(U, W)]) => RDD[(U, (V,W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)	


    21. cogroup	=> Is used to join RDDs with duplicate keys and you want unique keys in the output
		  -> groupByKey -> fullOuterJoin

	rdd1 => [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

	rdd2 => [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4, 7]) (key4, [15])

	cogroup: (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4, 7])) (key3, ([6], [])) (key4, ([], [15]))


  RDD Actions
  ------------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce			P: (U, U) -> U
				reduces an entire RDD into one final value of the same type by iterativly
				applying the reduce function on each each RDD in the first stage and further
				reduces the outputs of all partitions into final value in the next stage.		
		rdd1				 
	
		P0: 9, 7, 6, 2, 1, 3, 5, 0  -> reduce -> -15 -> reduce -> 37
		P1: 7, 8, 6, 4, 6, 5, 2, 1  -> reduce -> -25
		P2: 4, 7, 9, 8, 0, 0, 3, 4  -> reduce -> -27

		rdd1.reduce(lambda x, y: x - y)

   5. aggregate		

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )


   6. first

   7. take

   8. takeOrdered

		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: x%2)

   9. takeSample

		rdd1.takeSample(True, 10)
		rdd1.takeSample(True, 10, 7567)
		rdd1.takeSample(True, 100, 3123)

		rdd1.takeSample(False, 10, 3534)

		
   10. countByValue

   11. countByKey

   12. foreach		=> P: function that is applied on the objects of the RDD but does not return things.

	rddWc.foreach(lambda x: print( "key: " + x[0] + ",value: " + str(x[1]) ))

   13. saveAsSequenceFile : Saves the RDD in Sequence file format

	rddWc.saveAsSequenceFile("E:\\PySpark\\output\\seq")


   Use-Case
   --------

	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	-> Find out the avarage-weight of each make of cars of American origin
	-> Arrange the data in the DESC order of average weight of each make
	-> Save the output as a single text file.
   
	=> Please try it yourself...


  Shared Variables
  -----------------

   Closure: Is all code (variables and methods) that must visible to a task inside an executor for
   the task to perform its comuputations on the RDD.

   -> The closure is serialized, and driver sends copy of this serialized code is sent to every executor.

	c = 0 

	def isPrime(n):
		if n is prime number return 1
		else return 0

	def f1(n):
		global c
		if ( isPrime(n) == 1 ) c += 1
		return n*2
	
	rdd1 = sc.paralellize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)   // 0

	Limitation of Closures: We can not use local variables to implement a global counter.


  Shared Variables
  -----------------

   1. Accumulator Variable

	=> Is a shared variable, accessed by all the tasks
	=> One copy is maintain in the driver
	=> All tasks add to this single copy
	=> This is not part of the function closure, and hence is not a local copy.
	=> Accumulator is write-only variable for the tasks.

	c = sc.accumulator(0)

	def isPrime(n):
		if n is prime number return 1
		else return 0

	def f1(n):
		global c
		if ( isPrime(n) == 1 ) c.add(1)
		return n*2
	
	rdd1 = sc.paralellize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)   // 0

        -----------------------------


  2. Broadcast Variable

	=> Large immutable collections can be converted into broadcast variables.

	=> BC varibales are not part of function closure
	=> One copy is sent to every executor node and tasks can look up that variable.
	=> Is a read-only variable for tasks. 


	d = {1:a, 2:b, 3:c, 4:d, 5:e, 6:f, ....}    // 100 MB
	bc = sc.broadcast(d)


	def f1(n):
	   global bc
	   return bc.value[n]
	
	rdd1 = sc.parallelize([1,2,3,4,5,6,...], 4)
	rdd2 = rdd1.map( f1 )
 =====================================================

  spark-submit command
  -------------------- 

    => Is a single command that is used to submit any spark application (Scala, Java, Python, R)
       to any cluster manager (local, spark standalone, YARN, mesos, kebernetes)


    	spark-submit [options] <app jar | python file | R file> [app arguments]


	=> spark-submit E:\PySpark\spark_core\examples\wordcount.py

	=> spark-sumit --master yarn 
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 4
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		E:\PySpark\spark_core\examples\wordcount.py  [app args]


	spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 1
	spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount.py

 ========================================================
     Spark SQL (pyspark.sql)
 ========================================================

  -> Spark's high-level API for structured data process

	Structured Data File : Parquet (default), ORC, JSON, CSV (delimited text files)
	JDBC Format : RDBMS, NoSQL Databases
	Hive

   -> SparkSession
	
	-> Is the starting point of execution. 
	-> SparkSession represents a user session insode an application.

		spark = SparkSession \
        		.builder \
        		.appName("Dataframe Operations") \
        		.config("spark.master", "local[*]") \
        		.getOrCreate()    

   -> DataFrame (DF)
	-> Is the data abstraction of Spark SQL

	-> Is a collection of distributed in-memory partitions.
	-> DFs are immutable and lazily-evaluated. 
	-> DF is a collection of "Row" objects  (pyspark.sql.Row)
		-> Each row is a colelction of columns
		-> Columns are processed using "Spark internal types"
	-> Two components:
		-> Data   : Collection of partitions containing Row objects
		-> Schema : StructType object.

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)
  

    show command
    ------------
	df1.show(n=20, truncate=True, verticle=False)

	df1.show()
	df1.show(10, False)
	df1.show(5, True, True)


    Steps in a Spark SQL application:
    --------------------------------

    1. Read/Load the data from some data source into a DataFrame

	inputPath = "E:\\PySpark\\data\\users.json" 

	df1 = spark.read.format("json").load(inputPath)
	df1 = spark.read.json(inputPath)
      
	df1.show()
	df1.printSchema()


    2. Transform the DF using DF API methods or using SQL

	  Using DF API:
	  -------------
		df2 = df1.select("userid", "name", "age", "gender") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)

	  Using SQL
	  ---------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
         		 from users
         		 where age is not null
         		 group by age
         		 order by age
         		 limit 4"""
         
		df3 = spark.sql(qry)
		df3.show()

		spark2.catalog.dropTempView("users")  => drop the tempview created above.


    3. Write/Save the DF into a destination ( directory / DB / Hive )

	outputPath = "E:\\PySpark\\output\\json"

	df3.write.format("json").save(outputPath)
	df3.write.json(outputPath)

  
  Save Modes
  ----------
    -> define what to do when you are writing to an existing directory

	-> ignore
	-> append
	-> overwrite

	df3.write.mode("overwrite").json(outputPath)	
	df3.write.json(outputPath, mode="overwrite")


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

		

  DataFrame Transformations
  --------------------------

from pyspark.sql import SparkSession

from pyspark.sql.functions import broadcast, lit, round, sum, max
from pyspark.sql.functions import col, column, expr, asc, desc, count, avg, when, lit
from pyspark.sql.types import StructType, StructField, LongType, StringType, IntegerType


  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select( col("DEST_COUNTRY_NAME").alias("destination"),
                  column("ORIGIN_COUNTRY_NAME").alias("origin"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )


  2. where / filter

	df3 = df2.where("domestic = false and count > 100")
	df3 = df2.where( col("count") > 1000 )

	df3 = df2.filter( col("count") > 1000 )


  3. orderBy / sort

	df3 = df2.orderBy("count", "destination")
	df3 = df2.orderBy(desc("count"), asc("destination"))
	df3 = df2.sort(desc("count"), asc("destination"))


  4. groupBy  => Returns a 'GroupedData; object; Apply an aggregation method to return a DF

	df3 = df2.groupBy("domestic", "highFrequency").count()
	df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	df3 = df2.groupBy("domestic", "highFrequency").max("count")
	df3 = df2.groupBy("domestic", "highFrequency").avg("count")

	df3 = df2.groupBy("domestic", "highFrequency") \
        	.agg(   count("count").alias("count"), 
              		sum("count").alias("sum"), 
              		max("count").alias("max"), 
              		avg("count").alias("avg")  )


  5. limit
	 df2 = df1.limit(5)

  6. selectExpr

	df2 = df1.selectExpr( "DEST_COUNTRY_NAME as destination",
                  "ORIGIN_COUNTRY_NAME as origin",
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"
                )

  7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
         	.withColumn("highFrequency", col("count") > 200) \
         	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
         	.withColumn("count", col("count").cast("int"))

  8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	 .withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


  =======================================
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()
	userDf.printSchema()

	users2 = userDf.withColumn("ageGroup", 
                           when(userDf["age"] <= 12, "child")
                           .when(userDf["age"] < 20, "teenager")
                           .when(userDf["age"] < 60, "adult")
                           .otherwise("senior"))
  =======================================

  9. drop  => excludes specified columns 

	df5 = df4.drop("highFrequency", "newCount")
	df5.show(5)
	df5.printSchema()

  10. randomSplit

	dfList = df4.randomSplit([0.7, 0.3], 4564)
	dfList[0].count()
	dfList[1].count()

  11. sample

	df5 = df1.sample(True, 1.5, 4645)  	# withReplacement=True
	df5 = df1.sample(False, 0.5, 4645)

  12. distinct
	
	df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

  13. union, intersect, subtract

	df5 = df2.union(df3)
	df5 = df2.intersect(df3)
	df5 = df2.subtract(df3)

  14. repartition

	df2 = df1.repartition(4)

	df3 = df2.repartition(2)

	df4 = df1.repartition(3, col("DEST_COUNTRY_NAME"))

	df5 = df1.repartition(col("DEST_COUNTRY_NAME"))
		=> The number of output partitions is determined by the value of 
		   "spark.sql.shuffle.partitions" configuration whose default value is 200. 

		spark.conf.set("spark.sql.shuffle.partitions", "5")	

  15. coalesce
	
	df5 = df2.coalesce(2)
	df5.rdd.getNumPartitions()

  16. dropDuplicates

	userDf.dropDuplicates().show()
	userDf.dropDuplicates(['name', 'age']).show()	

  17. dropna

	usersDf.dropna().show()
	  -> Drops any row that contains null in any column.

	usersDf.dropna(subset=['phone', 'age']).show()
          -> Drops any row that contains null in ['phone', 'age'] columns.

 18. udf -> Creating and using user-defined functions 	 

------------------------------------------------------
def getAgeGroup( age ):
    if (age <= 12):
        return "child"
    elif (age >= 13 and age <= 19):
        return "teenager"
    elif (age >= 20 and age < 60):
        return "adult"
    else:
        return "senior"
    
getAgeGroupUDF = udf(getAgeGroup, StringType())

users2 = userDf.withColumn("ageGroup", getAgeGroupUDF(col("age")))
users2.show()
------------------------------------------------------
@udf(returnType=StringType()) 
def getAgeGroup( age ):
    if (age <= 12):
        return "child"
    elif (age >= 13 and age <= 19):
        return "teenager"
    elif (age >= 20 and age < 60):
        return "adult"
    else:
        return "senior"

users2 = userDf.withColumn("ageGroup", getAgeGroup(col("age")))
------------------------------------------------------
spark.udf.register("getAgeGroupUDF", getAgeGroup, StringType())

userDf.createOrReplaceTempView("users")
qry = "select id, name, age, getAgeGroupUDF(age) as ageGroup from users"
spark.sql(qry).show(truncate=False)
------------------------------------------------------
  
   19. join


   


   Working with different file formats
   ------------------------------------

	Structured Data File : Parquet (default), ORC, JSON, CSV (delimited text files)


	JSON
	-----
		read
			df1 = spark.read.format("json").load(inputPath)
			df1 = spark.read.json(inputPath)

		write
			df3.write.format("json").save(outputPath)
			df3.write.json(outputPath)

	Parquet
	-------
		read
			df1 = spark.read.format("parquet").load(inputPath)
			df1 = spark.read.parquet(inputPath)

		write
			df3.write.format("parquet").save(outputPath)
			df3.write.parquet(outputPath)
	ORC
	-----
		read
			df1 = spark.read.format("orc").load(inputPath)
			df1 = spark.read.orc(inputPath)

		write
			df3.write.format("orc").save(outputPath)
			df3.write.orc(outputPath)
	

        CSV (demilited text format)
	---------------------------
	
		read
			df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
			df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
			df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
			df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

		write
			df2.write.csv(outputPath, mode="overwrite", header=True)
			df2.write.csv(outputPath, mode="overwrite", header=True, sep="|")


    Creating an RDD from a DF
    -------------------------
	rdd1 = df1.rdd
	rdd1.take(5)


    Creating a DataFrame from programmatic Data
    --------------------------------------------
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Rajesh", 25),
             (4, "Ravi", 15),
             (5, "Ramya", 25),
             (6, "Radhika", 35)]

	df1 = spark.createDataFrame(listUsers)    # cols => _1, _2, _3

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


    Creating a DataFrame from RDD
    -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1.show()


   Creating a DataFrame with custom schema
   ---------------------------------------	
   
  	 mySchema = StructType([StructField("id", IntegerType(), True),
            	StructField("name", StringType(), True),
           	StructField("age", IntegerType(), True)])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)

	--------------------------------------------------

	mySchema = StructType(
               [
                StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
                StructField("DEST_COUNTRY_NAME", StringType(), True),
                StructField("count", IntegerType(), True)
               ]
           )
        
	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json" 

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)

   Joins
   -----

    Supported Joins: inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

    left-semi join
    ---------------
	-> Similar to inner join but data comes only from left-side table.
	-> Equivalent to sub-query:
		select * from emp where deptid in (select id from dept)

    left-anti join
    ---------------
	-> Equivalent to sub-query:
		select * from emp where deptid not in (select id from dept)

    	SQL Way
	--------
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*
         	from emp left anti join dept
         	on emp.deptid = dept.id"""
         
	joinedDf = spark.sql(qry)

	joinedDf.show()


	DF API Way
	---------
	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")
	joinedDf.show()


  Use-Case
  --------

  Datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

   From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average user-rating 
  	-> Consider only those movies that are rated by atleast 30 users.
  	-> Data: movieId, title, totalRatings, averageRating
  	-> Arrange the data in the DESC order of averageRating
  	-> Save the output as a single pipe-serated CSV file with header.

    => Please try yourself.


  Working with MySQL (JDBC Format)
  --------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


   Working Hive
   ------------
	
    Hive -> Data Warehousing Platform built on top of Hadoop.	
	Warehouse: directory where hive stores all its data-files
	MetaStore: Is where hive stores all its metadata (embedded metastore: derby, production: mysql)
   
   import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()

 =====================================================
    Spark Streaming
 =====================================================

  Two APIs
  --------

	Spark Streaming	 
		-> Built on top of Spark low-level API (RDDs)
		-> DStream API
		-> micro-batch based processing
		-> Provides 'seconds' scala latency (1 sec, 2 sec)
		-> near real time processing
		-> Streaming sources nativly supported: Sockets, File streams
		-> Event-time processing is not supported

	Structured Streaming  (IMP)
		-> Built on top of Spark SQL
		-> DataFrames based API
		-> micro-batch as well as continuous processing
		-> Provides 'milli-seconds' scala latency (10 ms, 1 ms)
		-> real time processing
		-> Streaming sources nativly supported: Sockets, File streams, Kafka
		-> Event-time processing is supported.

 
   Spark Streaming
   ---------------- 

       StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	-> Is a continuous flow of RDDs.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 
   

   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch










