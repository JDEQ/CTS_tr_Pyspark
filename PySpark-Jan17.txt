
 Agenda (PySpark)
 ----------------
   Spark - Basics & Architecture
   Spark Core API Basics
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes
	=> Databricks Notebooks
        => Github: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----   
    => Spark is in-memory distributed computing framework for big data analytics.

    	in-memory: ability to persist intermediate results and subsequent operations
		   can directly work on these persisted intermediate results. 

    => Spark is written in Scala programming language.

    => Spark is polyglot
	 -> Scala, Java, Python, R (and SQL)

    => Spark is a unified analytics framework

    => Spark can run on multiple cluster managers
	  local, spark standalone scheduler, YARN, Mesos, Kubernetes
  
  
  Spark Unified Framework
  -----------------------

	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming, Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib
	Graph parallel computations  		-> Spark GraphX


   Getting started with Spark
   --------------------------

	1. Setting up local dev environment on your personal machine.

		-> Install Anaconda distribution for Python. 
		   URL: https://www.anaconda.com/download

		-> Follow the instructions given in the shared document 
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

	* 2. Signup to Databricks Community Edition (free edition)
 		
		Signup: https://www.databricks.com/try-databricks

			Screen 1: Fill up the details with valid email address
			Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button)

		Login: https://community.cloud.databricks.com/login.html

		Downloading a file from Databricks
		----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439

		Example:
		file path to be downloaded: dbfs:/FileStore/ctsdatasets/output/wc/part-00000
		https://community.cloud.databricks.com/files/ctsdatasets/output/wc/part-00000?o=1072576993312365

  
  Spark Architecture
  ------------------

    	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution
	   -> Action commands trigger execution.	
 
  Creating RDDs
  -------------	
    Three ways:

	1. Create an RDD from external data files.

		rddFile = sc.textFile( <filePath> , 4 )

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,4,3,1,5,6,7,8,6,8,9,0,7,5,4,6,8], 2)
		
		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


  RDD Lineage DAG
  ---------------
  Driver maintains a Lineage DAG for every RDD
  Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD	
  Lineage DAG is a logical plan on how to create the RDD.

   	rddFile = sc.textFile("E:\Spark\wordcount.txt", 4)
	   Lineage of rddFile -> (4) rddFile -> sc.textFile on E:\Spark\wordcount.txt
	
	rddWords = rddFile.flatMap(lambda x: x.split())
	   Lineage of rddWords -> (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage of rddPairs -> (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	   Lineage of rddWc -> (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  Types of RDD Transformation
  ---------------------------

    Two types:

	 1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  Spark execution flow
  --------------------

	Application (created by a SparkContext)
	|
	|=> Jobs (each action command lauches a job)
	    |
	    |=> Stages (one or more stages; each wide transformation create a new stage)
		|
		|=> Tasks ( set of transformations that can run in partition)
		    |
  		    |=> Transformations (that can run in parallel)
 
  RDD Persistence
  ---------------
	rdd1 = sc.textFile(<file>, 4)
	rdd2 = rdd1.t2(....)
	rdd3 = rdd1.t3(....)
	rdd4 = rdd3.t4(....)
	rdd5 = rdd3.t5(....)
	rdd6 = rdd5.t6(....)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )         --> instruction to spark to save the rdd6 partitions
	rdd7 = rdd6.t7(....)

	rdd6.collect()	
		Lineage DAG of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile, t3, t5, t6] -> collect

	rdd7.collect()
		Lineage DAG of rdd6: (4) rdd7 -> rdd6.t7
		[t7]

	rdd6.unpersist()	-> deletes the partitions


	StorageLevels
	-------------
	MEMORY_ONLY		=> default, Memory Serialized 1x Replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x Replicated
	DISK_ONLY		=> Disk Serialized 1x Replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x Replicated
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x Replicated


	Commands
	--------
	rdd1.cache()     -> in-memory persistence
	rdd1.persist()	 -> in-memory persistence
	rdd1.persist(StorageLevel.MEMORY_AND_DISK)   -> custom storage-level persistence

	rdd1.unpersist()	


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creation and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional memory upto the quota allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.


  RDD Transformations
  -------------------
  
  1. map		P: U -> V
			Object to object transformation
			Input RDD: N objects, Output RDD: N objects					

  		rdd2 = rddFile.map(lambda x : x.split())


  2. filter		P: U -> Boolean
			Filters the objects based on the function
			Input RDD: N objects, Output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split()) > 8 ).collect()

  3. glom		P: None
			Return one list object per partition with all the objects of the partition

		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,3,2,4 -> glom -> P0: [3,2,1,3,2,4]
		P1: 5,4,3,6,7,8 -> glom -> P1: [5,4,3,6,7,8]
		P2: 3,2,6,5,7,0 -> glom -> P2: [3,2,6,5,7,0]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)
	
	
  4. flatMap		P: U -> Iterable[V]
			flattens the iterabels generated by the function
 			Input RDD: N objects, Output RDD: >= N objects	


		rddWords = rddFile.flatMap(lambda x: x.split())


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get the partition index as an additional parameter.

		rdd1 \
		.mapPartitionsWithIndex(lambda i,p: map(lambda x: (i, x), p)) \
		.filter(lambda x: x[0] == 1) \
		.map(lambda x: x[1]) \
		.collect()


  Types of RDDs
  -------------
	Generic RDDs: RDD[U]            
	Pair RDD: RDD[(K, V)]


  7. mapValues		P: U => V
			Applied only on Pair RDD
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs


  8. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

			rddWords.distinct().collect()
 

  9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the RDDs based on the function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 2).glom().collect()


  10. groupBy		P: U -> V, Optional: numPartitions

			Returns a Pair-RDD where:
			    key: Each unique value of the function output
		            value: ResultIterable. Grouped objects of the RDD that produced the key


  11. partitionBy	P: numPartitions, Optional: partitioning function
			Applied only on Pair RDDs
			Partitions the data based on the key by applying the partitioning function.


  12. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           

  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])

	
  ..ByKey Transformations
  -----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only
	

  13. sortByKey		P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

			rddPairs.sortByKey().glom().collect()
			rddPairs.sortByKey(False).glom().collect()
			rddPairs.sortByKey(False, 3).glom().collect()


  14. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: avoid groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)

  15. reduceByKey	P: (U, U) -> U
			Reduce all the values of each key by iterativly applying the reduce function.
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)

  16. repartition	P: numPartitions
			Is used to increase or decrease the number of output partitions	
			Global shuffle	

			
		rdd2 = rdd1.repartition(5)



  17. coalesce		P: numPartitions
			Is used to only decrease the number of output partitions	
			Partition merging


		rdd2 = rdd1.coalesce(5)


		Recommendations
		---------------
		-> The size of each partition should be between 100 MB to 1 GB
		   (Ideally 128 MB if you are running on Hadoop)
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor should be 5




  RDD Actions
  -----------

 1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one object by iterativly applying the function first on
			each partition and then across the reduced values of all partitions. 	
		
		rdd1									
	     	P0: 1, 2, 4, 3, 5, 7, 6, 8, 7, 0    -> reduce -> 43 -> reduce -> 153
		P1: 9, 8, 7, 9, 7, 9, 6, 8, 4, 2    -> reduce -> 69
		P2: 1, 3, 2, 1, 3, 5, 9, 0, 9, 0, 8 -> reduce -> 41

		rdd1.reduce(lambda x, y: x + y)

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )


  5. aggregate		Three parameters
			
			1. zero-value : starting value of the type of final output you want to produce
			2. sequence fn : Operate on each partition
					Iterativly merges all the values of the partition with the zero-value
			3. combine fn : Reduces all the per-partition outputs generated by seq-fn.

	rdd1.aggregate( (0,0), lambda z,v:(z[0]+v, z[1]+1), lambda x,y: (x[0]+y[0], x[1]+y[1]) )
	
  
  6. take(n)

		rdd1.take(10)  -> returns a list of first 10 objects


  7. takeOrdered(n, [fn])

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, len)


  8. takeSample(withReplacement, n, [seed])

		with-replacement sampling
			rdd1.takeSample(True, 10)
			rdd1.takeSample(True, 10, 45645)    # 45645 is a seed

		without-replacement sampling
			rdd1.takeSample(False, 10)
			rdd1.takeSample(False, 10, 45645)   # 45645 is a seed


  9. countByValue


  10. countByKey

  11. foreach  => P: function;
		  Returns nothing. 
		  Executes the function on all objects of the RDD.

  12. saveAsSequenceFile => Sequence File is Hadoop file format that stores key-value pairs.



   Spark Closures
   --------------

	In Spark, a closure constitutes all the variables and methods which must be visible 
	for the executor to perform its computations on the RDD. 

	-> This closure is serialized and sent to each executor.


	c = 0

	def isPrime(n):
	   return True if n is Prime
	   return False if n is not Prime

	def f1(n):
	   global c
	   if (isPrime(n)) c += 1
	   return n * 2	

	rdd1 = sc.parallelize(range(1, 4001), 4)
	rdd2 = rdd1.map(f1)

	rdd2.collect()

	print(c)      // 0


	Limitation: We can not use local variables (which are part of the closure) to implement global counter. 
	Solution: Use 'Accumulators' 


  Distributed Shared Variables
  ----------------------------

    1. Accumulator variable

	-> Is a shared variable, not part of a closure
	-> Not a local copy
	-> One variable maintained by the driver
	-> All tasks can add to this variable (the driver copy is updated)
	-> Is used to implement "global counter"

	c = sc.accumulator(0)

	def isPrime(n):
	   return True if n is Prime
	   return False if n is not Prime

	def f1(n):
	   global c
	   if (isPrime(n)) c.add(1)
	   return n * 2	

	rdd1 = sc.parallelize(range(1, 4001), 4)
	rdd2 = rdd1.map(f1)

	rdd2.collect()

	print(c.value()) 


  2. Broadcast variable

	-> Is a shared variable that is not part of the closure
	-> Variabe is broadcasted to each executor node.
	-> All tasks in that executor can read from that copy.

	d = sc.broadcast({1:a, 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, ....})   #100 MB

	def f1(n):
	   global d
	   return d.value[n]

	rdd1 = sc.parallelize(range(1, 4001), 4)
	rdd2 = rdd1.map(f1)

	rdd2.collect()    => [a, b, c, d, ...]
	
		
   Case-Study
   ----------
		
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, get the average weight of all the models of each make 
	of American origin cars. 

	-> Arrange in the DESC order of average weight. 
        -> Save the output as a single text file. 

	=> Try it yourself



  Spark-Submit Command
  ====================

     -> Is a single command to submit any spark application (Python, Scala, Java, R) to any cluster
        manager (local, spark standalone, YARN, Mesos, Kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
		--executor-memory 10G \
		--driver-cores 2 \
		--executor-cores 5 \
		--num-executors 10 \
		E:\PySpark\wordcount.py [app args]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2


  ====================
    Spark SQL
  ====================

    -> High Level API built on top of Spark Core

    -> Spark's structured data processing API

           File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	   JDBC Format  : RDBMS, NoSQL
	   Hive Format  : Hive Warehouse


    SparkSession
    ------------

	-> Starting point of execution
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()


   DataFrame (DF)
   --------------
	-> Main data abstraction of Spark SQL

	-> Is a collection of distributed in-memory partitions
	-> Immutable
	-> Lazily evaluated

	-> DataFrame contains two components:
		-> Data    : Collection of 'Row' object
		-> Schema  : StructType object

  		StructType(
		   [
		     	StructField('age', LongType(), True), 
			StructField('gender', StringType(), True), 
			StructField('name', StringType(), True), 
			StructField('phone', StringType(), True), 
			StructField('userid', LongType(), True)
		   ]
		)


  Basic steps in creating a Spark SQL Application
  -----------------------------------------------
	
   1. Create a DataFrame from some external / programmatic data. 

		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()

   2. Transform the DF using DF transformation methods or using SQL

		
		Using DF transformation
		-----------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)

		Using SQL
		---------
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()


   3. Save the dataframe into some external destination (such as files/databases/hive etc)   

		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

		df3.write.json(outputPath, mode="overwrite")

  Save Modes
  ----------
    -> Control the behaviour when saving a DF into an existing directory.

	1. errorIfExists (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


 DataFrame Transformations
 -------------------------

  1. select


  2. where / filter

   
  3. orderBy


  4. groupBy   => returns a pyspark.sql.group.GroupedData
		  Use an aggregation method to return a DataFrame


  5. limit 



  Working with different file formats
  -----------------------------------
  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)	


  CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")


  Text
	read
		df1 = spark.read.text(inputPath)

	write
		df1.write.text(outputPath)





























