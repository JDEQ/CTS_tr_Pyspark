
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 =====================================================================

   Spark

        -> Spark is a unified in-memory distributed computing framework

	-> Spark is written in SCALA programming language.

	in-memory => the intermediate results can be persisted in the RAM (memory) and subsequent tasks can
	read these results carry the processing forward.

	=> Is a framework to perform bigdata analytics.
	
   
    Spark unified framework
    -----------------------

	Spark provides a consistent set of APIs for processing different analytical work loads
	using the same execution engine.

		Batch processing of unstructured data	: Spark Core API  (low-level API)
		Batch processing of structured data	: Spark SQL
		Streaming analytics			: Spark Streaming
		Predictive analytics			: Spark MLlib
		Graph parallel computations		: Spark GraphX

      Spark is a polyglot
	  -> Scala, Java, Python and R  

      Spark supports multiple cluster managers
	  -> local, spark standalond scheduler, YARN, Mesos, Kubernetes

	  
   Getting started with Spark
   --------------------------

      1. Working in your vLab 

	   -> PySpark shell
	       -> Open a terminal and type 'pyspark'

	   -> Spyder IDE

      2. Setting up your own dev. environment on your personal system. 

	   => Try pip install  (pip install pyspark)
		
		Else..

	   => Follow the instructions given in the shared document. 
		-> Make sure you install Anaconda before using the above document.

      3. Try Databricks community edition (free cloud service)
		
		Signup URL: https://www.databricks.com/try-databricks
		NOTE: Install "Community edition" only 
		
		Login URL: https://community.cloud.databricks.com/login.html


		To Download a file from databricks:
		-----------------------------------

		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

		/FileStore/tables/wordcount/part-00000
		https://community.cloud.databricks.com/files/tables/wordcount/part-00000?o=1072576993312365#tables/new/dbfs
    	

   Spark Architecture
   ------------------
	
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks is reported to the driver.


   Deploy Mode
   -----------
	=> Tells where to run the driver process - on the client (client mode) or on the cluster (cluster mode)


   Resilient Distributed Dataset (RDD)
   -----------------------------------

    => RDD is the fundamental data abstraction in Spark core API.

    => A RDD is a collection of distributed in-memory partitions.
	   -> A partition is a collection of object

    => Components of RDD
		RDD meta-data   : Lineage DAG
		RDD data	: Partitions

    => RDDs are lazily evaluated
	 -> Transformations does not cause executions
	 -> Only action commands trigger execution.

    => RDDs are immutable
     

   Creating RDDs
   -------------
	Three ways:

	1. Create an RDD from some external data source:

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

	2. Create an RDD from programmatic data.

		rdd1 = sc.parallelize( range(1,100), 3 )

	3. By applying transformations on exiting RDD

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))   

   RDD Operations
   --------------

	Two operations:

	1. Transformations
		-> Only create RDD lineage DAGs
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


   RDD Lineage DAGs
   ----------------
   RDD Lineage DAG, which is a logical plan, is created when an RDD is created and maintained by the Driver
   RDD Lineage DAG id a heirarchy of dependent RDDs  all the way from the very first RDD.
	

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage of rdd1: (4) rdd1 -> sc.textFile


	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
	Lineage of rdd2: (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))
	Lineage of rdd3: (4) rdd3  -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x,y: x + y)
	Lineage of rdd4: (4) rdd4 -> rdd3.reduceByKey  -> rdd2.map -> rdd1.flatMap -> sc.textFile


  RDD Persistence
  ---------------
	rdd1 = sc.textFile( <file>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.DISK_ONLY )       -> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	
	Lineage of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	(sc.textFile, t3, t5, t6) -> rdd6 

	rdd7.collect()
	
	Lineage of rdd6: (4) rdd7 -> rdd6.t7
	(t7) -> rdd6 

	rdd6.unpersist()

	StorageLevel
	------------
	1. MEMORY_ONLY		: default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		: Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated


	Commands
	---------
	rdd1.cache()   -> in-memory
	rdd1.persist() -> in-memory
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()


  Executor's memory structure
  ---------------------------  
       Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)
	


  Types of transformations
  ------------------------
        Two types:

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


   RDD Transformations
   -------------------

   1. map		P: U -> V
			Transforms each object by applying the function
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: len(x.split(" ")) ).collect()

   2. filter		P: U -> Boolean
			Filters the objects based on the function
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x) < 51).collect()

  3. glom		P: None
			Returns one list per partition with all the objects of the partition.

		rdd1		     rdd2 = rdd1.glom()
		P0: 4,3,2,1,0,8 -> glom -> P0: [4,3,2,1,0,8]
		P1: 4,5,6,7,8,9 -> glom -> P1: [4,5,6,7,8,9]
		P2: 5,4,3,2,6,0 -> glom -> P2: [5,4,3,2,6,0]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

		rdd1.glom().map(max).collect()

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterable produced by the function
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation


		rdd1		 rdd2 = rdd1.mapPartitions( lambda p: [sum(p)] )   
		P0: 4,3,2,1,0,8 -> mapPartitions -> P0: 18
		P1: 4,5,6,7,8,9 -> mapPartitions -> P1: 39
		P2: 5,4,3,2,6,0 -> mapPartitions -> P2: 20

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()


   6. mapPartitionsWithIndex  P: Int, Iterable[U] -> Iterable[V]
			    Similar to mapPartitions, but we get the partition-index as an additional 
			    function parameter.

	 	rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))]).collect()

   7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

		rddWords.distinct().glom().collect()

   Types of RDDs
   -------------
	Generic RDDs: RDD[U]
	Pair RDDs: RDD[(K, V)]
	
   8. mapValues		P: U -> V
			Applied only to pair RDDs
			Transforms the 'value' part of the (K,V) pairs by applying the function.


   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the value of the function output.

			rddWords.sortBy(lambda x: x[-1]).collect()
			rddWords.sortBy(lambda x: x[-1], False).collect()
			rddWords.sortBy(lambda x: x[-1], False, 5).collect()


   10. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair RDD where:
				key: each unique value of the function output
				value: ResultIterable containing all the objects of the RDD that returned the key.

		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           			.flatMap(lambda x: x.split(" ")) \
           			.groupBy(lambda x: x) \
           			.mapValues(len) \
           			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit	P: List of weights (eg: [0.6, 0.4])
			Returns a list of RDDs split randomly in the specified weights.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 4645)

   12. repartition	P: numPartitions
			You can increase or decrease the number of partitions of the output RDD
			Performs global shuffling

		rdd2 = rdd1.repartition(5)


   13. coalesce		P: numPartitions 
			You can only decrease the number of partitions of the output RDD
			Performs partition merging.

		rdd2 = rdd1.repartition(5)


	Recommendations
	---------------
	1. The size of the partition should be between 100MB and 1GB  (ideally 128 MB).
	2. The number of partitions should be a multiple of number of cores.
	3. If the number of partitions is less than but close to 2000, bump it up to 2000
	4. The number of cores per executor should be 5


   14. partitionBy	P: numPartitons, Optional: partitioning-function
			Applied only to Pair RDD
			Controls which pairs go to which partition based on partitioning-function 
			applied to the key.

transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;      

rdd1 = sc.parallelize(transactions, 3) \
        .map(lambda d: (d['city'], d) ) \
        .partitionBy(4, custom_partitioner)

rdd1.glom().collect()
rdd1.partitioner


   15. union, intersection, subtract, caresian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				partitions & type
        --------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide
	

    ...ByKey transformations
    ------------------------
	-> Are wide transformations
	-> Applied only on pair RDDs	
	-> Operate based on the value of 'key'


   16. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Elements are sorted based on the key.

		rdd4.sortByKey().glom().collect()
		rdd4.sortByKey(False).glom().collect()
		rdd4.sortByKey(False, 3).glom().collect()

   17. groupByKey	P: None, Optional: numPartitions
			Elements of the RDD are grouped based on the key.

			CAUTION: AVOID groupByKey if you can

		output = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
           			.flatMap(lambda x: x.split(" ")) \
           			.map(lambda x: (x,1)) \
           			.groupByKey() \
           			.mapValues(sum) \
           			.sortBy(lambda x: x[1], False, 1)

   18. reduceByKey	P: (U, U) -> U
			Reduces the all the values of each unique key by iterativly applying the
			reduce function on each partitions in first stage and across partitions in next stage

		output = sc.textFile(inputPath, 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.map(lambda x: (x, 1)) \
            		.reduceByKey(lambda x, y: x + y)

    19. aggregateByKey    	 P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


    Use-case
    --------
	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	Find the average weight of all the makes of cars with 'American' origin.
	Arrange the data in the DESC order of average-weight
	Save the output as single text file.

	=> Try to do it yourself. 

   
   Actions
   -------

   1. collect

   2. count  

   3. saveAsTextFile 

   4. reduce			P: (U, U) -> U
				Reduces the entire RDD to one value of the same type by iterativly applying the
				reduce function on each partitions in first stage and across partitions in next stage

		rdd1 
		P0: 3,2,1,5,3,5 -> reduce -> 5  -> reduce -> 8
		P1: 6,4,8,0,6,1 -> reduce -> 8
		P2: 7,2,4,3,2,1 -> reduce -> 7

		rdd1.reduce( lambda x, y: max(x, y) )
		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )	

   5. aggregate
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. take(n)

   7. takeOrdered(n, [ordering-fn])

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, len)

   8. takeSample( withReplacement, n, [seed] )

		rdd1.takeSample(True, 10)	   	# True means withReplacement = True
		rdd1.takeSample(True, 1000000)
		rdd1.takeSample(True, 100, 433)    	# 433 is a seed here

		rdd1.takeSample(False, 100, 433)	# False means withReplacement = false
		rdd1.takeSample(False, 100, 433)

   9. countByValue  => Returns the Count of each unique object

		rdd2.countByValue()

   10. countByKey    => applied on PairRDDs. Returns the Count of each unique key

		rdd2.countByKey()

   11. first

   12. foreach     => Runs a function on all objects of the RDD. Does not return any value. 
		
		rddWc.foreach(lambda a: print("key:" + a[0] + ",value:" + str(a[1])) )

   13. saveAsSequenceFile
		
		RDD[(K,V)].saveAsSequenceFile( <outputDir> )
		rddWc.saveAsSequenceFile("E:\\PySpark\\output\\seq")
	

   spark-submit command
   --------------------

        Is the single command that is used to submit any spark application (scala, java, python, R)
	to any cluster manager (local, standalone scheduler, YARN, Mesos, Kubernetes)     

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
	        --driver-cores 2 \		
		--executor-memory 5G \
		--executor-cores 5 \
	        --num-execures 4 \
		E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt cmd_xmpl 1
	
 
  Spark Closure
  -------------
	-> A closure constitute all the variables and methods that must be visible inside an executor
           for the tasks to perform their computations on the RDD

        -> The closure is serialized (by driver) and a separate copy is sent to every executor.



	c = 0

	def isPrime(n):
	   return True if n is prime
	   else return false

	def f1(n):
	   global c
	   if (isPrime(n)) c += 1
	   return n * 10

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
	
	rdd2.collect()

	print(c)      # c = 0

	
	Limitation: Local variables can not be used to implement global counters. 
	Solutions: Use "Accumulator" variable


  Shared Variables
  ----------------
 
    Accumulator: 
  
  
 
   	c = sc.accumulator(0)

	def isPrime(n):
	   return True if n is prime
	   else return false

	def f1(n):
	   global c
	   if (isPrime(n)) c.add(1)
	   return n * 10

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
	
	rdd2.collect()

	print( c.value() )      # c = 80



   Broadcast variable
   ------------------

               
	d = sc.broadcast({1:10, 2:20, 3:30, 4:40, ...... })    # 100 MB

	def f1(n):
	   global d
	   return d.value[n]  
	
	rdd1 = sc.parallelize( [1,2,3,4,5,6, ..], 4)

	rdd2 = rdd1.map( f1 )    
	  
	rdd2.collect()    => [10,20,30,40, ...] 

    
  ===================================================
      Spark SQL  (pyspark.sql)
  ===================================================
    
    -> Spark SQL is a structured data processing API, built on top of Spark Core. 

	   Structured file formats: Parquet (default), ORC, JSON, CSV (delimited text)
		       JDBC format: RDBMS databases, NoSQL databases
		       Hive format: Hive warehouse.
	
    -> SparkSession
	  -> Starting point of execution for Spark SQL
	  -> Represents a user session within an appication

		spark = SparkSession \
    			.builder \
    			.appName("Basic Dataframe Operations") \
    			.config("spark.master", "local[*]") \
    			.getOrCreate() 

    -> DataFrame (DF)

	 -> Is a collection of in-memeory partitions that are immutable and lazily evaluated.
	 -> Is a collection of Row objects

	  DataFrame has two components:
		-> Data: Collection of Row objects
		-> Schema: StructType object

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)



   Basic steps in Spark SQL
   ------------------------

    1. Read/load data from some data source into a DataFrame

		df1 = spark.read.format("json").load(inputFile)
		df1 = spark.read.load(inputFile, format="json")
		df1 = spark.read.json(inputFile)

		df1.show()
		df1.printSchema()


    2. Apply transformations on the DF using transformation methods or using SQL

		Using transformation methods
		----------------------------

			df2 = df1.select("userid", "name", "age", "gender") \
         			.where("age is not null") \
         			.orderBy("gender", "age") \
         			.groupBy("age").count() \
         			.limit(4)

		Using SQL
		---------
			df1.createOrReplaceTempView("users")
			
			df3 = spark.sql("""select age, count(1) as count
         			from users
        			where age is not null
         			group by age
         			order by count
         			limit 4""")


    3. Write/save the content of the DF to some destinations
		
		outputDir = "E:\\PySpark\\output\\json"

		df3.write.format("json").save(outputDir)
		df3.write.save(outputDir, format="json")
		df3.write.json(outputDir)
		df3.write.mode("overwrite").json(outputDir)


  Save Modes
  ----------
	
    => Determine the behaviour when you saving a DF to an existing directory

	errorIfExists  (default)
	ignore
	append
	overwrite
	
	df3.write.mode("overwrite").json(outputDir)
	df3.write.json(outputDir, mode="overwrite")	

  
  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessible only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

  DF Transformations
  ------------------

  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select( col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                  column("DEST_COUNTRY_NAME").alias("destination"),
                  expr("count").cast("int"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
                )

  2. where / filter

	df3 = df2.where("highFrequency=True and domestic=False")
	df3 = df2.filter("highFrequency=True and domestic=False")

	df3 = df2.where( col("count") > 1000 )
	df3 = df2.where( expr("count > 1000") )

  3. orderBy / sort

	df3 = df2.orderBy( "count", "origin" )
	df3 = df2.orderBy( desc("count"), asc("origin") )

	df3 = df2.sort( desc("count"), asc("origin") )
	df3.show()


  4. groupBy -> returns a 'GroupedData' object
		use some aggregation function to return a DF

	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
         	.agg(  count("count").alias("count"),
                	sum("count").alias("sum"),
                	round(avg("count"), 1).alias("avg"),
                	max("count").alias("max")
             	)
	df3.show()


  5. limit
	  df2 = df1.limit(10)


  6. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
         	.withColumn("highFrequency", expr("count > 200")) \
         	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
         	.withColumn("count", col("count").cast("int")) \
         	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	df3 = userDf.withColumn("ageGroup", when( col("age") < 13, "child" )
                                    	    .when( col("age") < 20, "teenager" )
                                    	    .when( col("age") < 60, "adult" )
                                    	    .otherwise("senior"))

  7. udf (user defined function)

	
            def getAgeGroup( age ):
                if (age <= 12):
                    return "child"
                elif (age >= 13 and age <= 19):
                    return "teenager"
                elif (age >= 20 and age < 60):
                    return "adult"
                else:
                    return "senior"
            
            getAgeGroupUdf = udf(getAgeGroup, StringType())
                
            df3 = userDf.withColumn("ageGroup", getAgeGroupUdf("age") )

	   --------------------------
	    @udf(returnType = StringType())
            def getAgeGroup( age ):
                if (age <= 12):
                    return "child"
                elif (age >= 13 and age <= 19):
                    return "teenager"
                elif (age >= 20 and age < 60):
                    return "adult"
                else:
                    return "senior"
                
            df3 = userDf.withColumn("ageGroup", getAgeGroup("age") )
		
	    ---------------------------
	     userDf.createOrReplaceTempView("users")
	     spark.catalog.listTables()

	     spark.udf.register("get_age_group", getAgeGroup, StringType())

	     qry = "select id, name, age, get_age_group(age) as ageGroup from users"

	     df4 = spark.sql(qry)

   8. drop 

	    df3 = df2.drop("newCount", "highFrequency")
	    df3.printSchema()

   9. dropna

	    df3 = usersDf.dropna(subset=["phone", "age"])
	    df3.show()

   10. dropDuplicates

	   userDf.dropDuplicates().show()
	   userDf.dropDuplicates(["name", "age"]).show()
	  

   11. distinct

	  userDf.distinct().show()

   12. sample

	  	df2 = df1.sample(True, 0.5)
	 	df2 = df1.sample(True, 0.5, 4564)
	  	df2 = df1.sample(True, 1.5, 4564)   # fraction > 1 is allowed

	  	df2 = df1.sample(False, 0.5)
	  	df2 = df1.sample(False, 1.5, 4564)  # ERROR: fraction > 1 is NOT allowed

   13. ramdomSplit
	 
		dfList = df1.randomSplit([0.6, 0.4], 6867)
		print( dfList[0].count(),  dfList[1].count() )

   14. union, intersect, subtract
        
	df2 = df1.where("ORIGIN_COUNTRY_NAME = 'India'")
        df2.show()  # rows = 1
        df2.rdd.getNumPartitions()
        
        df3 = df1.where("count > 1000")
        df3.show()
        df3.count() # rows = 14
        df3.rdd.getNumPartitions()
        
        df4 = df2.union(df3)        
        df4.show()
        df4.rdd.getNumPartitions()        
        
        df5 = df4.intersect(df2)
        df5.show()
        df5.rdd.getNumPartitions()
        
        df6 = df4.subtract(df2)
        df6.show()
        df6.rdd.getNumPartitions()


   15. repartition
        
        df2 = df1.repartition(6)
        df2.rdd.getNumPartitions()
        
        df3 = df2.repartition(4)
        df3.rdd.getNumPartitions()
        
        df4 = df2.repartition(4, col("DEST_COUNTRY_NAME"))
        df4.rdd.getNumPartitions()
        
        df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
        df5.rdd.getNumPartitions()
        

   16. coalesce		

        df6 = df2.coalesce(2)
        df6.rdd.getNumPartitions()


   17. Joins  => discussed separatly


   Working with different file formats
   -----------------------------------

   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")


  
   Creating an RDD from DF
   -----------------------
	rdd1 = df1.rdd


   Creating a DF from programmatic data
   -------------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df2.show()


   Creating a DF from RDD
   ----------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	rdd1 = spark.sparkContext.parallelize(listUsers)
	df2 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df2 = rdd1.toDF(["id", "name", "age"])
	df2.show()


  Using Programmatic schema on a DF
  ---------------------------------
	mySchema = StructType([
             StructField("id", IntegerType(), True),
             StructField("name", StringType(), True),
             StructField("age", IntegerType(), True)
           ])

	df2 = spark.createDataFrame(listUsers, schema = mySchema)
	df3 = rdd1.toDF(schema = mySchema)
       ----------------------
	inputFile = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	mySchema = StructType([
             StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
             StructField("DEST_COUNTRY_NAME", StringType(), True),
             StructField("count", IntegerType(), True)
           ])

	df1 = spark.read.json(inputFile, schema=mySchema)


  Joins
  ======
      Supported Joins:  inner, left, right, full, left_semi, left_anti

      left_semi join
      --------------
	Similar to inner join, but fetches the data only from the leftside table.
	
	Equivalent to the following subquery:
		select * from emp where deptid in (select id from dept)		
    
      left_anti join
      --------------		
	Equivalent to the following subquery:
		select * from emp where deptid not in (select id from dept)



        employee = spark.createDataFrame([
            (1, "Raju", 25, 101),
            (2, "Ramesh", 26, 101),
            (3, "Amrita", 30, 102),
            (4, "Madhu", 32, 102),
            (5, "Aditya", 28, 102),
            (6, "Pranav", 28, 100)])\
          .toDF("id", "name", "age", "deptid")
          
        department = spark.createDataFrame([
            (101, "IT", 1),
            (102, "ITES", 1),
            (103, "Opearation", 1),
            (104, "HRD", 2)])\
          .toDF("id", "deptname", "locationid")
        
        employee.show()     
        department.show()
        
        employee.createOrReplaceTempView("emp")
        department.createOrReplaceTempView("dept")
        
        spark.catalog.listTables()
        
        qry = """select emp.*
                 from emp left anti join dept
                 on emp.deptid = dept.id"""
        
        joinedDf = spark.sql(qry)
        
        joinedDf.show()






  

















        




























   







