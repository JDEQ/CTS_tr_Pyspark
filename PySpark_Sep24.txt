
  Agenda
  ------

   Prerequisite: Python

   	-> Spark Basics, Architecture
	-> Spark Core API (Low Level API)
		-> RDDs - RDD Transformations & Actions
	-> Spark SQL 
	-> Spark MLlib (Machine Learning)
	-> Introduction to Spark Streaming (Real-time analytics)

  ----------------------------------------------------

     Materials
	-> PDF versions of the presentations
	-> Daily class notes 
	-> Code modules
	-> GitHub: https://github.com/ykanakaraju/pyspark

 ---------------------------------------------------

  Big Data
  --------   
     -> Large and Complex Data
     -> Data that is hard to store and process using traditional technologies that rely on single machines.

	How ig Big Data characterized:
	-> Volume
	-> Velocity
	-> Variety (unstructured & semi-structured data)
	-> Veracity
	-> Value
	
    -> A single machine become a limitation, the solution is to use the combined resources of many machines
       This is called a cluster.


    Computing Cluster	
    -----------------
	-> Is a unified entity consisting of many nodes (machines) whose cumulative resources 
	   (Disk, RAM, CPU Cores) to store and process big data.

    Hadoop
    ------
	-> Is a opensource framework for storing and processing bigdata using a computing 
	   cluster made of commodity hardware. 

	   -> HDFS (Hadoop Distributed File System) : Distributed Storage Framework. 
	   	-> Data is distributed among many machines as blocks of 128MB each
			-> 10 GB file is stored as 80 blocks spread on many nodes.
		-> For each block, 3 replicas are created (by default)
			-> Hence, to store a 10GB file on HDFS, we need 30GB disk space.

	  -> MapReduce -> Distributed computing Framework
		-> MR application contains two classes called Mapper & Reducer
		-> Many instances of the mapper are launched parallelly in the containers allocated 
		   on many nodes in the cluster.
		-> The intermediate outputs produced by the mappers are written to disk (local disk)
		-> This data is collected by the framework (shuffling) and then sent to the Reducer class
		-> The Reducer aggregates all these results and produce final output.

	  -> YARN -> Cluster Manager (or Resource Manager)
		-> Accepts the jobs and allocates resource containers to those jobs across lot of nodes
		-> The containers are allocated on the machine where the data blocks of the file are located.
	
   What is Spark?
   --------------
	-> Written in Scala language.
	-> Is a unified in-memory distributed/cluster computing framework.
	-> Spark supports multiple languages
		-> Scala, Java, Python, R 

	* in-memory distributed computing framework 
		-> The intermediate outputs of distributed tasks can be persisted in-memory (RAM)
		-> The subsequent tasks can operate on these in-memory persisted outputs. 
		
	
   Spark Unified Framework
   -----------------------
	-> Spark provides a consistent set of APIs (libraries) running on the same execution engine 
	   for processing different types of analytics workloads.	

	  	Batch Analytics on unstructred data	 -> Spark Core API (RDD)
		Batch Analytics on structured data       -> Spark SQL
		Streaming Analytics (real time process)	 -> Spark Streaming, Structred Streaming
		Predictive Analytics (Machine Learning)	 -> Spark MLlib
		Graph Parallel Computations		 -> Spark GraphX


   Spark Architecture & Building Blocks
   ------------------------------------        
	1. Cluster Manager

		-> CM receives the job sumissions
		-> Allocates executor containers to the applications
		-> Supports Spark Standalone Scheduler, YARN, Mesos, Kubernetes.

	2. Driver Process
		-> Whenever an application is launched, the driver process is created first
		-> master process which analysis the user-code and send tasks to be executed in the executors.
		-> Driver process contains a "SparkContext" object.

		Deploy-Modes
                ------------
		1. Client Mode (default)  -> The driver	runs on the client machine
		2. Cluster Mode		  -> The driver runs in one of the nodes inside the cluster	


	3. SparkContext
		-> Represents an application and a connection to the cluster.
		-> Link between the driver process and various tasks running on the cluster
		-> Starting point og any SparkCore application
		    NOTE: In case of Spark SQL, we use a "SparkSession" in place of SparkContext.

	4. Executors
		-> We have many executors allocated by Cluster manager and multiple processes
		   run with in each executor
		-> Driver sends tasks to be executed in the executors by analyzing the user-code
		-> All tasks does the same function, but on different partitions. 
		-> Executors report the status of the tasks to the driver.


    Getting started with PySpark
    ----------------------------

     1. Installing Spark locally and work with PySpark shell

	1.1 Download Spark from the URL: https://spark.apache.org/downloads.html
	    and extract it to a suitable folder.

	1.2 Setup the environment variables
		SPARK_HOME  	-> spark installation location
		HADOOP_HOME	-> spark installation location
		PATH		-> Add the <Spark>\bin 
	

    2. Using some IDE such as Spyder or Jupiter Notebook (or PyCharm)
	
	2.1 Install Anaconda Navigator
		https://docs.anaconda.com/anaconda/install/windows/

	2.2 To setup PySpark with Spyder or Jupyter Notebook, follow the instructions in the
	    document shared in the github.

    3. Using Databricks Community Edition Account (***)

	-> Signup to Databricks Community Edition Account
		=> https://databricks.com/try-databricks
		=> Read "QuickStart Tutorial"   		
	
	
    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> Fundamental data abstraction of Spark Core API

	-> RDD is a set of distributed in-memory partitions
	    -> Partition is a collection of objects
	    -> If you load a textfile into an RDD, you will have an RDD of Strings where each line
	       of the textfile is one object.

	-> RDD has two components:
		
		1. Meta Data : Lineage DAG (numPartitions, Persistence level..)
			       Maintained by the driver.

		2. Data : in-memory partitions

	-> RDD partitions are immutable.

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution.
		-> Execution is trigger by action commands only. 

	-> RDDs are resilient
		-> RDDs can automatically recreate missing partitions by reexecuting the tasks.


    Creating RDD
    ------------

	-> 3 ways

	1. Create an RDD from some external file.
		
		filePath = "E:\\Spark\\wordcount.txt"
		rddFile = sc.textFile( filePath  )
		rddFile = sc.textFile( filePath, 4 )

		-> the default number of partitions is given by the value of sc.defaultMinPartitions

	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize( range(1, 101) )
		rdd2 = sc.parallelize( range(1, 101), 3 )  // rdd2 will have 3 partitions

		-> the default number of partitions is given by the value of sc.defaultParallelism
		-> sc.defaultParallelism = total number of cores allocated to the application

	3. By applying transformations on existing RDDs, we can create new RDDs.

		rdd2 = rdd1.map(lambda x: x.upper())

	
   NOTE: rddFile.getNumPartitions()  -> get the partition count of an RDD



   What can we do with an RDD ?
   ----------------------------

	Only two things:

	1. Transformations
	   -> The output of a transformation is an RDD
	   -> Does not cause execution. 
	   -> They only cause the creation of Lineage DAG (at the driver)

	2. Actions
	   -> Produce some output by triggering excution on the RDDs
	   -> Cause execution.


   RDD Lineage DAG
   ---------------
	Lineage refers the a DAG of all dependencies all the way from the very first RDD 
	which caused the creation of this RDD.

	-> Lineage is a "Logical Plab"

	rddFile = sc.textFile( filePath, 4 )
	Lineage: rddFile -> sc.textFile

	rdd2 = rdd1.map(lambda x: x+10)
	Lineage: rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.filter(lambda x: x > 18)
	Lineage: rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile

	rdd4 = rdd3.map(lambda x: [x, x+10])
	Lineage: rdd4 -> rdd3.map -> rdd2.filter -> rdd1.map -> sc.textFile

	rdd4.collect()
	Physical Plan => sc.textFile (rddFile) -> map (rdd2) -> filter (rdd3) -> map (rdd4) --> collect()


   Types of Transformations
   ------------------------
	1. Narrow Transformations
		-> Does not cause shuffling of the data from one partition to other partitions
		-> Partition to partition transformations
		-> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
		-> Causes shuffling of the data
		-> One output partition may need data from multiple input partitions
		-> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------
	rdd1 = sc.textFile( .... )
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd1.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd3.t6(..)
	rdd7 = rdd6.t7(..)
	rdd7.persist(StorageLevel.MEMORY_ONLY)   -----> instruction to not GC the rdd7. save the partitions.
	rdd8 = rdd7.t8(..)

	rdd7.collect()

	lineage of rdd7:  rdd7 -> rdd6.t7 -> rdd3.t6 -> rdd1.t3 -> sc.textFile	
	tasks: sc.textFile -> t3 -> t6 -> t7 -> collect()

	rdd8.collect()
	
	lineage of rdd8: rdd8 -> rdd7.t8 -> rdd6.t7 -> rdd3.t6 -> rdd1.t3 -> sc.textFile
	tasks: t8 -> collect	

	Persistence types   ---> in-memory in deserialized format
			    ---> in-memory in serialized format
			    ---> disk

	Storage Levels
	--------------
	1. MEMORY_ONLY (default)	-> deserilized
	2. MEMORY_AND_DISK		-> deserilized	
	3. DISK_ONLY
	4. MEMORY_ONLY_SER		-> serialized
	5. MEMORY_AND_DISK_SER		-> serialized	
	6. MEMORY_ONLY_2	
	7. MEMORY_AND_DISK_2
	
	Persistence Commands:
	
		1. rdd.persist( <StorageLevel> )
		2. rdd.cache()   --> in-memory persistence
		3. rdd.unpersist()


   Executor Memory Structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------
	-> Output of a transformations is an RDD
	-> The number of partitions of output RDD = number of partitions of input RDD (by default)
		-> there are few exceptions..


    1. map		P: U -> V
			Element to element transformation
			input RDD: N objects, output RDD: N objects

    2. filter		P: U -> Boolean
			Only those elements for which the function returns True will be in the output.
			input RDD: N objects, output RDD: <= N objects

    3. glom		P: None
			Returns one array object will thte elements per partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 1,2,1,3,5,6 -> glom -> P0: [1,2,1,3,5,6]
		P1: 3,0,7,8,9,0 -> glom -> P1: [3,0,7,8,9,0]
		P2: 4,3,4,8,9,0 -> glom -> P2: [4,3,4,8,9,0]
	
		rdd1.count: 18		   rdd2.count: 3

    4. flatMap		P: U -> Iterable[V] 
			FlapMap flattens all the elements of the iterables returned by the function.

	
    5. mapPartitions	P: Iterator[U] -> Iterator[V]
			Applies the function to the entire partition

		rdd1		   rdd2 = rdd1.mapPartitions(lambda x : [sum(x)])

		P0: 1,2,1,3,5,6 -> glom -> P0: 18
		P1: 3,0,7,8,9,0 -> glom -> P1: 27
		P2: 4,3,4,8,9,0 -> glom -> P2: 28


    6. mapPartitionsWithIndex   P: Int, Iterator[U] -> Iterator[V]
				Applies the function to the entire partition. We get partition-id also as input.

	rdd1.mapPartitionsWithIndex(lambda i, data : [(i, sum(data))] ).collect()

    7. distinct			P: None, Optional: number of output partitions
				Returns distinct elements of the array.

    8. sortBy			P: U -> V, optional: number of output partitions
				The objects of the RDD are sorted based on the function output (V)

		rddWords.sortBy(lambda x: len(x)).collect()
		rddWords.sortBy(lambda x: len(x), False).collect()
		rdd1.sortBy(lambda x: x%4, True, 5).glom().collect()


    Types of RDDs from useage standpoint:

	-> Generic RDDs  : RDD[U]
	-> Pair RDDs	 : RDD[(U, V)]


    9. groupBy			P: U -> V
				Returns a pair RDD where each unique function output will the 'key' and an
				iterable of all RDD objects that gave the same key will be the 'value'

				RDD[U].groupBy(U -> V) => RDD[(V, ResultIterable[U])]

		rddWords.groupBy(lambda x: x[0]).mapValues(list).collect()
		rdd1.groupBy(lambda z: z%4, 4).mapValues(list).glom().collect()

   10. mapValues		P: U -> V
				Can be applied only to pair RDDs
				The function transforms only the 'value' part of the (K, V) pairs.		
				
		rdd = sc.textFile(filePath) \
        		.flatMap(lambda x: x.split(" ")) \
        		.groupBy(lambda x: x) \
        		.mapValues(len) \
        		.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: Array of ratios
				Will return an array of RDDs split in the given ratios.

		rddArr = rdd1.randomSplit([0.5, 0.5])
		rddArr = rdd1.randomSplit([0.5, 0.5], 46456)   // seed = 46456


   12. repartition		P: # of partitions
				Is used to increase or decrease the number of partitions.
				Causes global shuffle.

   13. coalesce			P: # of partitions
				Is used only to decrease the number of output partitions
				Causes partition merging.


	Recommendations:
	=> Size of the partition should be around 128 MB  (100MB to 150MB)
	=> Number of partition can be 2 to 3 times the number of cores allocated.
	=> Do not create too big partitions. (max shuffle partition size of 2 GB)
	=> The number of cores in an executor sould be 5 (or 4)


   14. partitionBy		P: # of partitions, optional: partitioning-function
				-> Applied only on Pair RDDs.

	transactions = [
    		{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    		{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    		{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    		{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    		{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    		{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    		{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    		{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    		{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    		{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

	rdd1 = sc.parallelize(transactions, 3)

	rdd2 = rdd1.map(lambda x: (x['city'], x))

	def custom_partitioner(city): 
    		return len(city);  

	rdd3 = rdd2.partitionBy(4, custom_partitioner)


    15. union, intersection, subtract, cartesian

	 => Let us say rdd1 has M partitions and rdd2 has N partitions
	
		command			output partitions
		-----------------------------------------
		rdd1.union(rdd2)		M + N, narrow	
		rdd1.intersection(rdd2)		M + N, wide
		rdd1.subtract(rdd2)		M + N, wide
		rdd1.cartesian(rdd2)		M * N, wide

	

     ...ByKey Transformations
	-> Applied only to pair RDDs
        -> Are all wide transformations
	-> They operate values of each unique key
	-> Optionally take a parameter for number of output partitions.

	
    16. sortByKey		P: None, optional: Ascending: True/False, number of partitions

		rdd12.sortByKey().collect()         	# ASC sort by key
		rdd12.sortByKey(False).collect()	# DESC sort by key
		rdd12.sortByKey(False, 5).collect()	# DESC sort by key with 5 partitions

    17. groupByKey		P: None, Optional: number of partitions
				Will group all the values of each unique key of the input RDD.	
				NOTE: Avoid if possible (very inefficient)

		rdd = sc.textFile(filePath) \
        		.flatMap(lambda x: x.split(" ")) \
			.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False, 1)

     18. reduceByKey		P: U,U -> U   optional: number of partitions

				-> Will reduce all the different values of each unique by 
				  iterativly applying a reduce function. First, it will reduce 
				  within each partition (narrow) and then across partitions (wide)

`		rdd = sc.textFile(filePath) \
        		.flatMap(lambda x: x.split(" ")) \
			.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False, 1)

     19. aggregateByKey   ---> to be discussed

     20. joins transformations   -> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
					
				  RDD[(U, V)].join( RDD[(U, W)] ) -> RDD[(U, (V, W))]			

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

     21.cogroup           ---> to be discussed


     





    
  
  RDD Actions
  -----------

   1. collect
   2. count
   3. saveAsTextFile
   4. reduce		  P: U, U -> U
			  reduce will produce an output of the same type as RDD elements by 
			  reducing the RDD  within each partition and then across the partitions 
                          by iterativly applying a reduce function.

		rdd1
		P0: 1, 2, 1, 3, 4, 5, 6   ->  6   -> 9
		P1: 7, 8, 9, 3, 2, 1, 4   ->  9
		P2: 5, 6, 8, 9, 9, 0, 0   ->  9 

		rdd1.reduce(lambda a, b: a if a>b else b)

		







