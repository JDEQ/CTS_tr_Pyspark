
  Agenda - PySpark
  ----------------
  -> Spark : Basics & Architecture
  -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming


  Materials
  ---------
    -> PDF Presentations
    -> Code Modules
    -> Class Notes
    -> https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
    -> Written in "Scala" programming language.
	
    -> Is a unified in-memory distribute computing framework. 

    -> Spark is a polyglot
	-> Scala, Java, Python and R

    -> Spark applications can run on mulitple cluster managers
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes
	
  
   Cluster
   -------
	-> Is a unified entity comprising of a group of nodes whose cumulative resources can be used 
	   to distribute your storage and processsing. 

   in-memory computation
   ---------------------
	-> The intermediate results (partitions) of tasks can be persisted in-memory (RAM) and
           subsequent tasks can run on these persisted partitions. 

   Spark Unified Framework
   -----------------------
	-> Provides a consistent set of APIs running on the same execution engine to support
	   different analytics workloads. 

	   -> Batch Processsing of unstructured data   	: Spark Core API
	   -> Batch Processsing of structured data	: Spark SQL
	   -> Stream Processing (real-time)		: Spark Streaming, Structured Streaming
	   -> Predictive Analytics (Machine Learning)	: Spark MLlib
	   -> Graph Parallel Computations		: Spark GraphX


   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.

       
   Getting started with Spark
   --------------------------
    
    1. Working in your vLab

	-> Connect to your vLab (Windows machine)
	-> Click on CentOS 7 icon (this is your vLab)
	-> Login using the credentials ( refer to ReadMe.txt file for username & passsword)
        
	PySpark Shell
        -------------
	-> Open a terminal and type "pyspark" at the prompt
	-> Open a browser (FireFox) and go to "localhost:4040" -> Web UI of PySpark Shell
	
       Jupyter Notebook
       ----------------
	-> Open a terminal
	-> Type the following command(s)
		$ jupyter notebook  (this may not work)
		$ jupyter notebook --allow-root

   2. Setting up your own working environment

	-> Download and install "Anaconda Navigator" 
		URL: https://www.anaconda.com/products/individual
	-> Follow the instructions given in the shared document to setup pyspark on 
	   Jupyter Notebooks and Spyder
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
   3. Signup to Databricks Community Edition
	
	-> https://databricks.com/try-databricks


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	=> Fundamental data abstraction of Spark. 

	=> Is a  collection of distributed in-memory partitions
		-> Each partition is a collection of objects

	=> RDD partitions are immutable

	=> RDDs are lazily evaluation
		-> Transformation does not cause execution
		-> Only actions cause execution

	=> RDDs are resilient
		-> RDDs can recreate the missing partitions at runtime by reapplying
		   the transformations. 


   How to create RDDs ?
   --------------------   
     Three ways:

	1.  From some external data file such as a text file

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The default numPartitions is defined by "sc.defaultMinPartitions" whose value is 2
		   if you have atleast two cores allocated.
	
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From some programmatic collection

		rdd1 = sc.parallelize( range(1, 100) )
		-> The default numPartitions is defined by "sc.defaultParallelism" whose value is equals to
		   the number of cores allocated.

	3. By applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())


   What can we do with an RDD ?
   ----------------------------
	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.


   RDD Lineage DAG
   ---------------
	=> An RDD object can be thought of as a set of instructions on how to create the partitions
          of that RDD called "Lineage DAG" (which is a logical plan)

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
		Lineage DAG: rddFile -> sc.textFile 

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage DAG: rdd2 -> rddFile.map -> sc.textFile 

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))	
		Lineage DAG: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 

        rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile 


   RDD Transformations
   -------------------

    1. map 		P: U -> V
			Element to element transformations
			input RDD: N object, out PDD: N objects

















