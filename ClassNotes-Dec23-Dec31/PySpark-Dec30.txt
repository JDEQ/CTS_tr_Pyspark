
  Agenda - PySpark
  ----------------
  -> Spark : Basics & Architecture
  -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming


  Materials
  ---------
    -> PDF Presentations
    -> Code Modules
    -> Class Notes
    -> https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
    -> Written in "Scala" programming language.
	
    -> Is a unified in-memory distribute computing framework. 

    -> Spark is a polyglot
	-> Scala, Java, Python and R

    -> Spark applications can run on mulitple cluster managers
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes
	
  
   Cluster
   -------
	-> Is a unified entity comprising of a group of nodes whose cumulative resources can be used 
	   to distribute your storage and processsing. 

   in-memory computation
   ---------------------
	-> The intermediate results (partitions) of tasks can be persisted in-memory (RAM) and
           subsequent tasks can run on these persisted partitions. 

   Spark Unified Framework
   -----------------------
	-> Provides a consistent set of APIs running on the same execution engine to support
	   different analytics workloads. 

	   -> Batch Processsing of unstructured data   	: Spark Core API
	   -> Batch Processsing of structured data	: Spark SQL
	   -> Stream Processing (real-time)		: Spark Streaming, Structured Streaming
	   -> Predictive Analytics (Machine Learning)	: Spark MLlib
	   -> Graph Parallel Computations		: Spark GraphX

   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.

       
   Getting started with Spark
   --------------------------
    
    1. Working in your vLab

	-> Connect to your vLab (Windows machine)
	-> Click on CentOS 7 icon (this is your vLab)
	-> Login using the credentials ( refer to ReadMe.txt file for username & passsword)
        
	PySpark Shell
        -------------
	-> Open a terminal and type "pyspark" at the prompt
	-> Open a browser (FireFox) and go to "localhost:4040" -> Web UI of PySpark Shell
	
       Jupyter Notebook
       ----------------
	-> Open a terminal
	-> Type the following command(s)
		$ jupyter notebook  (this may not work)
		$ jupyter notebook --allow-root


   2. Setting up your own working environment

	-> Download and install "Anaconda Navigator" 
		URL: https://www.anaconda.com/products/individual

	-> Follow the instructions given in the shared document to setup pyspark on 
	   Jupyter Notebooks and Spyder
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	

   3. Signup to Databricks Community Edition
	
	-> https://databricks.com/try-databricks


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	=> Fundamental data abstraction of Spark. 

	=> Is a  collection of distributed in-memory partitions
		-> Each partition is a collection of objects

	=> RDD partitions are immutable

	=> RDDs are lazily evaluation
		-> Transformation does not cause execution
		-> Only actions cause execution

	=> RDDs are resilient
		-> RDDs can recreate the missing partitions at runtime by reapplying
		   the transformations. 


   How to create RDDs ?
   --------------------   
     Three ways:

	1.  From some external data file such as a text file

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The default numPartitions is defined by "sc.defaultMinPartitions" whose value is 2
		   if you have atleast two cores allocated.
	
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From some programmatic collection

		rdd1 = sc.parallelize( range(1, 100) )
		-> The default numPartitions is defined by "sc.defaultParallelism" whose value is equals to
		   the number of cores allocated.

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())


   What can we do with an RDD ?
   ----------------------------
	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.


   RDD Lineage DAG
   ---------------
	=> An RDD object can be thought of as a set of instructions on how to create the partitions
          of that RDD called "Lineage DAG" (which is a logical plan)

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
		Lineage DAG: rddFile -> sc.textFile 

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage DAG: rdd2 -> rddFile.map -> sc.textFile 

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))	
		Lineage DAG: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 

        rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile 

   RDD Actions
   -----------

	1. collect

	2. count

   
   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Persistence
   ---------------

	rdd1 = sc.textFile(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevels.MEMORY_ONLY  )       -> instruction to persist the rdd6 partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	Lineage of rdd6	: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	Tasks: sc.textFile, t3, t5, t6 -> rdd6 -> collected. 

	rdd7.collect()
	Lineage of rdd7	: rdd7 -> rdd6.t7
	Tasks: t7 -> rdd7 -> collected.

	rdd6.unpersist()


	StorageLevels  (supported in PySpark)
        -------------------------------------
	1. MEMORY_ONLY      	-  memory serialized & in RAM (default)
	2. MEMORY_AND_DISK	-  disk memory serialized - preferebly RAM, else Disk.
	3. DISK_ONLY		
	4. MEMORY_ONLY_2	- 2x replication
	5. MEMORY_AND_DISK_2    - 2x replication


	Commands
        --------
	-> rdd1.persist()    => default: MEMORY_ONLY
	-> rdd1.persist( StorageLevel.DISK_ONLY )
	-> rdd1.cache()
	
	-> rdd1.unpersist()   
   

  Executor Memory Structure
  -------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD in-mwmory persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------

    1. map 		P: U -> V
			Element to element transformations
			input RDD: N objects, output RDD: N objects  =>(number of objects)

    		rdd = rddFile.map(lambda x : x.upper())


    2. filter		P: U -> Boolean
			Output RDD will have only those objects for which the function returns True.
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) >= 9).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, output RDD: # of partitions

		rdd1			rdd2 = rdd1.glom()  

		P0: 1,2,3,6,5 -> glom -> P0 : [1,2,3,6,5]
		P1: 7,3,4,5,6 -> glom -> P1 : [7,3,4,5,6]
		P2: 9,3,5,4,7 -> glom -> P2 : [9,3,5,4,7]
	        rdd1.count() = 15 (int)	      rdd2.count() = 3 (lists)

		rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap		P: U -> Iterable[V] 
			flatMap flattens the individual elements of the iterable objects produced by the function
  			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Applies a function to an entire partition
			Function takes the entire partition as input	
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda x: map(lambda a: a*10, x))

		P0: 1,2,3,6,5 -> mapPartitions -> P0 : 10,20,30,60,50
		P1: 7,3,4,5,6 -> mapPartitions -> P1 : 70,30,40,50,60
		P2: 9,3,5,4,7 -> mapPartitions -> P2 : 90,30,50,40,70

		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).glom().collect()
		rdd1.mapPartitions(lambda x: [sum(x)]).glom().collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Same as mapPartitions, exept that we get partition-index as additional 
				parameter.
  
		rdd1.mapPartitionsWithIndex(lambda index, data: [(index, sum(data))] ).glom().collect()
		rdd1.mapPartitionsWithIndex(lambda i, x: map(lambda a: (i, "A" + str(a)), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the input RDD.

		rddWords.distinct().collect()
		rdd2 = rddWords.distinct(5)   # rdd2 will have 5 partitions


   8. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				The elements of the RDD are sorted based on the function output. 
  
		rddWords.sortBy(lambda x: x[0]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 6).collect()


   Types of RDDs:
	-> Generic RDDs => RDD[U]
	-> Pair RDDs	=> RDD[(K, V)]

   9. mapValues			P: U-> V
				Applied only to pair RDD
				Will transform the value part of the pair RDD by applying the function.


   10. groupBy			P: U -> V, Optional: numPartitions

				The objects of the RDD are grouped by the function output.

				Returns a Pair RDD where:
				key: Unique value of function output
				value: ResultIterable of all objects of the RDD that produced the key.

		rddWords.groupBy(lambda x: x[0]).mapValues(list).collect()
		rddWords.groupBy(lambda x: x[0], 6).mapValues(list).glom().collect()

		# wordcount example
		file = "E:\\Spark\\wordcount.txt"

		outputRdd = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.groupBy(lambda x: x) \
            			.mapValues(len) \
            			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of ratios ex: [0.4, 0.3, 0.3]  Optional: Seed
				Retuns a list of RDDs split randomly approx. in the given ratios.
	
		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 97234234)

   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of output RDD.
				Performs global shuffle.


   13. coalesce			P: numPartitions
				Used to only decrease the number of partitions of output RDD.
				Performs partition-merging.

	Recommendations
        ---------------
	-> The size of each partition should be around 128MB  (100 -150 MB)  
	-> The number of partitions should be a multiple of number of cores
	-> The number of cores in each executor should be 5


   14. partitionBy 		P: numPartitions, Optional: Partitioning function : U -> Int

				-> Applied only to Pair RDD and partioning happend based on the keys.
				-> Used to control which keys go to which partitions
		                -> default partitioning fun: hash of the key
 		transactions = [
    			{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    			{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    			{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

			def custom_partitioner(city): 
    				if (city == 'Chennai'):
        				return 0;
    				elif (city == 'Hyderabad'):
        				return 1;
    				elif (city == 'Vijayawada'):
        				return 1;
    				elif (city == 'Pune'):
        				return 2;
    				else:
        				return 3;      

			rdd1 = sc.parallelize(transactions, 3).map(lambda d: (d['city'], d))
			rdd1.glom().collect()

			rdd2 = rdd1.partitionBy(3, custom_partitioner)
			rdd2.glom().collect()

   15. union, intersection, subtract, cartesian

	Let us say, rdd1 has M partitions and rdd2 has N partitions

	command				# output partitions
        ---------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


  ..ByKey Transformations
  ------------------------
      	-> Wide Transformations
	-> Applied only on pair RDDs


  16. sortByKey			P: None, Optional: ascending (True/False), numPartitions
				RDD is sorted based on the key	
   
		rddpairs1.sortByKey()
		rddpairs1.sortByKey(False)
		rddpairs1.sortByKey(True, 3)

   17. groupByKey		P: None, Optional: numPartitions
				Groups the RDD base on the key
				Returns a Pair RDD where key is the "unique-key" and value is a 
				ResultIterable with grouped elements.
				NOTE: Try to avoid groupByKey.

		# wordcount example
		file = "E:\\Spark\\wordcount.txt"

		outputRdd = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
            			.groupByKey() \
            			.mapValues(sum)


   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Will reduce all the values of each unique key into one final value
				(per unique key) nu iterativly applying the function.

		# wordcount example
		file = "E:\\Spark\\wordcount.txt"

		outputRdd = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
            			.reduceByKey(lambda x, y: x + y, 1)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.

		student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		avg_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
          		.aggregateByKey((0,0),
                          lambda z, v: (z[0] + v, z[1] + 1),
                          lambda a, b: (a[0] + b[0], a[1] + b[1])) \
         		.mapValues(lambda x: x[0]/x[1])


    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

     21. cogroup	 -> Used when the RDDs that you join may have duplicate keys
			   Performs groupByKey on each RDD and then performs fullOuterJoin on the 
			   grouped data. 
   

   RDD Actions
   ------------

    1. collect

    2. count

    3. saveAsTextFile

    4. reduce			P: (U, U) -> U
				Reduces the entire RDD into one value of the same type by iterativly
				applying the reduce on the objects of each partition first and then 
				reduces the reduced values of each partition.

		P0:   9, 1, 8, 7, 5, 1, 2    	   => -15  => 48
		P1:   3, 5, 4, 3, 6, 7, 8	   => -30	
		P2:   9, 0, 3, 4, 5, 6, 7, 8, 9    => -33

		rdd1.reduce(lambda x, y: x - y)   => output: 48

    5. aggregate	-> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final values of the type
                  of the zero-value.

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]))


    6. take		
		rdd1.take(10) 

    7. takeOrdered

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, lambda x: len(x))

    8. takeSample

		rdd1.takeSample(True, 10)	# with replacement sampling
		rdd1.takeSample(True, 10, 456)  # with replacement sampling with a seed
		rdd1.takeSample(False, 10, 456) # without replacement sampling with seed

    9. countByValue

    10. countByKey

    11. first

    12. foreach	-> does not return anything.
		   applies a function on all the values of the rdd.
		
    13. saveAsSequenceFile


   Use-Case
   --------
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
	
	From the cars.tsv dataset, find out the average weight of each make of American origin.
	Sort the data in the DESC order of average weight.
	Save the output as a singke text-file.
	Use RDD API ONLY
		
       => Please try to solve the above.


   spark-submit
   ------------
	-> is a single command to submit any spark application (python, scala, java etc..) to
	   any cluster manager. 
	
	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G 
		--executor-memory 10G
		--executor-cores 5
		--num-executors 10
		wordcount.py [app arguments]


	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout2 3



   Closure
   -------

	Closure is those variables and methods that must be visible within an executor for a task to 
	perform its computations on the RDD.

	This closure is serialized and a separate local copy will be sent to each executor.

	// the following will not work as intended ..

	count = 0

	def isPrime( n ) :
		if (n is prime) return 1 
		else returns 0

	def f1 ( n ) :
		global count
		if ( isPrime(n) == 1 ) count += 1
		return n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(count)    // output = 0  

	
	The closure problem:
		-> In the above the count referred inside the function is part of the closure
		   and hense isa local copy in every task and this changes to this value can not 
		   be propagated back to the driver. 

		=> Hence, WE CAN NOT USE LOCAL VARIABLES for implementing GLOBAL COUNTERS.


   Shared Variables
   ----------------

   1. Accumulator
   --------------

	-> Is a shared variable.
	-> Maintained by driver.
	-> All tasks can add to this variables.
	-> Not part of function closure.

	-> Accumulators are used implement global counters.

	count = sc.accumulator(0)

	def isPrime( n ) :
		if (n is prime) return 1 
		else returns 0

	def f1 ( n ) :
		global count
		if ( isPrime(n) == 1 ) count.add(1)
		return n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(count.value) 


   2. Broadcast Variable
   ----------------------

	d1 = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, ......})   # 100 MB

	def f1( n ):
		global d1
		return d1.value[n]

        rdd1 = sc.parallelize([1,2,3,4,5, ...], 4)	
        rdd2 = rdd1.map( f1 )
        rdd2.collect()     # output:  a,b,c,....

 ===============================================   
     Spark SQL  (pyspark.sql)
 ===============================================

   => Spark's Structured Data Processing API
	
	  -> High Level API built on top of Spark Core

	  Structured File Formats => Parquet (default), ORC, JSON, CSV (delimited text)
	  Hive
	  JDBC format :  RDBMS, NoSQL

   => SparkSession
	-> Starting point of Spark SQL application
	-> Introduced from Spark 2.0 onwards
	-> Represents a user-session within an application
		-> An application (sparkcontext) can have multiple sessions (SparkSession)   

		spark = SparkSession \
        		.builder \
        		.appName("Dataframe Operations") \
        		.config("spark.master", "local[*]") \
        		.getOrCreate() 

   => DataFrame (DF)
	
	-> A collection on distributed in-memory partitions that are immutable and lazily-evaluated. 
	
	-> DF partitions contain only "Row" objects  (pyspark.sql.Row)
		-> A Row is a collection of Column objects
		    -> Each column object's data is stored using Spark SQL interna type resentations.

	-> DF contains two components:
		-> Data   : Several partitions of Row objects
		-> Schema : Is metadata and represents the structure of the DF
			    Is a StructType object (A StructType contains a list of StructField)

			StructType(
			   List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			   )
			)

    
   Steps in a Spark SQL Program
   ----------------------------

	1. Read/load the data from some datasource into a dataframe.

		df1 = spark.read.format("json").load(inputFile)
		df1 = spark.read.json(inputFile)

	2. Apply transformations of the DataFrame using DF API methods or using SQL
		
		Using DF API approach
		---------------------
		df2 = df1.select("userid", "name", "age") \
        		.where("age is not null") \
        		.groupBy("age").count() \
        		.limit(4) \
        		.orderBy("count")

		Using SQL approach
                -------------------
		spark.catalog.listTables()
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
        		from users
        		where age is not null
        		group by age
        		order by count
        		limit 4"""
        
		df3 = spark.sql(qry) 

		# spark.catalog.dropTempView("users")    


	3. Write/save the DF into a structure file or database etc. 
	  
		outputDir = "E:\\PySpark\\output\\json"
		df2.write.format("json").save(outputDir)
		df2.write.json(outputDir)

   LocalTempViews & GlobalTempViews
   ---------------------------------

	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.


  DataFrame Transformations
  -------------------------

   1. select
		df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

		df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 	column("DEST_COUNTRY_NAME").alias("destination"), 
                 	expr("count"),
                 	expr("count + 10 as newCount"),
                	expr("count > 365 as highFrequency"),
                 	expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

   2. where / filter

		df3 = df2.where(col("highFrequency") == "true")
		df3 = df2.where("domestic = false and count > 100")

   3. orderBy / sort
		df3 = df2.orderBy("count", "destination")
		df3 = df2.orderBy(desc("count"), asc("destination"))

   4. groupBy  with aggregation methods
	=> groupBy returns a "GroupedData" object on which you apply some aggregation method 
           to return a DataFrame.

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").max("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")	

		df3 = df2.groupBy("domestic", "highFrequency") \
        		.agg( count("count").alias("count"), 
             			sum("count").alias("sum"), 
             			max("count").alias("max"), 
             			avg("count").alias("avg") )
	

   5. limit
		df2 = df1.limit(10)

   6 selectExpr
	
		df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin", 
                 		"DEST_COUNTRY_NAME as destination", 
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 365 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
   7. withColumn

		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", col("count") > 365) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
			.withColumn("country", lit("India"))


   8. withCoumnRenamed

		df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", col("count") > 365) \
        		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


   9. union, intersect, subtract

		df4 = df2.union(df3)
		df4 = df2.union(df3)
		df5 = df4.subtract(df2)

   10. distinct

		df1.select("DEST_COUNTRY_NAME").distinct().count()

   11. sample

		df2 = df1.sample(True, 0.4)		# withReplacement sampling
		df2 = df1.sample(True, 0.4, 45656)	# withReplacement sampling with seed
		df2 = df1.sample(True, 1.4)		# withReplacement sampling ratio can be  > 1

		df2 = df1.sample(False, 0.4)		# withOutReplacement sampling
		df2 = df1.sample(False, 0.4, 24)	# withOutReplacement sampling with seed
		df2 = df1.sample(False, 1.4)		# ERROR - ratio can NOT be  > 1

   12. drop

		df3 =  df2.drop("newCount", "highFrequency")


   13. randomSplit

		df3, df4 = df2.randomSplit([0.6, 0.4])  # here df3 & df4 are DataFrames
		dfList = df2.randomSplit([0.6, 0.4])    # dfList is a list of DFs

		df3, df4 = df2.randomSplit([0.6, 0.4], 45)

   14. repartition

		df3.rdd.getNumPartitions()   # command to list the numPartitions of a DF

		df3 = df2.repartition(4)
		df4 = df3.repartition(2)

		df5 = df2.repartition( col("origin") )
			=> here the numPartitions is governed by the config parameter
			   spark.sql.shuffle.partitions, whose default value is 200.
			
			   spark.conf.set("spark.sql.shuffle.partitions", "10")
					

		df5 = df2.repartition( 6, col("origin") )		

   15. coalesce

		df6 = df5.coalesce(3)

   16. join
	  --> discussed separatly

 
   Save Modes
   ----------
	-> errorIfExists
	-> ignore
	-> append
	-> overwrite

	df1.write.json(outputDir, mode="overwrite")
	df1.write.mode("overwrite").json(outputDir)


 
   Working with different file formats
   -----------------------------------

	JSON
		read
			df1 = spark.read.format("json").load(inputFile)
			df1 = spark.read.json(inputFile)

		write
			df2.write.format("json").save(outputDir)
			df2.write.json(outputDir)
			df2.write.json(outputDir, mode="overwrite")
			df2.write.mode("overwrite").json(outputDir)

	Parquet
		read
			df1 = spark.read.format("parquet").load(inputFile)
			df1 = spark.read.parquet(inputFile)

		write
			df2.write.format("parquet").save(outputDir)
			df2.write.parquet(outputDir)
			df2.write.parquet(outputDir, mode="overwrite")
			df2.write.mode("overwrite").parquet(outputDir)

	ORC
		read
			df1 = spark.read.format("orc").load(inputFile)
			df1 = spark.read.orc(inputFile)

		write
			df2.write.format("orc").save(outputDir)
			df2.write.orc(outputDir)
			df2.write.orc(outputDir, mode="overwrite")
			df2.write.mode("overwrite").orc(outputDir)

	CSV (delimited text file)

		read
			df1 = spark.read.format("csv").load(inputFile, header=True, inferSchema=True)
			df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputFile)
			df1 = spark.read.csv(inputFile, header=True, inferSchema=True)
			df1 = spark.read.csv(inputFile, header=True, inferSchema=True, sep="|")

		write
			df2.write.format("csv").save(outputDir, mode="overwrite", header=True)
			df2.write.csv(outputDir, mode="overwrite", header=True)
			df2.write.csv(outputDir, mode="overwrite", header=True, sep="|")


   Create an RDD from DataFrame
   ----------------------------
	rdd1 = df1.rdd
	rdd1.collect()


   Create a DF from programmatic data
   ----------------------------------
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Raghu", 25),
             (4, "Rajeev", 41),
             (5, "Rajesh", 15),
             (6, "Ramya", 25),
             (7, "Radhika", 40)]

	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Create a DF from an RDD
   -----------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")


   Create a DF using programmatic schema
   -------------------------------------
	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)
      
        ----------------------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	inputFile = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json" 
	df1 = spark.read.json(inputFile, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputFile)


   Joins
   -----

     Supported Joins:  inner (default), left_outer, right_outer, full_outer, left_semi, left_anti
	
     Left-Semi	
     ---------
	-> Like inner join, but the data comes only from the left-side table.
	-> Is equivalent to the following subquery:
		select * from emp where deptid IN (select is from dept)

     Left-Anti
     ---------
	-> Is equivalent to the following subquery:
		select * from emp where deptid NOT IN (select is from dept)

     Data
     ----
	employee = spark.createDataFrame([
    	(1, "Raju", 25, 101),
    	(2, "Ramesh", 26, 101),
    	(3, "Amrita", 30, 102),
    	(4, "Madhu", 32, 102),
    	(5, "Aditya", 28, 102),
    	(6, "Pranav", 28, 100)]).toDF("id", "name", "age", "deptid")
  
	department = spark.createDataFrame([
    	(101, "IT", 1),
    	(102, "ITES", 1),
   	 (103, "Opearation", 1),
    	(104, "HRD", 2)]).toDF("id", "deptname", "locationid") 

	employee.show()
	department.show() 

	
     SQL Approach
     ------------	
	spark.catalog.listTables()
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*, dept.* 
         from emp full outer join dept
         on emp.deptid = dept.id"""
         
	joinedDf = spark.sql(qry)  

	joinedDf.show()
	spark.stop()

	
     DF Transformations API
     ----------------------
 	Supported Joins:  inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")

	joinedDf.show()
	
	# Enforcing Broadcast Join
	joinedDf = employee.join( broadcast(department), joinCol, "left_outer")
		=> broadcast join can be enforced using the above command as long as the
		   broadcasted table size is < 8 GB


   Use-Case
   --------
	From movies,csv and ratings.csv datasets, fetch the top 10 movies with highest average user rating.
	-> Consider only those movies with atleast 30 total number of ratings. 
	-> data:  movieId, title, totalRatings, avgRating
	-> Sort the data in the DESC order of avgRating
	-> Save the output as a single pipe-separated CSV file with Header
	-> Use only DF Transformations API (not SQL)

	datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

	=> Try this yourself.

 
   JDBC Format - Working with MySQL
   --------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


    Working with Hive
    -----------------

    Hive : Data Warehousing platform of Hadoop

	Warehouse: is a directory where hive stores all its databases' and tables' data files
	Metastore: is a external RDBMS (MySQL) where Hive stores all its metadata.  

	
# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()
----------------------------------------------------------------

 ================================================
     Machine Learning & Spark MLlib
 ================================================
  
  ML Model   =>  Learned Entity
		 Learns from historic data
		 Trained by one or more algorithms.  

  Terminology   
  -----------
   1. Training Data
   2. Features		: inputs
   3. Label		: output
   4. Algorithm		: iterative mathematical computation that establishes a relation between the
			  label and features with a goal to mimize the loss (computed with some loss fn)
   5. Model		: Output of the algorithm that contain the relation between label and features. 
   6. Error		: vriation between the actual and prediction for a given data point.
   7. Loss		: error of the entire dataset
   
            model = algorithm( data )

	X	Y	Z (lbl)  pred.  error 
	--------------------------------------
	1000	1000	3100	3000	100
	1200	600	2950	3000	-50	
	1500	500	3550	3500	 50
	2000	100	4000	4100	-100
	1300	400	3060	3000	 60
	1000	500	 ??     
	---------------------------------------
				Loss:    72
      

        model 1 :   z = 2x + y		72 
	model 2 :   z = 1.9x + 0.9y	68 
	....
        ....    => The returned model is the one with minimal loss


  Steps in a ML projects
  ----------------------

   1. Data Collection

	Goal: Collection as much relavent data as you can from various sources

   2. Data Preparation
	
	Goal: Convert the data into a format that can be used by an algorithm.

	-> Convert all the data into numeric format (Double/Float)
	-> There should be no null/empty data - drop or fill the missing data.
	-> Remove outlier
	
	=> Perform EDA (exploratory data analysis) 
	=> Perform Feature Engineer

   3. Train your Model using one or more Algorithm

   4. Evaluate the Model

   5. Deploy the model
   
















