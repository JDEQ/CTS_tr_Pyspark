
  Agenda - PySpark
  ----------------
  -> Spark : Basics & Architecture
  -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming


  Materials
  ---------
    -> PDF Presentations
    -> Code Modules
    -> Class Notes
    -> https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
    -> Written in "Scala" programming language.
	
    -> Is a unified in-memory distribute computing framework. 

    -> Spark is a polyglot
	-> Scala, Java, Python and R

    -> Spark applications can run on mulitple cluster managers
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes
	
  
   Cluster
   -------
	-> Is a unified entity comprising of a group of nodes whose cumulative resources can be used 
	   to distribute your storage and processsing. 

   in-memory computation
   ---------------------
	-> The intermediate results (partitions) of tasks can be persisted in-memory (RAM) and
           subsequent tasks can run on these persisted partitions. 

   Spark Unified Framework
   -----------------------
	-> Provides a consistent set of APIs running on the same execution engine to support
	   different analytics workloads. 

	   -> Batch Processsing of unstructured data   	: Spark Core API
	   -> Batch Processsing of structured data	: Spark SQL
	   -> Stream Processing (real-time)		: Spark Streaming, Structured Streaming
	   -> Predictive Analytics (Machine Learning)	: Spark MLlib
	   -> Graph Parallel Computations		: Spark GraphX

   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.

       
   Getting started with Spark
   --------------------------
    
    1. Working in your vLab

	-> Connect to your vLab (Windows machine)
	-> Click on CentOS 7 icon (this is your vLab)
	-> Login using the credentials ( refer to ReadMe.txt file for username & passsword)
        
	PySpark Shell
        -------------
	-> Open a terminal and type "pyspark" at the prompt
	-> Open a browser (FireFox) and go to "localhost:4040" -> Web UI of PySpark Shell
	
       Jupyter Notebook
       ----------------
	-> Open a terminal
	-> Type the following command(s)
		$ jupyter notebook  (this may not work)
		$ jupyter notebook --allow-root

   2. Setting up your own working environment

	-> Download and install "Anaconda Navigator" 
		URL: https://www.anaconda.com/products/individual
	-> Follow the instructions given in the shared document to setup pyspark on 
	   Jupyter Notebooks and Spyder
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
   3. Signup to Databricks Community Edition
	
	-> https://databricks.com/try-databricks


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	=> Fundamental data abstraction of Spark. 

	=> Is a  collection of distributed in-memory partitions
		-> Each partition is a collection of objects

	=> RDD partitions are immutable

	=> RDDs are lazily evaluation
		-> Transformation does not cause execution
		-> Only actions cause execution

	=> RDDs are resilient
		-> RDDs can recreate the missing partitions at runtime by reapplying
		   the transformations. 


   How to create RDDs ?
   --------------------   
     Three ways:

	1.  From some external data file such as a text file

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The default numPartitions is defined by "sc.defaultMinPartitions" whose value is 2
		   if you have atleast two cores allocated.
	
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From some programmatic collection

		rdd1 = sc.parallelize( range(1, 100) )
		-> The default numPartitions is defined by "sc.defaultParallelism" whose value is equals to
		   the number of cores allocated.

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())


   What can we do with an RDD ?
   ----------------------------
	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.


   RDD Lineage DAG
   ---------------
	=> An RDD object can be thought of as a set of instructions on how to create the partitions
          of that RDD called "Lineage DAG" (which is a logical plan)

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
		Lineage DAG: rddFile -> sc.textFile 

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage DAG: rdd2 -> rddFile.map -> sc.textFile 

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))	
		Lineage DAG: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 

        rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile 



   RDD Actions
   -----------

	1. collect

	2. count

   
   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevels.MEMORY_ONLY  )       -> instruction to persist the rdd6 partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	Lineage of rdd6	: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	Tasks: sc.textFile, t3, t5, t6 -> rdd6 -> collected. 

	rdd7.collect()
	Lineage of rdd7	: rdd7 -> rdd6.t7
	Tasks: t7 -> rdd7 -> collected.

	rdd6.unpersist()


	StorageLevels  (supported in PySpark)
        -------------------------------------
	1. MEMORY_ONLY      	-  memory serialized & in RAM (default)
	2. MEMORY_AND_DISK	-  disk memory serialized - preferebly RAM, else Disk.
	3. DISK_ONLY		
	4. MEMORY_ONLY_2	- 2x replication
	5. MEMORY_AND_DISK_2    - 2x replication


	Commands
        --------
	-> rdd1.persist()    => default: MEMORY_ONLY
	-> rdd1.persist( StorageLevel.DISK_ONLY )
	-> rdd1.cache()
	
	-> rdd1.unpersist()   
   

  Executor Memory Structure
  -------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------

    1. map 		P: U -> V
			Element to element transformations
			input RDD: N objects, output RDD: N objects  =>(number of objects)

    		rdd = rddFile.map(lambda x : x.upper())


    2. filter		P: U -> Boolean
			Output RDD will have only those objects for which the function returns True.
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) >= 9).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, output RDD: # of partitions

		rdd1			rdd2 = rdd1.glom()  

		P0: 1,2,3,6,5 -> glom -> P0 : [1,2,3,6,5]
		P1: 7,3,4,5,6 -> glom -> P1 : [7,3,4,5,6]
		P2: 9,3,5,4,7 -> glom -> P2 : [9,3,5,4,7]
	        rdd1.count() = 15 (int)	      rdd2.count() = 3 (lists)

		rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap		P: U -> Iterable[V] 
			flatMap flattens the individual elements of the iterable objects produced by the function
  			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Applies a function to an entire partition
			Function takes the entire partition as input	
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda x: map(lambda a: a*10, x))

		P0: 1,2,3,6,5 -> mapPartitions -> P0 : 10,20,30,60,50
		P1: 7,3,4,5,6 -> mapPartitions -> P1 : 70,30,40,50,60
		P2: 9,3,5,4,7 -> mapPartitions -> P2 : 90,30,50,40,70

		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).glom().collect()
		rdd1.mapPartitions(lambda x: [sum(x)]).glom().collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Same as mapPartitions, exept that we get partition-index as additional 
				parameter.
  
		rdd1.mapPartitionsWithIndex(lambda index, data: [(index, sum(data))] ).glom().collect()
		rdd1.mapPartitionsWithIndex(lambda i, x: map(lambda a: (i, "A" + str(a)), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the input RDD.

		rddWords.distinct().collect()
		rdd2 = rddWords.distinct(5)   # rdd2 will have 5 partitions


   8. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				The elements of the RDD are sorted based on the function output. 
  
		rddWords.sortBy(lambda x: x[0]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()

















