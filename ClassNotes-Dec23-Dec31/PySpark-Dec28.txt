
  Agenda - PySpark
  ----------------
  -> Spark : Basics & Architecture
  -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming


  Materials
  ---------
    -> PDF Presentations
    -> Code Modules
    -> Class Notes
    -> https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
    -> Written in "Scala" programming language.
	
    -> Is a unified in-memory distribute computing framework. 

    -> Spark is a polyglot
	-> Scala, Java, Python and R

    -> Spark applications can run on mulitple cluster managers
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes
	
  
   Cluster
   -------
	-> Is a unified entity comprising of a group of nodes whose cumulative resources can be used 
	   to distribute your storage and processsing. 

   in-memory computation
   ---------------------
	-> The intermediate results (partitions) of tasks can be persisted in-memory (RAM) and
           subsequent tasks can run on these persisted partitions. 

   Spark Unified Framework
   -----------------------
	-> Provides a consistent set of APIs running on the same execution engine to support
	   different analytics workloads. 

	   -> Batch Processsing of unstructured data   	: Spark Core API
	   -> Batch Processsing of structured data	: Spark SQL
	   -> Stream Processing (real-time)		: Spark Streaming, Structured Streaming
	   -> Predictive Analytics (Machine Learning)	: Spark MLlib
	   -> Graph Parallel Computations		: Spark GraphX

   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.

       
   Getting started with Spark
   --------------------------
    
    1. Working in your vLab

	-> Connect to your vLab (Windows machine)
	-> Click on CentOS 7 icon (this is your vLab)
	-> Login using the credentials ( refer to ReadMe.txt file for username & passsword)
        
	PySpark Shell
        -------------
	-> Open a terminal and type "pyspark" at the prompt
	-> Open a browser (FireFox) and go to "localhost:4040" -> Web UI of PySpark Shell
	
       Jupyter Notebook
       ----------------
	-> Open a terminal
	-> Type the following command(s)
		$ jupyter notebook  (this may not work)
		$ jupyter notebook --allow-root


   2. Setting up your own working environment

	-> Download and install "Anaconda Navigator" 
		URL: https://www.anaconda.com/products/individual

	-> Follow the instructions given in the shared document to setup pyspark on 
	   Jupyter Notebooks and Spyder
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	

   3. Signup to Databricks Community Edition
	
	-> https://databricks.com/try-databricks


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	=> Fundamental data abstraction of Spark. 

	=> Is a  collection of distributed in-memory partitions
		-> Each partition is a collection of objects

	=> RDD partitions are immutable

	=> RDDs are lazily evaluation
		-> Transformation does not cause execution
		-> Only actions cause execution

	=> RDDs are resilient
		-> RDDs can recreate the missing partitions at runtime by reapplying
		   the transformations. 


   How to create RDDs ?
   --------------------   
     Three ways:

	1.  From some external data file such as a text file

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		-> The default numPartitions is defined by "sc.defaultMinPartitions" whose value is 2
		   if you have atleast two cores allocated.
	
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From some programmatic collection

		rdd1 = sc.parallelize( range(1, 100) )
		-> The default numPartitions is defined by "sc.defaultParallelism" whose value is equals to
		   the number of cores allocated.

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())


   What can we do with an RDD ?
   ----------------------------
	Two things:

	1. Transformations
		-> They only create Lineage of RDDs
		-> Does not cause execution

	2. Actions
		-> Trigger execution
		-> Converts in the logical plan to physical plan ans set of tasks are launched on the cluster.


   RDD Lineage DAG
   ---------------
	=> An RDD object can be thought of as a set of instructions on how to create the partitions
          of that RDD called "Lineage DAG" (which is a logical plan)

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
		Lineage DAG: rddFile -> sc.textFile 

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage DAG: rdd2 -> rddFile.map -> sc.textFile 

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))	
		Lineage DAG: rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile 

        rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG: rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile 

   RDD Actions
   -----------

	1. collect

	2. count

   
   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD Persistence
   ---------------

	rdd1 = sc.textFile(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevels.MEMORY_ONLY  )       -> instruction to persist the rdd6 partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	Lineage of rdd6	: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	Tasks: sc.textFile, t3, t5, t6 -> rdd6 -> collected. 

	rdd7.collect()
	Lineage of rdd7	: rdd7 -> rdd6.t7
	Tasks: t7 -> rdd7 -> collected.

	rdd6.unpersist()


	StorageLevels  (supported in PySpark)
        -------------------------------------
	1. MEMORY_ONLY      	-  memory serialized & in RAM (default)
	2. MEMORY_AND_DISK	-  disk memory serialized - preferebly RAM, else Disk.
	3. DISK_ONLY		
	4. MEMORY_ONLY_2	- 2x replication
	5. MEMORY_AND_DISK_2    - 2x replication


	Commands
        --------
	-> rdd1.persist()    => default: MEMORY_ONLY
	-> rdd1.persist( StorageLevel.DISK_ONLY )
	-> rdd1.cache()
	
	-> rdd1.unpersist()   
   

  Executor Memory Structure
  -------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   RDD Transformations
   -------------------

    1. map 		P: U -> V
			Element to element transformations
			input RDD: N objects, output RDD: N objects  =>(number of objects)

    		rdd = rddFile.map(lambda x : x.upper())


    2. filter		P: U -> Boolean
			Output RDD will have only those objects for which the function returns True.
			input RDD: N objects, output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) >= 9).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, output RDD: # of partitions

		rdd1			rdd2 = rdd1.glom()  

		P0: 1,2,3,6,5 -> glom -> P0 : [1,2,3,6,5]
		P1: 7,3,4,5,6 -> glom -> P1 : [7,3,4,5,6]
		P2: 9,3,5,4,7 -> glom -> P2 : [9,3,5,4,7]
	        rdd1.count() = 15 (int)	      rdd2.count() = 3 (lists)

		rdd1.glom().map(lambda x: len(x)).collect()

   4. flatMap		P: U -> Iterable[V] 
			flatMap flattens the individual elements of the iterable objects produced by the function
  			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Applies a function to an entire partition
			Function takes the entire partition as input	
			partition to partition transformation

		rdd1		rdd2 = rdd1.mapPartitions(lambda x: map(lambda a: a*10, x))

		P0: 1,2,3,6,5 -> mapPartitions -> P0 : 10,20,30,60,50
		P1: 7,3,4,5,6 -> mapPartitions -> P1 : 70,30,40,50,60
		P2: 9,3,5,4,7 -> mapPartitions -> P2 : 90,30,50,40,70

		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).glom().collect()
		rdd1.mapPartitions(lambda x: [sum(x)]).glom().collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Same as mapPartitions, exept that we get partition-index as additional 
				parameter.
  
		rdd1.mapPartitionsWithIndex(lambda index, data: [(index, sum(data))] ).glom().collect()
		rdd1.mapPartitionsWithIndex(lambda i, x: map(lambda a: (i, "A" + str(a)), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the input RDD.

		rddWords.distinct().collect()
		rdd2 = rddWords.distinct(5)   # rdd2 will have 5 partitions


   8. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				The elements of the RDD are sorted based on the function output. 
  
		rddWords.sortBy(lambda x: x[0]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 6).collect()


   Types of RDDs:
	-> Generic RDDs => RDD[U]
	-> Pair RDDs	=> RDD[(K, V)]

   9. mapValues			P: U-> V
				Applied only to pair RDD
				Will transform the value part of the pair RDD by applying the function.


   10. groupBy			P: U -> V, Optional: numPartitions

				The objects of the RDD are grouped by the function output.

				Returns a Pair RDD where:
				key: Unique value of function output
				value: ResultIterable of all objects of the RDD that produced the key.

		rddWords.groupBy(lambda x: x[0]).mapValues(list).collect()
		rddWords.groupBy(lambda x: x[0], 6).mapValues(list).glom().collect()

		# wordcount example
		file = "E:\\Spark\\wordcount.txt"

		outputRdd = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.groupBy(lambda x: x) \
            			.mapValues(len) \
            			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of ratios ex: [0.4, 0.3, 0.3]  Optional: Seed
				Retuns a list of RDDs split randomly approx. in the given ratios.
	
		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 97234234)

   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of output RDD.
				Performs global shuffle.


   13. coalesce			P: numPartitions
				Used to only decrease the number of partitions of output RDD.
				Performs partition-merging.

	Recommendations
        ---------------
	-> The size of each partition should be around 128MB  (100 -150 MB)  
	-> The number of partitions should be a multiple of number of cores
	-> The number of cores in each executor should be 5


   14. partitionBy 		P: numPartitions, Optional: Partitioning function : U -> Int

				-> Applied only to Pair RDD and partioning happend based on the keys.
				-> Used to control which keys go to which partitions
		                -> default partitioning fun: hash of the key
 		transactions = [
    			{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    			{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    			{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
   			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

			def custom_partitioner(city): 
    				if (city == 'Chennai'):
        				return 0;
    				elif (city == 'Hyderabad'):
        				return 1;
    				elif (city == 'Vijayawada'):
        				return 1;
    				elif (city == 'Pune'):
        				return 2;
    				else:
        				return 3;      

			rdd1 = sc.parallelize(transactions, 3).map(lambda d: (d['city'], d))
			rdd1.glom().collect()

			rdd2 = rdd1.partitionBy(3, custom_partitioner)
			rdd2.glom().collect()

   15. union, intersection, subtract, cartesian

	Let us say, rdd1 has M partitions and rdd2 has N partitions

	command				# output partitions
        ---------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


  ..ByKey Transformations
  ------------------------
      	-> Wide Transformations
	-> Applied only on pair RDDs


  16. sortByKey			P: None, Optional: ascending (True/False), numPartitions
				RDD is sorted based on the key	
   
		rddpairs1.sortByKey()
		rddpairs1.sortByKey(False)
		rddpairs1.sortByKey(True, 3)

   17. groupByKey		P: None, Optional: numPartitions
				Groups the RDD base on the key
				Returns a Pair RDD where key is the "unique-key" and value is a 
				ResultIterable with grouped elements.
				NOTE: Try to avoid groupByKey.

		# wordcount example
		file = "E:\\Spark\\wordcount.txt"

		outputRdd = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
            			.groupByKey() \
            			.mapValues(sum)


   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Will reduce all the values of each unique key into one final value
				(per unique key) nu iterativly applying the function.

		# wordcount example
		file = "E:\\Spark\\wordcount.txt"

		outputRdd = sc.textFile(file, 4) \
            			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
            			.reduceByKey(lambda x, y: x + y, 1)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.

		student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		avg_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
          		.aggregateByKey((0,0),
                          lambda z, v: (z[0] + v, z[1] + 1),
                          lambda a, b: (a[0] + b[0], a[1] + b[1])) \
         		.mapValues(lambda x: x[0]/x[1])


    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

     21. cogroup	 -> Used when the RDDs that you join may have duplicate keys
			   Performs groupByKey on each RDD and then performs fullOuterJoin on the 
			   grouped data. 
   

   RDD Actions
   ------------

    1. collect

    2. count

    3. saveAsTextFile

    4. reduce			P: (U, U) -> U
				Reduces the entire RDD into one value of the same type by iterativly
				applying the reduce on the objects of each partition first and then 
				reduces the reduced values of each partition.

		P0:   9, 1, 8, 7, 5, 1, 2    	   => -15  => 48
		P1:   3, 5, 4, 3, 6, 7, 8	   => -30	
		P2:   9, 0, 3, 4, 5, 6, 7, 8, 9    => -33

		rdd1.reduce(lambda x, y: x - y)   => output: 48

    5. aggregate	-> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final values of the type
                  of the zero-value.

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]))


    6. take		
		rdd1.take(10) 

    7. takeOrdered

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, lambda x: len(x))

    8. takeSample

		rdd1.takeSample(True, 10)	# with replacement sampling
		rdd1.takeSample(True, 10, 456)  # with replacement sampling with a seed
		rdd1.takeSample(False, 10, 456) # without replacement sampling with seed

    9. countByValue

    10. countByKey

    11. first

    12. foreach	-> does not return anything.
		   applies a function on all the values of the rdd.
		
    13. saveAsSequenceFile


   Use-Case
   --------
	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
	
	From the cars.tsv dataset, find out the average weight of each make of American origin.
	Sort the data in the DESC order of average weight.
	Save the output as a singke text-file.
	Use RDD API ONLY
		
       => Please try to solve the above.


   spark-submit
   ------------
	-> is a single command to submit any spark application (python, scala, java etc..) to
	   any cluster manager. 
	
	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G 
		--executor-memory 10G
		--executor-cores 5
		--num-executors 10
		wordcount.py [app arguments]


	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout2 3



   Closure
   -------

	Closure is those variables and methods that must be visible within an executor for a task to 
	perform its computations on the RDD.

	This closure is serialized and a separate local copy will be sent to each executor.

	// the following will not work as intended ..

	count = 0

	def isPrime( n ) :
		if (n is prime) return 1 
		else returns 0

	def f1 ( n ) :
		global count
		if ( isPrime(n) == 1 ) count += 1
		return n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(count)    // output = 0  

	
	The closure problem:
		-> In the above the count referred inside the function is part of the closure
		   and hense isa local copy in every task and this changes to this value can not 
		   be propagated back to the driver. 

		=> Hence, WE CAN NOT USE LOCAL VARIABLES for implementing GLOBAL COUNTERS.


   Shared Variables
   ----------------

   1. Accumulator
   --------------

	-> Is a shared variable.
	-> Maintained by driver.
	-> All tasks can add to this variables.
	-> Not part of function closure.

	-> Accumulators are used implement global counters.

	count = sc.accumulator(0)

	def isPrime( n ) :
		if (n is prime) return 1 
		else returns 0

	def f1 ( n ) :
		global count
		if ( isPrime(n) == 1 ) count.add(1)
		return n*2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(count.value) 


   2. Broadcast Variable
   ----------------------

	d1 = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, ......})   # 100 MB

	def f1( n ):
		global d1
		return d1.value[n]

        rdd1 = sc.parallelize([1,2,3,4,5, ...], 4)	
        rdd2 = rdd1.map( f1 )
        rdd2.collect()     # output:  a,b,c,....












