
  Agenda (PySpark)
  -----------------

   Databricks - Basics
   Spark - Basics & Architecture 
   Spark Core API 
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark-submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Notebooks
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
	
    Spark is written in Scala programming language

    ** Spark is a unified in-memory distributed computing framework for performing big-data analytics **

    Cluster => is a unified system containing group of nodes whose combined resources (RAM & CPU cores) 
               can be used to distributed the processing among all the nodes

    in-memory -> spark can persist intermediate results in memory so that the subsequent transformations
		 can directly use these persisted results. 

    Spark is a polyglot
	-> Supports Scala, Java, Python, R and SQL

    Spark supports multiple cluster manager
	-> Local, Spark Standalone, YARN, Mesos, Kubernetes


    Unified framework
    -----------------
	
      Spark provides a consistent set of API running on the same execution engine and using standard
      data abstractions for processing different analytical workloads. 


	Batch Processing  	   => RDD API, Spark SQL
	Stream Processing	   => Spark Streaming, Structured Streaming
	Predictive Analytics (ML)  => Spark MLlib
	Graph Processing	   => Spark GraphX


   Spark Architecture	
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


  Getting started with PySpark on Local Machine
  ---------------------------------------------
  
    -> Install Anaconda Distribution (for Python)
	URL: https://www.anaconda.com/download
	-> Download and install Anaconda distribution

    -> Follow the instructions given in the shared document.
	https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


  RDD (Resilient Distributed Dataset)
  -----------------------------------
	
   -> RDD is the fundamental data abstraction of Spark Core
   -> RDD represents a collection of distributed in-memory partitions


  Creating RDDs
  -------------
	
	Three ways:

	1. Creating an RDD from external file


		rdd1 = sc.textFile( <filePath>, n )
		rdd1 = sc.textile("E:\\Spark\\wordcount.txt", 4)


	2. Creating an RDD from programmatic data

	3. Creating an RDD by applying transformations on exiting RDD



  RDD Operations
  --------------
	Two operations

	1. Transformations
		-> Creates an RDD Lineage DAG


	2. Actions
		-> Executes an RDD and launchs a Job in the cluster



  RDD Lineage DAG
  ---------------

   








  












































  


