
  Agenda (PySpark on Databricks)
  ------------------------------

   Databricks - Basics
   Spark - Basics & Architecture 
   Spark Core API 
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark-submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Notebooks
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
	
    Spark is written in Scala programming language

    ** Spark is a unified in-memory distributed computing framework for performing big-data analytics **

    Cluster => is a unified system containing group of nodes whose combined resources (RAM & CPU cores) 
               can be used to distributed the processing among all the nodes

    in-memory -> spark can persist intermediate results in memory so that the subsequent transformations
		 can directly use these persisted results. 

    Spark is a polyglot
	-> Supports Scala, Java, Python, R and SQL

    Spark supports multiple cluster manager
	-> Local, Spark Standalone, YARN, Mesos, Kubernetes


   Unified framework
   -----------------
	
      Spark provides a consistent set of API running on the same execution engine and using standard
      data abstractions for processing different analytical workloads. 


	Batch Processing  	   => RDD API, Spark SQL
	Stream Processing	   => Spark Streaming, Structured Streaming
	Predictive Analytics (ML)  => Spark MLlib
	Graph Processing	   => Spark GraphX


   Spark Architecture	
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


  Getting started with PySpark on Local Machine
  ---------------------------------------------
  
    -> Install Anaconda Distribution (for Python)
	URL: https://www.anaconda.com/download
	-> Download and install Anaconda distribution

    -> Follow the instructions given in the shared document.
	https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


  RDD (Resilient Distributed Dataset)
  -----------------------------------
	
   -> RDD is the fundamental data abstraction of Spark Core

   -> RDD represents a collection of distributed in-memory partitions
	 -> Partition is a collections of objects (of some type)

   -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
	-> Actions trigger execution.

   -> RDDs are immutable
	-> We can not change the content of an RDD once created. 
	-> We can only transform to other RDDs

   -> RDDs are resilient
	-> RDDs can recompute (missing) partitions at run time 


  Creating RDDs
  -------------
	
    Three ways:

	1. Creating an RDD from external file

		rddFile = sc.textFile( <filePath>, n )
		rddFile = sc.textile("E:\\Spark\\wordcount.txt", 4)

		=> default number of partitions is as per the value of "sc.defaultMinPartitions"
		=> "sc.defaultMinPartitions" = 2 if core >= 2, else 1


	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize(<some python collection>, n)
		rdd1 = sc.parallelize([3,2,1,4,2,3,5,4,6,7,8,9,0,8,9,0,8,6,4,1,2,3,2,4,6,5,7,8,9,6,7,5], 3)

		=> default number of partitions is as per the value of "sc.defaultParallelism"
		=> "sc.defaultMinPartitions" = number of cores allocated to your application


	3. Creating an RDD by applying transformations on exiting RDD

		rddWords = rddFile.flatMap(lambda x: x.split())

		-> By default the output RDD will have the same number of partitions as input partitions


  RDD Operations
  --------------
	Two operations

	1. Transformations
		-> Creates an RDD Lineage DAG
		-> Does not cause execution

	2. Actions
		-> Executes an RDD and launchs a Job in the cluster
		-> Produces some output
		-> Converts logical plan (DAG) to a physical plan for execution. 


  RDD Lineage DAG   
  ---------------
   (DAG: Directed Acyclic Graph)
   -> Represents a logical plan on how to execute the RDD.
   -> Lineage DAG stores the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rddFile => (4) rddFile -> sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG of rddWords => (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords => (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc => (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
  


  RDD Execution Flow
  ------------------

	Application (ex: PySpark Shell or You App in Spyder/PyCharm)
	|
	|--> Jobs (Each action command launches one job)
		|
		|--> Stages (one or more stages per Job)
			|
			|--> Tasks (one task per partition of the RDD in that stage)
				|
				|--> Transformations (one or more transformations per task)				


  Getting started with Databricks
  -------------------------------


    Signup to Databricks community edition
    --------------------------------------
	-> URL: https://www.databricks.com/try-databricks
		-> Fill in the details with valid email address (Preferably personal email)
		-> In the next page, click on "Get started with Community Edition" link
		   (Do not click on 'Continue' button)


    Databricks Storage
    ------------------	
	DBFS (Databricks File System) --> Upload your data into this
		
	 File access format: dbfs:/FileStore/flight-data  or  /FileStore/flight-data

	 FileStore => this is the folder into which you can upload user data.


  RDD Transformations
  -------------------

  1. map		P:  U -> V
			Object to object transformation
			Transforms each input object into an output object by applying the function
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: x.upper()).collect()


  2. filter		P: U -> Boolean	
			Filters the data to the output DataFrame based on the condition
			input RDD: N objects, output RDD: < N objects

		rdd1.filter(lambda x: x > 5).collect()


  3. glom		P: None
			Returns one list object per partition with the content of entire partition.

	
		rdd1			     rdd2 = rdd1.glom()

		P0: 2,3,4,3,5,6,7 -> glom -> P0: [2,3,4,3,5,6,7]
		P1: 4,6,7,0,5,1,2 -> glom -> P1: [4,6,7,0,5,1,2]
		P2: 5,7,2,3,1,2,1 -> glom -> P2: [5,7,2,3,1,2,1]

		rdd1.count() = 21 (int)		rdd2.count() = 3 (list)


  4. mapPartitions   	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()


  5. flatMap		P: U -> Iterable[V]
			flattens the output produced by the function

		rddWords = rddFile.flatMap(lambda x: x.split())


  Types of RDDs
  -------------
	
	1. Generic RDDs  : RDD[U]
	2. Pair RDDs : RDD[(K, V)]


  6. mapValues		P: U -> V
			Applied only on Pair RDD
			Transforms only the 'value' part of the (K, V) pairs by applying the function
			

		rdd3 = rdd2.mapValues(lambda v: (v, v+10))
		rdd3.collect()


  7. distinct		P: None, Optional: numPartition
			returns distinct objects of the RDD

		rdd1.distinct().glom().collect()
		rdd1.distinct(3).glom().collect()
		rdd1.distinct(2).glom().collect()


  8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the data of the RDD based on the function output

		rddWords.sortBy(lambda x: len(x)).collect()
		rddWords.sortBy(lambda x: len(x), False).collect()
		rddWords.sortBy(lambda x: len(x), False, 3).collect()


  9. groupBy		P: U -> V, Optional: numPartitions
			Returns a Pair-RDD where:
				key: Each unique value of the function output
				value: ResultIterable. Grouped objectsof the RDD that produced the key



  10. partitionBy	P: numPartitions, Optional: partition-function (defaut: hash)
			Applied on Pair RDDs only

  
			
  11. sortByKey		P: None, Optional: ascending (True/False), numPartitions
			Applied on Pair RDDs only
			Sorts based on the Key


  12. groupByKey	P: None, Optional: numPartitions
			Applied on Pair RDDs only
			Returns a Pair RDD where:
			  key: each unique key of the (K, V) pairs
			  value: Results Iterable. Grouped values with the same key.

  
  13. reduceByKey	P: (U, U) -> U, Optional: numPartitions
			Applied on Pair RDDs only
			Reduces all the values of each unique key.


  14. repartition	P: numPartition
			Is used to increase or decrease the number of partitions
			Performs global shuffle

		rddWords.repartition(3).glom().collect()
		

  15. coalesce		P: numPartition
			Is used only to decrease the number of partitions
			Performs partition merging

		 rddWords.coalesce(2).glom().collect()	



  RDD Persistence
  ---------------
	rdd1 = sc.textFile(....., 40)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.DISK_ONLY )      --> instruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	   lineage DAG of rdd6: (40) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   physical plan: [sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	   lineage DAG of rdd7: (40) rdd7 -> rdd6.t7
	   physical plan: [t7] -> collected

	rdd6.unpersist()


     Storage Levels
     --------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2    -> Disk Memory Serialized 2x Replicated

    
    Commands
    --------
	rdd1.cache()        			-> in-memory persistence
	rdd1.persist(StorageLevel.DISK_ONLY)

	rdd1.unpersist()
		
  
  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) => U
			Reduces an entire RDD to one value of the same type.
			In the first stage each partition is reduced and in the second stage the outputs
			of all the partitions are further reduced by iterativly running the function.

		rdd1
		P0: 3,2,1,4,2,4,5 -> reduce -> 21 -> reduce -> 73
		P1: 5,7,6,8,9,0,1 -> reduce -> 36
		P2: 2,1,1,3,2,4,3 -> reduce -> 16

		rdd1.reduce(lambda x, y: x + y) => 73

  5. take

  6. takeOrdered

  7. takeSample

  8. countByValue

  9. countByKey


  Use-Case
  ---------

   Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

   From cars.tsv file, get the average weight of all the models of each make of American origin cars
	-> Arrange the data in the DESC order of average weight
	-> Save the output in a single text file.

  
  Closure
  -------

	In Spark, a closure constitutes all the variables and methods which must be visible for 
        the executor to perform its computations on the RDD

	-> Driver serializes the closure and a separate copy is sent to each executor. 



        c = 0    # count the number of primes
    
	def f1(n):
	   global c
	   if is_prime(n):
	      c += 1
	   return n*2

	def is_prime(n):
	   return True if n is prime
	   else return False

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map(f1)

	rdd2.collect()

	print(c)      # 0


    Limitatiton: You can not use local variables to implement global counter. 
    Solution: Use Accumulator variable.


  Shared Variables
  ----------------

   1. Accumulator Variable

	-> Is a shared variable that is not part of the closure
	-> Maintained by driver
	-> All tasks can add to it using 'add' method.	
	-> Only driver can read the value of accumulator. Tasks can only write to it.
	-> Use accumulators to implement global counters. 


	c = sc.accumulator(0)    # count the number of primes
    
	def f1(n):
	   global c
	   if is_prime(n):
	      c.add(1)
	   return n*2

	def is_prime(n):
	   return True if n is prime
	   else return False

	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map(f1)

	rdd2.collect()

	print( c.value() )  


   2. Broadcast Variable
  
	-> Broadcast variable is a shared variable, hence, not a part of closure
	-> Driver sends a copy of the broadcast variable to every executor (not to task)
	-> All the tasks within that executor can read from the one copy (of the executor)
	-> You can convert large immutable collections into broadcast variables. 	

	bc = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, ........})     # 100 MB

	def f1(n):
           global bc 
           return bc.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,....], 4)
	rdd2 = rdd1.map(f1)

	rdd2.collect()
	
	
  Spark-submit command
  --------------------

    -> Is a single command to submit any spark application (Python, Scala, Java, R) to any cluster
       manager (local, spark standalone, YARN, Mesos, Kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt st_output 2


  ===========================
     Spark SQL
  ===========================
    
    => This is the main API of Spark for data analysis/data engineering/data science
	
	-> Is a high-level API built on top of Spark Core.	
	-> Spark Structured/Semi-structured data processing API


   => Supported Formats:
	
	File Formats: Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format:  RDBMS, NoSQL Databases
	Hive Format:  Hive warehouse


   => SparkSession
	
	-> Represents a user-session inside an application
	-> An application can contain multiple sessions.
	-> SparkSession is the starting point of executions

		application => SparkContext
		session	=> SparkSession

	In Databricks:
		A cluster -> Represents an application
		Notebooks -> Represent Sessions (in the application)
	

		spark = SparkSession \
			.builder \
			.appName("Basic Dataframe Operations") \
			.config("spark.master", "local[*]") \
			.getOrCreate()    


  
   => DataFrame

	-> Is the main data abstraction	

	-> DataFrame represents a collection of in-memory partitions that are immutable and lazily evaluated
	   -> each partition contains "Row" objects

        -> DataFrame has two components

		-> Data   : Row objects
		-> Schema : StructType object

		StructType(
		    [
			StructField('age', LongType(), True), 
			StructField('gender', StringType(), True), 
			StructField('name', StringType(), True), 
			StructField('phone', StringType(), True), 
			StructField('userid', LongType(), True)
		    ]
		)


  
   Basic Steps in Spark SQL 		
   ------------------------
	
	1. Load/Read the data into a DataFrame	

		df1 = spark.read.format("json").load("/FileStore/users.json")
		df1 = spark.read.load("/FileStore/users.json", format="json")
		df1 = spark.read.json("/FileStore/users.json")
		
		Applying schema
		---------------
		users_schema = StructType(
		    [
			StructField('age', LongType(), True), 
			StructField('gender', StringType(), True), 
			StructField('name', StringType(), True), 
			StructField('phone', StringType(), True), 
			StructField('userid', LongType(), True)
		    ])

		df1 = spark.read.schema(users_schema).json("/FileStore/users.json")


	2. Transform the DataFrame using Transformations API or using SQL

		Using Transformation methods
		----------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("gender", "age").count() \
        		.limit(4)


		Using SQL
		---------
		df1.createOrReplaceTempView("users")

		df3 = spark.sql("""SELECT age, count(1) as count
        			   FROM users
        			   WHERE age IS NOT NULL
        			   GROUP BY gender, age
        			   ORDER BY gender, age
        			   LIMIT 4""")
		display(df3)	


 	3. Save/Write the DataFrame into a target location 
	
		df3.write.format("json").save("dbfs:/FileStore/output/json")
		df3.write.save("dbfs:/FileStore/output/json", format="json")
		df3.write.json("dbfs:/FileStore/output/json")

 
  Save Modes
  ----------
	- Define what should happen when you are writing to an existing directory
  		- ErrorIfExists (default)
  		- Ignore
  		- Append   (appends additional files to the existing directory)
  		- Overwrite (overwrites old directory)


	df3.write.mode("append").format("json").save("dbfs:/FileStore/output/json")
	df3.write.mode("overwrite").format("json").save("dbfs:/FileStore/output/json")

	df3.write.format("json").save("dbfs:/FileStore/output/json", mode="append")


  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")

  DataFrame Transformations
  -------------------------

   1. select
	
		df2 = df1.select("ORIGIN_COUNTRY_NAME",
				"DEST_COUNTRY_NAME",
				"count")
		----------------------------------------
		df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
			expr("DEST_COUNTRY_NAME as destination"),
			expr("count").cast("int"),
			expr("count+10 as newCount"),
			expr("count > 100 as highFrequency"),
			expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME").alias("domestic"))


		df2.show()
		df2.printSchema()


  2. where / filter

	df3 = df2.where("count > 500 and domestic = false")
	df3 = df2.filter("count > 500 and domestic = false")
	df3 = df2.where( col("count") > 200 )

  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")
	df3 = df2.sort(desc("count"), asc("origin"))


  4. groupBy   => returns a 'pyspark.sql.group.GroupedData' object (not a DataFrame)
		  Apply aggregation methods to return a DataFrame


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"),2).alias("avg"))

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	expr("sum(count) as sum"),
              		expr("max(count) as max"),
              		expr("avg(count) as avg"))


  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
			"DEST_COUNTRY_NAME as destination",
			"count",
			"count+10 as newCount",
			"count > 100 as highFrequency",
			"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
	df2.show()


  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
		.withColumn("highFrequency", expr("count > 100")) \
		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
		.withColumn("Country", lit("India"))

	----------------------

	df4 = df3.withColumn("ageGroup", when( col("age") < 13, "child")
                                	.when( col("age") < 20, "teenager")
                                	.when( col("age") < 60, "adult")
                                	.otherwise("senior"))

	df4 = df3.withColumn("ageGroup", when( expr("age < 13"), "child")
                                	.when( expr("age between 13 and 19"), "teenager")
                                	.when( expr("age < 60"), "adult")
                                	.otherwise("senior"))


  8. udf (User Defined Function)


	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"    

	get_age_group = udf(getAgeGroup, StringType())

	df4 = df3.withColumn("ageGroup", get_age_group(col("age")))

	---------------------------

	@udf(returnType = StringType())
	def getAgeGroup( age ):
		if (age <= 12):
			return "child"
		elif (age >= 13 and age <= 19):
			return "teenager"
		elif (age >= 20 and age < 60):
			return "adult"
		else:
			return "senior"    


	df4 = df3.withColumn("ageGroup", getAgeGroup(col("age")))
	
	------------------------------

	spark.udf.register("get_age_group", getAgeGroup, StringType())

	df3.createOrReplaceTempView("users")

	qry = """select id, name, age, get_age_group(age) as ageGroup
		   from users"""       

	spark.sql(qry).show()


  9. drop


  10. dropDuplicates

 
  11. distinct

  
  12. repartition


  13. coalesce


  14. join



   Working with different file formats
   -----------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)


   CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")

   Text
	read
		df1 = spark.read.text(inputPath)
		=> df1 will have one columns called 'value' of 'string' type

	write
		df1.write.text(outputPath)
		=> You can only save a DF with a single text column in 'text' format.


  Creating an RDD from DataFrame
  ------------------------------
   
        rdd1 = df1.rdd


  Creating a DataFrame from programmatic data
  -------------------------------------------
   
     	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])


  Creating a DataFrame from RDD
  -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame with custom/programmatic schema
  ----------------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)  
	-----------------------------------------------------
        filePath = "E:\\PySpark\data\\flight-data\\2015-summary-nh.csv"

	mySchema = "origin STRING, destination STRING, count INT"
	df1 = spark.read.csv(filePath, schema=mySchema) 
	-----------------------------------------------------
	mySchema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

	df1 = spark.read.json(filePath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()
	-----------------------------------------------------
	mySchema = StructType([
				 StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
				 StructField("DEST_COUNTRY_NAME", StringType(), True),
				 StructField("count", IntegerType(), True)
			   ])

	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()


  Use-Case
  --------

    Dataset: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

    From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average user-rating
	-> Consider only those movies that are rated by atleast 50 users
	-> Data: movieId, title, totalRatings, averageRating
	-> Arrange the data in the DESC order of averageRating
	-> Save the output as a single pipe-separated CSV file with header
	-> Use only DF transformation methods (not SQL)

	=> Try it yourself



  Creating an RDD from DataFrame
  ------------------------------
   
        rdd1 = df1.rdd


  Creating a DataFrame from programmatic data
  -------------------------------------------
   
     	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])


  Creating a DataFrame from RDD
  -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame with custom/programmatic schema
  ----------------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)  
	-----------------------------------------------------
        filePath = "E:\\PySpark\data\\flight-data\\2015-summary-nh.csv"

	mySchema = "origin STRING, destination STRING, count INT"
	df1 = spark.read.csv(filePath, schema=mySchema) 
	-----------------------------------------------------
	mySchema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

	df1 = spark.read.json(filePath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()
	-----------------------------------------------------
	mySchema = StructType([
				 StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
				 StructField("DEST_COUNTRY_NAME", StringType(), True),
				 StructField("count", IntegerType(), True)
			   ])

	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()

           
  JDBC Format - Working with MySQL
  --------------------------------
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()



  Working with Hive
  -----------------

# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()  

spark.catalog.currentDatabase()
spark.catalog.listDatabases()

spark.sql("show databases").show()

spark.sql("drop database sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")
spark.sql("use sparkdemo")

spark.catalog.listTables()
spark.catalog.currentDatabase()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
 
summaryDf2.write \
   .mode("overwrite") \
   .format("hive") \
   .saveAsTable("topRatedMovies")
   
   
spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()


 ===========================================
   Spark Streaming (Structured Streaming) 
 ===========================================

   Spark Streaming APIs:

	-> Spark Streaming (DStreams API) built on top of Spark Core
		-> pyspark.streaming
		-> Old & legacy API

	-> Structured Streaming built on top of Spark SQL 
		-> pyspark.sql.streaming
		-> Current and preferred API

  Notes:

  The key idea in Structured Streaming is to treat a live data stream as a table that is being 
  continuously appended.

  You express your streaming computation as standard batch-like query as on a static table, 
  and Spark runs it as an incremental query on the unbounded input table.

  Programming Model
  -----------------
	A query on the input will generate the “Result Table”.

	Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	which eventually updates the Result Table.

	Whenever the result table gets updated, we would want to write the changed result rows 
	to an external sink.

	Structured Streaming does not materialize the entire table.

	It reads the latest available data, processes it incrementally to update the result, and 
	then discards the source data.

	Spark is responsible for updating the Result Table when there is new data, thus relieving 
	the users from reasoning about fault-tolerance and data consistency.


  Sources
  -------

	Socket Stream (host & port)	
	File Stream (directory) => text, csv, json, parquet, orc
	Rate stream
	Kafka Stream


  Sinks
  -----

	Console Sink
	File Sinks  (directory) => text, csv, json, parquet, orc
	Kafka sink
	ForEachBatch sink
	ForEach sink 
	Memory sink




 =============================================
    Spark Streaming (Structured Streaming)
 =============================================






















