
  Agenda (PySpark on Databricks)
  ------------------------------

   Databricks - Basics
   Spark - Basics & Architecture 
   Spark Core API 
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark-submit command
   Spark SQL 
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Notebooks
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----
	
    Spark is written in Scala programming language

    ** Spark is a unified in-memory distributed computing framework for performing big-data analytics **

    Cluster => is a unified system containing group of nodes whose combined resources (RAM & CPU cores) 
               can be used to distributed the processing among all the nodes

    in-memory -> spark can persist intermediate results in memory so that the subsequent transformations
		 can directly use these persisted results. 

    Spark is a polyglot
	-> Supports Scala, Java, Python, R and SQL

    Spark supports multiple cluster manager
	-> Local, Spark Standalone, YARN, Mesos, Kubernetes


   Unified framework
   -----------------
	
      Spark provides a consistent set of API running on the same execution engine and using standard
      data abstractions for processing different analytical workloads. 


	Batch Processing  	   => RDD API, Spark SQL
	Stream Processing	   => Spark Streaming, Structured Streaming
	Predictive Analytics (ML)  => Spark MLlib
	Graph Processing	   => Spark GraphX


   Spark Architecture	
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


  Getting started with PySpark on Local Machine
  ---------------------------------------------
  
    -> Install Anaconda Distribution (for Python)
	URL: https://www.anaconda.com/download
	-> Download and install Anaconda distribution

    -> Follow the instructions given in the shared document.
	https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


  RDD (Resilient Distributed Dataset)
  -----------------------------------
	
   -> RDD is the fundamental data abstraction of Spark Core

   -> RDD represents a collection of distributed in-memory partitions
	 -> Partition is a collections of objects (of some type)

   -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
	-> Actions trigger execution.

   -> RDDs are immutable
	-> We can not change the content of an RDD once created. 
	-> We can only transform to other RDDs

   -> RDDs are resilient
	-> RDDs can recompute (missing) partitions at run time 


  Creating RDDs
  -------------
	
    Three ways:

	1. Creating an RDD from external file

		rddFile = sc.textFile( <filePath>, n )
		rddFile = sc.textile("E:\\Spark\\wordcount.txt", 4)

		=> default number of partitions is as per the value of "sc.defaultMinPartitions"
		=> "sc.defaultMinPartitions" = 2 if core >= 2, else 1


	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize(<some python collection>, n)
		rdd1 = sc.parallelize([3,2,1,4,2,3,5,4,6,7,8,9,0,8,9,0,8,6,4,1,2,3,2,4,6,5,7,8,9,6,7,5], 3)

		=> default number of partitions is as per the value of "sc.defaultParallelism"
		=> "sc.defaultMinPartitions" = number of cores allocated to your application


	3. Creating an RDD by applying transformations on exiting RDD

		rddWords = rddFile.flatMap(lambda x: x.split())

		-> By default the output RDD will have the same number of partitions as input partitions


  RDD Operations
  --------------
	Two operations

	1. Transformations
		-> Creates an RDD Lineage DAG
		-> Does not cause execution

	2. Actions
		-> Executes an RDD and launchs a Job in the cluster
		-> Produces some output
		-> Converts logical plan (DAG) to a physical plan for execution. 


  RDD Lineage DAG   
  ---------------
   (DAG: Directed Acyclic Graph)
   -> Represents a logical plan on how to execute the RDD.
   -> Lineage DAG stores the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rddFile => (4) rddFile -> sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG of rddWords => (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords => (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc => (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
  


  RDD Execution Flow
  ------------------

	Application (ex: PySpark Shell or You App in Spyder/PyCharm)
	|
	|--> Jobs (Each action command launches one job)
		|
		|--> Stages (one or more stages per Job)
			|
			|--> Tasks (one task per partition of the RDD in that stage)
				|
				|--> Transformations (one or more transformations per task)				


  Getting started with Databricks
  -------------------------------


    Signup to Databricks community edition
    --------------------------------------
	-> URL: https://www.databricks.com/try-databricks
		-> Fill in the details with valid email address (Preferably personal email)
		-> In the next page, click on "Get started with Community Edition" link
		   (Do not click on 'Continue' button)


    Databricks Storage
    ------------------	
	DBFS (Databricks File System) --> Upload your data into this
		
	 File access format: dbfs:/FileStore/flight-data  or  /FileStore/flight-data

	 FileStore => this is the folder into which you can upload user data.



  
		

  RDD Transformations
  -------------------

  1. map		P:  U -> V
			Object to object transformation
			Transforms each input object into an output object by applying the function
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: x.upper()).collect()


  2. filter		P: U -> Boolean	
			Filters the data to the output DataFrame based on the condition
			input RDD: N objects, output RDD: < N objects

		rdd1.filter(lambda x: x > 5).collect()


  3. glom		P: None
			Returns one list object per partition with the content of entire partition.

	
		rdd1			     rdd2 = rdd1.glom()

		P0: 2,3,4,3,5,6,7 -> glom -> P0: [2,3,4,3,5,6,7]
		P1: 4,6,7,0,5,1,2 -> glom -> P1: [4,6,7,0,5,1,2]
		P2: 5,7,2,3,1,2,1 -> glom -> P2: [5,7,2,3,1,2,1]

		rdd1.count() = 21 (int)		rdd2.count() = 3 (list)


  4. mapPartitions   	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()


  5. flatMap		P: U -> Iterable[V]
			flattens the output produced by the function

		rddWords = rddFile.flatMap(lambda x: x.split())


  Types of RDDs
  -------------
	
	1. Generic RDDs  : RDD[U]
	2. Pair RDDs : RDD[(K, V)]


  6. mapValues		P: U -> V
			Applied only on Pair RDD
			Transforms only the 'value' part of the (K, V) pairs by applying the function
			

		rdd3 = rdd2.mapValues(lambda v: (v, v+10))
		rdd3.collect()


  7. distinct		P: None, Optional: numPartition
			returns distinct objects of the RDD

		rdd1.distinct().glom().collect()
		rdd1.distinct(3).glom().collect()
		rdd1.distinct(2).glom().collect()


  8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the data of the RDD based on the function output

		rddWords.sortBy(lambda x: len(x)).collect()
		rddWords.sortBy(lambda x: len(x), False).collect()
		rddWords.sortBy(lambda x: len(x), False, 3).collect()







  RDD Persistence
  ---------------
	rdd1 = sc.textFile(....., 40)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.DISK_ONLY )      --> instruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	   lineage DAG of rdd6: (40) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   physical plan: [sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	   lineage DAG of rdd7: (40) rdd7 -> rdd6.t7
	   physical plan: [t7] -> collected

	rdd6.unpersist()


     Storage Levels
     --------------
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2    -> Disk Memory Serialized 2x Replicated

    
    Commands
    --------
	rdd1.cache()        			-> in-memory persistence
	rdd1.persist(StorageLevel.DISK_ONLY)

	rdd1.unpersist()
	
	

























  


