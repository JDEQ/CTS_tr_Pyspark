  
  Agenda
  ---------   
   1. PySpark Basics & Spark SQL 	8 hours
   2. AWS Basics & AWS Infra		4 hours
   3. Amazon S3				4 hours
   4. AWS Lambda			4 hours
   5. AWS Glue				8 hours
   6. AWS Athena			2 hours
   7. Project				4 hours


  Course Materials
  ----------------
    -> PDF Presentations
    -> Code Modules, Instructions, Scripts
    -> Github: 	https://github.com/ykanakaraju/pyspark
		https://github.com/ykanakaraju/aws-cts  
  
  Spark
  -----

   -> Spark is an in-memory distributed computing framework.

   -> Spark is written in SCALA language

   -> Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

   -> Spark has unified stack for data analytics

   -> Spark APIs
	-> Spark Core API  : Low-level API. Uses RDDs
	-> Spark SQL	   : Structured/semi-structured data processing (Batch)
	-> Spark Streaming : DStreams API, Structured streaming
	-> Spark MLLib	   : Predictive analytics using ML
	-> Spark GraphX	   : Graph Processing


  Spark Architecture
  ------------------
	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


  
  RDD (Resilient Distributed Dataset)
  -----------------------------------
	
   -> RDD is the fundamental data abstraction of Spark Core

   -> RDD represents a collection of distributed in-memory partitions
	 -> Partition is a collections of objects (of some type)

   -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
	-> Actions trigger execution.

   -> RDDs are immutable
	-> We can not change the content of an RDD once created. 
	-> We can only transform to other RDDs

   -> RDDs are resilient
	-> RDDs can recompute (missing) partitions at run time 


  Getting started with Spark
  --------------------------
 
    1. Creating a local development environment

	-> Install Anaconda distribution  
	-> Follow the instructions given in the document:
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    2. Signup to Databricks Community Edition (free edition)
	
	 -> Signup: https://www.databricks.com/try-databricks
		-> Click on "Get started with Community Edition" in second screen
  
	-> Login: https://community.cloud.databricks.com/login.html

 
  Creating RDDs
  -------------

    Three ways:

	1. Creating an RDD from external file

		rddFile = sc.textFile( <filePath>, n )
		rddFile = sc.textile("E:\\Spark\\wordcount.txt", 4)

		=> default number of partitions is as per the value of "sc.defaultMinPartitions"
		=> "sc.defaultMinPartitions" = 2 if core >= 2, else 1


	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize(<some python collection>, n)
		rdd1 = sc.parallelize([3,2,1,4,2,3,5,4,6,7,8,9,0,8,9,0,8,6,4,1,2,3,2,4,6,5,7,8,9,6,7,5], 3)

		=> default number of partitions is as per the value of "sc.defaultParallelism"
		=> "sc.defaultMinPartitions" = number of cores allocated to your application


	3. Creating an RDD by applying transformations on exiting RDD

		rddWords = rddFile.flatMap(lambda x: x.split())

		-> By default the output RDD will have the same number of partitions as input partitions


  RDD Operations
  --------------
	Two operations

	1. Transformations
		-> Creates an RDD Lineage DAG
		-> Does not cause execution

	2. Actions
		-> Executes an RDD and launchs a Job in the cluster
		-> Produces some output
		-> Converts logical plan (DAG) to a physical plan for execution. 


  RDD Lineage DAG   
  ---------------
   (DAG: Directed Acyclic Graph)
   -> Represents a logical plan on how to execute the RDD.
   -> Lineage DAG stores the heirarchy of dependencies all the way from the very first RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rddFile => (4) rddFile -> sc.textFile on wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG of rddWords => (4) rddWords -> rddFile.flatMap -> sc.textFile on wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG of rddWords => (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG of rddWc => (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
  


  RDD Execution Flow
  ------------------

	Application (ex: PySpark Shell or You App in Spyder/PyCharm)
	|
	|--> Jobs (Each action command launches one job)
		|
		|--> Stages (one or more stages per Job)
			|
			|--> Tasks (one task per partition of the RDD in that stage)
				|
				|--> Transformations (one or more transformations per task)	




  Spark SQL
  ---------

   -> Spark's structured / semi-structured data processing API
   
	File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format  : RDBMS, NoSQL
	Hive Format  : Hive Warehouse

   -> SparkSession

	-> Starting point of execution
	-> Represents a session inside an application
	-> Each session can have its own configuration. 

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate()  

   -> DataFrames

		Data => Rows
		Schema => StructType

			StructType(
			     [
				StructField('age', LongType(), True), 
				StructField('gender', StringType(), True), 
				StructField('name', StringType(), True), 
				StructField('phone', StringType(), True), 
				StructField('userid', LongType(), True)	
			     ]
			)


   Spark SQL Basic Steps
   ---------------------

    1. Reading/Loading data into a DF

		df1 = spark.read.format("json").load(input_path)
		df1 = spark.read.load(input_path, format="json")
		df1 = spark.read.json(input_path)


    2. Transforming a DF 

		Using DF Transformation methods
		-------------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)
            
		display(df2)


		Using SQL
		-------------------------------
		df1.createOrReplaceTempView("users")

		df3 = spark.sql("""SELECT age, count(1) as count
        			   FROM users
        			   WHERE age IS NOT NULL
        			   GROUP BY gender, age
        			   ORDER BY gender, age
        			   LIMIT 4""")
		display(df3)



    3. Writing/Saving the DF data into some destination

		df2.write.format("json").save(output_path)
		df2.write.save(output_path, format="json")
		df2.write.json(output_path)

		df2.write.mode("overwrite").json(output_path)

 
  Save Modes
  ----------
	- Define what should happen when you are writing to an existing directory
  		- ErrorIfExists (default)
  		- Ignore
  		- Append   (appends additional files to the existing directory)
  		- Overwrite (overwrites old directory)


	df3.write.mode("append").format("json").save("dbfs:/FileStore/output/json")
	df3.write.mode("overwrite").format("json").save("dbfs:/FileStore/output/json")

	df3.write.format("json").save("dbfs:/FileStore/output/json", mode="append")


  DF Transformations
  ------------------

   1. select
	
		df2 = df1.select("ORIGIN_COUNTRY_NAME",
				"DEST_COUNTRY_NAME",
				"count")
		----------------------------------------
		df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
			expr("DEST_COUNTRY_NAME as destination"),
			expr("count").cast("int"),
			expr("count+10 as newCount"),
			expr("count > 100 as highFrequency"),
			expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME").alias("domestic"))


		df2.show()
		df2.printSchema()


  2. where / filter

	df3 = df2.where("count > 500 and domestic = false")
	df3 = df2.filter("count > 500 and domestic = false")
	df3 = df2.where( col("count") > 200 )

  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")
	df3 = df2.sort(desc("count"), asc("origin"))


  4. groupBy   => returns a 'pyspark.sql.group.GroupedData' object (not a DataFrame)
		  Apply aggregation methods to return a DataFrame


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"),2).alias("avg"))

	df3 = df2.groupBy("highFrequency", "domestic") \
        	.agg( 	expr("sum(count) as sum"),
              		expr("max(count) as max"),
              		expr("avg(count) as avg"))


  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
			"DEST_COUNTRY_NAME as destination",
			"count",
			"count+10 as newCount",
			"count > 100 as highFrequency",
			"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")
	df2.show()


  7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
		.withColumn("highFrequency", expr("count > 100")) \
		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
		.withColumn("Country", lit("India"))

	----------------------

	df4 = df3.withColumn("ageGroup", when( col("age") < 13, "child")
                                	.when( col("age") < 20, "teenager")
                                	.when( col("age") < 60, "adult")
                                	.otherwise("senior"))

	df4 = df3.withColumn("ageGroup", when( expr("age < 13"), "child")
                                	.when( expr("age between 13 and 19"), "teenager")
                                	.when( expr("age < 60"), "adult")
                                	.otherwise("senior"))



  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")


  Working with different file formats
  -----------------------------------
  JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

  Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


  ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)


   CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")

   Text
	read
		df1 = spark.read.text(inputPath)
		=> df1 will have one columns called 'value' of 'string' type

	write
		df1.write.text(outputPath)
		=> You can only save a DF with a single text column in 'text' format.




  Creating an RDD from DataFrame
  ------------------------------
   
        rdd1 = df1.rdd


  Creating a DataFrame from programmatic data
  -------------------------------------------
   
     	listUsers = [(1, "Raju", 5),
		(2, "Ramesh", 15),
		(3, "Rajesh", 18),
		(4, "Raghu", 35),
		(5, "Ramya", 25),
		(6, "Radhika", 35),
		(7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])


  Creating a DataFrame from RDD
  -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = rdd1.toDF(["id", "name", "age"])

	df1.show()
	df1.printSchema()


  Creating a DataFrame with custom/programmatic schema
  ----------------------------------------------------

	mySchema = "id INT, name STRING, age INT"
	df1 = spark.createDataFrame(listUsers, schema=mySchema)  
	-----------------------------------------------------
        filePath = "E:\\PySpark\data\\flight-data\\2015-summary-nh.csv"

	mySchema = "origin STRING, destination STRING, count INT"
	df1 = spark.read.csv(filePath, schema=mySchema) 
	-----------------------------------------------------
	mySchema = "ORIGIN_COUNTRY_NAME STRING, DEST_COUNTRY_NAME STRING, count INT"

	df1 = spark.read.json(filePath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()
	-----------------------------------------------------
	mySchema = StructType([
				 StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
				 StructField("DEST_COUNTRY_NAME", StringType(), True),
				 StructField("count", IntegerType(), True)
			   ])

	df1 = spark.read.schema(mySchema).json(filePath)
	df1.show(5)
	df1.printSchema()


























