
  Agenda - PySpark
  -----------------
  Spark - Basics & Architecture
  Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  Spark SQL
  Spark MLLib & Machine Learning
  Introduction to Spark Streaming

  
  Materials
  ----------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   -> GibHub: https://github.com/ykanakaraju/pyspark

 
   
  Spark
  ------
    => Spark is a open source distributed computing framework to process huge amount of data
       using in-memory computation on a cluster using simple programming constructs. 

       	-> distributed computing framework
	-> in-memory processing
	-> runs on the cluster

    => Supports various programming languages (Spark is a polyglot)	
	-> Scala (native), Java, Python, R

    => Spark is a unified framework.

	 -> Provides a consistent set of APIs for performing processsing for different analytical workloads
	    running on the same computing engine. 

	 -> Components of Spark
	      -> Batch Processing		 : Spark Core, Spark SQL
	      -> Real-time processing		 : Spark Streaming, Structured Streaming
	      -> Predictive analytics (using ML) : Spark MLlib
	      -> Graph Parallel Computations	 : Spark GraphX


    => Spark applications can run on multiple cluster managers
	 -> Spark Standalone, YARN, Mesos, Kubernetes, local


    => Spark Layers EcoSystem	
	=> Programming Lang:	Scala, Python, Java, R
	=> Spark High-Level:	Spark SQL, Spark MLlib, Spark Streaming, Spark Graphx
	=> Spark Low Level:	Spark Core (RDD)
	=> Cluster Managers:	Standalone, YARN, Mesos, Kubernetes
	=> Storage: 		Linux, HDFS, KafKa, NoSQL, RDDMS


    Spark Architecture
    ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


    Getting started with Spark
    --------------------------

     1. Working in your vLab
	-> Follow the instructions given in the email. 
	-> Click on "Bigdata Enbl" icon
	-> This will send you to a Windows server.
	-> Double Click on "CentOS7" icon and login to 
		-> Type your password (refer to README.txt file)
	-> This will connect to CentOS 7 lab

	PySpark Shell
		-> Open a terminal
		-> Type "pyspark"
		-> Open the WebUI:
			-> Open FireFox browser (Applications menu -> Firfox)
			-> Type "localhost:4040" in address bar. 

       Working with JuPyter Notebook
		-> Open a terminal
		-> type :  "jupyter notebook --allow-root"

   2. Setting up PySpark in our personal machine
	-> Make sure you have "Anaconda Navigator" installen on your machine.
		URL: https://www.anaconda.com/products/individual

	-> One option: You can pip install spark (pip install pyspark)

	-> Follow the instruction given the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


   3. Signup to Databricks Comminity Edition (free account)
	URL: https://databricks.com/try-databricks

	Read through "Guide: Quickstart tutorial" article.


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    -> Fundamental data abstraction of Spark Core API

    -> RDD is a distributed collection of in-memory partitions
	-> Partitions are a collection of objects.

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution of the RDDs
	-> Action commands trigger execution.

    -> RDDs are resilient
	-> RDD partitions can be created on-the-fly (at runtime) when any RDD partition is missing.
	   (because of node failures are because memory leaks, evictions etc)


   How to create RDDs?
   ------------------
	Three ways:

	1. From external data files:

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic python collections

		rdd1 = sc.parallelize( range(1, 100), 4)
		rdd1 = sc.parallelize( [1,2,1,2,4,5,6,4,6,7,7,9], 4)

        3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*2)	


   What can you do with an RDD?
   ----------------------------

	-> Transformations
		-> Transformations return an RDD
		-> Transformations does not cause execution of the RDDs
			-> They only create Lineage DAGs @ the driver side. 
		
	-> Actions
		-> Trigger execution of the RDD
		-> Produces some output
		-> Converts the logical Plan adnd physical execution plan and several tasks
		   are launched on the cluster

   RDD Lineage
   ------------
    RDD LIneage is a logical plan maintained for every RDD by the driver
    RDD Lineage DAGs are created by transformations
    RDD Lineage all the dependencies of the RDD all the way from the very first RDD.

	rddFile = sc.textFile(file, 4)
	Lineage DAG: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile
	
	rdd4.collect()


  RDD Persistence
  ---------------

	rdd1 = sc.paralleize(range(1, 100), 3)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StoargeLevel.MEMORY_ONLY )  ---> instruction to Spark to persist rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	    rdd6 lineage => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.paralleize(range(1, 100), 3)
	        [ sc.paralleize -> t3 -> t5 -> t6 ] -> rdd 6 ==> collect

        rdd7.collect()
	    rdd7lineage => rdd7 -> rdd6.t7
		[ t7 ] -> rdd 7 ==> collect

	rdd6.unpersist()


        Storage Levels
        --------------
	MEMORY_ONLY	   -> (default) memory serilized 1x replication
	MEMORY_AND_DISK    -> disk memory serilized 1x replication
	DISK_ONLY	
	MEMORY_ONLY_2	   -> memory serilized 1x replication
			    (2 copies of the partitions are persisted on 2 different executor)
	MEMORY_AND_DISK_2  -> disk memory serilized 2x replication


     Commands
     ---------

	rdd1.persist() or rdd1.cache()   => MEMORY_ONLY	
	rdd1.persist( StoargeLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  --------------------		
	-> The output of any transformation is an RDD
	-> The transformation create RDD Lineage DAG.   


   1. map		P: U -> V
			Object to Object transformations
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).collect()


   2. filter		P: U -> Boolean
			Filter the data based on the function
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8 ).collect()


   3. glom		P: None
			Created on list object per partition with all the elements of the partitions

		rdd1			   rdd2 = rdd1.glom()

		P0: 2,1,3,2,4,5 -> glom -> P0: [2,1,3,2,4,5]
		P1: 5,2,3,4,7,9 -> glom -> P1: [5,2,3,4,7,9]
		P2: 5,0,8,9,0,3 -> glom -> P2: [5,0,8,9,0,3]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)


   4. flapMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

 
   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1		   rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )

		P0: 7, 2, 4, 3, 2, 5, 6, 7 -> mapPartitions -> P0: 36
		P1: 8, 9, 0, 5, 6, 7, 8, 9 -> mapPartitions -> P1: 52
		P2: 2, 4, 2, 5, 7, 8, 9, 6 -> mapPartitions -> P2: 43

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions, but we get the partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()


   7. distinct			P: None, Optional: numPartitions
				Return an RDD with distinct objects of the input RDD.

   Types of RDDs
   -------------
	Generic RDDs :  RDD[U]
	Pair RDDs : RDD[(U, V)]

   8. mapValues			P: U -> V
				Applied only on Pair RDD
				The function is applied only on the 'value; of the key-value pairs

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the objects of the RDD based on the output of the function.

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 5).collect()


   10. groupBy			P: U -> V, Optional: numPartitions
				Objects of the RDD are grouped (as (k,v) pairs) based on the function output.

				RDD[U].groupBy(U -> V)  => RDD[(V, ResultIterable[U])]


   11. randomSplit		P: list of ratios (ex: [0.4, 0.4, 0.3])
				Returns a list of RDDs split randomly in the specified ratios. 

			rddList = rdd1.randomSplit([0.6, 0.4])
			rddList = rdd1.randomSplit([0.6, 0.4], 456)    # 456 is a seed to fix the randomness.


   12. repartition		P: numPartitions
				Used to increase or decrease the number of output partitions
				Global shuffle

			rdd2 = rdd1.repartition(10)   // rdd2 will have 10 partitions

   13. coalesce			P: numPartitions
				Used to only decrease the number of output partitions
				Partition merging

			rdd2 = rdd1.repartition(5)	// rdd2 will have 5 partitions
	
        Recommendations
        ----------------
	-> The size of the partition should be around 128 MB
	-> The number of partitions should be a multiple of number of CPU cores alloted to your application.
	-> The number of cores in each executor should be 5.


  14. union, intersection, subtract, cartesian

	rdd3 = rdd1.union(rdd2)	
	

  15. partitionBy		P: numPartitions, Optional: partitioning-function  (U -> Int)
				Applied only on pair RDDs
				This used to control which elements goes to which based on the 'key' applied on
				a partitioning-function


transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]


def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;   
    
    
rdd1 = sc.parallelize(transactions, 3) \
         .map(lambda d: (d['city'], d)) \
         .partitionBy(4, custom_partitioner)


   ..ByKey transformations 
   -----------------------
    -> Wide transformations
    -> Applied only on Pair RDD
 
	
    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				RDD is sorted based on the key

		rdd2.sortByKey(False, 2).glom().collect()


    16. groupByKey		P: None, Optional: numPartitions
				Groups the RDD based on the key.
				Returns a Pair RDD, where key is a unique key and value is the grouped (ResultIterable) values that
				have the same key. 
				
				Note: Avaoid groupByKey if possible. 
					
				RDD[(U, V)].groupBy() => RDD[(U, ResultIterable[V])]

		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
             			.flatMap(lambda x: x.split(" ")) \
             			.map(lambda x: (x, 1)) \
             			.groupByKey() \
             			.mapValues(len) \
             			.coalesce(1)

    17. reduceByKey		P: (U, U) -> U,  Optional: numPartitions
				Will reduce all the values of each unique key with in each partition, 
				and then across partitions by iterativly applying the reduce function.	

		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
             			.flatMap(lambda x: x.split(" ")) \
             			.map(lambda x: (x, 1)) \
             			.reduceByKey(lambda x, y: x + y) \
             			.sortBy(lambda x: x[1], False, 1)	

    18. aggregateByKey 		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                        		lambda z, v: (z[0] + v, z[1] + 1),
                        		lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                			.mapValues(lambda x: x[0]/x[1])	


    
		student_rdd = sc.parallelize([
  				("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  				("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  				("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
 				("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  				("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  				("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  				("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey((0,0),
                                		lambda z,v: (z[0]+v, z[1]+1),
                                		lambda a,b:(a[0]+b[0], a[1]+b[1])) \
                		.mapValues(lambda p: p[0]/p[1])


    19. joins 		=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			   Performed on two pair RDDs

			   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

			join = names1.join(names2)   #inner Join
			leftOuterJoin = names1.leftOuterJoin(names2)
			rightOuterJoin = names1.rightOuterJoin(names2)
			fullOuterJoin = names1.fullOuterJoin(names2)

    20. cogroup		   => Is used to join RDDs with duplicate keys and you need unique joined keys
			      in the output
			   => groupByKey on each RDD -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4, 7]) (key4, [17])

		=> (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4, 7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile
		rddOutput.saveAsTextFile("E:\\PySpark\\output\\rddOutput")


   4. reduce		P: (U, U) -> U
			Reduces the entire RDD into one final value of the same type by iterativly
			applying a reduce function with in partitions and then across partitions

		rdd1

		P0: 7, 2, 4, 3, 2, 5, 6, 7 -> reduce -> -22 -> 53
		P1: 8, 9, 0, 5, 6, 7, 8, 9 -> reduce -> -36
		P2: 2, 4, 2, 5, 7, 8, 9, 6 -> reduce -> -39
		
		rdd1.reduce(lambda x, y : x - y )

   5. aggregate

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )

	rdd2.aggregate( ("", 0, 0), 
			lambda z, v: (z[0] + "," + str(v[0]), z[1]+v[1], z[2] if (z[2] > v[1]) else v[1]), 
			lambda a, b: (a[0] + "," + b[0], a[1] + b[1], a[2] if (a[2] > b[2]) else b[2]) 
		      )


   6. take
		rddWords.take(20)

   7. takeOrdered

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, lambda x: len(x))

   8. takeSample   
		
		rdd1.takeSample(True, 15)
		rdd1.takeSample(True, 150, 567) 
		rdd1.takeSample(False, 15)
		rdd1.takeSample(False, 15, 567)

   9. countByValue

   10. countByKey

   11. first

   12. foreach  => Applies some function on all the objects of the RDD.
		   Does not return any value

   13. saveAsSequenceFile


    Use-Case
    ---------

    dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
	
    From cars.tsv dataset, find out the average weight of each make of American origin
    Arrange the data in the DESC order of average weight
    Save the output rdd in a single text file.

    => Try this 


 
   Shared Variables
   ----------------

    Shared Variables: Accumulator, Broadcast

    Closure : A closure is all the code (all variables and methods) that must be visible for a task
	      to perform its computations on the RDD. 

    -> A closure a serialized and a copy is sent to every executor. 

    Limitation: Closure variables (local variables inside a task) can not be used to implement
		global counter.

	c = 0  

	def isPrime( n ) :
		if n is prime return 1
		else 
		if n is not prime return 0		

	def f1( n ) :
		global c
		if (isPrime(n) == 1) c = c + 1
		n * 2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c )       // 0


   1. Accumulator Variable

	-> Is a shared variable
	-> Not part of closure (and hence not a local variable)
	-> Manintained by the driver
	-> All tasks can write to this variable.
	-> Is used to implement global counters.

	=> Tasks can not red the value of accumulator variable
	   Tasks can only write to it. 
    

	c = sc.accumulator(0) 

	def isPrime( n ) :
		if n is prime return 1
		else 
		if n is not prime return 0		

	def f1( n ) :
		global c
		if (isPrime(n) == 1) c.add(1)		
		n * 2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c )


  2. Broadcast Variable

	=> A large immutable collection when required inside a closure method can be converted
	   into a broadcast variable (instead of a local variable) to save lot of execution
	   memory.

	=> A single copy of the broadcast variable to sent to all executor node
	   (one copy per executor, not per task)

        => A broadcats variable can read by all the tasks running in the executor

  
	bc = sc.broadcast({1:'a', 2:'b', 3:'c', 4:'d', 5:'e', 6:'f', ....})   // 100 MB
   
	def f1( n ) :
	   global bc
	   return bc.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5, ...], 4)

	rdd2 = rdd1.map( f1 )

	rdd2.collect()


   spark-submit
   =============

    => Is a single commandthat is used to submit any spark application (scala, java, python, R)
       to any cluster manager (local, standalone, yarn, mesos, kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]


	=> spark-submit E:\PySpark\spark_core\examples\wordcount.py 

	=> spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executor-memory 10G
		--executor-cores 5
		--num-executor 20
		E:\PySpark\spark_core\examples\wordcount.py [app arguments] 


	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wordcountout 2
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


   ===============================================
        Spark SQL (pyspark.sql)
   ===============================================

    => Spark's structured data processing API

	Structured File Formats:  Parquet (default), ORC, JSON, CSV (delimited text)
	JDBC Format: RDBMS, NoSQL
	Hive

   => High-level API built on top of Spark Core API

   => SparkSession
	-> Starting point of execution
	-> Introduced in Spark 2.0 onwards 
	-> Represents a user-session inside an application
		-> An application (SparkContext) can have multiple user session (SparkSession)

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local") \
        	.getOrCreate()   

   => DataFrame (DF)
	-> Data abstraction of Spark SQL
	
	-> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	-> A DF is a collection of "Row" object  (pyspark.sql.Row)

	DataFrame:
		-> Data : Collection of 'Row' objects
		-> Schema : StructType objects 

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)

	
  Steps in creating  a Spark SQL application
  ------------------------------------------

	1.  Read/load data from some data source into a DataFrame

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()


	2. Apply tranformations on the DataFrame using DF API methods or using SQL

		Using DataFrame API
		--------------------
		df2 = df1.select("userid", "name", "age", "gender") \
         		.where("age is not null") \
         		.orderBy("gender", "age") \
         		.groupBy("age").count() \
         		.limit(4)

		Using SQL
                ---------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
         		from users
         		where age is not null
         		group by age 
         		order by age
         		limit 4"""

		df4 = spark.sql(qry)
		df4.show()

	
	3. Write/save the data of the DF into some external directory / database.

		df4.write.format("json").save(outputPath)
		df4.write.json(outputPath)


  Save Modes (Write Modes)
  ------------------------
   Define the behaviour when you are writing to an existing directory.	

	-> ignore
	-> append
	-> overwrite

	df4.write.json(outputPath, mode="overwrite")
	df4.write.mode("overwrite").json(outputPath)


  LocalTempViews & GlobalTempViews
  --------------------------------

	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"
	

  DataFrame Transformations
  --------------------------

   1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

   2. where / filter
	
		df3 = df2.where("count > 100 and domestic = false")
		df3 = df2.where( col("count") > 100 )

		df3 = df2.filter( col("count") > 100 )

   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.orderBy(desc("count"), asc("origin"))

		df3 = df2.sort(desc("count"), asc("origin"))
	

   4. groupBy   => returns  a "GroupedData" object on which you have to invoke a aggrgation method

		df3 = df2.groupBy("highFrequency", "domestic").count()
		df3 = df2.groupBy("highFrequency", "domestic").avg("count")
		df3 = df2.groupBy("highFrequency", "domestic").sum("count")
		df3 = df2.groupBy("highFrequency", "domestic").max("count")

		df3 = df2.groupBy("highFrequency", "domestic") \
        		.agg(   count("count").alias("count"),
              			sum("count").alias("sum"),
              			avg("count").alias("avg"),
              			max("count").alias("max") )

   5. limit
		df2 = df1.limit(20)

   6. selectExpr

	df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
                 expr("DEST_COUNTRY_NAME as destination"),
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

	is same as

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 "DEST_COUNTRY_NAME as destination",
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

   7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) 

		----------------------
		listUsers = [(1, "Raju", 5),
             		(2, "Ramesh", 15),
             		(3, "Rajesh", 18),
             		(4, "Raghu", 35),
             		(5, "Ramya", 25),
             		(6, "Radhika", 35),
             		(7, "Ravi", 70)]


		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])

		users2 = userDf.withColumn("ageGroup", 
                            when(userDf["age"] < 12, "child")
                           .when(userDf["age"] < 20, "teenager")
                           .when(userDf["age"] < 60, "adult")
                           .otherwise("senior"))
		---------------------------------------

   8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


   9. union, intersect, subtract

		df5 = df3.union(df4)
		df5 = df3.intersect(df4)	# numPartitions = 200
		df5 = df3.subtract(df4)		# numPartitions = 200

   10. sample

		df3 = df1.sample(True, 0.5)
		df3 = df1.sample(True, 1.5, 7565)

		df3 = df1.sample(False, 0.5)
		df3 = df1.sample(False, 1.5, 7565)   => Invalid, fraction must be in the range [0,1]

   11. distinct

		df1.select("DEST_COUNTRY_NAME").distinct().show()
		df1.select("DEST_COUNTRY_NAME").distinct().count()

   12. randomSplit

		df10, df11 = df1.randomSplit([0.5, 0.5], 467)

		df10.count()
		df11.count()

   13. repartition

	df1.rdd.getNumPartitions()

	df2 = df1.repartition(4)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(2)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition(4, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions() 

	df4 = df2.repartition(col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()  
	
		=> Here the number of output partitions is decided based on the value of 
		  "spark.sql.shuffle.partitions" config property, whose default value is 200

		    spark.conf.set("spark.sql.shuffle.partitions", "5")

   14. coalesce

	df3 = df2.coalesce(2)
	df3.rdd.getNumPartitions()


   15. drop

		df2 = df1.drop("col1", col2")


   Working with different file formats
   -----------------------------------

   JSON
	
	read:
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	write:
		df4.write.format("json").save(outputPath)
		df4.write.json(outputPath)
		df4.write.json(outputPath, mode="overwrite")

   Parquet
	
	read:
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.parquet(inputPath)

	write:
		df4.write.format("parquet").save(outputPath)
		df4.write.parquet(outputPath)
		df4.write.parquet(outputPath, mode="overwrite")

   ORC
	
	read:
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.orc(inputPath)

	write:
		df4.write.format("orc").save(outputPath)
		df4.write.orc(outputPath)
		df4.write.orc(outputPath, mode="overwrite")


   CSV (delimited text file)

	read:

		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath) 
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True) 
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True) 
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|") 

	write:

		df2.write.csv(outputPath, header=True, mode="overwrite")
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")
		

 
   Creating an RDD from DataFrame
   ------------------------------

	rdd1 = df1.rdd
	rdd1.take(5)


   Creating a DataFrame from programmatic data
   -------------------------------------------

	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Mahesh", 25),
             (4, "Rajesh", 15),
             (5, "Suresh", 25),
             (6, "Aditya", 35),
             (7, "Pranav", 45)]

	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

   
   Creating a DataFrame from RDD
   -----------------------------

	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

	df1.show()


   Creating a DataFrame with programmatic schema
   ---------------------------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)

	----------------------------------------------------

	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json" 

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)

 
   Joins
   -----

    Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti

     left-semi
     ---------
	Similar to inner join but the data comes only from the left table. 

	Equivalent to the following sub-query:	
	  => select * from emp where deptid IN (select id from dept)

     left-anti
     ---------	
       Equivalent to the following sub-query:	
	  => select * from emp where deptid NOT IN (select id from dept)  

     SQL Way
     -------
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*
         	from emp left anti join dept on
         	emp.deptid = dept.id"""
         
	joinedDf = spark.sql(qry)  

	joinedDf.show()
 
     DF API way
     ----------
	Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")

     Enforcing a broadcast join
     --------------------------
	joinedDf = employee.join(broadcast(department), joinCol, "left_anti")
   	

    Use-Case
    --------
      From movies.csv and ratings.csv datasets, fetch the top 10 movies with higheset average rating.
	-> Consider only those movies which are rated by atleast 30 users
	-> Data: movieId, title, totalRatings, averageRating
	-> Sort the data in the DESC order of averageRating
	-> Save the content as a single CSV file with pipe-separated-values with header.

	Datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

	=> Try it...

  
   Working with JDBC Format (MySQL)
   ---------------------------------
  import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


   Working with Hive
   -----------------

      Hive is a data warehousing platform built on top of Hadoop
      Hive is abstraction buit on top of MapReduce
      Hive uses 'HQL (Hive Query Language)' which is a dialect of SQL

      warehouse => directory where hive stores all its data file.
		   (default:  spark-warehouse)	

     -------------------------------

import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()

---------------------------------
      
 ==============================================
      Machine Learning & Spark MLLib
 ==============================================

   ML Model =>  Learned Entity
		Learns from historic data
		Trained by 'Algorithms' 


   Terminology
   -----------

    1. Training data

    2. Features	   : inputs, dimensions

    3. Label	   : output

    4. Algorithm   : An iterative mathematical computation that establishes a relation between inputs and output
	             with a goal to minimize a loss function.  
		     The relation between the label and features is returned as a Model.

    5. ML Model	   : Output of algorithm
		     Can predict the label when new features are given. 
 
    6. Error	   : Difference between the actual label and prediction for a given data point

    7. Loss        : Error of the entire dataset computed using some loss function

    
        X	Y	Z(lbl)  Prediction Error
	-----------------------------------------
	1000	1000	3100	3000	   100
	1000	500	2550	2500	    50
	500	1000	1950	2000	   -50
	1100	600	2740    2800	   -60
	1200	500	        2700
	---------------------------------------
			     Loss:  260/5 = 65 

        Model 1  =>  Z = 2X + Y		  Loss: 65
	Model 2  =>  Z = 2.1X + Y + 10	  Loss: 60
	Model 3  => 			  Loss: 54
	Model 4				  Loss: 55
	Model 5				  Loss: 55.5
	Model 6				  Loss: 55.45
	Model 7				  Loss: 55.65


    Steps in an ML project
    ----------------------

      1. Data Collection
	 
           output: Raw training data

      2. Data Preparation

	    output: Prepared Data  (we create a 'Feature Vector')

	     -> All the data must be in numerical type  (Double/Float)
	     -> There should be no nulls or empty strings
	     -> Remove the outliers.	
	     -> Identify the features

	     EDA : Exploratory Data Analysis
	     FE  : Feature Engineering

     3. Train the model using an algorithm
	   output : Unevaluated Model

     4. Evaluate the model
	   output: Evaluated Model

     5. Deploy the model


  Types of Machine Learning
  --------------------------

    1. Supervised Learning

	=> Training data is labelled (Features and Label)

	1.1 Classification
		-> Label is one of two/few fixed values.
		-> Ex: Email Spam prediction,  Survival Prediction

        1.2 Regression
		-> Label is a continuous value
		-> Ex: House price prediction 

    2. Unsupervised Learning

	=> Training data is unlabelled (only features and no label)
	
	2.1  Clustering ( & Collaborative Filtering )
        2.2  Dimesionality Reduction

    3. Reinforcement Learning

	 -> Semi supervised learning



   Spark MLLib
   -----------

	Some popular ML libraries => Spark MLLib, SciKitLearn, SAS, PyTorch
		     DL libraries => TensorFlow, Keras, PyTorch

	
	Two APIs are available:
	----------------------------
		-> pyspark.mllib   : Legacy (based on RDD)
		-> pyspark.ml	   : Current (based on DataFrames)	


        Components of Spark MLLib
        -------------------------

	1. Features Tools    	 : Feature Extractors, Feature Transformers, Feature Selectors
	2. ML Algorithms     	 : Classification, Regression, Clustering & Collaborative Filtering
	3. Pipeline	     	 : Define workflows
	4. Evaluation Toools 	 : To evaluate ML model such as MultiClassClassificationEvaluator
	5. Model Selection Tools : Cross-Validation, Train-Validation Split 
	6. Utilities		 : Stats, LinAlg


       Bulding Block of Spark MLLib
       ----------------------------
	
	1. DataFrame

	2. Feature Vector   : Is a vector object containing all the features in numeric type (Double type)

		Two formats:

		-> Dense Vector  :  Vectors.dense(0,0,0,2,6,0,0,0,5,0,0,7,0,0,0)
		-> Sparse Vector :  Vector.sparse(15, [3,4,8,11], [2,6,5,7])

       3. Estimator
		input  : DataFrame
		output : Model
		method : fit

		<model> = <estimator>.fit( <dataFrame> )
		Ex: All ML Algorithms, Several Feature tools (StringIndexer, OneHotEncoder)	


       4. Transformer
		input: DataFrame
		output: DataFrame   (output DF will have or more additional columns in addition to all the input cols)
		method: transform

		<outputDf> = <transformer>.transform( <inputDf> )

		Ex: All ML models, Several Feature tools (Tokenizer, HashingTF)


      5. Pipeline   -> Contains stages of transformers and estimators   
		    -> Pipeline is an estimator

		pl = Pipeline(stages = [T1, T2, T3, E4])
		plModel = pl.fit(df)

		df -> T1 -> df1 -> T2 -> df2 -> T3 -> df3 -> E4 -> plModel

		

    Mini Project : Titanic - Machine Learning from Disaster
    -------------------------------------------------------

     URL: https://www.kaggle.com/c/titanic
   

PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,"Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs.Braund, Mr. Owen  John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S

 	
	Label: 	   Survived
	Features:  Pclass,Sex,Age,SibSp,Parch,Fare,Embarked
		
		Numerical:  Pclass,Age,SibSp,Parch,Fare
		Categorical: Sex,Embarked
				=> StringIndexer, OneHotEncoder
			
 ==========================================================
     Introduction Spark Streaming
 ==========================================================

      Spark Streaming

	  => microbatch based processing
	  => Provides "seconds" scale latency.  (near-real-time processing)


      StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	-> Is a continuous flow of RDDs.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 

	


















