
  Agenda - PySpark
  -----------------
  Spark - Basics & Architecture
  Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  Spark SQL
  Spark MLLib & Machine Learning
  Introduction to Spark Streaming

  
  Materials
  ----------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   -> GibHub: https://github.com/ykanakaraju/pyspark

 
   
  Spark
  ------
    => Spark is a open source distributed computing framework to process huge amount of data
       using in-memory computation on a cluster using simple programming constructs. 

       	-> distributed computing framework
	-> in-memory processing
	-> runs on the cluster

    => Supports various programming languages (Spark is a polyglot)	
	-> Scala (native), Java, Python, R

    => Spark is a unified framework.

	 -> Provides a consistent set of APIs for performing processsing for different analytical workloads
	    running on the same computing engine. 

	 -> Components of Spark
	      -> Batch Processing		 : Spark Core, Spark SQL
	      -> Real-time processing		 : Spark Streaming, Structured Streaming
	      -> Predictive analytics (using ML) : Spark MLlib
	      -> Graph Parallel Computations	 : Spark GraphX


    => Spark applications can run on multiple cluster managers
	 -> Spark Standalone, YARN, Mesos, Kubernetes, local


    => Spark Layers EcoSystem	
	=> Programming Lang:	Scala, Python, Java, R
	=> Spark High-Level:	Spark SQL, Spark MLlib, Spark Streaming, Spark Graphx
	=> Spark Low Level:	Spark Core (RDD)
	=> Cluster Managers:	Standalone, YARN, Mesos, Kubernetes
	=> Storage: 		Linux, HDFS, KafKa, NoSQL, RDDMS


    Spark Architecture
    ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


    Getting started with Spark
    --------------------------

     1. Working in your vLab
	-> Follow the instructions given in the email. 
	-> Click on "Bigdata Enbl" icon
	-> This will send you to a Windows server.
	-> Double Click on "CentOS7" icon and login to 
		-> Type your password (refer to README.txt file)
	-> This will connect to CentOS 7 lab

	PySpark Shell
		-> Open a terminal
		-> Type "pyspark"
		-> Open the WebUI:
			-> Open FireFox browser (Applications menu -> Firfox)
			-> Type "localhost:4040" in address bar. 

       Working with JuPyter Notebook
		-> Open a terminal
		-> type :  "jupyter notebook --allow-root"

   2. Setting up PySpark in our personal machine
	-> Make sure you have "Anaconda Navigator" installen on your machine.
		URL: https://www.anaconda.com/products/individual

	-> One option: You can pip install spark (pip install pyspark)

	-> Follow the instruction given the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


   3. Signup to Databricks Comminity Edition (free account)
	URL: https://databricks.com/try-databricks

	Read through "Guide: Quickstart tutorial" article.


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    -> Fundamental data abstraction of Spark Core API

    -> RDD is a distributed collection of in-memory partitions
	-> Partitions are a collection of objects.

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution of the RDDs
	-> Action commands trigger execution.

    -> RDDs are resilient
	-> RDD partitions can be created on-the-fly (at runtime) when any RDD partition is missing.
	   (because of node failures are because memory leaks, evictions etc)


   How to create RDDs?
   ------------------
	Three ways:

	1. From external data files:

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic python collections

		rdd1 = sc.parallelize( range(1, 100), 4)
		rdd1 = sc.parallelize( [1,2,1,2,4,5,6,4,6,7,7,9], 4)

        3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*2)	


   What can you do with an RDD?
   ----------------------------

	-> Transformations
		-> Transformations return an RDD
		-> Transformations does not cause execution of the RDDs
			-> They only create Lineage DAGs @ the driver side. 
		
	-> Actions
		-> Trigger execution of the RDD
		-> Produces some output
		-> Converts the logical Plan adnd physical execution plan and several tasks
		   are launched on the cluster

   RDD Lineage
   ------------
    RDD LIneage is a logical plan maintained for every RDD by the driver
    RDD Lineage DAGs are created by transformations
    RDD Lineage all the dependencies of the RDD all the way from the very first RDD.

	rddFile = sc.textFile(file, 4)
	Lineage DAG: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile
	
	rdd4.collect()


  RDD Persistence
  ---------------

	rdd1 = sc.paralleize(range(1, 100), 3)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StoargeLevel.MEMORY_ONLY )  ---> instruction to Spark to persist rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	    rdd6 lineage => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.paralleize(range(1, 100), 3)
	        [ sc.paralleize -> t3 -> t5 -> t6 ] -> rdd 6 ==> collect

        rdd7.collect()
	    rdd7lineage => rdd7 -> rdd6.t7
		[ t7 ] -> rdd 7 ==> collect

	rdd6.unpersist()


        Storage Levels
        --------------
	MEMORY_ONLY	   -> (default) memory serilized 1x replication
	MEMORY_AND_DISK    -> disk memory serilized 1x replication
	DISK_ONLY	
	MEMORY_ONLY_2	   -> memory serilized 1x replication
			    (2 copies of the partitions are persisted on 2 different executor)
	MEMORY_AND_DISK_2  -> disk memory serilized 2x replication


     Commands
     ---------

	rdd1.persist() or rdd1.cache()   => MEMORY_ONLY	
	rdd1.persist( StoargeLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  --------------------		
	-> The output of any transformation is an RDD
	-> The transformation create RDD Lineage DAG.   


   1. map		P: U -> V
			Object to Object transformations
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).collect()


   2. filter		P: U -> Boolean
			Filter the data based on the function
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8 ).collect()


   3. glom		P: None
			Created on list object per partition with all the elements of the partitions

		rdd1			   rdd2 = rdd1.glom()

		P0: 2,1,3,2,4,5 -> glom -> P0: [2,1,3,2,4,5]
		P1: 5,2,3,4,7,9 -> glom -> P1: [5,2,3,4,7,9]
		P2: 5,0,8,9,0,3 -> glom -> P2: [5,0,8,9,0,3]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)


   4. flapMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

 
   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1		   rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )

		P0: 7, 2, 4, 3, 2, 5, 6, 7 -> mapPartitions -> P0: 36
		P1: 8, 9, 0, 5, 6, 7, 8, 9 -> mapPartitions -> P1: 52
		P2: 2, 4, 2, 5, 7, 8, 9, 6 -> mapPartitions -> P2: 43

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions, but we get the partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()


   7. distinct			P: None, Optional: numPartitions
				Return an RDD with distinct objects of the input RDD.

   Types of RDDs
   -------------
	Generic RDDs :  RDD[U]
	Pair RDDs : RDD[(U, V)]

   8. mapValues			P: U -> V
				Applied only on Pair RDD
				The function is applied only on the 'value; of the key-value pairs

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the objects of the RDD based on the output of the function.

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 5).collect()


   10. groupBy			P: U -> V, Optional: numPartitions
				Objects of the RDD are grouped (as (k,v) pairs) based on the function output.

				RDD[U].groupBy(U -> V)  => RDD[(V, ResultIterable[U])]


   11. randomSplit		P: list of ratios (ex: [0.4, 0.4, 0.3])
				Returns a list of RDDs split randomly in the specified ratios. 

			rddList = rdd1.randomSplit([0.6, 0.4])
			rddList = rdd1.randomSplit([0.6, 0.4], 456)    # 456 is a seed to fix the randomness.


   12. repartition		P: numPartitions
				Used to increase or decrease the number of output partitions
				Global shuffle

			rdd2 = rdd1.repartition(10)   // rdd2 will have 10 partitions

   13. coalesce			P: numPartitions
				Used to only decrease the number of output partitions
				Partition merging

			rdd2 = rdd1.repartition(5)	// rdd2 will have 5 partitions
	
        Recommendations
        ----------------
	-> The size of the partition should be around 128 MB
	-> The number of partitions should be a multiple of number of CPU cores alloted to your application.
	-> The number of cores in each executor should be 5.


  14. union, intersection, subtract, cartesian

	rdd3 = rdd1.union(rdd2)	
	

  15. partitionBy		P: numPartitions, Optional: partitioning-function  (U -> Int)
				Applied only on pair RDDs
				This used to control which elements goes to which based on the 'key' applied on
				a partitioning-function


transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]


def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;   
    
    
rdd1 = sc.parallelize(transactions, 3) \
         .map(lambda d: (d['city'], d)) \
         .partitionBy(4, custom_partitioner)


   ..ByKey transformations 
   -----------------------
    -> Wide transformations
    -> Applied only on Pair RDD
 
	
    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				RDD is sorted based on the key

		rdd2.sortByKey(False, 2).glom().collect()


    16. groupByKey		P: None, Optional: numPartitions
				Groups the RDD based on the key.
				Returns a Pair RDD, where key is a unique key and value is the grouped (ResultIterable) values that
				have the same key. 
				
				Note: Avaoid groupByKey if possible. 
					
				RDD[(U, V)].groupBy() => RDD[(U, ResultIterable[V])]

		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
             			.flatMap(lambda x: x.split(" ")) \
             			.map(lambda x: (x, 1)) \
             			.groupByKey() \
             			.mapValues(len) \
             			.coalesce(1)

    17. reduceByKey		P: (U, U) -> U,  Optional: numPartitions
				Will reduce all the values of each unique key with in each partition, 
				and then across partitions by iterativly applying the reduce function.	

		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
             			.flatMap(lambda x: x.split(" ")) \
             			.map(lambda x: (x, 1)) \
             			.reduceByKey(lambda x, y: x + y) \
             			.sortBy(lambda x: x[1], False, 1)	

    18. aggregateByKey 		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                        		lambda z, v: (z[0] + v, z[1] + 1),
                        		lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                			.mapValues(lambda x: x[0]/x[1])	


    
		student_rdd = sc.parallelize([
  				("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  				("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  				("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
 				("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  				("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  				("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  				("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey((0,0),
                                		lambda z,v: (z[0]+v, z[1]+1),
                                		lambda a,b:(a[0]+b[0], a[1]+b[1])) \
                		.mapValues(lambda p: p[0]/p[1])


    19. joins 		=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			   Performed on two pair RDDs

			   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

			join = names1.join(names2)   #inner Join
			leftOuterJoin = names1.leftOuterJoin(names2)
			rightOuterJoin = names1.rightOuterJoin(names2)
			fullOuterJoin = names1.fullOuterJoin(names2)

    20. cogroup		   => Is used to join RDDs with duplicate keys and you need unique joined keys
			      in the output
			   => groupByKey on each RDD -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4, 7]) (key4, [17])

		=> (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4, 7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile
		rddOutput.saveAsTextFile("E:\\PySpark\\output\\rddOutput")


   4. reduce		P: (U, U) -> U
			Reduces the entire RDD into one final value of the same type by iterativly
			applying a reduce function with in partitions and then across partitions

		rdd1

		P0: 7, 2, 4, 3, 2, 5, 6, 7 -> reduce -> -22 -> 53
		P1: 8, 9, 0, 5, 6, 7, 8, 9 -> reduce -> -36
		P2: 2, 4, 2, 5, 7, 8, 9, 6 -> reduce -> -39
		
		rdd1.reduce(lambda x, y : x - y )

   5. aggregate

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )

	rdd2.aggregate( ("", 0, 0), 
			lambda z, v: (z[0] + "," + str(v[0]), z[1]+v[1], z[2] if (z[2] > v[1]) else v[1]), 
			lambda a, b: (a[0] + "," + b[0], a[1] + b[1], a[2] if (a[2] > b[2]) else b[2]) 
		      )


   6. take
		rddWords.take(20)

   7. takeOrdered

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, lambda x: len(x))

   8. takeSample   
		
		rdd1.takeSample(True, 15)
		rdd1.takeSample(True, 150, 567) 
		rdd1.takeSample(False, 15)
		rdd1.takeSample(False, 15, 567)

   9. countByValue

   10. countByKey

   11. first

   12. foreach  => Applies some function on all the objects of the RDD.
		   Does not return any value

   13. saveAsSequenceFile


    Use-Case
    ---------

		















 
 


