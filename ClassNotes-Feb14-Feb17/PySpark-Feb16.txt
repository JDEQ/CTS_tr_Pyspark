
  Agenda - PySpark
  -----------------
  Spark - Basics & Architecture
  Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  Spark SQL
  Spark MLLib & Machine Learning
  Introduction to Spark Streaming

  
  Materials
  ----------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   -> GibHub: https://github.com/ykanakaraju/pyspark

 
   
  Spark
  ------
    => Spark is a open source distributed computing framework to process huge amount of data
       using in-memory computation on a cluster using simple programming constructs. 

       	-> distributed computing framework
	-> in-memory processing
	-> runs on the cluster

    => Supports various programming languages (Spark is a polyglot)	
	-> Scala (native), Java, Python, R

    => Spark is a unified framework.

	 -> Provides a consistent set of APIs for performing processsing for different analytical workloads
	    running on the same computing engine. 

	 -> Components of Spark
	      -> Batch Processing		 : Spark Core, Spark SQL
	      -> Real-time processing		 : Spark Streaming, Structured Streaming
	      -> Predictive analytics (using ML) : Spark MLlib
	      -> Graph Parallel Computations	 : Spark GraphX


    => Spark applications can run on multiple cluster managers
	 -> Spark Standalone, YARN, Mesos, Kubernetes, local


    => Spark Layers EcoSystem	
	=> Programming Lang:	Scala, Python, Java, R
	=> Spark High-Level:	Spark SQL, Spark MLlib, Spark Streaming, Spark Graphx
	=> Spark Low Level:	Spark Core (RDD)
	=> Cluster Managers:	Standalone, YARN, Mesos, Kubernetes
	=> Storage: 		Linux, HDFS, KafKa, NoSQL, RDDMS


    Spark Architecture
    ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


    Getting started with Spark
    --------------------------

     1. Working in your vLab
	-> Follow the instructions given in the email. 
	-> Click on "Bigdata Enbl" icon
	-> This will send you to a Windows server.
	-> Double Click on "CentOS7" icon and login to 
		-> Type your password (refer to README.txt file)
	-> This will connect to CentOS 7 lab

	PySpark Shell
		-> Open a terminal
		-> Type "pyspark"
		-> Open the WebUI:
			-> Open FireFox browser (Applications menu -> Firfox)
			-> Type "localhost:4040" in address bar. 

       Working with JuPyter Notebook
		-> Open a terminal
		-> type :  "jupyter notebook --allow-root"

   2. Setting up PySpark in our personal machine
	-> Make sure you have "Anaconda Navigator" installen on your machine.
		URL: https://www.anaconda.com/products/individual

	-> One option: You can pip install spark (pip install pyspark)

	-> Follow the instruction given the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


   3. Signup to Databricks Comminity Edition (free account)
	URL: https://databricks.com/try-databricks

	Read through "Guide: Quickstart tutorial" article.


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    -> Fundamental data abstraction of Spark Core API

    -> RDD is a distributed collection of in-memory partitions
	-> Partitions are a collection of objects.

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution of the RDDs
	-> Action commands trigger execution.

    -> RDDs are resilient
	-> RDD partitions can be created on-the-fly (at runtime) when any RDD partition is missing.
	   (because of node failures are because memory leaks, evictions etc)


   How to create RDDs?
   ------------------
	Three ways:

	1. From external data files:

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. From programmatic python collections

		rdd1 = sc.parallelize( range(1, 100), 4)
		rdd1 = sc.parallelize( [1,2,1,2,4,5,6,4,6,7,7,9], 4)

        3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*2)	


   What can you do with an RDD?
   ----------------------------

	-> Transformations
		-> Transformations return an RDD
		-> Transformations does not cause execution of the RDDs
			-> They only create Lineage DAGs @ the driver side. 
		
	-> Actions
		-> Trigger execution of the RDD
		-> Produces some output
		-> Converts the logical Plan adnd physical execution plan and several tasks
		   are launched on the cluster

   RDD Lineage
   ------------
    RDD LIneage is a logical plan maintained for every RDD by the driver
    RDD Lineage DAGs are created by transformations
    RDD Lineage all the dependencies of the RDD all the way from the very first RDD.

	rddFile = sc.textFile(file, 4)
	Lineage DAG: (4) rddFile -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
	Lineage DAG: (4) rdd2 -> rddFile.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
	Lineage DAG: (4) rdd3 -> rdd2.flatMap -> rddFile.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rddFile.map -> sc.textFile
	
	rdd4.collect()


  RDD Persistence
  ---------------

	rdd1 = sc.paralleize(range(1, 100), 3)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StoargeLevel.MEMORY_ONLY )  ---> instruction to Spark to persist rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	    rdd6 lineage => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.paralleize(range(1, 100), 3)
	        [ sc.paralleize -> t3 -> t5 -> t6 ] -> rdd 6 ==> collect

        rdd7.collect()
	    rdd7lineage => rdd7 -> rdd6.t7
		[ t7 ] -> rdd 7 ==> collect

	rdd6.unpersist()


        Storage Levels
        --------------
	MEMORY_ONLY	   -> (default) memory serilized 1x replication
	MEMORY_AND_DISK    -> disk memory serilized 1x replication
	DISK_ONLY	
	MEMORY_ONLY_2	   -> memory serilized 1x replication
			    (2 copies of the partitions are persisted on 2 different executor)
	MEMORY_AND_DISK_2  -> disk memory serilized 2x replication


     Commands
     ---------

	rdd1.persist() or rdd1.cache()   => MEMORY_ONLY	
	rdd1.persist( StoargeLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  --------------------		
	-> The output of any transformation is an RDD
	-> The transformation create RDD Lineage DAG.   


   1. map		P: U -> V
			Object to Object transformations
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).collect()


   2. filter		P: U -> Boolean
			Filter the data based on the function
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8 ).collect()


   3. glom		P: None
			Created on list object per partition with all the elements of the partitions

		rdd1			   rdd2 = rdd1.glom()

		P0: 2,1,3,2,4,5 -> glom -> P0: [2,1,3,2,4,5]
		P1: 5,2,3,4,7,9 -> glom -> P1: [5,2,3,4,7,9]
		P2: 5,0,8,9,0,3 -> glom -> P2: [5,0,8,9,0,3]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)


   4. flapMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

 
   5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1		   rdd2 = rdd1.mapPartitions( lambda x : [sum(x)] )

		P0: 7, 2, 4, 3, 2, 5, 6, 7 -> mapPartitions -> P0: 36
		P1: 8, 9, 0, 5, 6, 7, 8, 9 -> mapPartitions -> P1: 52
		P2: 2, 4, 2, 5, 7, 8, 9, 6 -> mapPartitions -> P2: 43

		rdd1.mapPartitions(lambda p : map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions, but we get the partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()


   7. distinct			P: None, Optional: numPartitions
				Return an RDD with distinct objects of the input RDD.

   Types of RDDs
   -------------
	Generic RDDs :  RDD[U]
	Pair RDDs : RDD[(U, V)]

   8. mapValues			P: U -> V
				Applied only on Pair RDD
				The function is applied only on the 'value; of the key-value pairs

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the objects of the RDD based on the output of the function.

		rddWords.sortBy(lambda x: x[-1]).collect()
		rddWords.sortBy(lambda x: x[-1], False).collect()
		rddWords.sortBy(lambda x: x[-1], False, 5).collect()


   10. groupBy			P: U -> V, Optional: numPartitions
				Objects of the RDD are grouped (as (k,v) pairs) based on the function output.

				RDD[U].groupBy(U -> V)  => RDD[(V, ResultIterable[U])]


   11. randomSplit		P: list of ratios (ex: [0.4, 0.4, 0.3])
				Returns a list of RDDs split randomly in the specified ratios. 

			rddList = rdd1.randomSplit([0.6, 0.4])
			rddList = rdd1.randomSplit([0.6, 0.4], 456)    # 456 is a seed to fix the randomness.


   12. repartition		P: numPartitions
				Used to increase or decrease the number of output partitions
				Global shuffle

			rdd2 = rdd1.repartition(10)   // rdd2 will have 10 partitions

   13. coalesce			P: numPartitions
				Used to only decrease the number of output partitions
				Partition merging

			rdd2 = rdd1.repartition(5)	// rdd2 will have 5 partitions
	
        Recommendations
        ----------------
	-> The size of the partition should be around 128 MB
	-> The number of partitions should be a multiple of number of CPU cores alloted to your application.
	-> The number of cores in each executor should be 5.


  14. union, intersection, subtract, cartesian

	rdd3 = rdd1.union(rdd2)	
	

  15. partitionBy		P: numPartitions, Optional: partitioning-function  (U -> Int)
				Applied only on pair RDDs
				This used to control which elements goes to which based on the 'key' applied on
				a partitioning-function


transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]


def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;   
    
    
rdd1 = sc.parallelize(transactions, 3) \
         .map(lambda d: (d['city'], d)) \
         .partitionBy(4, custom_partitioner)


   ..ByKey transformations 
   -----------------------
    -> Wide transformations
    -> Applied only on Pair RDD
 
	
    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				RDD is sorted based on the key

		rdd2.sortByKey(False, 2).glom().collect()


    16. groupByKey		P: None, Optional: numPartitions
				Groups the RDD based on the key.
				Returns a Pair RDD, where key is a unique key and value is the grouped (ResultIterable) values that
				have the same key. 
				
				Note: Avaoid groupByKey if possible. 
					
				RDD[(U, V)].groupBy() => RDD[(U, ResultIterable[V])]

		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
             			.flatMap(lambda x: x.split(" ")) \
             			.map(lambda x: (x, 1)) \
             			.groupByKey() \
             			.mapValues(len) \
             			.coalesce(1)

    17. reduceByKey		P: (U, U) -> U,  Optional: numPartitions
				Will reduce all the values of each unique key with in each partition, 
				and then across partitions by iterativly applying the reduce function.	

		rddOutput = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
             			.flatMap(lambda x: x.split(" ")) \
             			.map(lambda x: (x, 1)) \
             			.reduceByKey(lambda x, y: x + y) \
             			.sortBy(lambda x: x[1], False, 1)	

    18. aggregateByKey 		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				Applied on only pair RDD.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()


		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey( (0,0),
                        		lambda z, v: (z[0] + v, z[1] + 1),
                        		lambda a, b: (a[0]+b[0], a[1]+b[1])) \
                			.mapValues(lambda x: x[0]/x[1])	


    
		student_rdd = sc.parallelize([
  				("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  				("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  				("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
 				("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  				("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  				("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  				("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

		output_rdd = student_rdd.map(lambda t: (t[0], t[2])) \
                		.aggregateByKey((0,0),
                                		lambda z,v: (z[0]+v, z[1]+1),
                                		lambda a,b:(a[0]+b[0], a[1]+b[1])) \
                		.mapValues(lambda p: p[0]/p[1])


    19. joins 		=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
			   Performed on two pair RDDs

			   RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

			join = names1.join(names2)   #inner Join
			leftOuterJoin = names1.leftOuterJoin(names2)
			rightOuterJoin = names1.rightOuterJoin(names2)
			fullOuterJoin = names1.fullOuterJoin(names2)

    20. cogroup		   => Is used to join RDDs with duplicate keys and you need unique joined keys
			      in the output
			   => groupByKey on each RDD -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4, 7]) (key4, [17])

		=> (key1, ([10, 7], [5, 17])) (key2, ([12, 6], [4, 7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile
		rddOutput.saveAsTextFile("E:\\PySpark\\output\\rddOutput")


   4. reduce		P: (U, U) -> U
			Reduces the entire RDD into one final value of the same type by iterativly
			applying a reduce function with in partitions and then across partitions

		rdd1

		P0: 7, 2, 4, 3, 2, 5, 6, 7 -> reduce -> -22 -> 53
		P1: 8, 9, 0, 5, 6, 7, 8, 9 -> reduce -> -36
		P2: 2, 4, 2, 5, 7, 8, 9, 6 -> reduce -> -39
		
		rdd1.reduce(lambda x, y : x - y )

   5. aggregate

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.		

	rdd1.aggregate( (0,0), lambda z,v: (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )

	rdd2.aggregate( ("", 0, 0), 
			lambda z, v: (z[0] + "," + str(v[0]), z[1]+v[1], z[2] if (z[2] > v[1]) else v[1]), 
			lambda a, b: (a[0] + "," + b[0], a[1] + b[1], a[2] if (a[2] > b[2]) else b[2]) 
		      )


   6. take
		rddWords.take(20)

   7. takeOrdered

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20, lambda x: len(x))

   8. takeSample   
		
		rdd1.takeSample(True, 15)
		rdd1.takeSample(True, 150, 567) 
		rdd1.takeSample(False, 15)
		rdd1.takeSample(False, 15, 567)

   9. countByValue

   10. countByKey

   11. first

   12. foreach  => Applies some function on all the objects of the RDD.
		   Does not return any value

   13. saveAsSequenceFile


    Use-Case
    ---------

    dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv
	
    From cars.tsv dataset, find out the average weight of each make of American origin
    Arrange the data in the DESC order of average weight
    Save the output rdd in a single text file.

    => Try this 


 
   Shared Variables
   ----------------

    Shared Variables: Accumulator, Broadcast

    Closure : A closure is all the code (all variables and methods) that must be visible for a task
	      to perform its computations on the RDD. 

    -> A closure a serialized and a copy is sent to every executor. 

    Limitation: Closure variables (local variables inside a task) can not be used to implement
		global counter.

	c = 0  

	def isPrime( n ) :
		if n is prime return 1
		else 
		if n is not prime return 0		

	def f1( n ) :
		global c
		if (isPrime(n) == 1) c = c + 1
		n * 2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c )       // 0


   1. Accumulator Variable

	-> Is a shared variable
	-> Not part of closure (and hence not a local variable)
	-> Manintained by the driver
	-> All tasks can write to this variable.
	-> Is used to implement global counters.

	=> Tasks can not red the value of accumulator variable
	   Tasks can only write to it. 
    

	c = sc.accumulator(0) 

	def isPrime( n ) :
		if n is prime return 1
		else 
		if n is not prime return 0		

	def f1( n ) :
		global c
		if (isPrime(n) == 1) c.add(1)		
		n * 2
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print( c )


  2. Broadcast Variable

	=> A large immutable collection when required inside a closure method can be converted
	   into a broadcast variable (instead of a local variable) to save lot of execution
	   memory.

	=> A single copy of the broadcast variable to sent to all executor node
	   (one copy per executor, not per task)

        => A broadcats variable can read by all the tasks running in the executor

  
	bc = sc.broadcast({1:'a', 2:'b', 3:'c', 4:'d', 5:'e', 6:'f', ....})   // 100 MB
   
	def f1( n ) :
	   global bc
	   return bc.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5, ...], 4)

	rdd2 = rdd1.map( f1 )

	rdd2.collect()


   spark-submit
   =============

    => Is a single commandthat is used to submit any spark application (scala, java, python, R)
       to any cluster manager (local, standalone, yarn, mesos, kubernetes)

	spark-submit [options] <app jar | python file | R file> [app arguments]


	=> spark-submit E:\PySpark\spark_core\examples\wordcount.py 

	=> spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executor-memory 10G
		--executor-cores 5
		--num-executor 20
		E:\PySpark\spark_core\examples\wordcount.py [app arguments] 


	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wordcountout 2
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


   ===============================================
        Spark SQL (pyspark.sql)
   ===============================================

    => Spark's structured data processing API

	Structured File Formats:  Parquet (default), ORC, JSON, CSV (delimited text)
	JDBC Format: RDBMS, NoSQL
	Hive

   => High-level API built on top of Spark Core API

   => SparkSession
	-> Starting point of execution
	-> Introduced in Spark 2.0 onwards 
	-> Represents a user-session inside an application
		-> An application (SparkContext) can have multiple user session (SparkSession)

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local") \
        	.getOrCreate()   

   => DataFrame (DF)
	-> Data abstraction of Spark SQL
	
	-> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	-> A DF is a collection of "Row" object  (pyspark.sql.Row)

	DataFrame:
		-> Data : Collection of 'Row' objects
		-> Schema : StructType objects 

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)

	
  Steps in creating  a Spark SQL application
  ------------------------------------------

	1.  Read/load data from some data source into a DataFrame

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()


	2. Apply tranformations on the DataFrame using DF API methods or using SQL

		Using DataFrame API
		--------------------
		df2 = df1.select("userid", "name", "age", "gender") \
         		.where("age is not null") \
         		.orderBy("gender", "age") \
         		.groupBy("age").count() \
         		.limit(4)

		Using SQL
                ---------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
         		from users
         		where age is not null
         		group by age 
         		order by age
         		limit 4"""

		df4 = spark.sql(qry)
		df4.show()

	
	3. Write/save the data of the DF into some external directory / database.

		df4.write.format("json").save(outputPath)
		df4.write.json(outputPath)


  Save Modes (Write Modes)
  ------------------------
   Define the behaviour when you are writing to an existing directory.	

	-> ignore
	-> append
	-> overwrite

	df4.write.json(outputPath, mode="overwrite")
	df4.write.mode("overwrite").json(outputPath)


  LocalTempViews & GlobalTempViews
  --------------------------------

	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"
	

  DataFrame Transformations
  --------------------------

   1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

   2. where / filter
	
		df3 = df2.where("count > 100 and domestic = false")
		df3 = df2.where( col("count") > 100 )

		df3 = df2.filter( col("count") > 100 )

   3. orderBy / sort

		df3 = df2.orderBy("count", "origin")
		df3 = df2.orderBy(desc("count"), asc("origin"))

		df3 = df2.sort(desc("count"), asc("origin"))
	

   4. groupBy   => returns  a "GroupedData" object on which you have to invoke a aggrgation method

		df3 = df2.groupBy("highFrequency", "domestic").count()
		df3 = df2.groupBy("highFrequency", "domestic").avg("count")
		df3 = df2.groupBy("highFrequency", "domestic").sum("count")
		df3 = df2.groupBy("highFrequency", "domestic").max("count")

		df3 = df2.groupBy("highFrequency", "domestic") \
        		.agg(   count("count").alias("count"),
              			sum("count").alias("sum"),
              			avg("count").alias("avg"),
              			max("count").alias("max") )

   5. limit
		df2 = df1.limit(20)

   6. selectExpr

	df2 = df1.select(expr("ORIGIN_COUNTRY_NAME as origin"),
                 expr("DEST_COUNTRY_NAME as destination"),
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

	is same as

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 "DEST_COUNTRY_NAME as destination",
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")

   7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) 

		----------------------
		listUsers = [(1, "Raju", 5),
             		(2, "Ramesh", 15),
             		(3, "Rajesh", 18),
             		(4, "Raghu", 35),
             		(5, "Ramya", 25),
             		(6, "Radhika", 35),
             		(7, "Ravi", 70)]


		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])

		users2 = userDf.withColumn("ageGroup", 
                            when(userDf["age"] < 12, "child")
                           .when(userDf["age"] < 20, "teenager")
                           .when(userDf["age"] < 60, "adult")
                           .otherwise("senior"))
		---------------------------------------

   8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


   9. union, intersect, subtract

		df5 = df3.union(df4)
		df5 = df3.intersect(df4)	# numPartitions = 200
		df5 = df3.subtract(df4)		# numPartitions = 200

   10. sample

		df3 = df1.sample(True, 0.5)
		df3 = df1.sample(True, 1.5, 7565)

		df3 = df1.sample(False, 0.5)
		df3 = df1.sample(False, 1.5, 7565)   => Invalid, fraction must be in the range [0,1]

   11. distinct

		df1.select("DEST_COUNTRY_NAME").distinct().show()
		df1.select("DEST_COUNTRY_NAME").distinct().count()

   12. randomSplit

		df10, df11 = df1.randomSplit([0.5, 0.5], 467)

		df10.count()
		df11.count()

   13. repartition

	df1.rdd.getNumPartitions()

	df2 = df1.repartition(4)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(2)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition(4, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions() 

	df4 = df2.repartition(col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()  
	
		=> Here the number of output partitions is decided based on the value of 
		  "spark.sql.shuffle.partitions" config property, whose default value is 200

		    spark.conf.set("spark.sql.shuffle.partitions", "5")

   14. coalesce

	df3 = df2.coalesce(2)
	df3.rdd.getNumPartitions()


   15. joins





   Working with different file formats
   -----------------------------------

   JSON
	
	read:
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	write:
		df4.write.format("json").save(outputPath)
		df4.write.json(outputPath)
		df4.write.json(outputPath, mode="overwrite")

   Parquet
	
	read:
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.parquet(inputPath)

	write:
		df4.write.format("parquet").save(outputPath)
		df4.write.parquet(outputPath)
		df4.write.parquet(outputPath, mode="overwrite")

   ORC
	
	read:
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.orc(inputPath)

	write:
		df4.write.format("orc").save(outputPath)
		df4.write.orc(outputPath)
		df4.write.orc(outputPath, mode="overwrite")


   CSV (delimited text file)

	read:

		df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputPath) 
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True) 
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True) 
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|") 

	write:

		df2.write.csv(outputPath, header=True, mode="overwrite")
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")
		

 
   Creating an RDD from DataFrame
   ------------------------------

	rdd1 = df1.rdd
	rdd1.take(5)


   Creating a DataFrame from programmatic data
   -------------------------------------------

	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Mahesh", 25),
             (4, "Rajesh", 15),
             (5, "Suresh", 25),
             (6, "Aditya", 35),
             (7, "Pranav", 45)]

	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

   
   Creating a DataFrame from RDD
   -----------------------------

	rdd1 = spark.sparkContext.parallelize(listUsers)

	df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

	df1.show()


   Creating a DataFrame with programmatic schema
   ---------------------------------------------

	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df1 = spark.createDataFrame(rdd1, schema=mySchema)

	----------------------------------------------------

	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json" 

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)


 
   Joins
   ------


	













