
  Agenda (PySpark)
  ----------------
   -> Spark - Basic & Architecture
   -> Spark Core (Low level API)
	-> RDD - Transformations and Actions
	-> Shared Variables
   -> Spark-Submit
   -> Spark SQL
	-> Data Frame Operations
        -> Integrations - MySQL & Hive
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ----------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   -> GitHub: https://github.com/ykanakaraju/pyspark


  Spark
  -----

   => Spark is written in Scala
    
   => Spark is a unified in-memory distributed computing framework for performing big data analytics
      running on a cluster.

   => Spark supports multiple programming languages
	=> Scala, Java, Python, R

   => Spark application can be submitted to multiple cluster managers
	=> local, Spark standalone scheduler, YARN, Mesos, Kubernetes


   Spark Unified Stack
   -------------------
     Spark provides a consistent set of APIs for performing different analytical workloads using the same
     execution engine.

	Batch Processing of unstructured data 	=> Spark Core API (RDDs)
	Batch Processing of structured data	=> Spark SQL (DataFrames)
	Stream processing (real-time)		=> Structured Streaming, DStreaming API
	Predictive analytics (using ML)		=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX


   Spark Architecture
   ------------------

       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with PySpark
   ----------------------------
    1. Working in the vLab
          -> Follow the instructions in the attached document
          -> On the desktop of the windows there is Oracle VM VitualBox. Click on it and lauch your lab.
		-> Check for the userids etc in the document provided on the desktop.

	  => Lauch pyspark shell (open a terminal and type 'pyspark')
	  => Lauch Spyder IDE (open a terminal and type 'spyder')

    2. Installing Spark on your personal machine
	=> Make sure you install "Anaconda Distribution". It comes with Spyder and Jupyter Notes.
	=> Setup PySpark to work with Spyder.
		-> Follow the instruction mentianed in 
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
    3. Signup for Databricks community edition account
		Link to Signup: https://databricks.com/try-databricks
		Link to Login: https://community.cloud.databricks.com/login.html


   RDD (Resilent Distributed Dataset)
   ----------------------------------
	
     => Is the fundamental data abstraction of Spark Core API

     => RDD is a collection of distributed in-memory partitions.
	   -> Partition is a collection of objects of some type.

     => RDDs are immutable

     => RDDs are lazily evaluated
	   -> transformations does not cause execution
	   -> execution is triggered by action commands
	
     => RDDs are resilient
	   -> RDDs are resilient to missing in-memory partitions
	   -> Missing in-memory partitions can be recreated at run-time.	
	

      Create RDDs 
      ------------
	1. Create RDD from external files

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([3,2,1,4,2,5,,4,6,7,8,9,0,6,7,8,9,6], 2)

	3. Applying transformations on existing RDDs
	
		rdd2 = rdd1.map(lambda x: x*2)

     RDD Operations
     --------------
	Two types of operations:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan. 


     RDD Lineage DAG
     ----------------
	
	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	    Lineage DAG : (4) rddFile -> textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	    Lineage DAG : (4) rddWords -> rddFile -> textFile

        rddPairs = rddWords.map(lambda x: (x, 1))
	     Lineage DAG : (4) rddPairs -> rddWords -> rddFile -> textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
	     Lineage DAG : (4) rddWc -> rddPairs -> rddWords -> rddFile -> textFile	
	

     Note: rdd1.getNumPartitions() returns the number of partitions of the rdd


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...., 3)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( Storagelevel.MEMORY_AND_DISK )       ==> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

        rdd6.collect() 	
	        Lineage of rdd6:  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile(....)
		textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect()

        rdd7.collect()
	        Lineage of rdd6:  rdd7 -> rdd6.t7 
		t7 (rdd7) -> collect()

        rdd6.unpersist()


        Storage Levels
        --------------

	1. MEMORY_ONLY		: (default) Memory Serialized 1x Replicated
				  RDD may fully, partitially or not persisted at all subjected to the
				  availability of storage memory.

	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated

        3. DISK_ONLY 		: Disk Serialized 1x Replicated

	4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated	

	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated


        Commands
        ---------
            rdd1.cache()   	=> in-memory persistence
	    rdd1.persistence()  => in-memory persistence
	    rdd1.persistence( StorageLevel.MEMORY_AND_DISK) 

	    rdd1.unpersist()		
	

   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Transformations
   -------------------
 
   1. map			P: U -> V
				Object to Object transformation
				input RDD: N object, output RDD: N objects			

		rddFile.map(lambda x: x.split(" ")).collect()

   2. filter			P: U -> Boolean
				Only those objects for which the function return True will be there in the output.
				input RDD: N object, output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

   3. glom			P: None
				Returns one list object per partition with all objects of the partition.


		rdd1 		rdd2 = rdd1.glom()

		P0: 3,2,1,4,5 -> glom -> P0: [3,2,1,4,5]
		P1: 5,6,7,8,2 -> glom -> P1: [5,6,7,8,2]
		P2: 9,0,4,2,1 -> glom -> P2: [9,0,4,2,1]

		rdd1.count() = 15 (int)   rdd2.count() = 3 (list)

  4. flatMap			P: U -> Iterable[V]
				flatMap flattens the elements of the iterables produced by the function.
				input RDD: N object, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions		P: Iterable[U] -> Iterable[V]
				partition to partition transformation

		rdd1 		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 3,2,1,4,5 -> mapPartitions -> P0: 15
		P1: 5,6,7,8,2 -> mapPartitions -> P1: 28
		P2: 9,0,4,2,1 -> mapPartitions -> P2: 16

		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V] 
				Similar to mapPartitions, but we get partition-index as additional parameter.		

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))]) \
		    .filter(lambda x: x[0] == 1) \
                    .flatMap(lambda x: x[1]).collect()

   Types of RDDs
   -------------
	
	Generic RDDs :  RDD[U]
	Pair RDDs : RDD[(K, V)]

   7. mapValues			P: U -> V
				Applied only to pair RDDs
				Transforms the value part of the pair RDD.

		rdd4.mapValues(lambda x: x[0]).collect()

   8. distinct			P: None, Optional: numPartitions
				Returns only distinct objects of the input RDD.

		rdd1.distinct().glom().collect()
		rdd1.distinct(4).glom().collect()

   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Objects of the RDD are sorted based on the function output.  

		rddWords.sortBy(lambda x: x[0]).collect()
		rddWords.sortBy(lambda x: x[0], False).collect()
		rdd1.sortBy(lambda x: x%2, True, 2).glom().collect()

  10. groupBy			P: U -> V, Optional: numPartitions
				Returns a Pair RDD, where:
				   key: each unique value of the function output
				   value: ResultIterable object containing the objects that returned the key.

		rddOut = sc.textFile("E:\\Spark\\wordcount.txt") \
           		.flatMap(lambda x: x.split(" ")) \
           		.groupBy(lambda x: x, 1) \
           		.mapValues(len)

  11. randomSplit		P: Weights, Optional: seed
				Splits thr RDD into multiple RDDs in the specified weights.

		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 6464)


  12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions of the output RDD
				Performs  global shuffle

		rdd2 = rdd1.repartition(6)
		rdd2 = rdd1.repartition(2)

  
  13. coalesce			P: numPartitions
				Is used to only decrease the number of partitions of the output RDD
				Performs partition merging

		rdd2 = rdd1.repartition(2)

      Recommendations
      ---------------
	=> The optimal size of an RDD partition is 128 MB (or between 100 MB to 500 MB)
        => The number of partitions of the RDD shoud be a multiple of number of CPU cores allocated.
        => The number of cores per executor should be 5

  
  14. partitionBy		P: numPartition, Optional: partition-function (default: hash)
				Applied only on Pair RDDs
				Controls which keys go into which partition 
	
		rdd2.partitionBy(4, lambda x: x+10).glom().collect()


   15. union, intersection, subtract, cartesian

	let us say rdd1 has M partitions and rdd2 has N partitions

	 command			output partitions
         --------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2)	M + N, wide
	 rdd1.subtract(rdd2)		M + N, wide
	 rdd1.cartesian(rdd2)		M * N, wide


    ..ByKey transformations
    ------------------------
	=> Are all wide transformations.
	=> Applied only on pair RDDs. 

   16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the objects by the key

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()
		rdd2.sortByKey(True, 5).glom().collect()
		rdd2.sortByKey(numPartitions = 5).glom().collect()

   17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD with unique-keys and grouped values.

				WARNING: AVOID groupByKey

		rddOut = sc.textFile("E:\\Spark\\wordcount.txt") \
           		   .flatMap(lambda x: x.split(" ")) \
           		   .map(lambda x: (x, 1)) \
           		   .groupByKey() \
           		   .mapValues(sum)

   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				reduces all the values of each unique key

		rddOut = sc.textFile("E:\\Spark\\wordcount.txt") \
           		.flatMap(lambda x: x.split(" ")) \
           		.map(lambda x: (x, 1)) \
           		.reduceByKey(lambda x,y: x + y)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])	


   20. join, leftOuterJoin, rightOuterJoin, fullOuterJoin
				-> Operate on two pair RDDs

			RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V,W))]

		names1 = sc.parallelize(["vijay", "aditya", "raju", "amrita"]).map(lambda a: (a, 1))
		names2 = sc.parallelize(["amrita", "raju", "pavan", "pranav"]).map(lambda a: (a, 2))

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

  21 cogroup

  

  RDD Actions
  ------------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce			P: (U, U) -> U
				Reduces the entire RDD to one final value of the same type by iterativly
				applying the reduce function first within each partition and then across
				partitions.
 

	P0: 3, 2, 1, 4, 3, 2, 5, 4 -> reduce -> -18 -> reduce -> 35
	P1: 7, 6, 8, 9, 0, 8, 7, 4 -> reduce -> -35
	P2: 3, 2, 1, 3, 4, 5, 6, 0 -> reduce -> -18

		rdd1.reduce(lambda a,b : a-b)
		rddWc.reduce(lambda x, y : (x[0] + "," + y[0], x[1] + y[1]) )

   5. aggregate

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))
		

   6. first

   7. take
		rddWords.take(30)

   8. takeOrdered
		rddWc.takeOrdered(8)
		rddWc.takeOrdered(8, lambda x: x[1])

   9. takeSample

		rdd1.takeSample(True, 10)
		rdd1.takeSample(True, 10, 6777)	

		rdd1.takeSample(False, 10)
		rdd1.takeSample(False, 10, 646)

  10. countByValue

		rdd1.countByValue()

  11. countByKey

		rdd4.countByKey()

  12. foreach	-> applies the function on all the objects of the RDD.
		   does not return anything.

  13. saveAsSequenceFile



   Use-Case
   ========

	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset find the average weight of all models of each make of American origin cars.
	-> Arrange the data in the DESC order of average-weight
	-> Save the output as a single text file.

        => Try this yourself.


   Closures
   --------
	In Spark, a Closure constitute all the variables and methods that must be visible inside an executor
        for the tasks to perform their computations on the RDDs.

	The driver serializes the closure and sends a separate copy to every task.


	c = 0

	def isPrime(n):
		return True of n is Prime
		return False otherwise

	def f1(n):
                global c
		if ( isPrime(n) ) c += 1
		return n*10

	rdd1 = sc.parallelize(range(1, 4001), 4)

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

	print(c)     # c = 0


	Limitation: We can not use local variable to implement global counter.
	Solution: Use 'Accumulator' variable.


   Shared Variables
   ----------------

    1. Accumulator Variable

 	-> Maintained by the driver
	-> Not part of closure (not a local copy)
	-> All tasks can add to this accumulator. 
	-> All tasks share one copy of the variable maintained at the driver side.
	-> Used to implement global counter

	c = sc.accumulator(0)

	def isPrime(n):
		return True of n is Prime
		return False otherwise

	def f1(n):
                global c
		if ( isPrime(n) ) c.add(1)
		return n*10

	rdd1 = sc.parallelize(range(1, 4001), 4)

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

	print(c.value)     # c = 80



  2. Broadcast Variable

	=> Large immutable collections can be converted into broadcast variables.
	=> BC varibales are not part of function closure
	=> One copy is sent to every executor node and tasks can look up that variable.
	=> Is a read-only variable for tasks. 

	d = sc.broadcast({1: a, 2: b, 3: c, 4: d, 5: e, 6: f, 7: g, ... })   #100 MB 

	def f1(n):
	    global d
	    return d.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,6,7,.....], 4)

	rdd2 = rdd1.map( f1 )

	rdd2.collect()     # a,b,c,d,..


   spark-submit
   ------------

	spark-submit is a single command that is used to submit any spark application (scala, java, python, R)
	to any cluster manager (local, spark standalone, YARN, Mesos, Kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn 
		--driver-memory 2G 
		--driver-cores 2
		--executor-memory 10G
		--executor-cores 5
		--num-executor 5
		E:\Spark\wordcount.py [app-args]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py sampletext.txt out_wordcount 1

 ====================================
   Spark SQL  (pyspark.sql)
 ====================================
   
   => Is a high-level API built on top of Spark-Core

   => Is Spark's structured data processing API
	
	 -> Structured file formats: Parquet (default), ORC, JSON, CSV (delimted text files)
	 -> JDBC format : RDBMS, NoSQL
         -> Hive Format : To process data in Hive.

    SparkSession
    ------------

	=> Starting point of execution of Spark SQL application.
	=> Represents a user-session inside a spark-application (represented by SparkContext)
	   -> We can have more than ne user-session in one application.
  
	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[3]") \
    		.getOrCreate()   


    DataFrame (DF)
    --------------

	=> Is a collection of distributed in-memory partitions that are immutable and lazily-evaluated.

	=> DataFrame is a collection of 'Row' objects
		-> A Row contains one or more 'columns'
			-> Each colunmn is processing by Catalyst engine using Spark SQL
		           internal types. 

        => DataFrame contains two components
		
		-> Data: Collection of 'Rows' 
		-> Schema: 'StructType' object

			StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			 )


  Basic steps in a Spark SQL application
  --------------------------------------

	1. Read/load a DataFrame with data from some external or programmatic data source.

		inputPath = "E:\\PySpark\\data\\users.json"

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")	
		df1 = spark.read.json(inputPath)

        2. Apply transformations on the DF using transformation methods or using SQL

		Using DF transformation methods
                -------------------------------

		df2 = df1.select("userid", "name", "age", "gender") \
         		 .where("age is not null") \
         		 .orderBy("gender", "age") \
         		 .groupBy("age").count() \
         		 .limit(4)

		Using SQL
                ---------

		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		dfUsers = spark.table("users")

		qry = """select age, count(*) as count
        		from users 
        		where age is not null
        		group by age
        		order by age
        		limit 4"""        
        
		df3 = spark.sql(qry)
		df3.show()


        3. Write/Save the content of the DF to some external storage.
   
		outputPath = "E:\\PySpark\\output\\json"

		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df1.write.json(outputPath)

  Save Modes
  ----------
	1. errorifexists (default)
	2. ignore
	3. append
        4. overwrite

	df2.write.mode("overwrite").json(outputPath)
	df2.write.json(outputPath, mode="overwrite")


  LocalTempViews & GlobalTempViews
   --------------------------------     
 
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessible only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


  DF Transformations
  ------------------
  1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME",
                 	"DEST_COUNTRY_NAME",
                 	"count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))

  2. where / filter

	df3 = df2.where("domestic = false and count >= 1000")
	df3 = df2.filter("domestic = false and count >= 1000")

	df3 = df2.where( col("count") > 1000 )

  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.sort("count", "origin")

	df3 = df2.orderBy(desc("count"), asc("origin"))

  4. groupBy (with aggregation methods)
	
	-> groupBy returns GroupedData object
	-> Use an aggregation method to return a DataFrame

	df3 = df2.groupBy("domestic", "highFrequency").count()
	df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	df3 = df2.groupBy("domestic", "highFrequency").avg("count")
	df3 = df2.groupBy("domestic", "highFrequency").max("count")

	df3 = df2.groupBy("domestic", "highFrequency") \
         	.agg(count("count").alias("count"),
              	    sum("count").alias("sum"),
                    avg("count").alias("avg"),
                    max("count").alias("max"))

  5. limit
		
	df2 = df1.limit(10)
     
  6. selectExpr

	df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 "DEST_COUNTRY_NAME as destination",
                 "count",
                 "count + 10 as newCount",
                 "count > 200 as highFrequency",
                 "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")


   7. withColumn & withColumnRenamed

	df3 = df1.withColumn("newCount", expr("count + 10")) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	-----------------------------------------------------------

	df4 = userDf.withColumn("ageGroup",  when(userDf["age"] <= 12, "child")
                           		     .when(userDf["age"] < 20, "teenager")
                           		     .when(userDf["age"] < 60, "adult")
                           		     .otherwise("senior") )

   8. udf (user-defined-functions)


	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"    
    
	get_age_group = udf(getAgeGroup, StringType())  

	df4 = userDf.withColumn("ageGroup", get_age_group(col("age")) )

       =====================================================

	@udf(returnType=StringType()) 
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"  

	df4 = userDf.withColumn("ageGroup", getAgeGroup(col("age")) )

	======================================================

	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"  
    
	spark.udf.register("getAgeGroupUDF", getAgeGroup, StringType())

	spark.catalog.listFunctions()
	userDf.createOrReplaceTempView("users")

	spark.sql("select id, name, age, getAgeGroupUDF(age) as ageGroup from users").show()


   9. drop   -> drop Columns
	
	   	df4 = df3.drop("domestic", "newCount")

   10. dropna

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		df3 = usersDf.dropna()
		df3 = usersDf.dropna(subset=["phone", "age"])

		df3.show()

   11. dropDuplicates

		listUsers = [(1, "Raju", 5),
             		(1, "Raju", 5),
             		(3, "Raju", 5),
             		(4, "Raghu", 35),
             		(4, "Raghu", 35),
             		(6, "Raghu", 35),
             		(7, "Ravi", 70)]

		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
		userDf.show()

		userDf.dropDuplicates().show()
		userDf.dropDuplicates(["name", "age"]).show()

   12. distinct

		userDf.distinct().show()

		Q: Give me the count of "DEST_COUNTRY_NAME" in flights dataframe

			df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()
			df1.select("DEST_COUNTRY_NAME").distinct().count()

   13. union, intersect, subtract

		df4 = df2.union(df3)
		df5 = df4.intersect(df3)
		df6 = df4.subtract(df3)

   14. sample

		df2 = df1.sample(True, 0.5)         # True: withReplacement Sampling
		df2 = df1.sample(True, 1.5, 797)    # fraction > 1 allowed

		df2 = df1.sample(False, 0.5, 797)   # fALSE: with out replacement sampling
		df2 = df1.sample(False, 1.5, 797)   # ERROR: fraction > 1 NOT allowed

   15. randomSplit

		dfList= df1.randomSplit([0.6, 0.4])

		dfList[0].count()
		dfList[1].count()

   16. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df1.repartition(4, df1["ORIGIN_COUNTRY_NAME"])
		df4.rdd.getNumPartitions()

		df5 = df1.repartition(df1["ORIGIN_COUNTRY_NAME"])
		df5.rdd.getNumPartitions()

   17. coalesce

		df6 = df2.coalesce(2)
		df6.rdd.getNumPartitions()

   18. join => discussed separatly


  Working with different file formats
  -----------------------------------

   1. JSON
	
	  read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")	
		df1 = spark.read.json(inputPath)

	  write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df1.write.json(outputPath)

   2. Parquet (default)

	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")	
		df1 = spark.read.parquet(inputPath)

	  write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df1.write.parquet(outputPath)

   3. ORC

	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")	
		df1 = spark.read.orc(inputPath)

	  write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df1.write.orc(outputPath)


   4. CSV (delimited text file)

	read: 
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

		df1 = spark.read \
			.format("csv") \
			.option("header", True) \
			.option("inferSchema", True) \
			.load(inputPath)

  
	write
		df2.write.mode("overwrite").csv(outputPath, header=True)
		df2.write.mode("overwrite").csv(outputPath, header=True, sep="|")


   Create an RDD from DF
   ---------------------	
	rdd1 = df1.rdd    

   Create a DF from programmatic collection
   ----------------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df2 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")

   Create a DF from RDD
   ---------------------
	df2 = rdd1.toDF(["id", "name", "age"])
	df2.show()

   Create a DF with programmatic schema
   ------------------------------------
	mySchema = StructType([
             StructField("id", IntegerType(), True),
             StructField("name", StringType(), True),
             StructField("age", IntegerType(), True)])

	df1 = spark.createDataFrame(listUsers, mySchema)
	----------------------------------------------------
	mySchema = StructType([
             StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
             StructField("DEST_COUNTRY_NAME", StringType(), True),
             StructField("count", IntegerType(), True)])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)


    Joins
    ------	
	Supports => inner, left, right, full, left_semi, left_anti

	
	left-semi join
	--------------	
	=> Is like inner join, but data comes only from left-side table.

	=> Is equivalent to the following sub-query:
		select * from emp where deptid IN (select id from dept)

	left-anti join
	--------------		
	=> Is equivalent to the following sub-query:
		select * from emp where deptid NOT IN (select id from dept)

        Data
        ----

	employee = spark.createDataFrame([
    		(1, "Raju", 25, 101),
    		(2, "Ramesh", 26, 101),
    		(3, "Amrita", 30, 102),
    		(4, "Madhu", 32, 102),
    		(5, "Aditya", 28, 102),
    		(6, "Pranav", 28, 100)])\
  		.toDF("id", "name", "age", "deptid")
  
	employee.printSchema()
	employee.show()  
  
	department = spark.createDataFrame([
    		(101, "IT", 1),
    		(102, "ITES", 1),
    		(103, "Opearation", 1),
    		(104, "HRD", 2)])\
  		.toDF("id", "deptname", "locationid")
  
	department.show()  
	department.printSchema()   


     SQL Approach
     ------------
	spark.catalog.listTables()

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select *
         	from emp left anti join dept
         	on emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)

	joinedDf.show()

   
     DF Transformation method: join
     ------------------------------
	Supported Joins: inner, left/left_outer, right/right_outer, full/full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")

	joinedDf.show()


       Enforcing a broadcast join
       --------------------------
	joinedDf = employee.join(broadcast(department), joinCol, "left")




  Setting and getting configuration on a SparkSesssion
  ----------------------------------------------------

	spark.conf.get("spark.sql.shuffle.partitions")    # the get the value of existing configuratin value
	spark.conf.set("spark.sql.shuffle.partitions", "5")

     
   Use-Case
   --------
      datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens   

      From movies.csv and ratings.csv files, fetch the top 10 movies with highest average rating.
      	=> Consider only those movies that are rated by atleast 30 users
	=> data to be fetched: movieId, title, ratingCount, averageRating
	=> Arrange the data in DESC order of averageRating
	=> Save the output as a single Pipe-separated CSV file with header.


	=> Try it yourself.


   Hive Format - Working with Hive
   -------------------------------

	=> Hive is a data warehousing framework built on top of Hadoop.
	=> Hive is an abstraction ovr MapReduce.

	Warehouse: Is a directory where Hive stores all its managed tables
	Metastore: Is (usually) an external RDBMS service (MySQL is a popular choice) to store meta data.


warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.catalog.currentDatabase()

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

#spark.sql("DROP TABLE IF EXISTS movies")
#spark.sql("DROP TABLE IF EXISTS ratings")
#spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL


moviesDF = spark.table("movies")
ratingsDF = spark.table("ratings")

#moviesDF = spark.sql("SELECT * FROM movies")
#ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()

  
  JDBC Format - Working with MySQL
  --------------------------------
    
import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


=======================================
  Spark Streaming 
=======================================


     Spark's real-time data processing API

     Two APIs are available:

	1. Spark Streaming a.k.a DStreams API    (out-dated)
	2. Structured Streaming	(current and preferred API)

  Spark Streaming (DStreams API)
  ------------------------------

     => microbatch based processing
     => Provides "seconds" scale latency.  (near-real-time processing)
     => does not support event-time processing

       StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch




 


















    