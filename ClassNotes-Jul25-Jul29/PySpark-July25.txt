
  Agenda (PySpark)
  ----------------
   -> Spark - Basic & Architecture
   -> Spark Core (Low level API)
	-> RDD - Transformations and Actions
	-> Shared Variables
   -> Spark-Submit
   -> Spark SQL
	-> Data Frame Operations
        -> Integrations - MySQL & Hive
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ----------
   -> PDF presentations
   -> Code Modules
   -> Class Notes
   -> GitHub: https://github.com/ykanakaraju/pyspark


  Spark
  -----

   => Spark is written in Scala
    
   => Spark is a unified in-memory distributed computing framework for performing big data analytics
      running on a cluster.

   => Spark supports multiple programming languages
	=> Scala, Java, Python, R

   => Spark application can be submitted to multiple cluster managers
	=> local, Spark standalone scheduler, YARN, Mesos, Kubernetes


   Spark Unified Stack
   -------------------
     Spark provides a consistent set of APIs for performing different analytical workloads using the same
     execution engine.

	Batch Processing of unstructured data 	=> Spark Core API (RDDs)
	Batch Processing of structured data	=> Spark SQL (DataFrames)
	Stream processing (real-time)		=> Structured Streaming, DStreaming API
	Predictive analytics (using ML)		=> Spark MLLib
	Graph Parallel Computations		=> Spark GraphX


   Spark Architecture
   ------------------

    1. Cluster Manager

    2. Driver Master

    3. Executor Processes

    4. Spark Context


   Getting started with PySpark
   ----------------------------
    1. Working in the vLab
          -> Follow the instructions in the attached document
          -> On the desktop of the windows there is Oracle VM VitualBox. Click on it and lauch your lab.
		-> Check for the userids etc in the document provided on the desktop.

	  => Lauch pyspark shell (open a terminal and type 'pyspark')
	  => Lauch Spyder IDE (open a terminal and type 'spyder')

    2. Installing Spark on your personal machine
	=> Make sure you install "Anaconda Distribution". It comes with Spyder and Jupyter Notes.
	=> Setup PySpark to work with Spyder.
		-> Follow the instruction mentianed in 
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf
	
    3. Signup for Databricks community edition account
		Link to Signup: https://databricks.com/try-databricks
		Link to Login: https://community.cloud.databricks.com/login.html


   RDD (Resilent Distributed Dataset)
   ----------------------------------
	
     => Is the fundamental data abstraction of Spark Core API

     => RDD is a collection of distributed in-memory partitions.
	   -> Partition is a collection of objects of some type.

     => RDDs are immutable

     => RDDs are lazily evaluated
	   -> transformations does not cause execution
	   -> execution is triggered by action commands
	
     => RDDs are resilient
	   -> RDDs are resilient to missing in-memory partitions
	   -> Missing in-memory partitions can be recreated at run-time.	
	

      Create RDDs 
      ------------
	1. Create RDD from external files

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([3,2,1,4,2,5,,4,6,7,8,9,0,6,7,8,9,6], 2)

	3. Applying transformations on existing RDDs
	
		rdd2 = rdd1.map(lambda x: x*2)

     RDD Operations
     --------------
	Two types of operations:

	1. Transformations

	2. Actions 


     RDD Lineage DAG
     ----------------
	
	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	    Lineage DAG : (4) rddFile -> textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	    Lineage DAG : (4) rddWords -> rddFile -> textFile

        rddPairs = rddWords.map(lambda x: (x, 1))
	     Lineage DAG : (4) rddPairs -> rddWords -> rddFile -> textFile

	rddWc = rddPairs.reduceByKey(lambda x,y: x + y)
	     Lineage DAG : (4) rddWc -> rddPairs -> rddWords -> rddFile -> textFile	
	

     Note: rdd1.getNumPartitions() returns the number of partitions of the rdd



   Types of Transformations
   ------------------------

	1. Narrow Transformation


	2. Wide Transformations


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...., 3)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( Storagelevel.MEMORY_AND_DISK )       ==> instruction to spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

        rdd6.collect() 	
	        Lineage of rdd6:  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile(....)
		textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> collect()

        rdd7.collect()
	        Lineage of rdd6:  rdd7 -> rdd6.t7 
		t7 (rdd7) -> collect()

        rdd6.unpersist()


        Storage Levels
        --------------

	1. MEMORY_ONLY		: (default) Memory Serialized 1x Replicated
				  RDD may fully, partitially or not persisted at all subjected to the
				  availability of storage memory.

	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated

        3. DISK_ONLY 		: Disk Serialized 1x Replicated

	4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated	

	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated


        Commands
        ---------
            rdd1.cache()   	=> in-memory persistence
	    rdd1.persistence()  => in-memory persistence
	    rdd1.persistence( StorageLevel.MEMORY_AND_DISK) 

	    rdd1.unpersist()		
	

   Executor's memory structure
   ---------------------------



   RDD Transformations
   -------------------
 
   1. map			P: U -> V
				Object to Object transformation
				input RDD: N object, output RDD: N objects			

		rddFile.map(lambda x: x.split(" ")).collect()

   2. filter			P: U -> Boolean
				Only those objects for which the function return True will be there in the output.
				input RDD: N object, output RDD: <= N objects

		rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

   3. glom			P: None
				Returns one list object per partition with all objects of the partition.


		rdd1 		rdd2 = rdd1.glom()

		P0: 3,2,1,4,5 -> glom -> P0: [3,2,1,4,5]
		P1: 5,6,7,8,2 -> glom -> P1: [5,6,7,8,2]
		P2: 9,0,4,2,1 -> glom -> P2: [9,0,4,2,1]

		rdd1.count() = 15 (int)   rdd2.count() = 3 (list)

  4. flatMap			P: U -> Iterable[V]
				flatMap flattens the elements of the iterables produced by the function.
				input RDD: N object, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions		P: Iterable[U] -> Iterable[V]
				partition to partition transformation

		rdd1 		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )

		P0: 3,2,1,4,5 -> mapPartitions -> P0: 15
		P1: 5,6,7,8,2 -> mapPartitions -> P1: 28
		P2: 9,0,4,2,1 -> mapPartitions -> P2: 16

		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V] 
				Similar to mapPartitions, but we get partition-index as additional parameter.		

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))]).collect()

		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))]) \
		    .filter(lambda x: x[0] == 1) \
                    .flatMap(lambda x: x[1]).collect()

   Types of RDDs
   -------------
	
	Generic RDDs :  RDD[U]
	Pair RDDs : RDD[(K, V)]

   7. mapValues			P: U -> V
				Applied only to pair RDDs
				Transforms the value part of the pair RDD.

		rdd4.mapValues(lambda x: x[0]).collect()

   8. distinct			P: None, Optional: numPartitions
				Returns only distinct objects of the input RDD.

		rdd1.distinct().glom().collect()
		rdd1.distinct(4).glom().collect()

   9. 
  


























 



    