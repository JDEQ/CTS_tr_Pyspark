
  Agenda
  ------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD Transformation & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming

 -------------------------------------------

   Materials
   ----------

       	-> PDF presentations
	-> Core Modules 
	-> Class Notes
	-> Github: https://github.com/ykanakaraju/pyspark

 -------------------------------------------

  Cluster
  -------
  
  -> Is a group of nodes whose cumulative resources can be used to distribute your storage
       and processing across many nodes. 

	-> distributed storage
	-> distributed processing

  Spark
  -----

   -> Is a unified in-memory distributed computing framework.
   -> Spark is written in Scala programming language.  

   -> Spark is polyglot
	-> Scala, Java, Python, R

   -> Spark applications can run on multiple cluster manages.
	-> local, Spark Standalone, YARN, Mesos, Kubernetis
 

  In-memory computation
  ---------------------
 
	-> The intermediate results of computations can be persisted in-memory and subsequent
	   tasks can be launched in these persisted im-memory data. 

  Spark unified framework
  -----------------------
      -> Spark provides a consistent set of APIs for performing different analytical workloads
	 using the same execution engine.

	-> Batch Processing of unstructured data   :  Spark Core API
	-> Batch Processing of Structured data	   :  Spark SQL
	-> Stream Processing			   :  Spark Streaming 
        -> Predictive Analytics	(ML Models)        :  Spark MLlib
	-> Graph Parallel Computations             :  Spark GraphX
     

   Getting started with Spark
   --------------------------

    1. Working in you vLab

	-> Login to your vLab as per the instruction given in the email.
	-> You will be logged in to a Windows server.
	-> On the desktop you have a "CentOS 7" icon. Click on the icon and login.

	1.1 Working with PySpark shell

	 	-> Open a terminal and type "pyspark" at the prompt
		-> This shall launch the PySpark shell.
	
        1.2 Working with Jupyter Notebooks

		-> Open a terminal
		-> Type the following at the prompt

		    $ jupyter notebook    (this may not work)
		    $ jupyter notebook --allow-root

		-> This will lauch Jupyter Notebook browser link
		-> Here you can create your notebooks.

    2. Installing PySpark environment in your own machine. 

	-> Download and install "Anaconda Navigator"
		URL: https://www.anaconda.com/products/individual

	-> Pip Install PySpark
		C:\> pip install pyspark

        -> If pip install is not working, try to follow the steps mentioned in the shared document.
	  https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup Databricks Community Edition
	    URL: https://databricks.com/try-databricks
	
	    -> Signup by filling your details.
	    -> Follow the instaruction sent to your email to create your username and password.
	    -> Login to Databrick Account
	    -> Read "Guide: Quickstart tutorial".	


   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.



  RDD (Resilient Distributed Dataset)
  -----------------------------------

     -> Is the fundamental data abstraction of Spark framework. 

     -> Is a collection of distributed in-memory partitions.
	-> A partition is a collection of objects

     -> RDDs are immutable

     -> RDDs have two components:

		data 	    : distributed in-memory partitions
		lineage DAG : logical plan 

     -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   How to create RDDs ?
   --------------------
	3 ways:

	1. Create an RDD from some external file:

		rdd1 = sc.textFile( <filePath>, n )    // n: numPartitions
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data (such as Python collection)
	
		rdd1 = sc.parallelize( range(1, 100), 3 )
		rdd1 = sc.parallelize( [3,2,1,4,5,6,7,8,9,0,12,3,4,5,6,7,8,9,0], 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with an RDD ?
   ------------------------------

	Only two things: 

	1. Transformations
	
		-> Does not cause execution
		-> They only create Lineage DAG (logical plan) of the RDD

	2. Actions
		-> Triggers execution on the RDD
		-> Converts the logical plan into physical plan and cause a set of tasks to be 
		   launched on the cluster.

   RDD Lineage DAG
   ---------------

	=> Lineage of an RDD is a logical plan about how to create that RDD.
	=> This tracks all the dependencies (hierarchy) all the way from the very first RDD.

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage DAG:  (4) rdd1 -> sc.textFile

	rdd2 = rdd1.map(lambda x: x.upper())
		Lineage DAG:  (4) rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage DAG:  (4) rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG:  (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rdd1.map -> sc.textFile


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partition
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   Persistence of RDDs
   -------------------	
	rdd1 = sc.parallelize(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_ONLY )       -> instruction to spark to persist the rdd6 partition
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.parallelize
	sc.parallelize (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> collect

	rdd7.collect()
	
	lineage of rdd7: rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.parallelize
  	rdd6 --> t7 (rdd7) -> collect

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		-> serialized, RAM only
	2. MEMORY_AND_DISK	-> serialized, RAM if available, else Disk
	3. DISK_ONLY
	4. MEMORY_ONLY_2
	5. MEMORY_AND_DISK_2


	Persistence Commands
        --------------------
		-> persist()   // default storage level
		-> persist(StorageLevel.DISK_ONLY)
		-> cache()     // in-memory persistence

		-> unpersist()


   RDD Transformations
   -------------------

    1. map			P: U -> V 
				Element to element transformation
				input RDD: N objects, output RDD: N objects      

    2. filter			P: U -> Boolean 
				Only those objects for which the function returns True will be in the output RDD.
				input RDD: N objects, output RDD: <= N objects 

    3. glom			P: None
				Returns a list object per partition with all the elements of the partition
				input RDD: N objects, output RDD: number of objects = num of partition  
	
		rdd1			   rdd2 = rdd1.glom()

		P0:  4,7,6,4,7,8  -> glom ->  P0: [4,7,6,4,7,8]
		P1:  9,7,0,8,6,3  -> glom ->  P1: [9,7,0,8,6,3]
		P2:  3,4,1,2,6,6  -> glom ->  P2: [3,4,1,2,6,6]
			
       		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)

   4. flatMap			P: U -> Iterable[V]
				flapMap flattens the iterables returned by the function
				input RDD: N objects, output RDD: > N objects 

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies a function on the entire partition.

		rdd1			   rdd2 = rdd1.mapPartitions(lambda x: lambda x : map(lambda a: a*10, x))

		P0:  4,7,6,4,7,8  -> mapPartitions ->  P0: 40,70,60,40,70,80
		P1:  9,7,0,8,6,3  -> mapPartitions ->  P1: ...
		P2:  3,4,1,2,6,6  -> mapPartitions ->  P2: ...

		rdd1.mapPartitions(lambda x : [sum(x)] ).collect()
		rdd1.mapPartitions(lambda x : map(lambda a: a*10, x)).collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Applies a function on the entire partition.
				In addition to partition data, it also gets the partition-index as an additional
				parameter.

		rdd1.mapPartitionsWithIndex(lambda index, data : [(index, sum(data))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, x : map(lambda a: (i, a*10), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an output RDD with distinct objects

		rddWords.distinct().collect()

   8. sortBy			P: U -> V, Optional: ascending, numPartitions
				Objects of RDD are sorted based on the fucntion output.		
	
	rdd1.sortBy(lambda x: x%2).glom().collect()
	rdd1.sortBy(lambda x: x%2, False).glom().collect()  // desc sort
		
 	rddWords.sortBy(len).glom().collect()
	rddWords.sortBy(len, numPartitions=6).glom().collect()
	rddWords.sortBy(len, False, 6).glom().collect()


   Types of RDDs:

	-> Generic RDDs : RDD[U]
	-> Pair RDDs	: RDD[(U, V)]

   9. mapValues			P: U -> V
				Applied only on Pair RDD
				Transforms the value part only of the RDD using the function. It won't
				chamge the 'key' part

	rddPairs.mapValues(lambda x: str(x) + 'A').collect()


   10. groupBy			P: U -> V    optional: numPartitions
				Objects are grouped based on the function output.
				Returns a pair RDD, where:
				  key: each unique value of the function output
				  value: ResultIterable containing all the objects of the RDD that produced the key.
				
	rdd1.groupBy(lambda x: x%2, 1).glom().collect()


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            	.flatMap(lambda x: x.split(" ")) \
            	.groupBy(len) \
            	.mapValues(len) \
            	.sortBy(lambda x: x[1], False, 1)


        # wordcount example....
        rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            .flatMap(lambda x: x.split(" ")) \
            .groupBy(lambda x: x) \
            .mapValues(len) \
            .sortBy(lambda x: x[1], False, 1)

   
    11. randomSplit		P: list of ratios
				Returns a list of RDDs split randomly in the given ratios.

		rddList = rdd3.randomSplit([0.4, 0.3, 0.3])
		rddList = rdd3.randomSplit([0.5, 0.5], 45634)   # 45634 is a seed. seed fixes the randomness. 


    12. repartition		p: numPartitions	
				Returns an RDD with specified number of partitions
				Is used to increase or decrease the number of partitions.
				results in global shuffle of partitions

		rdd2 = rdd1.repartition(n)   # rdd2 will have n partitions


    13. coalesce		p: numPartitions	
				Returns an RDD with specified number of partitions
				Is used only to decrease the number of partitions.
				Results in partition-merging


    14. partitionBy		p: numPartitions, Optional: partitioning function :  U -> Int
				Applied on pair RDDs
				The partitions in the objects are stored id decided by a partitioninn function
				based the 'key' of the (K, V) pairs.
				default partition function:  key : hash(key)


		transactions = [
    			{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    			{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    			{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

		rdd1 = sc.parallelize(transactions, 3) \
         		.map(lambda d: (d['city'], d))

		rdd1.glom().collect()

		def custom_partitioner(city): 
    			if (city == 'Chennai'):
       				return 0;
    			elif (city == 'Hyderabad'):
        			return 1;
    			elif (city == 'Vijayawada'):
        			return 1;
    			elif (city == 'Pune'):
        			return 2;
    			else:
        			return 3;

		rdd2 = rdd1.partitionBy(5, custom_partitioner)


    15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				# of output partitions
        ------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow	
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


    ...ByKey transformations
    ------------------------
	=> wide transformations
	=> applied only on pair RDDs

    16. sortByKey		P: None, Optional: ascending, numPartitions
				Sorts the objects of the RDD based on the key.

		rdd2.sortByKey().collect()
		rdd2.sortByKey(False).collect()
		rdd2.sortByKey(False, 3).collect()

		rdd2.sortByKey(0).collect()


    17. groupByKey		P: None, Optional: numPartitions
				Groups the objects based on the key
				Output is a pair RDD with unique keys and grouped values.
				NOTE: Performs global shuffle. So try to avoid it.

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.map(lambda x: (x, 1)) \
            		.groupByKey() \
            		.mapValues(sum) \
            		.sortBy(lambda x: x[1], False, 1)

    18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Will apply the reduce function iterativly to reduce all the values
				of each unique key.
		
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.map(lambda x: (x, 1)) \
            		.reduceByKey(lambda x, y: x + y) \
            		.sortBy(lambda x: x[1], False, 1)

    19. aggregateByKey	Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.  

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

	output = student_rdd.map(lambda s: (s[0], s[2])) \
            		.aggregateByKey((0,0),
                		lambda z, v: (z[0] + v, z[1] + 1),
                		lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            		.mapValues(lambda x: x[0]/x[1] )


     20. Joins:	 		join, leftOuterJoin, rightOuterJoin, fullOuterJoin

				=> RDD[(U, V)].join(RDD[(U, W)]) -> RDD[(U, (V,W))]	

				join = names1.join(names2)   #inner Join
				leftOuterJoin = names1.leftOuterJoin(names2)
				rightOuterJoin = names1.rightOuterJoin(names2)
				fullOuterJoin = names1.fullOuterJoin(names2)

     21. cogroup		Used to join RDDs that have duplicate keys
				cogroup:  groupByKey -> fullOuterJoin

	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	=>  (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
	=>  (key1, [5, 17]) (key2, [4,7]) (key4, [17])

	=> (key1, ([10, 7], [5, 17]))  (key2, ([12, 6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


  RDD Actions
  --------------

   1. collect
   
   2. count

   3. saveAsTextFile

   4. reduce			P:  (U, U) -> U
				Reduces the entire RDD to one final value of the same type by iterativly
				applying the function on all partitions.

			rdd1
			P0 : 9, 2, 7, 3, 1  -> -4
			P1 : 8, 6, 2, 5, 1  -> -6
			P2 : 9, 3, 5, 1, 0  -> 0

			x = rdd1.reduce( lambda x, y : x - y )

   5. aggregate		-> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final values of the type
                  of the zero-value.

   6. take		rdd1.take(10)

   7. takeOrdered
			rddWords.takeOrdered(10)
			rddWords.takeOrdered(10, lambda x: len(x))

   8. takeSample
			rdd1.takeSample(True, 8)	# withReplacement: True
			rdd1.takeSample(False, 8)	# withReplacement: False
			rdd1.takeSample(False, 8, 456)  # 456 is a seed here

   9. countByValue

   10. countByKey

   11. foreach		- Take a function as a parameter, that does not return anything
			- Applies that function on all the objects


   Use-Case
   --------
   From cars.tsv file, get the average-weight of each make of American origin cars
   Arrange the data in the DESC order of average-weight
   Save the output as a single textfile. 

	-> Try this yourself
	-> Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv


   Spark-submit
   ------------
	=> Is a single command to submit any spark application (Scala, Java, Python, R) to 
	   any cluster manager
    
	=> spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--driver-cores 3
		--num-executors 20
                wordcount.py <command-line-args>

  	spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout 2
  	spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount.py

    
   Shared Variables
   ----------------

	Closures: 
	--------
	Closure constitutes all the variables and methods that must be visible for an executor
	to perform computation on the RDD. 

	This closure (code) is serialized and local copy of it is sent to every executor.

	// The following code will not work ..	
	counter = 0

	def f1( n ) :
		global counter
		if (isPrime(n) == 1) counter = counter + 1
		return n*2

	def isPrime( a ) :
		return 1 if a is prime number
		return 0 if a is not prime number
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
    
	rdd2.saveAsTextFile(...)

	print(counter)   // output = 0
   
	=> We CAN NOT use local variables to implement global counter. 
		

   Accumulator
   -----------
	-> Is a shared variable maintained by the driver
	-> Not part of closure
	-> All the tasks can add to this accumulator variable.

	counter = sc.accumulator(0)

	def f1( n ) :
		global counter
		if (isPrime(n) == 1) counter.add(1)
		return n*2

	def isPrime( a ) :
		return 1 if a is prime number
		return 0 if a is not prime number
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )
    
	rdd2.saveAsTextFile(...)

	print(counter.value)

 
   Broadcast Variable
   ------------------
      
        lookup = sc.broadcast({1: 'a', 2:'e', 3:'i', 4:'o', 5:'u'}) 
	result = sc.parallelize([2, 1, 3, 4, 5]).map(lambda x: lookup.value[x]) 
	print( result.collect() ) 


   ================================================
       Spark SQL (pyspark.sql)
   ================================================

    => Spark's Structured Data Processing API
    => High-level API built on top of Spark Core API

    SparkSession
    ------------
     -> Starting point of execution.
     -> Introduced in Spark 2.0 onwards     
     -> Is user-session with on an application

		spark = SparkSession \
        		.builder \
        		.appName("Dataframe Operations") \
        		.config("spark.master", "local[*]") \
        		.getOrCreate()   

   DataFrame (DF)
   ---------
    -> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
    -> Is a collection of "Row" objects   (pyspark.sql.Row)

    -> DataFrame has two components:

	 -> Data: Collection of Rows
	 -> Schema : StructType object 
		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)


   Types of data that Spark SQL can process
   ----------------------------------------
	-> Structured File Formats : Parquet (default), ORC, JSON, CSV (delimited text format) 
	-> Hive
	-> JDBC format : RDBMS, NoSQL DBs


    Steps in Working with Spark SQL
    --------------------------------

	1. Read/load the data from some external source (or programmatic data) into a DataFrame

		df1 = spark.read.format("json").load(inputFilePath)
		df1 = spark.read.json(inputFilePath)

	2. Apply transformations on the DF using DF API methods or using SQL

		Using DF API
		------------
		df2 = df1.select("userid", "name", "age", "gender") \
        		.where("age is not null") \
        		.groupBy("age").count() \
        		.limit(4) \
        		.orderBy("count")

		Using SQL
                ----------
		df1.createOrReplaceTempView("users")

		qry = """select age, count(*) as count
        		from users
        		where age is not null
       			group by age
        		order by count
        		limit 4"""

		df3 = spark.sql(qry)
		df3.show()

		#spark.catalog.dropTempView("users")


	3. Write/save into a structured file format / database etc. 

		df2.write.format("json").save(outputDir)
		df2.write.json(outputDir)
	
   LocalTempViews & GlobalTempViews
   ---------------------------------
	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.


   DataFrame Transformations
   -------------------------
    1. select

		df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

		df2 = df1.select(col("DEST_COUNTRY_NAME").alias("destination"),
                 		column("ORIGIN_COUNTRY_NAME").alias("origin"),
                 		expr("count"),
                 		expr("count + 10 as newCount"),
                 		expr("count > 300 as highFrequency"),
                 		expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                	)

    2. where / filter

		df3 = df2.where("domestic = false and count > 100")
		df3 = df2.filter("domestic = false and count > 100")

		df3 = df2.where(col("count") > 100)

    3. orderBy / sort

		df3 = df2.orderBy("count", "destination")
		df3 = df2.orderBy(desc("count"), asc("destination"))

    4. groupBy  -> returns a "GroupedData" object on which you have apply some aggregation method
	           to return a DataFrame.

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")
		df3 = df2.groupBy("domestic", "highFrequency").max("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
        		.agg(	count("count").alias("count"), 
             		     	sum("count").alias("sum"), 
             			avg("count").alias("avg"), 
             			max("count").alias("max")
			     )
    5. limit

		df1.limit(10)

    6. selectExpr

	df2 = df1.select(expr("DEST_COUNTRY_NAME as destination"),
                 expr("ORIGIN_COUNTRY_NAME as origin"),
                 expr("count"),
                 expr("count + 10 as newCount"),
                 expr("count > 300 as highFrequency"),
                 expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )

	IS SAME AS:

	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                 "ORIGIN_COUNTRY_NAME as origin",
                 "count",
                 "count + 10 as newCount",
                 "count > 300 as highFrequency",
                 "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

    7. withColumn

		df3 = df1.withColumn("newCount", col("count") + 10) \
         		.withColumn("highFrequency", expr("count > 300")) \
         		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))

    8. withColumnRenamed

		df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         		 .withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

    9. drop
		df5 = df4.drop("newCount", "highFrequency")

    10. union, intersect, subtract

		df2 = df1.where("DEST_COUNTRY_NAME = 'India'")
		df3 = df1.where("count > 1000")

		df4 = df2.union(df3)            # df2 + df3
		df5 = df4.intersect(df3)	# df3
		df6 = df4.subtract(df3)		# df2

    11. distinct

		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

    12. sample
	
		df5 = df1.sample(True, 0.75)
		df5 = df1.sample(True, 0.5, 4335)
		df5 = df1.sample(True, 1.5, 4335)   # valid

		df5 = df1.sample(False, 0.5)
		df5 = df1.sample(False, 0.5, 4335)
		df5 = df1.sample(False, 1.5, 4335)   # ERROR: sampling ration 1.5 is INVALID

    13. randomSplit

		df5, df6 = df1.randomSplit([0.7, 0.3], 56)

		df5.count()
		df6.count()

    14. repartition

		df2 = df1.repartition(4)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(2)
		df3.rdd.getNumPartitions()

		df4 = df1.repartition(col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		=> Here the numPartitions is determined by "sparl.sql.shuffle.partitions" config 
		   whose default value is 200. You can set it to whatever value you may want.
			spark.conf.set("spark.sql.shuffle.partitions", "10") 

		df4 = df1.repartition(4, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()		

    15. coalesce
		df5 = df4.coalesce(2)
		df5.rdd.getNumPartitions()

    16. join
	     -> discussed separatly.

   SaveModes
   ---------	
	-> errorIfExist
	-> ignore
	-> append
	-> overwrite

	df2.write.json(outputDir, mode="overwrite")
	df2.write.mode("overwrite").json(outputDir)



   Working with Structured File Formats
   -------------------------------------

	JSON
	----
		Read

			df1 = spark.read.format("json").load(inputFilePath)
			df1 = spark.read.json(inputFilePath)

		Write
			df2.write.format("json").save(outputDir)
			df2.write.json(outputDir)
	Parquet
	-------
		Read

			df1 = spark.read.format("parquet").load(inputFilePath)
			df1 = spark.read.parquet(inputFilePath)

		Write
			df2.write.format("parquet").save(outputDir)
			df2.write.parquet(outputDir)
	ORC
	----
		Read

			df1 = spark.read.format("orc").load(inputFilePath)
			df1 = spark.read.orc(inputFilePath)

		Write
			df2.write.format("orc").save(outputDir)
			df2.write.orc(outputDir)

	CSV (delimited text file)
	-------------------------
		Read			
			df1 = spark.read.csv(inputFilePath, inferSchema=True)   # for headerless csv file

			df1 = spark.read.option("header", True).option("inferSchema", True).format("csv").load(inputFilePath)
			df1 = spark.read.format("csv").load(inputFilePath, header=True, inferSchema=True)
			df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True)
			df1 = spark.read.csv(inputFilePath, header=True, inferSchema=True, sep="|")

		Write
			df2.write.mode("overwrite").csv(outputDir, header=True)
			df2.write.mode("overwrite").csv(outputDir, header=True, sep="|")
			

    Creating an RDD from dataframe
    ------------------------------
	rdd1 = df1.rdd   # return an RDD of Row objects


    Creating an dataframe from programmatic data
    --------------------------------------------
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Rajesh", 45),
             (4, "Rakesh", 35),
             (5, "Rajni", 15),
             (6, "Ramya", 40),
             (7, "Raghava", 25)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


    Creating an dataframe from an RDD
    ---------------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")


    Creating an dataframe with custom (programmatic) schema
    -------------------------------------------------------
	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)]) 

	df1 = spark.createDataFrame(rdd1, schema=mySchema)

       ====================================================
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)]) 

	inputFilePath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	df1 = spark.read.json(inputFilePath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputFilePath)


    Joins
    ------
	Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti.

	Left-semi
        ---------
	-> Equivalent to the following sub-query:		
		select * from emp where deptid IN (select id from dept)

	Left-anti
        ---------
	-> Equivalent to the following sub-query:		
		select * from emp where deptid NOT IN (select id from dept)
		
	SQL Approach
	------------
	spark.catalog.listTables()

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select emp.*, dept.*
        	from emp left outer join dept on
        	emp.deptid = dept.id"""
        
	joinedDf = spark.sql(qry)
	joinedDf.show()


	DF API Approach
        ---------------
	Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti.

	joinedCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinedCol, "left_anti")
   
    Use-Case
    --------
    From movies.csv & ratings.csv datasets, fetch the top 10 movies with highest average rating.
    -> Consider only those movies with totalratings atleast 30 ratings
    -> Data: movieId, title, totalRatings, averageRating
    -> Sort the data in the DESC order of averageRating
    -> Save the output as a single Pipe-separated CSV file. 

	=> Try it yourself ....
	=> Datasets are there in the github.

  
   Working with MySQL
   ------------------
      => Refer to the code from the GitHub

   
   Working with Hive
   -----------------          
      -> Hive : is a data warehouse platform for Hadoop
      
	Warehouse: Directory where hive stores all its managed data files
        MetaStore: An external rdbms (or embedded database) where Hive stores all its metadata
	
	 => Refer to the code from the GitHub
  
 ========================================
    Machine Learning & Spark MLlib
 ========================================   

   ML Model => Is a learned entity
	       Learns from historic data
	       An alrothm trains a model by analyzing the hostotic data.
	
	       => Based on the learning
		   -> A model can predict output on new inputs
		   -> A model can identify the hidden patterns in data
		   -> A model can recommend products etc
		   -> A model can project/forecast futuristic trends etc

  Terminology
  -----------

   1. Training Data   => Contains label and features
  
   2. Features 		: Inputs, dimensions

   3. Label		: Output  

   4. ML Algorithm	: Itrative mathematical computation that establishes a relation between the
			  label and the features with a goal to minimize the value of a loss function.

   5. ML Model		: Output of algorithm which has in-built relation between label and features.

   6. Error		: difference between actual value and predicted value

   7. Loss		: Cimulative value of all error.

   
	X	Y	Z(label)  pred.  error
        --------------------------------------
	1000	1000	3100	3000	100
	1500	500	3450	3500	-50
	2000	700	4650	4700	-50
	1100	800	3200	3000    200
	1200	600	?   
	------------------------------------
				 Loss:  100

        model 1:  Z = 2X + Y   		   Loss: 100
	model 2:  Z = 1.9X + Y  	   Loss:  90
	model 3:  Z = 1.8X + 1.1Y	   Loss:  85
	model 4:  Z = 1.75X + 1.1Y	   Loss:  92
	model 5:  Z = 1.81X + 1.05Y + 10   Loss:  87


    Steps in an ML Project
    ----------------------
     1. Data Collection

     2. Data Preparation

	   Output: Creating a "Feature Vector" that you can use with an algorithm
		
		-> Remove outliers
		-> Convert all data to numerical data
   		-> Fill/drop all missing data (such as null, empty strings)
		-> Scale/normalize data ranges
		-> Identify the features (remove, add, merge etc.)

    3. Train your model using an algorithm

	  -> Fit the prepared data to an algorithm
	  Output: A trained model

	  => Split the prepared data into training (70%) and validation (30%) datasets
	  => Train the model with training dataset
	  => get the prediction from the model on the validation dataset.

     4. Evaluate the model
	
     5. Deploy the model
	

   Types of Machine Learning
   --------------------------
    1. Supervised Learning

	=> The training data contains both label as well as features (labelled data)

	1.1 Classification
		-> Label is one of two/few fixed values
		-> Ex: 1/0, [1,2,3,4,5]

	1.2 Regression
		-> Label is a continuous value
		-> Ex: House Price Prediction

    2. Unsupervised Learning

	=> The training data contains only features, no label

	2.1  Clustering  (Collaborative Filtering)
		-> Customer Segmentation
		-> Recommendation and rating engines

	2.2  Dimesionality Reduction	

    3. Reinforcement Learning 
	-> Semi-supervised


   Spark MLlib
   -----------
    
      Two libraries:
	
		pyspark.mllib  -> legacy library (RDDs)
		pyspark.ml     -> Current library (DataFrames)


      1. Feature Tools  : Feature Extractors, Feature Transformers, Feature Selectors
      2. ML Algorithms  : Classification, Regression, Clustering, Collaborative Filtering
      3. Pipeline	: Creating workflows of processing steps. 
      4. Model Selection Tools : CrossValidation, TrainValidationSplit (hyperparameter tuning)
      5. Persistence
      6. Utilities       : LinAlg, Stats ..


      Building Blocks og Spark MLlib
      ------------------------------
	
	1. Feature Vector  : Vector object containing all features in numerical format
		
		-> Dense vector  :    Vectors.dense(0,0,0,0,9,3,0,0,5,0,0,0,7,0,0)
		-> Sparse vector :    Vectors.sparse(15, [5,6,9,13], [9,3,5,7])

        2. Estimator
		input :  DataFrame
		output:  Model
		method:  fit

		<model> = <estimator>.fit( <dataframe> )

		Ex: All ML algorithms, Several Feature Transformers

        3. Transformer 
		input:  DataFrame
		output: DataFrame
		method: transform

		<outputDf> = <transformer>.transform(<inputDf>)

		Ex: All models, Several Feature Transformers

         4. Pipeline		
		-> A Pipeline chains Transformers and Estimators together to specify an ML workflow

		pipeline = Pipeline(stages = [T1, T2, T3, E4])
		plModel = pipeline.fit( df1 )
		df1 => T1 => df2 => T2 => df3 => T3 => df4 => E4 => plModel


  Mini Project : Titanic - Machine Learning from Disaster
  -------------------------------------------------------
  URL:  https://www.kaggle.com/c/titanic
  Solution Code: https://github.com/ykanakaraju/pyspark/blob/master/spark_ml/radom_forest_titanic.py


PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs.Braund, Mr. Owen  John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S

  label:  Survived
  features: Pclass,Sex,Age,SibSp,Parch,Fare,Embarked 

	Numerical Features: Pclass,Age,SibSp,Parch,Fare
	Categorical Features: Sex,Embarked 
				--> StringIndexer, oneHotEncoder
 	
   =========================================================================

   Introduction to Spark Streaming
   -------------------------------

     Spark Streaming

	  => microbatch based processing
	  => Provides "seconds" scale latency.  (near-real-time processing)


      StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	-> Is a continuous flow of RDDs.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


		






	



















