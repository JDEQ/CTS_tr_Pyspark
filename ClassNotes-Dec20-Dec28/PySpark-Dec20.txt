
  Agenda
  ------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD Transformation & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming

 -------------------------------------------

   Materials
   ----------

       	-> PDF presentations
	-> Core Modules 
	-> Class Notes
	-> Github: https://github.com/ykanakaraju/pyspark

 -------------------------------------------

  Cluster
  -------
  
  -> Is a group of nodes whose cumulative resources can be used to distribute your storage
       and processing across many nodes. 

	-> distributed storage
	-> distributed processing

  Spark
  -----

   -> Is a unified in-memory distributed computing framework.
   -> Spark is written in Scala programming language.  

   -> Spark is polyglot
	-> Scala, Java, Python, R

   -> Spark applications can run on multiple cluster manages.
	-> local, Spark Standalone, YARN, Mesos, Kubernetis
 

  In-memory computation
  ---------------------
 
	-> The intermediate results of computations can be persisted in-memory and subsequent
	   tasks can be launched in these persisted im-memory data. 

  Spark unified framework
  -----------------------
      -> Spark provides a consistent set of APIs for performing different analytical workloads
	 using the same execution engine.

	-> Batch Processing of unstructured data   :  Spark Core API
	-> Batch Processing of Structured data	   :  Spark SQL
	-> Stream Processing			   :  Spark Streaming 
        -> Predictive Analytics	(ML Models)        :  Spark MLlib
	-> Graph Parallel Computations             :  Spark GraphX
     

   Getting started with Spark
   --------------------------

    1. Working in you vLab

	-> Login to your vLab as per the instruction given in the email.
	-> You will be logged in to a Windows server.
	-> On the desktop you have a "CentOS 7" icon. Click on the icon and login.

	1.1 Working with PySpark shell

	 	-> Open a terminal and type "pyspark" at the prompt
		-> This shall launch the PySpark shell.
	
        1.2 Working with Jupyter Notebooks

		-> Open a terminal
		-> Type the following at the prompt

		    $ jupyter notebook    (this may not work)
		    $ jupyter notebook --allow-root

		-> This will lauch Jupyter Notebook browser link
		-> Here you can create your notebooks.

    2. Installing PySpark environment in your own machine. 

	-> Download and install "Anaconda Navigator"
		URL: https://www.anaconda.com/products/individual

	-> Pip Install PySpark
		C:\> pip install pyspark

        -> If pip install is not working, try to follow the steps mentioned in the shared document.
	  https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup Databricks Community Edition
	    URL: https://databricks.com/try-databricks
	
	    -> Signup by filling your details.
	    -> Follow the instaruction sent to your email to create your username and password.
	    -> Login to Databrick Account
	    -> Read "Guide: Quickstart tutorial".	


   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.



  RDD (Resilient Distributed Dataset)
  -----------------------------------

     -> Is the fundamental data abstraction of Spark framework. 

     -> Is a collection of distributed in-memory partitions.
	-> A partition is a collection of objects

     -> RDDs are immutable

     -> RDDs have two components:

		data 	    : distributed in-memory partitions
		lineage DAG : logical plan 

     -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   How to create RDDs ?
   --------------------
	3 ways:

	1. Create an RDD from some external file:

		rdd1 = sc.textFile( <filePath>, n )    // n: numPartitions
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data (such as Python collection)
	
		rdd1 = sc.parallelize( range(1, 100), 3 )
		rdd1 = sc.parallelize( [3,2,1,4,5,6,7,8,9,0,12,3,4,5,6,7,8,9,0], 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())

   What can you do with an RDD ?
   ------------------------------

	Only two things: 

	1. Transformations
	
		-> Does not cause execution
		-> They only create Lineage DAG (logical plan) of the RDD

	2. Actions
		-> Triggers execution on the RDD
		-> Converts the logical plan into physical plan and cause a set of tasks to be 
		   launched on the cluster.

   RDD Lineage DAG
   ---------------

	=> Lineage of an RDD is a logical plan about how to create that RDD.
	=> This tracks all the dependencies (hierarchy) all the way from the very first RDD.

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage DAG:  (4) rdd1 -> sc.textFile

	rdd2 = rdd1.map(lambda x: x.upper())
		Lineage DAG:  (4) rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage DAG:  (4) rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG:  (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rdd1.map -> sc.textFile







