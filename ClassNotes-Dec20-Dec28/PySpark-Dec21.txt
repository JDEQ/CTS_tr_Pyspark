
  Agenda
  ------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD Transformation & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming

 -------------------------------------------

   Materials
   ----------

       	-> PDF presentations
	-> Core Modules 
	-> Class Notes
	-> Github: https://github.com/ykanakaraju/pyspark

 -------------------------------------------

  Cluster
  -------
  
  -> Is a group of nodes whose cumulative resources can be used to distribute your storage
       and processing across many nodes. 

	-> distributed storage
	-> distributed processing

  Spark
  -----

   -> Is a unified in-memory distributed computing framework.
   -> Spark is written in Scala programming language.  

   -> Spark is polyglot
	-> Scala, Java, Python, R

   -> Spark applications can run on multiple cluster manages.
	-> local, Spark Standalone, YARN, Mesos, Kubernetis
 

  In-memory computation
  ---------------------
 
	-> The intermediate results of computations can be persisted in-memory and subsequent
	   tasks can be launched in these persisted im-memory data. 

  Spark unified framework
  -----------------------
      -> Spark provides a consistent set of APIs for performing different analytical workloads
	 using the same execution engine.

	-> Batch Processing of unstructured data   :  Spark Core API
	-> Batch Processing of Structured data	   :  Spark SQL
	-> Stream Processing			   :  Spark Streaming 
        -> Predictive Analytics	(ML Models)        :  Spark MLlib
	-> Graph Parallel Computations             :  Spark GraphX
     

   Getting started with Spark
   --------------------------

    1. Working in you vLab

	-> Login to your vLab as per the instruction given in the email.
	-> You will be logged in to a Windows server.
	-> On the desktop you have a "CentOS 7" icon. Click on the icon and login.

	1.1 Working with PySpark shell

	 	-> Open a terminal and type "pyspark" at the prompt
		-> This shall launch the PySpark shell.
	
        1.2 Working with Jupyter Notebooks

		-> Open a terminal
		-> Type the following at the prompt

		    $ jupyter notebook    (this may not work)
		    $ jupyter notebook --allow-root

		-> This will lauch Jupyter Notebook browser link
		-> Here you can create your notebooks.

    2. Installing PySpark environment in your own machine. 

	-> Download and install "Anaconda Navigator"
		URL: https://www.anaconda.com/products/individual

	-> Pip Install PySpark
		C:\> pip install pyspark

        -> If pip install is not working, try to follow the steps mentioned in the shared document.
	  https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup Databricks Community Edition
	    URL: https://databricks.com/try-databricks
	
	    -> Signup by filling your details.
	    -> Follow the instaruction sent to your email to create your username and password.
	    -> Login to Databrick Account
	    -> Read "Guide: Quickstart tutorial".	


   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.



  RDD (Resilient Distributed Dataset)
  -----------------------------------

     -> Is the fundamental data abstraction of Spark framework. 

     -> Is a collection of distributed in-memory partitions.
	-> A partition is a collection of objects

     -> RDDs are immutable

     -> RDDs have two components:

		data 	    : distributed in-memory partitions
		lineage DAG : logical plan 

     -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   How to create RDDs ?
   --------------------
	3 ways:

	1. Create an RDD from some external file:

		rdd1 = sc.textFile( <filePath>, n )    // n: numPartitions
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data (such as Python collection)
	
		rdd1 = sc.parallelize( range(1, 100), 3 )
		rdd1 = sc.parallelize( [3,2,1,4,5,6,7,8,9,0,12,3,4,5,6,7,8,9,0], 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with an RDD ?
   ------------------------------

	Only two things: 

	1. Transformations
	
		-> Does not cause execution
		-> They only create Lineage DAG (logical plan) of the RDD

	2. Actions
		-> Triggers execution on the RDD
		-> Converts the logical plan into physical plan and cause a set of tasks to be 
		   launched on the cluster.

   RDD Lineage DAG
   ---------------

	=> Lineage of an RDD is a logical plan about how to create that RDD.
	=> This tracks all the dependencies (hierarchy) all the way from the very first RDD.

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage DAG:  (4) rdd1 -> sc.textFile

	rdd2 = rdd1.map(lambda x: x.upper())
		Lineage DAG:  (4) rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage DAG:  (4) rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG:  (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rdd1.map -> sc.textFile


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   Persistence of RDDs
   -------------------	
	rdd1 = sc.parallelize(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_ONLY )       -> instruction to spark to persist the rdd6 partition
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.parallelize
	sc.parallelize (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> collect

	rdd7.collect()
	
	lineage of rdd7: rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.parallelize
  	rdd6 --> t7 (rdd7) -> collect

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		-> serialized, RAM only
	2. MEMORY_AND_DISK	-> serialized, RAM if available, else Disk
	3. DISK_ONLY
	4. MEMORY_ONLY_2
	5. MEMORY_AND_DISK_2


	Persistence Commands
        --------------------
		-> persist()   // default storage level
		-> persist(StorageLevel.DISK_ONLY)
		-> cache()     // in-memory persistence

		-> unpersist()


   RDD Transformations
   -------------------

    1. map			P: U -> V 
				Element to element transformation
				input RDD: N objects, output RDD: N objects      

    2. filter			P: U -> Boolean 
				Only those objects for which the function returns True will be in the output RDD.
				input RDD: N objects, output RDD: <= N objects 

    3. glom			P: None
				Returns a list object per partition with all the elements of the partition
				input RDD: N objects, output RDD: number of objects = num of partition  
	
		rdd1			   rdd2 = rdd1.glom()

		P0:  4,7,6,4,7,8  -> glom ->  P0: [4,7,6,4,7,8]
		P1:  9,7,0,8,6,3  -> glom ->  P1: [9,7,0,8,6,3]
		P2:  3,4,1,2,6,6  -> glom ->  P2: [3,4,1,2,6,6]
			
       		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)

   4. flatMap			P: U -> Iterable[V]
				flapMap flattens the iterables returned by the function
				input RDD: N objects, output RDD: > N objects 

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies a function on the entire partition.

		rdd1			   rdd2 = rdd1.mapPartitions(lambda x: lambda x : map(lambda a: a*10, x))

		P0:  4,7,6,4,7,8  -> mapPartitions ->  P0: 40,70,60,40,70,80
		P1:  9,7,0,8,6,3  -> mapPartitions ->  P1: ...
		P2:  3,4,1,2,6,6  -> mapPartitions ->  P2: ...

		rdd1.mapPartitions(lambda x : [sum(x)] ).collect()
		rdd1.mapPartitions(lambda x : map(lambda a: a*10, x)).collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Applies a function on the entire partition.
				In addition to partition data, it also gets the partition-index as an additional
				parameter.

		rdd1.mapPartitionsWithIndex(lambda index, data : [(index, sum(data))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, x : map(lambda a: (i, a*10), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an output RDD with distinct objects

		rddWords.distinct().collect()

   8. sortBy			P: U -> V, Optional: ascending, numPartitions
				Objects of RDD are sorted based on the fucntion output.		
				==> to be discussed..
 	






