
  Agenda
  ------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD Transformation & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming

 -------------------------------------------

   Materials
   ----------

       	-> PDF presentations
	-> Core Modules 
	-> Class Notes
	-> Github: https://github.com/ykanakaraju/pyspark

 -------------------------------------------

  Cluster
  -------
  
  -> Is a group of nodes whose cumulative resources can be used to distribute your storage
       and processing across many nodes. 

	-> distributed storage
	-> distributed processing

  Spark
  -----

   -> Is a unified in-memory distributed computing framework.
   -> Spark is written in Scala programming language.  

   -> Spark is polyglot
	-> Scala, Java, Python, R

   -> Spark applications can run on multiple cluster manages.
	-> local, Spark Standalone, YARN, Mesos, Kubernetis
 

  In-memory computation
  ---------------------
 
	-> The intermediate results of computations can be persisted in-memory and subsequent
	   tasks can be launched in these persisted im-memory data. 

  Spark unified framework
  -----------------------
      -> Spark provides a consistent set of APIs for performing different analytical workloads
	 using the same execution engine.

	-> Batch Processing of unstructured data   :  Spark Core API
	-> Batch Processing of Structured data	   :  Spark SQL
	-> Stream Processing			   :  Spark Streaming 
        -> Predictive Analytics	(ML Models)        :  Spark MLlib
	-> Graph Parallel Computations             :  Spark GraphX
     

   Getting started with Spark
   --------------------------

    1. Working in you vLab

	-> Login to your vLab as per the instruction given in the email.
	-> You will be logged in to a Windows server.
	-> On the desktop you have a "CentOS 7" icon. Click on the icon and login.

	1.1 Working with PySpark shell

	 	-> Open a terminal and type "pyspark" at the prompt
		-> This shall launch the PySpark shell.
	
        1.2 Working with Jupyter Notebooks

		-> Open a terminal
		-> Type the following at the prompt

		    $ jupyter notebook    (this may not work)
		    $ jupyter notebook --allow-root

		-> This will lauch Jupyter Notebook browser link
		-> Here you can create your notebooks.

    2. Installing PySpark environment in your own machine. 

	-> Download and install "Anaconda Navigator"
		URL: https://www.anaconda.com/products/individual

	-> Pip Install PySpark
		C:\> pip install pyspark

        -> If pip install is not working, try to follow the steps mentioned in the shared document.
	  https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup Databricks Community Edition
	    URL: https://databricks.com/try-databricks
	
	    -> Signup by filling your details.
	    -> Follow the instaruction sent to your email to create your username and password.
	    -> Login to Databrick Account
	    -> Read "Guide: Quickstart tutorial".	


   Spark Architecture
   ------------------

     	1. Cluster Manager (CM)
		-> Applications are submitted to CM
		-> CM allocates resources (containers for executors and driver) for the application
		
	2. Driver
		-> Master Process that manages the execution
		-> Created a SparkContext object
		-> Analyses the user code and sends tasks to the executors

		Deploy Modes:
			-> client : default, Driver run in the client machine
			-> cluster : driver runs on one of the nodes in the cluster.

	3. Executors
		-> Runs the tasks that are assigned by the driver and status is reported back to the driver.
		-> All tasks does the same thing, but on different partitions of data

	4. SparkContext
		-> Starting point of execution and is created in a driver process
		-> Application context
		-> Link between the driver process and various tasks running on the cluster.



  RDD (Resilient Distributed Dataset)
  -----------------------------------

     -> Is the fundamental data abstraction of Spark framework. 

     -> Is a collection of distributed in-memory partitions.
	-> A partition is a collection of objects

     -> RDDs are immutable

     -> RDDs have two components:

		data 	    : distributed in-memory partitions
		lineage DAG : logical plan 

     -> RDDs are lazily evaluated.
	 -> Transformations does not cause execution
	 -> Only action commands trigger the execution.


   How to create RDDs ?
   --------------------
	3 ways:

	1. Create an RDD from some external file:

		rdd1 = sc.textFile( <filePath>, n )    // n: numPartitions
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data (such as Python collection)
	
		rdd1 = sc.parallelize( range(1, 100), 3 )
		rdd1 = sc.parallelize( [3,2,1,4,5,6,7,8,9,0,12,3,4,5,6,7,8,9,0], 3 )

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with an RDD ?
   ------------------------------

	Only two things: 

	1. Transformations
	
		-> Does not cause execution
		-> They only create Lineage DAG (logical plan) of the RDD

	2. Actions
		-> Triggers execution on the RDD
		-> Converts the logical plan into physical plan and cause a set of tasks to be 
		   launched on the cluster.

   RDD Lineage DAG
   ---------------

	=> Lineage of an RDD is a logical plan about how to create that RDD.
	=> This tracks all the dependencies (hierarchy) all the way from the very first RDD.

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage DAG:  (4) rdd1 -> sc.textFile

	rdd2 = rdd1.map(lambda x: x.upper())
		Lineage DAG:  (4) rdd2 -> rdd1.map -> sc.textFile

	rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		Lineage DAG:  (4) rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		Lineage DAG:  (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rdd1.map -> sc.textFile


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partition
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   Persistence of RDDs
   -------------------	
	rdd1 = sc.parallelize(....)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_ONLY )       -> instruction to spark to persist the rdd6 partition
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.parallelize
	sc.parallelize (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> collect

	rdd7.collect()
	
	lineage of rdd7: rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.parallelize
  	rdd6 --> t7 (rdd7) -> collect

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		-> serialized, RAM only
	2. MEMORY_AND_DISK	-> serialized, RAM if available, else Disk
	3. DISK_ONLY
	4. MEMORY_ONLY_2
	5. MEMORY_AND_DISK_2


	Persistence Commands
        --------------------
		-> persist()   // default storage level
		-> persist(StorageLevel.DISK_ONLY)
		-> cache()     // in-memory persistence

		-> unpersist()


   RDD Transformations
   -------------------

    1. map			P: U -> V 
				Element to element transformation
				input RDD: N objects, output RDD: N objects      

    2. filter			P: U -> Boolean 
				Only those objects for which the function returns True will be in the output RDD.
				input RDD: N objects, output RDD: <= N objects 

    3. glom			P: None
				Returns a list object per partition with all the elements of the partition
				input RDD: N objects, output RDD: number of objects = num of partition  
	
		rdd1			   rdd2 = rdd1.glom()

		P0:  4,7,6,4,7,8  -> glom ->  P0: [4,7,6,4,7,8]
		P1:  9,7,0,8,6,3  -> glom ->  P1: [9,7,0,8,6,3]
		P2:  3,4,1,2,6,6  -> glom ->  P2: [3,4,1,2,6,6]
			
       		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)

   4. flatMap			P: U -> Iterable[V]
				flapMap flattens the iterables returned by the function
				input RDD: N objects, output RDD: > N objects 

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies a function on the entire partition.

		rdd1			   rdd2 = rdd1.mapPartitions(lambda x: lambda x : map(lambda a: a*10, x))

		P0:  4,7,6,4,7,8  -> mapPartitions ->  P0: 40,70,60,40,70,80
		P1:  9,7,0,8,6,3  -> mapPartitions ->  P1: ...
		P2:  3,4,1,2,6,6  -> mapPartitions ->  P2: ...

		rdd1.mapPartitions(lambda x : [sum(x)] ).collect()
		rdd1.mapPartitions(lambda x : map(lambda a: a*10, x)).collect()


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Applies a function on the entire partition.
				In addition to partition data, it also gets the partition-index as an additional
				parameter.

		rdd1.mapPartitionsWithIndex(lambda index, data : [(index, sum(data))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, x : map(lambda a: (i, a*10), x)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an output RDD with distinct objects

		rddWords.distinct().collect()

   8. sortBy			P: U -> V, Optional: ascending, numPartitions
				Objects of RDD are sorted based on the fucntion output.		
	
	rdd1.sortBy(lambda x: x%2).glom().collect()
	rdd1.sortBy(lambda x: x%2, False).glom().collect()  // desc sort
		
 	rddWords.sortBy(len).glom().collect()
	rddWords.sortBy(len, numPartitions=6).glom().collect()
	rddWords.sortBy(len, False, 6).glom().collect()


   Types of RDDs:

	-> Generic RDDs : RDD[U]
	-> Pair RDDs	: RDD[(U, V)]

   9. mapValues			P: U -> V
				Applied only on Pair RDD
				Transforms the value part only of the RDD using the function. It won't
				chamge the 'key' part

	rddPairs.mapValues(lambda x: str(x) + 'A').collect()


   10. groupBy			P: U -> V    optional: numPartitions
				Objects are grouped based on the function output.
				Returns a pair RDD, where:
				  key: each unique value of the function output
				  value: ResultIterable containing all the objects of the RDD that produced the key.
				
	rdd1.groupBy(lambda x: x%2, 1).glom().collect()


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            	.flatMap(lambda x: x.split(" ")) \
            	.groupBy(len) \
            	.mapValues(len) \
            	.sortBy(lambda x: x[1], False, 1)


        # wordcount example....
        rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            .flatMap(lambda x: x.split(" ")) \
            .groupBy(lambda x: x) \
            .mapValues(len) \
            .sortBy(lambda x: x[1], False, 1)

   
    11. randomSplit		P: list of ratios
				Returns a list of RDDs split randomly in the given ratios.

		rddList = rdd3.randomSplit([0.4, 0.3, 0.3])
		rddList = rdd3.randomSplit([0.5, 0.5], 45634)   # 45634 is a seed. seed fixes the randomness. 


    12. repartition		p: numPartitions	
				Returns an RDD with specified number of partitions
				Is used to increase or decrease the number of partitions.
				results in global shuffle of partitions

		rdd2 = rdd1.repartition(n)   # rdd2 will have n partitions


    13. coalesce		p: numPartitions	
				Returns an RDD with specified number of partitions
				Is used only to decrease the number of partitions.
				Results in partition-merging


    14. partitionBy		p: numPartitions, Optional: partitioning function :  U -> Int
				Applied on pair RDDs
				The partitions in the objects are stored id decided by a partitioninn function
				based the 'key' of the (K, V) pairs.
				default partition function:  key : hash(key)


		transactions = [
    			{'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    			{'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    			{'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    			{'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    			{'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    			{'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

		rdd1 = sc.parallelize(transactions, 3) \
         		.map(lambda d: (d['city'], d))

		rdd1.glom().collect()

		def custom_partitioner(city): 
    			if (city == 'Chennai'):
       				return 0;
    			elif (city == 'Hyderabad'):
        			return 1;
    			elif (city == 'Vijayawada'):
        			return 1;
    			elif (city == 'Pune'):
        			return 2;
    			else:
        			return 3;

		rdd2 = rdd1.partitionBy(5, custom_partitioner)


    15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				# of output partitions
        ------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow	
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


    ...ByKey transformations
    ------------------------
	=> wide transformations
	=> applied only on pair RDDs

    16. sortByKey		P: None, Optional: ascending, numPartitions
				Sorts the objects of the RDD based on the key.

		rdd2.sortByKey().collect()
		rdd2.sortByKey(False).collect()
		rdd2.sortByKey(False, 3).collect()

		rdd2.sortByKey(0).collect()


    17. groupByKey		P: None, Optional: numPartitions
				Groups the objects based on the key
				Output is a pair RDD with unique keys and grouped values.
				NOTE: Performs global shuffle. So try to avoid it.

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.map(lambda x: (x, 1)) \
            		.groupByKey() \
            		.mapValues(sum) \
            		.sortBy(lambda x: x[1], False, 1)

    18. reduceByKey	
		



  RDD Actions
  --------------

   1. collect
   
   2. count

   3. saveAsTextFile

   4. reduce			P:  (U, U) -> U
				Reduces the entire RDD to one final value of the same type by iterativly
				applying the function on all partitions.

		rdd1
		P0 : 9, 2, 7, 3, 1  -> -4
		P1 : 8, 6, 2, 5, 1  -> -6
		P2 : 9, 3, 5, 1, 0  -> 0

		x = rdd1.reduce( lambda x, y : x - y )








