
   Agenda (PySpark)
   ----------------
   -> Spark - Basic & Architecture
   -> Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variable
   -> Spark SQL 
	-> DataFrame Operations
   -> Spark Streaming
	-> DStreams API (Spark Streaming)
	-> Structured Streaming
   -> Introduction to Spark MLlib

   Materials
   ---------
	1. PDF Presentations
	2. Code Modules
	3. Class Notes

	-> https://github.com/ykanakaraju/pyspark

  
  Spark
  -----    
     -> Spark is a unified in-memory distributed computing framework. 
    
     -> Used for big data analytics 

     -> Spark is written in Scala programming language
	
     -> Spark is a polyglot
	 -> Scala, Java, Python, R
    
     Cluster => Is a unified entity consisting of many nodes whose cumulative resources can be used
                to distribute your storage as well as processing. 

     In-memory => The intermediate results of computations can be persisted in RAM and subsequent
                tasks can directly work with these persisted results. 
     

     Spark Unified Framework
     ------------------------
      -> Spark provides a consistent set of API for processing different analytical workloads
         with the same execution engine. 	

	-> Batch Analytics of Unstructured Data		: Spark Core API (RDD)
        -> Batch Analytics of Structured Data		: Spark SQL (Dataframes)
        -> Stream Analytics (Real-time)			: Spark Streaming, Strcutured Streaming
	-> Predictive Analytics (Machine Learning)	: Spark MLLib
	-> Graph Parallel Computatitions		: Spark GraphX


   Spark Architecture
   ------------------

   1. Cluster Manager
	-> Spark apps are submitted to Cm using "Spark-submit" command
	-> CM allocates executors to the Spark application

   2. Driver
	-> Master process that runs either on the client machine or on one of the nodes of the cluster
	-> SparkContext object is created in the driver process.
	-> Manages the user code
	-> Sends tasks to the executors

   3. Executotrs
	-> Run the tasks sent by the driver
	-> report the status of the tasks back to the driver

   4. SparkContext
	-> Is the started point of execution
	-> Is an application context
	-> Link between the driver and several tasks running on the cluster.


   Getting started with Spark
   --------------------------
     1. Working in the vLab allocated to you
	-> Read the document attached and login to the lab
	-> You will be logging in to a Window server

	-> Click on the "CentOS 7" icon on the desktop and enter your password 
	   (Refer to README.txt file on the desktop)
	   ->  This is your vLab

        1. Launch PySpark shell
	   -> Open a terminal window
	   -> type "pyspark"
	   -> this will launch pyspark shell.

	   => Launch web UI @ localhost:4040

         2. Launch Jupyter Notebook
            -> Open a terminal
	    -> type "jupyter notebook --allow-root"
	    -> This will lauch the Jupyter notebook on the browser. 

    2. Setting-up pyspark environment in your own system

	1. Install "Anaconda Navigator" from https://www.anaconda.com/products/individual.

	2. Setup pyspark to run on Spyder or Jupyter Notebook

		-> Follow the instruction given in the document shared in the github.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (Free)
		
                Sign-up: https://databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html

		
   RDD (Risilient Distributed Dataset)
   -----------------------------------
   
       -> Fundamental Data Abstraction of Spark Core API
	
       -> RDD is a collection of distributed in-memory partitions.
            -> A partition is a collection of objects (of any type)


   How to create RDDs?
   -------------------

     3 ways:

	1. Create an RDD from some external data files

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		 => The default number of partitions is given by a configuration parameter 'sc.defaultMinPartitions'
		    whose value is 2 if you have atleast 2 cores allocated (otherwise 1)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

	3. Create RDDs by applying transformations on existing RDDs.

		rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with RDD?
   -------------------------
	Two things:

	1. Transformations
		-> Returns an RDD
		-> Create Lineage DAG of the RDD
		-> Does not cause execution

	2. Actions
		-> Triggers execution
		-> Produces some output.

   RDD Lineage DAG
   ----------------

     -> RDD Lineage is a logical plan on how to create the RDD from its hierarchy
     -> RDD Lineage tracks the hierachy of RDD that caused the creation of this RDD all the way from the
	 very first RDD. 	

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		rdd1 Lineage : (4) rdd1 -> sc.textFile -> E:\\Spark\\wordcount.txt

	rdd2 = rdd1.map(lambda x: x.upper())
		rdd2 lineage: (4) rdd2 -> rdd1.map -> sc.textFile
	
        rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		rdd3 lineage: (4) rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		rdd4 lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rdd1.map -> sc.textFile

	l1 = rdd4.collect()
	=> (sc.textFile -> map -> flatMap -> filter -> rdd4)
  




  














  
  
 
  



