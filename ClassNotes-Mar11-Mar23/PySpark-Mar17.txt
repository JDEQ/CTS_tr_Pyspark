
   Agenda (PySpark)
   ----------------
   -> Spark - Basic & Architecture
   -> Spark Core API
	-> RDD - Transformations & Actions
	-> Shared Variable
   -> Spark SQL 
	-> DataFrame Operations
   -> Spark Streaming
	-> DStreams API (Spark Streaming)
	-> Structured Streaming
   -> Introduction to Spark MLlib

   Materials
   ---------
	1. PDF Presentations
	2. Code Modules
	3. Class Notes

	-> https://github.com/ykanakaraju/pyspark

  
  Spark
  -----    
     -> Spark is a unified in-memory distributed computing framework. 
    
     -> Used for big data analytics 

     -> Spark is written in Scala programming language
	
     -> Spark is a polyglot
	 -> Scala, Java, Python, R
    
     Cluster => Is a unified entity consisting of many nodes whose cumulative resources can be used
                to distribute your storage as well as processing. 

     In-memory => The intermediate results of computations can be persisted in RAM and subsequent
                tasks can directly work with these persisted results. 
     

     Spark Unified Framework
     ------------------------
      -> Spark provides a consistent set of API for processing different analytical workloads
         with the same execution engine. 	

	-> Batch Analytics of Unstructured Data		: Spark Core API (RDD)
        -> Batch Analytics of Structured Data		: Spark SQL (Dataframes)
        -> Stream Analytics (Real-time)			: Spark Streaming, Strcutured Streaming
	-> Predictive Analytics (Machine Learning)	: Spark MLLib
	-> Graph Parallel Computatitions		: Spark GraphX


   Spark Architecture
   ------------------

   1. Cluster Manager
	-> Spark apps are submitted to Cm using "Spark-submit" command
	-> CM allocates executors to the Spark application

   2. Driver
	-> Master process that runs either on the client machine or on one of the nodes of the cluster
	-> SparkContext object is created in the driver process.
	-> Manages the user code
	-> Sends tasks to the executors

   3. Executotrs
	-> Run the tasks sent by the driver
	-> report the status of the tasks back to the driver

   4. SparkContext
	-> Is the started point of execution
	-> Is an application context
	-> Link between the driver and several tasks running on the cluster.


   Getting started with Spark
   --------------------------
    1. Working in the vLab allocated to you
	-> Read the document attached and login to the lab
	-> You will be logging in to a Window server

	-> Click on the "CentOS 7" icon on the desktop and enter your password 
	   (Refer to README.txt file on the desktop)
	   ->  This is your vLab

        1. Launch PySpark shell
	   -> Open a terminal window
	   -> type "pyspark"
	   -> this will launch pyspark shell.

	   => Launch web UI @ localhost:4040

         2. Launch Jupyter Notebook
            -> Open a terminal
	    -> type "jupyter notebook --allow-root"
	    -> This will lauch the Jupyter notebook on the browser. 

    2. Setting-up pyspark environment in your own system

	1. Install "Anaconda Navigator" from https://www.anaconda.com/products/individual.

	2. Setup pyspark to run on Spyder or Jupyter Notebook

		-> Follow the instruction given in the document shared in the github.
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Edition (Free)
		
                Sign-up: https://databricks.com/try-databricks
		Login: https://community.cloud.databricks.com/login.html

		
   RDD (Risilient Distributed Dataset)
   -----------------------------------
   
      -> Fundamental Data Abstraction of Spark Core API
	
      -> RDD is a collection of distributed in-memory partitions.
            -> A partition is a collection of objects (of any type)

      -> RDDs are lazily evaluated.
		-> Transformations only cause creation of RDD lineage DAG
		-> Action commands cause execution

      -> RDDs are immutable

      -> RDDs are resilient
	   -> RDDs can create missing partition on the fly at run (by performing recompuations on the RDD)


   How to create RDDs?
   -------------------
     3 ways:

	1. Create an RDD from some external data files

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		 => The default number of partitions is given by a configuration parameter 'sc.defaultMinPartitions'
		    whose value is 2 if you have atleast 2 cores allocated (otherwise 1)

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize(range(1, 100))
		=> The default number of partitions is given by a configuration parameter 'sc.defaultparallelism'
		    whose value is equal to the number of CPU cores allocated to the application. 

		rdd1 = sc.parallelize(range(1, 100), 3)


	3. Create RDDs by applying transformations on existing RDDs.

		rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with RDD?
   -------------------------
	Two things:

	1. Transformations
		-> Returns an RDD
		-> Create Lineage DAG of the RDD
		-> Does not cause execution

	2. Actions
		-> Triggers execution
		-> Produces some output.

   RDD Lineage DAG
   ----------------

     -> RDD Lineage is a logical plan on how to create the RDD from its hierarchy
     -> RDD Lineage tracks the hierachy of RDD that caused the creation of this RDD all the way from the
	 very first RDD. 	

	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		rdd1 Lineage : (4) rdd1 -> sc.textFile -> E:\\Spark\\wordcount.txt

	rdd2 = rdd1.map(lambda x: x.upper())
		rdd2 lineage: (4) rdd2 -> rdd1.map -> sc.textFile
	
        rdd3 = rdd2.flatMap(lambda x: x.split(" "))
		rdd3 lineage: (4) rdd3 -> rdd2.flatMap -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
		rdd4 lineage: (4) rdd4 -> rdd3.filter -> rdd2.flatMap -> rdd1.map -> sc.textFile

	l1 = rdd4.collect()
	=> (sc.textFile -> map -> flatMap -> filter -> rdd4)


  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  Executor's memory structure
  ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Persistence
  ---------------
	rdd1 = sc.textFile( <file> , 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK ) --> instruction to spark to not delete rdd6 partitions. 
	rdd7 = rdd6.t7(...)

        rdd6.collect()

	rdd6 lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) ---> collect	

        rdd7.collect()
		
	rdd7 lineage: rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	   rdd6 -> t7 (rdd7) ---> collect

        rdd6.unpersist()


        Storage Levels
        --------------	
	1. MEMORY_ONLY	   	-> default, Memory Serialized 1x Replicated 
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated 
	3. DISK_ONLY		-> Disk Serialized 1x Replicated 
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated 
				   (two copies of the partitions are persisted in two different executors) 
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated 
				   (two copies of the partitions are persisted in two different executors) 

	Commands
	--------
	rdd1.persist()
	rdd1.persist(StorageLevel.MEMORY_AND_DISK)
	rdd1.cache()   -> always MEMORY_ONLY persistence only

	rdd1.unpersist()	


  RDD Transformations
  --------------------
   
    => Every transformation returns an RDD
    => Only cause the lineage graph to be create at the driver side.

   
   1. map			P: U -> V
				Object to object transformation
				Transforms each object of the input RDD into another object by applying the 
				function.
				input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).collect()


   2. filter			P: U -> Boolean
				Only those objects for which the function returns True will be in the output RDD
				input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: x[-1] == "d").collect()


   3. glom			P: None
				Returns one list per partition with all the elements of the partition.
				input RDD: N objects, output RDD: = number of partitions

	
	 	rdd1			rdd2 = rdd1.glom()
		P0: 2,1,3,2,5 -> glom -> P0: [2,1,3,2,5]
		P1: 7,8,9,0,4 -> glom -> P1: [7,8,9,0,4]
		P2: 7,4,7,8,9 -> glom -> P2: [7,4,7,8,9]

		rdd1.count = 15	(int)	rdd2.count = 3 (list)

		ex:  rdd1.glom().map(len).collect()


    4. flatMap			P: U -> Iterable[V]
				flapMap flattens the elements of the iterables produced by the function.
				input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				partition to partition transformation

		rdd1	 rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )
		P0: 2,1,3,2,5 -> mapPartitions -> P0: 13
		P1: 7,8,9,0,4 -> mapPartitions -> P1: 28
		P2: 7,4,7,8,9 -> mapPartitions -> P2: 35

		rdd1.mapPartitions(lambda x: [len(list(x))] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()


   6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Similar to mapPartitions, but we get partition-index as an additional partition. 

		rdd1.mapPartitionsWithIndex(lambda i, x: [ (i, sum(x)) ] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD.
				input RDD: N objects, output RDD: <= N objects

                   rdd1.distinct()
		   rdd1.distinct(5)
 
   Types of RDDs:
	
	Two types of RDDs:	
	-> Generic RDD : RDD[U]
	-> Pair RDD : RDD[(U, V)]

   8. mapValues			P: U -> V
				Applied only on pair RDD
				Applies the function on only the value part of the (K, V) pairs
				input RDD: N objects, output RDD: N value

		rdd3 = rdd2.mapValues(lambda v: (v, v))


   9. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the RDD based on the value of the fucntion output.			
     				input RDD: N objects, output RDD: N value

		rdd1.sortBy(lambda x: x%5).glom().collect()
		rdd1.sortBy(lambda x: x%5, False).glom().collect()
		rdd1.sortBy(lambda x: x%5, False, 5).glom().collect()

   10.groupBy			P: U -> V, Optional: numPartitions
				The objects are grouped based on the function output.
				Returns a pair RDD where:
				  key: each unique value of the fucntion output
				  value: ResultIterable contains the objects of the RDD that produced the key as function output.
		
                 rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.groupBy(lambda x: x) \
            		.mapValues(len) \
            		.sortBy(lambda x: x[1], False, 1)

    11. randomSplit		P: list of ratios, Optional: seed
				Returns a list of RDDs split randomly in the specified ratios.		
			
		rddList = rdd1.randomSplit([0.5, 0.5])
		rddList = rdd1.randomSplit([0.5, 0.5], 3456)    # here 3456 is a seeed.

    12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of the output RDD
				(so that you may control the size of the partitions)
				Causes global shuffle


    13. coalesce		P: numPartitions
				Used to decrease the number of partitions of the output RDD
				Cause partition merging	

 	Recommandations
        ---------------
      	1. Size of the partition should be around 128 MB
	2. The number of partitions should be a a multiple of number of CPU core allocated
	3. If the number of partitions is close to but less than 2000, then bump it up to 2000. 
	4. The number of cores in each executor should be 5.


    14. union, intersection, subtract, cartesian

	  Let us say rdd1 has M partitions, and rdd2 has N partitions

	  command			number of output Partitions
	  ---------------------------------------------------------
	  rdd1.union(rdd2)		M + N, narrow
	  rdd1.intersection(rdd2)	M + N, wide
	  rdd1.subtract(rdd2)		M + N, wide		
	  rdd1.cartesian(rdd2)		M * N, wide	
    

    15. partitionBy		 P: numPartitions, Optional: partitioning-function (default: hash)
				 Applied only to pair RDDs
				 Is used to control which keys go to which partition by applying a custom 
				 partitioning function.
	
  		rdd2.partitionBy(3, lambda x: x + 2).glom().collect()


   ..ByKey transformations
   -----------------------
	=> Applied on Pair RDDs only
	=> Are all wide transformations.

    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the elemnts of the RDD based on the key

		rdd2.sortByKey().collect()
		rdd2.sortByKey(False).collect()
		rdd2.sortByKey(False, 5).collect()

    17. groupByKey		P: None, Optional: numPartitions
				Returns a Pair RDD where the key is each unique key of the RDD and 
				value is the ResultIterable with all the value of that key

				CAUTION: Avoid groupByKey transformation

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda x: (x, 1)) \
            			.groupByKey() \
            			.mapValues(sum)

  18. reduceByKey		P: U,U -> U, Optional: numPartitions
				Reduces all the values of each unique-key by iterativly applying the 
				reduce function on all the values of each unique-key within each partition,
				and then across partitions.

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda x: (x, 1)) \
            			.reduceByKey(lambda m,n: m+n)

  19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])


   20. joins	=>  join (inner), leftOuterJoin, rightOuterJoin, fullOuterJoin

			RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V,W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup	   => Is used when you want to get unique keys in the output RDD when the inputRDDs
		      have duplicate keys.

			=> groupByKey -> fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		     => (key1, [10,7]) (key2, [12, 6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		    => (key1, [5,17]) (key2, [4,7]) (key4, [17])

		=> (key1, ([10,7], [5,17])) (key2, ([12, 6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))



  RDD Actions
  -----------

  1. collect
  
  2. count
  
  3. saveAsTextFile

  4. reduce			P: U, U -> U
				Reduces the entire RDD into one value of the same type by iterativly applying
				the function
		
		P0: 9, 2, 4, 1, 7, 3, 5 -> reduce -> -13 -> 24
		P1: 6, 8, 9, 0, 3, 7, 4 -> reduce -> -25
		P2: 5, 6, 2, 3, 5, 0, 1 -> reduce -> -12

		rdd1.reduce(lambda x, y : x if (x > y) else y)

   5. aggregate	

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,v: (z[0]+v, z[1]+1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )				

   
   6. take	
		rdd1.take(5)

   7. takeOrdered
		 rdd1.takeOrdered(15)
		 rdd1.takeOrdered(15, lambda x: x%2)

   8. takeSample
		rdd1.takeSample(True, 15)
		rdd1.takeSample(True, 15, 5675)	   # seed : 5675

		rdd1.takeSample(False, 15)
		rdd1.takeSample(False, 15, 9796)    # seed : 9796

   9. first

   10. countByValue
		rdd2.countByValue()

   11. countByKey
		rdd2.countByKey()
			
   12. foreach		P: some function
			Applies the function on all the objects of the RDD
			Does not return anything.

		rdd2.foreach(lambda x : print("Key: " + str(x[0]) + ", Value: " + str(x[1])))

   13. saveAsSequenceFile
	
		output_rdd.saveAsSequenceFile("E:\\PySpark\\output\\seq")

  Use-Case
  --------

	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From the cars.tsv dataset fetch the average-weight of each make of 'American' origin cars. 
	Arrange the data in the DESC order of avererage weight
	Save the output as a single text file.

	=> Try to solve it yourself.


   Closure
   -------
	A closure refers to all the code (all variables and methods) that must be visible inside an executor
	for the tasks to perform their computations on the RDD.

	=> Spark sends a serialized copy of the closure to every executor. 

	c = 0

	def isPrime(n):
		return True if n is Prime
		return False if n is not prime
	
	def f1(n):
		global c		
		if (isPrime(n)) c += 1
	
		return n*2	

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     // c = 0


	Limitation: Local variables that are part of function closure can not be used to implement
		    global counter.  

	Solution => Use 'Accumulator' variables


   Shared variables
   ================
	-> Accumulator Variable
	-> Broadcast Variable

	
    Accumulator Variable
    --------------------
	-> Is a shared variable
	-> All the tasks can add to this single copy of the variable.
	-> Maintained by driver (only one copy)
	-> Not part of the closure
	-> All the tasks can add to this variable (using add method), but can not read this variable
	   This is read only by the driver. 

	=> Accumulator is used to implement "global counters" 
	

	c = sc.accumulator(0)

	def isPrime(n):
		return True if n is Prime
		return False if n is not prime
	
	def f1(n):
		global c		
		if (isPrime(n)) c.add(1)
	
		return n*2	

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

	rdd2.collect()

	print(c)     // c = 80


   
   Broadcast Variable
   ------------------

	d = {1:a, 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, .... }      # 100 MB
	bc = sc.broadcast(d)

	def f1( n ) :
		global bc		
		return bc.value[n]
	
	rdd1 = sc.parallelize([1,2,3,4,5,6,..], 4)
	rdd2 = rdd1.map( f1 )
        rdd2.collect()              




 
 =======================================================================
   spark-submit command
 ======================================================================= 

   Is a single command to submit any spark application (Scala, Java, Python, R) to any
   cluster managers (local, spark standalone, yarn, mesos, kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 3
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		<python-file-path> [app arguments]		

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput 1
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


  ============================================
      Spark SQL   (pyspark.sql)
  ============================================
   
    -> Spark's Structured data processsing library.
           -> Strcutured File Formats => Parquet (default), ORC, JSON, CSV (delimited file format)
	   -> JDBC Data Sources => RDBMS, NoSQL Databases
	   -> Hive (data warehousing platform built for Hadoop)

     -> SparkSession
	   -> Starting point of execution.
	   -> Represents a 'session' with its own configuration within an application
	   -> Introduced in Spark 2.0 (prior to that we used to have sqlContext)
	
	
	   spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()   
	
	   spark.conf.set( <name of config param>, <value of the config param>) 


    -> DataFrame (DF)

	-> Is the data abstraction of Spark SQL
	
	-> DF is a collection of distributed, in-memory partitions that are immutable and lazily evaluated. 
	-> DF is a collection of "Row" objects. 
		-> A row contains a list of column
		-> Each column is processed using 'Spark SQL Internal Types'
		
	-> DF contains two components:
		-> Data   : Collection of 'Rows'
		-> Schema : StructType object.

		StructType(
		   List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)


     Steps in a Spark SQL program
     ----------------------------

      1. Read/load the data from some data source into a DataFrame

		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

      2. Apply transformations on the DF using DF API methods or using SQL

		DF API Methods
		---------------
			df2 = df1.select("userid", "name", "gender", "age") \
        			.where("age is not null") \
        			.orderBy("gender", "age") \
        			.groupBy("age").count() \
        			.limit(4)

		Using SQL
		---------
			df1.createOrReplaceTempView("users")
			spark.catalog.listTables()

			qry = """select age, count(*) as count 
        			from users
        			where age is not null
        			group by age
        			order by age
        			limit 4"""

			df3 = spark.sql(qry)
			df3.show()
			
			spark.catalog.dropTempView("users")  # drop the temp view


      3. Write/save the DF into a structured file or database or hive. 

		outputPath = "E:\\PySpark\\output\\json"
		df2.write.format("json").save(outputPath)

		df2.write.json(outputPath)
		df2.write.json(outputPath, mode="overwrite")


   LocalTempView & GlobalTempView
   ------------------------------
    
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"


   Save Modes
   ----------
	-> default behavious: error if your write to an existing directory.

	-> ignore
	-> append
	-> overwrite

	df2.write.json(outputPath, mode="overwrite")	
	df2.write.mode("overwrite").json(outputPath)

      
   DF Transformations
   -------------------

   1. select

   2. where / filter

   3. orderBy / sort

   4. groupBy -> Returns a 'GroupedData' object
	      ->You have apply an aggregation command on the GroupedData to get a DF

   5. limit



   Working with different file formats
   -----------------------------------

    JSON
    ----
	read
		df1 = spark.read.json(inputPath)
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")		

	write		
		df2.write.json(outputPath)
		df2.write.json(outputPath, mode="overwrite")
		df2.write.format("json").save(outputPath)

    Parquet
    -------
	read
		df1 = spark.read.parquet(inputPath)
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")		

	write		
		df2.write.parquet(outputPath)
		df2.write.parquet(outputPath, mode="overwrite")
		df2.write.format("parquet").save(outputPath)

    ORC
    ----
	read
		df1 = spark.read.orc(inputPath)
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")		

	write		
		df2.write.orc(outputPath)
		df2.write.orc(outputPath, mode="overwrite")
		df2.write.format("orc").save(outputPath)


   CSV (delimited text files)
   ---------------------------

	read
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputPath)

	write
		df2.write.mode("overwrite").csv(outputPath, header=True)
		df2.write.mode("overwrite").csv(outputPath, header=True, sep="|")




   



