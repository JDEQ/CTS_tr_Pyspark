
  Curriculum
  ----------

   Understanding Spark
   RDD API - Transformations & Action 
   Spark SQL - DataFrames   
   Machine Learning & Spark MLLib
   Spark Streaming - Introduction

 ------------------------------------------------------------------------

   Big Data -> A set of data sets that are huge and complex that the traditional
               data management systems can not reasonbly process. 

    1. Volumn
    2. Velocity - Data growth.
    3. Variety  - Unstructured data or Semi-Structured
    4. Varacity - Data in doubt.  

   Traditional Data Management Systems -> Single server based system. 
 
   Cluster based computational systems are the solution. 

   Cluster -> Is a group of connected nodes that form a unified system whose cumulative 
              resources can be used to distribute your storage and processing requirements. 
   
   Hadoop Framework --> Distributed Storage and Processing Framework
                    --> HDFS: (Storage) 128 MB blocks
                        MapReduce (Distributed Parallel Processing) 
                        YARN  ( Cluster Manager / Resource Manager )

   MapReduce -> Is a disk based execution framework

   MapReduce -> Is not good for
                -> Iterative processing (ML)
                -> Lot of small files 
                -> Ad-hoc queries

   Spark is solution to the limitations of MapReduce Framework 

  -----------------------------------------------------------------------------

   Spark -> Open source unified in-memory distributed processing framework.
         -> Very memory intensive (needs lot of RAM)
         -> Is a polyglot ( Scala, Python, Java, R )
    
   Hadoop
   ------
      -> Batch processing of unstructured data    :  MapReduce
      -> Batch processing of structured data      :  Hive, Pig, Impala, Drill ...
      -> Stream data processing                   :  Storm, Kafka
      -> Machine Learning                         :  Mahout
      -> Graph Parallel Computations              :  Giraph
                 

   Spark unified framework
   ----------------------- 
   Spark provides a consistent set of APIs to process various analytics work loads.
   
      -> Batch processing of unstructured data    :  Spark Core (RDD)
      -> Batch processing of structured data      :  Spark SQL
      -> Stream data processing                   :  Spark Streaming, Structured Streaming
      -> Machine Learning                         :  Spark MLlib
      -> Graph Parallel Computations              :  Spark GraphX
   
  ----------------------------------------------------------------------------------

   Spark Building Blocks
   ---------------------

    1. Cluster Manager
          -> Spark apps are submitted to the cluster manager
          -> CM will register the job and schedules it by allocating some executors to the job
        
          -> Spark Standalone Scheduler, YARN, Mesos, Kubernetes.

    2. Driver Process
          -> Master Process which manages all the tasks of the spark job
          -> This is program where main() method is defined.
          -> Contains a SparkContext, which represents the connection to the cluster. 
          -> Will analyize the DAG and determines which tasks to be launched and when
             on the executors and accordingly send the tasks to the executors.

         Modes:
           1. Client Mode: (default mode) driver runs on the client machine.
           2. Cluster Mode: driver runs as one of the processes on the cluster.

    3. Executor Processes
          -> Tasks are launched in these executors by the driver process
          -> Each tasks is functinally same, but runs on a small chunk of data. Several
             tasks are launched parallelly across multiple executors.

    4. Spark Context
          -> Runs in the driver process
          -> represents an application's connection to the cluster with a defined config. 
  
 ----------------------------------------------------------------

   Programming Lang      =>  Scala, Python, Java & R
   Spark High Level API  =>  Spark SQL, Spark Streaming, Spark MLlib, Spark GraphX
   Spark Core API        =>  RDD based API
   Resource Managers     =>  Standalone Scheduler, YARN, Mesos, Kubernites   
   Storage Layer         =>  Linux, HDFS, RDMS, NoSQL, Kafka

------------------------------------------------------------------

   RDD - Resilient Distributed Dataset
   -----------------------------------

    RDD is the core data abstraction of Spark.


    RDD -> Partitioned 
           -> Distributed Dataset as partitions
           -> Each partition contains a collection of objects

        -> Immutable: you can not change the contents of an RDD   
     
        -> Lazilily Evaluated.
            -> Execution will happen only when you have an action command.
 
        -> Resilient (Fail Safety)
            -> RDDs can recreate the missing partitions by recomputing the partitions
               by executing the required tasks from the RDD lineage DAGs.
   

  How to create an RDD
  --------------------

   1. An RDD can be created from an external data file:
        rdd = sc.textFile ( file ) 

   2. An RDD can be created by parallelizing programmatic data
        rdd = sc.parallelize( [1,2,3,4,5,6,7,8] )

   3. By applying transformations on existing RDDs, we can create an other RDD
        rdd2 = rdd1.<transformation> 


  What can you do with an RDD
  ---------------------------

   1. Transformations
       -> represent in-memory transformation of input RDDs into output RDD
       -> tranformations create RDD
       -> transformations does not cause execution. 
       -> transformation cause the lineage DAG (directed acyclic graph) to 
          be created (at the driver side)
     

   2. Actions
       -> Action command produce output such as sending the data from the RDD to the driver
          or saving the contents of RDD into a file. 

       -> Actions trigger execution. 

       -> When an action command is seen by the driver, it will analyze the DAG, and 
          launches the set of tasks that need to be performed on the executors to get
          the output.

rdd1 = sc.textFile( filePath, 3 )
rdd2 = rdd1.map(lambda x: x.split(" "))
rdd3 = rdd2.map(lambda x: len(x))
rdd3.collect()  <-- action  

Lineage DAG:
------------
rdd1 -->  sc.textFile
rdd2 --> rdd1.map --> sc.textFile
rdd3 --> rdd2.map --> rdd1.map --> sc.textFile


  ================================================================================

  RDD Transformations
  -------------------

  1. map             => F: U => V
                     => Transforms each object to another object by applying the function
                     => Input RDD Count: N, Output RDD Count: N 


  2. filter          => F: U => Boolean
                     => The output RDD will have only those elements that return 'true'
		     => Input RDD Count: N, Output RDD Count: <= N 



















  
   
