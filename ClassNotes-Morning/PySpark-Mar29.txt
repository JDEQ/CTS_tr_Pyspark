
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark
   ----- 

     ** Spark is a unified in-memory distributed computing framework. **

     Spark is written in Scala programming language
     
     Spark is used for big data analytics

	
     Cluster: Is a unified entity comprising a group of nodes whose cumulative resources (CPU, RAM and disk)
	      can be used to distribute your storage and processing. 

     in-memory computation: facilitates to persist intermediate output and subsequents tasks can directly process 
     the data that stored in memory. 

	
   Spark unified framework
   -----------------------
	Spark provides consistent set of APIs for performing different analytics workloads
        based on same execution engine using well-defined data abstractions. 

	Batch Processing of unstructured data  => Spark Core API (RDD API)
	Batch Processing of structured data    => Spark SQL 
	Streaming analytics (real-time)	       => Spark Streaming (structured streaming)
	Predictive analytics (using ML)	       => Spark MLlib
	Graph parallel computations	       => Spark GraphX


    Spark is a polyglot
	=> Spark supports Scala, Java, Python and R 
   
    Spark can be executed on mutliple cluster managers:
	=> local, spark standalone, YARN, Mesos, Kubernetes.


   Getting started with Spark
   --------------------------
    
    1. Using the vLab allocated to you

            1. PySpark shell
	       -> Open a terminal window and type 'pyspark'

	    2. Spyder IDE
	       -> Search and lauch Spyder IDE


    2. Installing PySpark dev. env. on your local machine

	   => Try to use pip install
		pip install pyspark

	   => Make sure you have anaconda navigator installed

	   => Follow the steps mentioned in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup to Databricks Community edition  
		
	    Signup URL: https://www.databricks.com/try-databricks
			(Check for community edition)

	    Login URL: https://community.cloud.databricks.com/login.html


	    Downloading content from Databricks:

		Pattern
		---------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

		Example:
		-------
		/FileStore/tables/wordcountMar27/part-00000
		https://community.cloud.databricks.com/files/tables/wordcountMar27/part-00000?o=1072576993312365#tables/new/dbfs



  
   Spark Architecture
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks is reported to the driver.


   Resilient Distributed Dataset (RDD)
   -----------------------------------

	RDD is the fundamental data abstraction in a Spark Core API

	RDD is a collection of distributed in-memory partitions.
	   -> Each partition contains objects of some type. 

	RDD has two components:
	   -> RDD meta data : Lineage DAG 
	   -> RDD data : Collection of partitions

	=> RDDs are immutable

	=> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.


   Creating RDDs
   --------------

	Three ways:

	1. Create an RDD from some external data source

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize( [4,1,3,2,4,6,7,8,9,6,8,9,0,7], 2)


	3. By applying transformations of existing RDDs

		 rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   RDD Operations
   --------------
	
     Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


   RDD Lineage DAG
   ---------------
   Driver maintains the lineage DAG of each RDD when the RDD object is created
   Lineage DAG is a logical plan on how to create the RDD partitions
   This tracks all dependent RDDs (heirarchy) all the way from the very first RDD.


 	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
	-> Lineage of rdd1: rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
	-> Lineage of rdd2: rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))
	-> Lineage of rdd3: rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
	-> Lineage of rdd3: rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile

   ***  rdd1.getNumPartitions() => returns the number of partitions of the RDD
       

  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<filePath>, 40)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  -> instruction to spark to save the partitions of rdd6
	rdd7 = rdd6.t7(...)

	l1 = rdd6.collect()
	Lineage DAG of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		Plan: (textFile, t3, t5, t6) -> rdd6

	l2 = rdd7.collect()
	Lineage DAG of rdd7 => rdd7 -> rdd6.t7 
		Plan: (t7) -> rdd6	

	rdd6.unpersist()
        

	StorageLevel
	------------
	MEMORY_ONLY	   -> default, Memory serialized 1x replicated
	MEMORY_AND_DISK    -> Disk Memory serialized 1x replicated
	DISK_ONLY          -> Disk serialized 1x replicated
	MEMORY_ONLY_2      -> Memory serialized 2x replicated  
	MEMORY_AND_DISK_2  -> Disk Memory serialized 2x replicated
	

	Commands
	---------
	  rdd1.cache()     -> memory_only persistence
	  rdd1.persist()   -> memory_only persistence
	  rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	  rdd1.unpersist()
	
  
  Executor's memory strcuture
  ---------------------------
	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  Types of Transformations
  ------------------------     
	Two types:

	1. Narrow Transformations
		-> Cause do not cause data shuffling
		-> Each output partition is created entirly from input partitions

	2. Wide transformations
		-> Cause cause data shuffling
		-> We can control the number of partitions of the output RDD


  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to object transformation
			Transforms the objects by applying the function.
			input RDD: N objects, output RDD: N objects
	
		rdd2 = rddFile.map(lambda x: len(x))

  2. filter		P: U -> Boolean
			Filters the objects to the output RDD for which the function returns True
			input RDD: N objects, output RDD: <= N objects

		 rddFile.filter(lambda x: len(x.split(" ")) > 8 ).collect()

  3. glom 		P: None
			Returns one list per partition with all the objects of the partition
			input RDD: N objects, output RDD: objects = # of partitions

		rdd1			  rdd2 = rdd1.glom()
		P0: 3,1,2,4,3,6 -> glom -> P0: [3,1,2,4,3,6]
		P1: 4,6,3,5,4,7 -> glom -> P1: [4,6,3,5,4,7]
		P2: 6,7,8,0,4,8 -> glom -> P2: [6,7,8,0,4,8]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			flatMap flattens the output produced by the function.
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1	    rdd2 = rdd1.mapPartitions(lambda p: [sum(p)])  
		P0: 3,1,2,4,3,6 -> mapPartitions -> P0: 19
		P1: 4,6,3,5,4,7 -> mapPartitions -> P1: 29
		P2: 6,7,8,0,4,8 -> mapPartitions -> P2: 33

		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Partition to partition transformation
				You get the partition-index as additional function argument. 

	rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).filter(lambda p: p[0] == 1).collect()


   	
  7. distinct		P: None, Optional: numPartitions
			Returns the distinct objects of the input RDD.

		rdd1.distinct().glom().collect()
		rdd1.distinct(5).glom().collect()

  Types of RDDs
  -------------
	
	1. Generic RDDs : RDD[U]
	2. Pair RDDs : RDD[(K, V)]


  8. mapValues		P: U -> V
			Applied only to Pair RDDs
			Transforms the value of the (K, V) pairs by applying the function.

		rdd3.mapValues(lambda x: list(x)).collect()

  9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the objects of the RDD based on the function output.

		rddWords.sortBy(lambda x: x[-1], True).collect()
		rddWords.sortBy(lambda x: x[-1], True, 5).collect()
		rdd1.sortBy(lambda x: x%5, numPartitions=2).glom().collect()

  10. groupBy		P: U -> V, Optional: numPartitions
			Returns a pair RDD where:
			  key: unique value of the function output
			  value: ResultIterable object containing all objects of the RDD that produced the key.

		wordcount = sc.textFile(inputPath, 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len)

  11. randomSplit	P: List of weights  [ex: [0.6, 0.4])
			Splits an RDD into multiple RDDs at random in the specified weights

		rddList = rdd1.randomSplit([1, 2])
		rddList = rdd1.randomSplit([1, 2], 4568)
	

  12. repartition	P: numPartitions
			Is used to increase or decrease the partitions of the output RDD
			Causes global shuffle

		rdd2 = rdd1.repartition(2)     // decrease
		rdd2 = rdd1.repartition(6)     // increase

  13. coalesce		P: numPartitions
			Is used decrease the partitions of the output RDD
			Cause partition-merging

		rdd2 = rdd1.coalesce(2)

	Recommendations
	---------------
	1. The size of the partition should be between 100MB to 1GB. Ideally 128MB
	2. The number of partitions should be a multiple of number cores allocated.
	3. If the number of partitions is less than but close to 2000, bump it up to 2000
	4. The number of cores per executor should be 5 (between 4 & 6)


  14. partitionBy	P: numPartitions, Optional: partitioning-function (U -> Long/Int)	
			Applied only to pair-RDDs
			Partitioning will happen based on key.	


rdd3 = rdd1.map(lambda x: (x, 0)).partitionBy(2).map(lambda x: x[0])

   transactions = [
    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},
    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},    
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},
    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},
    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},
    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'},
    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}]

def custom_partitioner(city): 
    if (city == 'Chennai'):
        return 0;
    elif (city == 'Hyderabad'):
        return 1;
    elif (city == 'Vijayawada'):
        return 1;
    elif (city == 'Pune'):
        return 2;
    else:
        return 3;      

rdd1 = sc.parallelize(transactions, 3) \
        .map(lambda d: (d['city'], d)) \
        .partitionBy(3, custom_partitioner) \
        .map(lambda x: x[1])

rdd1.glom().collect()



  15. union, intersection, subtract, cartesian 
 
	Let us say rdd1 has M partitions and rdd2 has N partitions

	command				partitions & type
	--------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide

  
  ..ByKey transformations
  -----------------------
	Applied only on Pair RDDs
	Are wide transformations.
	Operate on the key of the (K, V) pairs


  16. sortByKey			P: None, Optional: ascending (True/False), numPartitions
				Sorts the RDD based on the key.

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()
		rdd2.sortByKey(False, 2).glom().collect()

  17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD where
				    key: Each unique key of the RDD
				    value: grouped values for that key

				WARNING: Avoid 'groupByKey' if possible.

		wordcount = sc.textFile(inputPath, 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.map(lambda x: (x, 1)) \
              			.groupByKey() \
              			.mapValues(sum)

  18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique-key by by iterativly applying
				the reduce function on each partition in the first stage and across the
				outputs of partitions in the next stage. 

		wordcount = sc.textFile(inputPath, 4) \
              			.flatMap(lambda x: x.split(" ")) \
              			.map(lambda x: (x, 1)) \
              			.reduceByKey(lambda x, y: x + y, 1)

  19. aggregateByKey	P: zero-value, seq-function, comb-function;  Optional: numPartitions

				Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin

		RDD[(K, V)].join( RDD[(K, W) ) => RDD[(K, (V, W))]

		join = names1.join(names2)   #inner Join
		print( join.collect() )

		leftOuterJoin = names1.leftOuterJoin(names2)
		print( leftOuterJoin.collect() )

		rightOuterJoin = names1.rightOuterJoin(names2)
		print( rightOuterJoin.collect() )

		fullOuterJoin = names1.fullOuterJoin(names2)
		print( fullOuterJoin.collect() )

    21. cogroup			Is used when you want to join RDDs with duplicate keys 
				and want unique keys in the output RDD

				groupByKey (on each RDD) => fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])


		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

		cogroup => (key1, ([10, 7], [5, 17])) (key2, ([5, 17], [4,7])) (key3, ([6], [])) (key4, ([], [17]))


   Use-case
   --------
	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	Find the average weight of all the makes of cars with 'American' origin.
	Arrange the data in the DESC order of average-weight
	Save the output as single text file.

       -> Try it yourself


  Actions
  -------
   
   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) => U
			Reduces an entire RDD to one value of the same type by iterativly applying
			the reduce function on each partition in the first stage and across the
			outputs of partitions in the next stage. 

			rdd1

			P0: 1, 2, 1, 3, 4, 5, 4, 6 -> reduce -> -24  -> reduce -> 37
			P1: 7, 8, 9, 0, 7, 8, 9, 0 -> reduce -> -34
			P2: 5, 3, 2, 4, 6, 8, 9, 0 -> reduce -> -27
		
			rdd1.reduce(lambda x,y: x - y)
  
   5. aggregate	
		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.


		rdd1.aggregate( (0,0), 
				lambda z,e: (z[0] + e, z[1] + 1), 
				lambda a,b: (a[0]+b[0], a[1]+b[1]) )

		rdd1.aggregate( (0,0,0),  	
				lambda z,v: (z[0]+v, z[1]+1, max(z[2],v)),  
				lambda a,b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])))

   6. take
		rdd1.take(10)

   7. takeOrdered(n)

		rdd1.takeOrdered(20)
		rddWords.takeOrdered(30, len)


   8. takeSample	=> rdd1.takeSample(withReplacement(True/False), n)
			   rdd1.takeSample(True, 10)

		  rdd1.takeSample(True, 10)		-> withReplacement sampling
		  rdd1.takeSample(True, 10, 345)	-> withReplacement sampling with seed
		  rdd1.takeSample(False, 100, 645)	-> withOutReplacement sampling with seed 

   9. countByValue

   10. countByKey

   11. foreach	-> Runs a function on all objects of the RDD
		   Does not return any value. 

   12. saveAsSequenceFile


   Spark-submit command
   --------------------

	Is a single command to submit any spark application (Scala, Java, Python, R) to any cluster manager
        (local, standalone scheduler, YARN, Mesos, Kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py 1	
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt sampletext_output 1

	spark-submit --master yarn \
		--deploy-mode cluster \
		--driver-memory 2G \
		--driver-cores 2 \
		--executor-memory 5G \
		--executor-cores 5 \
		--num-executors 4 \
		E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt sampletext_output 1


  Closure
  -------
	-> A closure is the code (variables and methods) that must be visible inside an executor for the 
	   tasks to perform their computation on the RDD partitions.

	-> The closure is serialized and a separate copy is sent to every executor.

	c = 0

	def isPrime(n):
		return True if n is Prime
		else return False

	def f1(n):
	   global c
	   if ( isPrime(n) ) c += 1	
	   return n*10	
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()
	
	print( c )     // c = 0


	Limitation: A local variable, which is part of closure, can not be used to implement
                    global counters. 

	Solution: Use Accumulator to implement global counters.



  Shared Variables
  ----------------

   1. Accumulator

	-> Is a shared variable maintained by the driver
	-> All distributed tasks can 'add' to the accumulator (write-only access)
	-> Used to implement global counters.
	

	c = sc.accumulator(0)

	def isPrime(n):
		return True if n is Prime
		else return False

	def f1(n):
	   global c
	   if ( isPrime(n) ) c.add(1)	
	   return n*10	
	
	rdd1 = sc.parallelize( range(1, 4001), 4 )
	rdd2 = rdd1.map( f1 )

	rdd2.collect()
	
	print( c.value() )     // c = 80


  2. Broadcast variable


	bc = sc.broadcast({1:10, 2:20, 3:30, 4:40, 5:50, .............})      # size: 100MB

	def f1(n):
	    global bc
	    return bc.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,6,...], 3)
	rdd2 = rdd1.map( f1 )    

	rdd2.collect()  

  ==================================
     Spark SQL  (pyspark.sql)
  ==================================

    Spark SQL is a structured data processing API built on top of Spark Core.
  
    Spark SQL is used to process:

	  Structured file formats : Parquet (default), ORC, JSON, CSV (delimited text)
	              JDBC format : RDBMS databases, NoSQL databases.
	              Hive format : Hive warehouse

    SparkSession
	-> Starting point of execution.
	-> SparkSesion represents a user session within an application.

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local[*]") \
    		.getOrCreate() 

	spark.conf.set("spark.sql.shuffle.partitions", "5")


     DataFrame (DF)
	-> A collections of in-memory partitions that are immutable and lazily evaluated.
		-> Partitions contain "Row" objects
	
	        DataFrame  has two components:
			DataFrame data     : Row objects
			DataFrame metadata : Schema  (is a StructType object) 		

				StructType(
				     List(
					StructField(age,LongType,true),
					StructField(gender,StringType,true),
					StructField(name,StringType,true),
					StructField(phone,StringType,true),
					StructField(userid,LongType,true)
				     )
				)


   Basic Steps for working with Spark SQL
   ---------------------------------------

	1. Read/load some data (external/programmatic) from a data-source into a DataFrame

		df1 = spark.read.format("json").load(inputFile)
		df1 = spark.read.load(inputFile, format="json")
		df1 = spark.read.json(inputFile)

	2. Apply transformations on the dataframe using DF API methods or using SQL

		Using DF transformation methods:
		--------------------------------
			df2 = df1.select("userid", "name", "age", "gender") \
         			.where("age is not null") \
         			.orderBy("gender", "age") \
         			.groupBy("age").count() \
         			.limit(4)

		Using SQL
		---------
		
			df1.createOrReplaceTempView("users")
			spark.catalog.listTables()

			qry = """select age, count(1) as count
        			from users
        			where age is not null
        			group by age
        			order by age
        			limit 4"""
        
			df3 = spark.sql(qry)


	3. Write/save the contents of the DF to some external destination. 
	
		outputDir = "E:\\PySpark\\output\\json"

		df2.write.format("json").save(outputDir)


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessible only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"	

  SaveModes
  ---------
     => Controls the behaviour when saving to an existing directory.

		ErrorIfExists (default)
		Ignore
		Append
		Overwrite

	df2.write.mode("overwrite").json(outputDir)
	df2.write.json(outputDir, mode="overwrite")
		

		

  DataFrame Transformations
  -------------------------

   1. select

   2. where / filter

   3. orderBy / sort

   4. groupBy  => returns GroupedData object

   5. limit


   Working with different file formats
   -----------------------------------

   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

   Parquet  (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)
   
   CSV (delimited text file)
	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.load(inputPath, format="csv", header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")
	
	write
		df2.write.format("csv").save(outputPath, header=True)
		df2.write.save(outputPath, format="csv", header=True)
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")	












   	

















	













