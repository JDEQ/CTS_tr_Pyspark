
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark
   ----- 

     ** Spark is a unified in-memory distributed computing framework. **

     Spark is written in Scala programming language
     
     Spark is used for big data analytics

	
     Cluster: Is a unified entity comprising a group of nodes whose cumulative resources (CPU, RAM and disk)
	      can be used to distribute your storage and processing. 

     in-memory computation: facilitates to persist intermediate output and subsequents tasks can directly process 
     the data that stored in memory. 

	
   Spark unified framework
   -----------------------
	Spark provides consistent set of APIs for performing different analytics workloads
        based on same execution engine using well-defined data abstractions. 

	Batch Processing of unstructured data  => Spark Core API (RDD API)
	Batch Processing of structured data    => Spark SQL 
	Streaming analytics (real-time)	       => Spark Streaming (structured streaming)
	Predictive analytics (using ML)	       => Spark MLlib
	Graph parallel computations	       => Spark GraphX


    Spark is a polyglot
	=> Spark supports Scala, Java, Python and R 
   
    Spark can be executed on mutliple cluster managers:
	=> local, spark standalone, YARN, Mesos, Kubernetes.


   Getting started with Spark
   --------------------------
    
    1. Using the vLab allocated to you

            1. PySpark shell
	       -> Open a terminal window and type 'pyspark'

	    2. Spyder IDE
	       -> Search and lauch Spyder IDE


    2. Installing PySpark dev. env. on your local machine

	   => Try to use pip install
		pip install pyspark

	   => Make sure you have anaconda navigator installed

	   => Follow the steps mentioned in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup to Databricks Community edition  
		
	    Signup URL: https://www.databricks.com/try-databricks
			(Check for community edition)

	    Login URL: https://community.cloud.databricks.com/login.html

  
   Spark Architecture
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks is reported to the driver.


   Resilient Distributed Dataset (RDD)
   -----------------------------------

	RDD is the fundamental data abstraction in a Spark Core API

	RDD is a collection of distributed in-memory partitions.

	RDD has two components:
	   -> RDD meta data : Lineage DAG 
	   -> RDD data : Collection of partitions

	=> RDDs are immutable

	=> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.


   Creating RDDs
   --------------

	Three ways:

	1. Create an RDD from some external data source

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize( [4,1,3,2,4,6,7,8,9,6,8,9,0,7], 2)


	3. By applying transformations of existing RDDs

		 rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   RDD Operations
   --------------
	
     Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


   RDD Lineage DAG
   ---------------
   Driver maintains the lineage DAG of each RDD when the RDD object is created
   Lineage DAG is a logical plan on how to create the RDD partitions
   This tracks all dependent RDDs (heirarchy) all the way from the very first RDD.


 	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
	-> Lineage of rdd1: rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
	-> Lineage of rdd2: rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))
	-> Lineage of rdd3: rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
	-> Lineage of rdd3: rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile
       
     
  RDD Transformations
  -------------------

   1. map		P: U -> V
			Object to object transformation
			Transforms the objects by applying the function.
			input RDD: N objects, output RDD: N objects
	














