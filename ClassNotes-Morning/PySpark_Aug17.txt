
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================
    
    Spark
    ------	
	-> Is a unified in-memory distributed computing framework for big data analytics.  

    	-> Spark is written Scala programming language.
	
	in-memory: ability to persist intermediate results and subsequent operations
		   can directly work on these persisted intermediate results. 

	-> Spark is a unified framework

	-> Spark is a polyglot
		-> Supports Scala, Java, Python, R

	-> Spark can run on multiple cluster managers
		local, spark standalone scheduler, YARN, Mesos, Kubernetes. 		


   Spark Unified Framework
   -----------------------

	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming, Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib
	Graph parallel computations  		-> Spark GraphX


 
   Getting started with Spark
   --------------------------
	
	1. Working in you vLab

	2. Setting up PySpark dev environment on your personal machine.

		Create a Python v-env
			cd <dir>
	     		python -m venv pyspark-venv
		Activate the v-env
			cd <dir>\pyspark-venv\Scripts
			activate
		Install PySpark
			pip install pyspark==3.4.1


	     => If the above approach is not working, you can also follow
                the instrauctions given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


		-> Install Anaconda distribution and then follow the steps.

	3. Signup to databricks community edition.

		Signup: https://www.databricks.com/try-databricks
		   -> Fill the details with valid email address.
		   -> In the next screen, click on 'Get started with Community Edition' link
		      (Do not click on 'Continue')

		Login: https://community.cloud.databricks.com/login.html


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
    -> RDD is the fundamental data abstraction in Spark Core API
	
    -> RDD is a collection of distributed in-memory partitions.
	-> Each partition is a collection of objects of some type.

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution
	-> Action commands cause execution. 

	
  Creating RDDs
  -------------
	
    Three ways:

	1. Create an RDD from external data files.

		rddFile = sc.textFile( <filePath> , 4 )

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,4,3,1,5,6,7,8,6,8,9,0,7,5,4,6,8], 2)
		
		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

	Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output
		

  RDD Lineage DAG
  ---------------
  Driver maintains a Lineage DAG for every RDD
  Lineage DAG is a heirarchy of dependencies of RDDs all the way upto th very first RDD	
  Lineage DAG is a logical plan on how to create the RDD.
    
     rddFile = sc.textFile(<filePath>, 4)	
     Lineage of rddFile: (4) rddFile -> textFile on <filePath>
	
     rddWords = rddFile.flatMap(lambda x: x.split(" "))
     Lineage of rddWords: (4) rddWords -> rddFile.flatMap -> textFile on <filePath>
  
     rddPairs = rddWords.map(lambda x: (x, 1))
     Lineage of rddPairs: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile on <filePath>

     rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
     Lineage of rddWc: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile on <filePath>


  Types of RDD Transformation
  ---------------------------

	Two types:

	 1. Narrow Transformations

           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations

           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<File>, 4)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.DISK_ONLY )      --> instruction to spark to save rdd6 partition
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	lineage of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile, t3, t5, t6] -> collected

	rdd7.collect()
	lineage of rdd6: (4) rdd7 -> rdd6.t7
		[t7] -> collected

	rdd6.unpersist()


       	Storage Levels
       	--------------	
	MEMORY_ONLY		=> default, Memory Serialized 1x replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x replicated
	DISK_ONLY		=> Disk Serialized 1x replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x replicated	
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x replicated	

	Commands
	--------	
	rdd1.cache()     -> in-memory persistence
	rdd1.persist()	 -> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()


   Spark Executor Memory Structure
   -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creationg and transformation
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.

   
  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects
			
	 rdd2 = rdd1.map(lambda x: (x%5, x))


  2. filter		P: U -> Boolean
			Objects for which the function returns True will be in the output RDDs
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Return one list object per partition with all the objects of that partition.

		rdd1			rdd2 = rdd1.glom()
		P0: 3,2,1,4,5,6 -> glom -> P0: [3,2,1,4,5,6]
		P1: 5,6,7,8,9,3 -> glom -> P1: [5,6,7,8,9,3]
		P2: 4,7,6,2,1,0 -> glom -> P2: [4,7,6,2,1,0]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			fatMap flattens the iterables produced by the function.

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			similat to mapPartitions, but we get the partition index as an additional parameter.

		rdd1 \
		.mapPartitionsWithIndex(lambda i,p: map(lambda x: (i, x), p)) \
		.filter(lambda x: x[0] == 1) \
		.map(lambda x: x[1]) \
		.collect()

  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

		rddWords.distinct().collect()


  Types of RDDs
  -------------

	Two types of RDDs:

	Generic RDD : RDD[U]
	Pair RDD :    RDD[(K, V)]


  8. mapValues		P: U -> V
			Applied only on Pair RDD
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs
			


  
  















































		




















 


  	















  





























   









  

