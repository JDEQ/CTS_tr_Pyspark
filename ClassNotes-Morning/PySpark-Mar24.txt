
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
	-> Spark Optimizations & Tuning
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================

   Spark
   ----- 

     ** Spark is a unified in-memory distributed computing framework. **

     Spark is written in Scala programming language
     
     Spark is used for big data analytics

	
     Cluster: Is a unified entity comprising a group of nodes whose cumulative resources (CPU, RAM and disk)
	      can be used to distribute your storage and processing. 

     in-memory computation: facilitates to persist intermediate output and subsequents tasks can directly process 
     the data that stored in memory. 

	
   Spark unified framework
   -----------------------
	Spark provides consistent set of APIs for performing different analytics workloads
        based on same execution engine using well-defined data abstractions. 

	Batch Processing of unstructured data  => Spark Core API (RDD API)
	Batch Processing of structured data    => Spark SQL 
	Streaming analytics (real-time)	       => Spark Streaming (structured streaming)
	Predictive analytics (using ML)	       => Spark MLlib
	Graph parallel computations	       => Spark GraphX


    Spark is a polyglot
	=> Spark supports Scala, Java, Python and R 
   
    Spark can be executed on mutliple cluster managers:
	=> local, spark standalone, YARN, Mesos, Kubernetes.


   Getting started with Spark
   --------------------------
    
    1. Using the vLab allocated to you

            1. PySpark shell
	       -> Open a terminal window and type 'pyspark'

	    2. Spyder IDE
	       -> Search and lauch Spyder IDE


    2. Installing PySpark dev. env. on your local machine

	   => Try to use pip install
		pip install pyspark

	   => Make sure you have anaconda navigator installed

	   => Follow the steps mentioned in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


    3. Signup to Databricks Community edition  
		
	    Signup URL: https://www.databricks.com/try-databricks
			(Check for community edition)

	    Login URL: https://community.cloud.databricks.com/login.html

  
   Spark Architecture
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks is reported to the driver.


   Resilient Distributed Dataset (RDD)
   -----------------------------------

	RDD is the fundamental data abstraction in a Spark Core API

	RDD is a collection of distributed in-memory partitions.
	   -> Each partition contains objects of some type. 

	RDD has two components:
	   -> RDD meta data : Lineage DAG 
	   -> RDD data : Collection of partitions

	=> RDDs are immutable

	=> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands cause execution.


   Creating RDDs
   --------------

	Three ways:

	1. Create an RDD from some external data source

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd2 = sc.parallelize( [4,1,3,2,4,6,7,8,9,6,8,9,0,7], 2)


	3. By applying transformations of existing RDDs

		 rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   RDD Operations
   --------------
	
     Two types of operations

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.


   RDD Lineage DAG
   ---------------
   Driver maintains the lineage DAG of each RDD when the RDD object is created
   Lineage DAG is a logical plan on how to create the RDD partitions
   This tracks all dependent RDDs (heirarchy) all the way from the very first RDD.


 	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	
	-> Lineage of rdd1: rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.split(" "))
	-> Lineage of rdd2: rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: (x, 1))
	-> Lineage of rdd3: rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
	-> Lineage of rdd3: rdd4 -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile

   ***  rdd1.getNumPartitions() => returns the number of partitions of the RDD
       

  RDD Persistence
  ---------------

	rdd1 = sc.textFile(<filePath>, 40)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )  -> instruction to spark to save the partitions of rdd6
	rdd7 = rdd6.t7(...)

	l1 = rdd6.collect()
	Lineage DAG of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		Plan: (textFile, t3, t5, t6) -> rdd6

	l2 = rdd7.collect()
	Lineage DAG of rdd7 => rdd7 -> rdd6.t7 
		Plan: (t7) -> rdd6	

	rdd6.unpersist()
        

	StorageLevel
	------------
	MEMORY_ONLY	   -> default, Memory serialized 1x replicated
	MEMORY_AND_DISK    -> Disk Memory serialized 1x replicated
	DISK_ONLY          -> Disk serialized 1x replicated
	MEMORY_ONLY_2      -> Memory serialized 2x replicated  
	MEMORY_AND_DISK_2  -> Disk Memory serialized 2x replicated
	

	Commands
	---------
	  rdd1.cache()     -> memory_only persistence
	  rdd1.persist()   -> memory_only persistence
	  rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	  rdd1.unpersist()
	
  
  Executor's memory strcuture
  ---------------------------
	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  Types of Transformations
  ------------------------     
	Two types:

	1. Narrow Transformations

	2. Wide transformations


  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to object transformation
			Transforms the objects by applying the function.
			input RDD: N objects, output RDD: N objects
	
		rdd2 = rddFile.map(lambda x: len(x))

  2. filter		P: U -> Boolean
			Filters the objects to the output RDD for which the function returns True
			input RDD: N objects, output RDD: <= N objects

		 rddFile.filter(lambda x: len(x.split(" ")) > 8 ).collect()

  3. glom 		P: None
			Returns one list per partition with all the objects of the partition
			input RDD: N objects, output RDD: objects = # of partitions

		rdd1			  rdd2 = rdd1.glom()
		P0: 3,1,2,4,3,6 -> glom -> P0: [3,1,2,4,3,6]
		P1: 4,6,3,5,4,7 -> glom -> P1: [4,6,3,5,4,7]
		P2: 6,7,8,0,4,8 -> glom -> P2: [6,7,8,0,4,8]

		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)

  4. flatMap		P: U -> Iterable[V]
			flatMap flattens the output produced by the function.
			input RDD: N objects, output RDD: >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			Partition to partition transformation

		rdd1	    rdd2 = rdd1.mapPartitions(lambda p: [sum(p)])  
		P0: 3,1,2,4,3,6 -> mapPartitions -> P0: 19
		P1: 4,6,3,5,4,7 -> mapPartitions -> P1: 29
		P2: 6,7,8,0,4,8 -> mapPartitions -> P2: 33

		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()
		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).glom().collect()


  6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V]
				Partition to partition transformation
				You get the partition-index as additional function argument. 

	rdd1.mapPartitionsWithIndex(lambda i, p: [(i, sum(p))] ).filter(lambda p: p[0] == 1).collect()


   	
  7. distinct		P: None, Optional: numPartitions
			Returns the distinct objects of the input RDD.

		rdd1.distinct().glom().collect()
		rdd1.distinct(5).glom().collect()


  8. sortBy



      

     















	













