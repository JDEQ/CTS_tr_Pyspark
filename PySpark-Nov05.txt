
  Curriculum
  ----------

   Understanding Spark
   RDD API - Transformations & Action 
   Spark SQL - DataFrames   
   Machine Learning & Spark MLLib
   Spark Streaming - Introduction

 ------------------------------------------------------------------------

   Big Data -> A set of data sets that are huge and complex that the traditional
               data management systems can not reasonbly process. 

    1. Volumn
    2. Velocity - Data growth.
    3. Variety  - Unstructured data or Semi-Structured
    4. Varacity - Data in doubt.  

   Traditional Data Management Systems -> Single server based system. 
 
   Cluster based computational systems are the solution. 

   Cluster -> Is a group of connected nodes that form a unified system whose cumulative 
              resources can be used to distribute your storage and processing requirements. 
   
   Hadoop Framework --> Distributed Storage and Processing Framework
                    --> HDFS: (Storage) 128 MB blocks
                        MapReduce (Distributed Parallel Processing) 
                        YARN  ( Cluster Manager / Resource Manager )

   MapReduce -> Is a disk based execution framework

   MapReduce -> Is not good for
                -> Iterative processing (ML)
                -> Lot of small files 
                -> Ad-hoc queries

   Spark is solution to the limitations of MapReduce Framework 

  -----------------------------------------------------------------------------

   Spark -> Open source unified in-memory distributed processing framework.
         -> Very memory intensive (needs lot of RAM)
         -> Is a polyglot ( Scala, Python, Java, R )
    
   Hadoop
   ------
      -> Batch processing of unstructured data    :  MapReduce
      -> Batch processing of structured data      :  Hive, Pig, Impala, Drill ...
      -> Stream data processing                   :  Storm, Kafka
      -> Machine Learning                         :  Mahout
      -> Graph Parallel Computations              :  Giraph
                 

   Spark unified framework
   ----------------------- 
   Spark provides a consistent set of APIs to process various analytics work loads.
   
      -> Batch processing of unstructured data    :  Spark Core (RDD)
      -> Batch processing of structured data      :  Spark SQL
      -> Stream data processing                   :  Spark Streaming, Structured Streaming
      -> Machine Learning                         :  Spark MLlib
      -> Graph Parallel Computations              :  Spark GraphX
   
  ----------------------------------------------------------------------------------

   Spark Building Blocks
   ---------------------

    1. Cluster Manager
          -> Spark apps are submitted to the cluster manager
          -> CM will register the job and schedules it by allocating some executors to the job
        
          -> Spark Standalone Scheduler, YARN, Mesos, Kubernetes.

    2. Driver Process
          -> Master Process which manages all the tasks of the spark job
          -> This is program where main() method is defined.
          -> Contains a SparkContext, which represents the connection to the cluster. 
          -> Will analyize the DAG and determines which tasks to be launched and when
             on the executors and accordingly send the tasks to the executors.

         Modes:
           1. Client Mode: (default mode) driver runs on the client machine.
           2. Cluster Mode: driver runs as one of the processes on the cluster.

    3. Executor Processes
          -> Tasks are launched in these executors by the driver process
          -> Each tasks is functinally same, but runs on a small chunk of data. Several
             tasks are launched parallelly across multiple executors.

    4. Spark Context
          -> Runs in the driver process
          -> represents an application's connection to the cluster with a defined config. 
  
 ----------------------------------------------------------------

   Programming Lang      =>  Scala, Python, Java & R
   Spark High Level API  =>  Spark SQL, Spark Streaming, Spark MLlib, Spark GraphX
   Spark Core API        =>  RDD based API
   Resource Managers     =>  Standalone Scheduler, YARN, Mesos, Kubernites   
   Storage Layer         =>  Linux, HDFS, RDMS, NoSQL, Kafka

------------------------------------------------------------------

   RDD - Resilient Distributed Dataset
   -----------------------------------

    RDD is the core data abstraction of Spark.


    RDD -> Partitioned 
           -> Distributed Dataset as partitions
           -> Each partition contains a collection of objects

        -> Immutable: you can not change the contents of an RDD   
     
        -> Lazilily Evaluated.
            -> Execution will happen only when you have an action command.
 
        -> Resilient (Fail Safety)
            -> RDDs can recreate the missing partitions by recomputing the partitions
               by executing the required tasks from the RDD lineage DAGs.
   

  How to create an RDD
  --------------------

   1. An RDD can be created from an external data file:
        rdd = sc.textFile ( file ) 

   2. An RDD can be created by parallelizing programmatic data
        rdd = sc.parallelize( [1,2,3,4,5,6,7,8] )

   3. By applying transformations on existing RDDs, we can create an other RDD
        rdd2 = rdd1.<transformation> 


  What can you do with an RDD
  ---------------------------

   1. Transformations
       -> represent in-memory transformation of input RDDs into output RDD
       -> tranformations create RDD
       -> transformations does not cause execution. 
       -> transformation cause the lineage DAG (directed acyclic graph) to 
          be created (at the driver side)
     

   2. Actions
       -> Action command produce output such as sending the data from the RDD to the driver
          or saving the contents of RDD into a file. 

       -> Actions trigger execution. 

       -> When an action command is seen by the driver, it will analyze the DAG, and 
          launches the set of tasks that need to be performed on the executors to get
          the output.

rdd1 = sc.textFile( filePath, 3 )
rdd2 = rdd1.map(lambda x: x.split(" "))
rdd3 = rdd2.map(lambda x: len(x))
rdd3.collect()  <-- action  

 Lineage DAG:
 ------------
 rdd1 -->  sc.textFile
 rdd2 --> rdd1.map --> sc.textFile
 rdd3 --> rdd2.map --> rdd1.map --> sc.textFile



  rdd1 = sc.textFile(filePath, 9)
  rdd2 = rdd1.flatMap(lambda x: x.split(" "))
  rdd3 = rdd2.map(lambda x: (x, 1))
  rdd4 = rdd3.reduceByKey(lambda a,b: a + b)

  rdd4.collect

  rdd4  -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile 

  sc.textFile -> flatMap -> map -> reduceByKey -> rdd4


  Lineage of RDDs
  ---------------

  rdd1  -> sc.textFile
  rdd2  -> rdd1.flatMap -> sc.textFile
  rdd3  -> rdd2.map -> rdd1.flatMap -> sc.textFile
  rdd4  -> rdd3.reduceByKey -> rdd2.map -> rdd1.flatMap -> sc.textFile


  ================================================================================
   
  Repository:  https://github.com/ykanakaraju/pyspark

  ===============================================================================
 
   Lab:
   ----
      CentOS --> Terminal --> pyspark
                          --> jupyter notebook --allow-root

  ==============================================================================

   RDD Parsistence
   ---------------

    rdd1 = sc.textFile( file )
    rdd2 = rdd1.t1()
    rdd2.persist()
    rdd3 = rdd2.t2()
    rdd4 = rdd2.t3()
    rdd5 = rdd2.t4()
    rdd6 = rdd5.t5()
    rdd7 = rdd6.t6()
    rdd8 = rdd6.t7()
    rdd8.persist()    --> The partitions of rdd8 are saved in memory, not GCed automatically.
    rdd9 = rdd8.t8()

    rdd9.collect()
       rdd9 => rdd8.t8 => rdd6.t7 => rdd5.t5 => rdd2.t4 => rdd1.t1 => sc.textFile
  

   Types of Persistence:
   ---------------------

     2 formats -> Deserialized format or serialized format

        ex: rdd8.persist( StorageLevel.MEMORY_ONLY )
            rdd8.cache() (same as above command ie storing in MEMORY_ONLY level)

            rdd8.unpersist()

    Storage Levels:
    ---------------

      1. MEMORY_ONLY :      Deserialized format in Storage memory ONLY.
      2. MEMORY_AND_DISK :  Deserialized format in MEMORY if available, else store on the disk.
      3. DISK_ONLY :        Deserialized format in disk only

      4. MEMORY_ONLY_SER :  Serialized format in Storage memory ONLY.    
      5. MEMORY_AND_DISK_SER

      6. MEMORY_ONLY_2
      7. MEMORY_AND_DISK_2   
 
  ==============================================================================

    Spark Executor Memory Structure
    -------------------------------

      Let us say we are requesting for exexutors with a memory of 10GB each.
      CM will allocate 10.3 GB exexutors to the Spark job.


      1. Reserved Memory: 300 MB


      2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB (unified memory area)
 
           2.1  Storage Memory (spark.memory.storageFraction : 0.5 ) => 3 GB  
                 -> Persist RDDs & Broadcast Variables

           2.2  Execution Memory => 3 GB
                 -> All RDD partition creation, running os tasks etc use this memory.
           

      3. User Memory   => 4 GB
           -> python/java/scala related code, objects use this memory

  ==============================================================================
    
    Types of Transformations
    ------------------------

      1. Narrow Transformations
           -> Simple and efficient
           -> Does not cause shuffling of the data across executors
           -> the computation of each partitions depends ONLY on the input partitions
              (partition-to-partition computations)
 

      2. Wide Transformations     
           -> Causes shuffling of the data across executors/nodes
           -> The computation of each partition requires all the partitions of the input RDD. 
           -> Every wide transformation results in stage-boundary.

        
   rdd9 => textFile, map, distict, map, flatmap, sortBy, filter

    stage 1: <textFile, map>  
    stage 2: <distinct, map, flatmap>
    stage 3: <sortBy, filter> 

   Job: Each action command results in a Job
    
   Stages: Each job is divided in stages. 
           Each stage has tasks that can run in parallel
           Stages have to launched one after other

   Tasks: Each Stage has a set of tasks that can run in parallel
          

   Job -> Stages -> Tasks


  RDD Transformations
  -------------------

  1. map             => F: U => V
                     => Transforms each object to another object by applying the function
                     => Input RDD Count: N, Output RDD Count: N 


  2. filter          => F: U => Boolean
                     => The output RDD will have only those elements that return 'true'
		     => Input RDD Count: N, Output RDD Count: <= N 


  3. glom            => F: None
                     => Will create an Array with all the elements of each partition of the RDD
                     => Input RDD Count: N, Output RDD Count: = number of partitions of input RDD


  4. distinct        => F: None
                     => Will output only distinct elements from the input RDD
		     => Input RDD Count: N, Output RDD Count: <= N 
                     => Results in global shuffle.

  
  5. flatMap         => F: U => Iterable[V]
                     => Will flatten all the elements of the iterables generated by the function. 
                     => Input RDD Count: N, Output RDD Count: >= N


  6. mapPartitions   => F: Iterable[U] => Iterable[V]
                     => Will take an entire partition (all the objects of each partition) 
                        as input an return an iterable.
                     => Input RDD Count: N, Output RDD Count: = number of partitions of input RDD

  
  7. mapPartitionsWithIndex => F: (Int, Iterable[U]) => Iterable[V]

  		     => Will take the partition index and the partition elements as functions inputs 
                        an return an iterable.

                     => Input RDD Count: N, Output RDD Count: = number of partitions of input RDD


   8. sortBy        => F: U => V, where V is the key based on which the elements are sorted.
                    => Will sort the elements of the RDD based on the output of the function.
		    => Input RDD Count: N, Output RDD Count: = N 


   9. randomSplit   => F: takes array of ratios
                    => The RDD is split into an array of RDDs based on the ratios specified. 
                    => You can specify a seed value as second argument to repeat the output across
                       multiple executions.

           ex: rddArr = wordsRdd.randomSplit( [0.5, 0.3, 0.2] )
               rddArr = wordsRdd.randomSplit( [0.5, 0.3, 0.2], 14 )


   Guidelines to determine how many partitions should you have.
   ------------------------------------------------------------
   -> The optimal size of each partition is ~ 128 MB ( 100MB to 200MB )
   -> The number partitions can be 2 to 3x the number of cores you have.
   -> If the number of partitions is close to 2000, then bump it up to 2000+ partitions

   
   10. repartition(n) => Is used to change the number of partitions of an RDD
                         Useful in the case narrow transformations. 

                         Is used to increase or decrease the number of partitions
                         Results in global shuffling.

   11. coalesce(n)  => Is used ONLY to decrese the number of partitions
                       Results in partition merging, does not global shuffling.
               
   
   Two types of RDDs
   -----------------   
       1. Generic RDD - RDD[U]
       2. Pair RDD    - RDD[(U, V)]
     

   12. partitionBy  => Allows you apply your own partitioning logic to partition the RDDs
                       (Hash Partitioner & Range Partitioners)
                       
                       Applied only on Pair RDD


   13. subtract, intersection, union, cartesian
                    => Applies on two RDDs
                    => ex: rdd3 = rdd1.union(rdd2)
  
          Let's say rdd1 has 2 partitions & rdd2 has 3 partitions:

          ex 1:  rdd3 = rdd1.subtract(rdd2)     -> rdd3 has 2 + 3 partitions. (wide)
          ex 2:  rdd4 = rdd1.intersection(rdd2) -> rdd4 has 2 + 3 partitions. (wide) 
          ex 3:  rdd5 = rdd1.union(rdd2)        -> rdd5 has 2 + 3 partitions. (narrow)
          ex 3:  rdd6 = rdd1.cartesian(rdd2)    -> rdd5 has 2 * 3 partitions. (wide) 



   xxxByKey => They work only on pair RDDs
   ---------------------------------------

   14. mapValues  => map function applied only on the values part of a pair RDD.
       

   15. groupByKey  => Applied on an RDD[(U, V)], groupBy returns an RDD of type RDD[(U, Iterable[V])]
                   
                   => It will aggregate all the values of each unique key, so that the output will
                      have only unique keys and aggregated value.

                   => Results in global shuffle. DO NOT USE IT.
    
   16. reduceByKey  => Take a reduce function as input and applies that reduce function
                       on all the values of each unique key in the RDD, so that the output will
                       have only unique keys and reduced values.

                       Is a two stage operation:
                          stage 1: Reduces the values in each partition (narrow)
                          stage 2: Reduce the outputs of all the partitions (wide)
   


   17. sortByKey      => sorts the data based on the key

   18. aggregateByKey => aggregateByKey(zero-value, seq-op, comb-op) 

                         zero-value: is the value you start with; is of the type of your final result.
                         seq-op: is a fold function, whose output is of the type of zero-value.
                                 is applied to indidual partitions.

                         comb-op: is a reduce operation that reduces all the values of each unique
                                  key across partitions (applied on the output of seq op)


  P0 : (a, 10) (a, 20) (a, 25) (b, 12) (b, 25) (c, 12) (c, 7) 
    stage 1:  (a, (55, 3)) (b, (27, 2))  (c, (19, 2))

  P1 : (a, 13) (a, 25) (a, 15) (b, 2) (b, 5)
    stage 1: (a, (53, 3))  (b, (7,2))


  P2 : (a, 11) (a, 10) (a, 22) (b, 52) (b, 35) 
    stage 1: (a, (43, 3))  (b, (87,2))


    zero-value:  (0, 0)
    seq-op: (lambda x, y: (x[0] + y, x[1] + 1) )
    comb-op: (lambda x, y: (x[0] + y[0], x[1] + y[1])



  



