
  Agenda ( PySpark - 8 session of 4 hours each)
  -----------------------------------------------
 
   -> Spark : Basics & Architecture
   -> Spark Core API (Low level API)
	-> RDD Operations - Transfomations & Actions
	-> Shared Variables
   -> Spark SQL
	-> DataFrame Operations
	-> Integrations - MySQL(JDBC), Hive
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
    1. PDF Presentations
    2. Code Modules (PySpark programs)   
    3. Class Notes
    => Git Hub: https://github.com/ykanakaraju/pyspark

  Pre-requisites
  --------------
	1. Python Programming language
	2. SQL

  Spark
  -----	
     => Spark is written in SCALA Programming language

     => Spark can be upto 100 times faster than MapReduce if you use 100% in-memory computations
	Spark can be upto 6 to 7 times faster than MapReduce even if use disk-based computations

     => Spark is a unified in-memory distributed computing framework open source framework
	for big-data analytics.    

    Cluster: Is a unified entity comprising of many nodes whose cumulative resources can be used
             to distribute the storage and processing.     

    In-Memory Distributed Computing: The intermediate results of parallel tasks that are running
	across the cluster are persisted in RAM and subsequent tasks can directly process these
	saved results. 

    Spark Unified Stack:  Spark provides a consistent set of API for processing different analytics
	workloads based on the same execution engine.

	=> Batch Processing of Unstructured Data	: Spark Core API
	=> Batch Processing of Structured Data		: Spark SQL
	=> Stream processing (real time)		: Spark Streaming & Structured streaming
	=> Predictive Analytics (using ML)		: Spark MLlib
	=> Graph Parallel Computations			: Spark GraphX

    Spark is a polyglot
	=> Supports multiple programming languages
	=> Scala, Java, Python, R
	

  Spark Architecture
  ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


	=> Web UI: http://localhost:4040/


  Getting started with PySpark
  ----------------------------
    1. Using your vLab
	  
          -> Follow the instaructions mentioned in the attachment
	
          -> You launch a Window server
		-> Click on the "Oracle VM Virtualbox" icon
		-> Your userid and password are found on a docuemnt on the desktop.
		-> Lanuch the VM from the "Oracle VM Virtualbox"

	  -> Launch "PySpark" shell.
		-> You can practice here...

          -> Lauch Jupyter Notebook
		-> Open a terminal and type the following command.
			-> juputer notebook   
		-> If the above command gives an exception:
			-> jupyter notebook --allow-root

    2. Setting up PySpark environment on your personal machine. 

	-> Make sure to install 'Anaconda Distribution'
		url: https://www.anaconda.com/products/distribution

	-> Just follow the instruction given in the shared document.
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf 

    3. Signup to 'Databricks Community Edition' account. 

	-> Signup: https://www.databricks.com/try-databricks
	-> Login:  https://community.cloud.databricks.com/login.html
	

  Spark Core API
  --------------
      => Low-level API (for unstructured data process)


  RDD (Resilient Distributed Dataset)
  -----------------------------------
      -> Fundamental data abstraction of Spark Core API.
      
      -> RDD is a collection of distributed in-memory partitions.
	  -> Each partition is a collection of object (of any type)

      -> RDD has two components:
		Metadata : Lineage DAG (logical execution plan)
		Data: distributed in-memory partitions

      -> RDDs are immutable
	   -> RDDs partition data can not changed once created
	   -> We can apply transformations to create new RDDs

      -> RDDs are lazily-evaluated.
	   -> Transformations does not cause execution.
	   -> Only action commands trigger execution


   Creating RDDs
   -------------

	Three ways:

	1. Creating RDDs from external data files
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		=> default number fo partitions is given by sc.defaultMinPartitions

	2. Creating RDDs from programmatic data
		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0], 3)

		rdd1 = sc.parallelize( [4,2,3,1,5,4,6,7,8,9,8,7,6,3,5,4,7,6,8,9,0])
		=> default number fo partitions is given by sc.defaultParallelism

        3. By applying transformations on existing RDDs
		
		rdd2 = rdd1.map(lambda x: x*100)




   RDD Operations
   --------------
	Only two types of operations:

	1. Transformations
		-> does not cause execution
		-> only create lineage DAGs of the RDDs

	2. Actions
		-> Triggers execution on the RDD and produces output.	


   RDD Lineage DAGs
   ----------------
    => Transformations does not cause execution. They only create lineage DAGs of the RDDs.

    => A Lineage DAG maintains the hierarchy of dependencies (on other RDDs) that caused the creation of
       this RDD all way from the very first RDD.

    => Use rddWc.toDebugString()  to see the plan


	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG: rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage DAG: rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG: rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG: rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


   















