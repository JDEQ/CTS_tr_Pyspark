
  Agenda - PySpark
  -----------------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming

  Materials
  ---------
     -> PDF presentations
     -> Code examples
     -> Class notes
    
  Spark
  -----

    -> Is a unified in-memory distributed computing framework.	
    -> Spark is written in 'Scala'

    Cluster => A group of nodes whose cumulative resources can be used to distribute 
               your storage and processing.

    In-Memory Computing => The intermediate results of tasks can be persisted in RAM.

  
    Unified Framework
    -----------------

	=> Spark provides a consistent set of APIs for processing different analytical workloads
	   using the same execution engine.

	-> Batch processsing of unstructured data	: Spark Core API (RDDs)
	-> Batch processsing of structured data		: Spark SQL
        -> Stream processing				: Spark Streaming, Structured Streaming
        -> Predictive analytics	(machine learning)	: Spark MLlib			
        -> Graph parallel computation			: Spark GraphX
 

    -> Spark is a polyglot
	-> You use Scala, Java, Python and R to write Spark application. 

    -> Spark applications can be submitted to muliple cluster manager.
	-> local, Spark Standalone, YARN, Mesos, Kubernetes. 


   Spark Layered Architecture
   ---------------------------   
    Programming Lang. :  Scala, Python, Java, R
    Spark High level  :  Spark SQL, Spark MLlib, Spark Streaming, Spark GraphX
    Spark Low Level   :  Spark Core API (RDDs)
    Cluster Managers  :  Spark Standalone, YARN, Mesos, Kubernetes
    Storage Layer     :  Linux, HDFS, Cloud, Kafka (Messageing systems), NoSQL, RDBMS


    Spark Architecture
    ------------------

     1. Cluster Manager (CM)
	
	-> Applications are submitted to CMs
	-> Schedules the application and lauches the application
	-> Allocates resources (executors) to the application.

     2. Driver 
	-> Master process
	-> manages the user code
	-> analyses the user code and sends tasks to the cluster. 

	Deploy modes
		1. client   -> default. Driver runs on the client machine.
		2. cluster  -> Driver runs on one of the nodes on the cluster.
     
     3. Executors
	-> Execute the tasks sent by the driver
	-> reports the status back to the driver
	-> All tasks does the same function but on different partitions of data. 

     4. SparkContext		
	 -> Starting point of exection that creates a driver process
	 -> Represents an application context
	 -> Is a link between the driver process and the tasks running in the cluster
	 -> Runs inside a driver process.


    Getting started with Spark
    ---------------------------
	1. In you vLab:
 
             -> Click on the CentOS 7 icon on the windows desktop. 
	     -> Login with your userid and password (Readme.txt)
	     -> Open a terminal
	    
	     	1. PySpark shell:   (type 'pyspark' at your prompt)  
			$ pyspark

		2. Working Jupyter Notebook
			$jupyter notebook   
			$jupyter notebook --allow-root	

			-> This will launch Jupyter notebook. 
			-> There is a "PySparkTest.ipynb". Open it to see how to get started. 

	2. Databricks Comminity Edition
		=> URL: https://databricks.com/try-databricks
		-> Signup using the above URL
		-> Complete the signup process by following the link that is sent to your email. 
  		-> Login to Databricks Cloud:
			Link: https://community.cloud.databricks.com/login.html
		-> Go thorugh "Guide: Quickstart tutorial" link & "Data import" link

       3. Setting up PySpark with Spyder (or Jupyter Notebook) on your local machine.
		-> Install "Anaconda Navigator" first
		-> Try "pip install pyspark" 
		        If this does not work: 
			-> Follow the instructions document" shared in the Github.


    RDD (Resilient Distributed Dataset)
    -----------------------------------

     => RDD is the fundamental data abstraction of Spark Core API

     => Is a collection of distributed in-memory partitions.
	  -> Each partition is a collection of objects.

     -> RDDs are immutable

     -> RDDs are lazily evaluated.
	-> Transformations does not cause execution
	-> Action commands trigger execution

     
    How to Create RDDs ?
    -------------------
	
	Three ways:

	1. Create an RDD from external data files

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

	2. Create RDDs from programmatic data (python collections)

		rdd10 = sc.parallelize( range(1, 101), 3 )

	3. Apply applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())
		rdd3 = rdd2.filter(lambda x: len(x) > 51)
	

    What can you do with an RDD ?
    -----------------------------

	Only two things:

	1. Transformations
		-> Cause creation of Lineage DAGs (logical plan) of RDDs
		-> Returns an RDD
		-> Does not cause execution.

	2. Actions
		-> Produces some output
		-> Cause the logical plan to be converted into a plan plan and set of tasks
		   to launched on the cluster.


    RDD Lineage DAG (Logical execution plan)
    -----------------------------------------
	-> Is a set of instruction maintained by driver on how to create the RDD.

	-> Is a logical plan, containing all the dependencies all the way from the very first RDD
           that caused creation of this RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage of rddFile: rddFile  -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage of rdd2:  rdd2 -> rddFile.map  -> sc.textFile

	rdd3 = rdd2.filter(lambda x: len(x) > 51)
		Lineage of rdd3:  rdd3  -> rdd2.filter -> rddFile.map  -> sc.textFile

	rdd4 = rdd3.flatMap(lambda x: x.split(" "))
		Lineage of rdd4:  rdd4 -> rdd3.flatMap -> rdd2.filter -> rddFile.map -> sc.textFile
    
	rdd4.collect() => This will cause execution..
		Transformations: sc.textFile -> map -> filter -> flatMap -> rdd4


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD persistence
   ---------------

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_ONLY )          --> instruction to spark to persist the rdd6 partitions. 
	rdd7 = rdd6.t7( .. )

	rdd6.collect() 
	
	rdd6 lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	execution: sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> list

	rdd7.collect()

	rdd7 lineage: rdd7 -> rdd6.t7
	execution: (rdd6) --> t7 (rdd7) --> list

	
	Storage Levels
        ---------------
	
	1. MEMORY_ONLY 		-> (not applicable to pyspark) 
				   deserialized, in-memory persistence

	2. MEMORY_AND_DISK	-> (not applicable to pyspark) 
	
	3. DISK_ONLY

        4. MEMORY_ONLY_SER	-> (default in pyspark) serialized, in-memory persistence

	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2        -> 2 replicas on on two different executors.

	7. MEMORY_AND_DISK_2


	Commands:
        ---------	
	  rdd1.persist()     
	  rdd1.persist( StorageLevel.DISK_ONLY )
	  rdd1.cache()  -> in-memory persistence

	  rdd1.unpersist()

     
   RDD Transformations
   -------------------

   1. map			P: U -> V
				Object to object transformation	
				Input RDD: N objects, OutputRDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).collect()

  2. filter			P: U -> Boolean
				Filters the objects that return True for the function to be in the
				output RDD.
				Input RDD: N objects, OutputRDD: <= N objects

	rddFile.filter(lambda x: len(x) > 51).collect()

  3. glom			P: None
				Returns a list object per partition with all the objects of the partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 5,2,7,3,4,8  -> glom -> P0: [5,2,7,3,4,8]
		P1: 7,5,6,8,2,3  -> glom -> P1: [7,5,6,8,2,3]
		P2: 6,2,3,5,1,0  -> glom -> P2: [6,2,3,5,1,0]

		rdd1.count() = 18 (int)	    rdd2.count() = 3 (list)


   4. flatMap			P: U -> Iterable[V]
				Flattens the elements of the iterable produced by the function.
				Input RDD: N objects, OutputRDD: >= N objects

	       rddWords = rddFile.flatMap(lambda x: x.split(" "))

  
   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies the function on the entire partition.

		rdd1			rdd2 = rdd1.mapPartitions( lambda x :  )

		P0: 5,2,7,3,4,8  -> mapPartitions -> P0: 
		P1: 7,5,6,8,2,3  -> mapPartitions -> P1: 
		P2: 6,2,3,5,1,0  -> mapPartitions -> P2: 

		rdd1.mapPartitions(lambda x : [max(x)]).collect()
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x) ).collect()				


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Same as mapPartitions, but partition-index as an additional function input
				parameter. 
				
	rdd1.mapPartitionsWithIndex(lambda i, x : [(i, max(x))]).collect()
	rdd1.mapPartitionsWithIndex(lambda pid, pdata: map(lambda a: (pid, a*10), pdata) ).glom().collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct elements of the input RDD.
				Input RDD: N objects, OutputRDD: <= N objects

		rddWords.distinct().collect()

		
   8. sortBy			P: U -> V, Optional: acsending: True/False,  numPartitions
				Objects of the output RDD are sorted based on the funtion output. 

		rdd1.sortBy(lambda x: x%4).glom().collect()
		rdd1.sortBy(lambda x: x%4, False).glom().collect()
		rdd1.sortBy(lambda x: x%4, True, 4).glom().collect()

   Types of RDDs:

	-> Generic RDDs: RDD[U]
	-> Pair RDDs: RDD[(U, V)]

  9. mapValues			P: U -> V
				Applied only to pair RDDs.
				Transforms only the 'value' part of (k, v) parts by applying the function.

	rdd2.mapValues(lambda x: x*10).collect()

  10. groupBy			P: U -> V, Optional: numPartitions
				Returns a pair RDD, where:
					key: each unique value of the function output
					value: A ResultIterable object with elements of the RDD that produced the key.

		rdd1.groupBy(lambda x : x%2).mapValues(list).glom().collect()
		rdd1.groupBy(lambda x : x%2, 2).mapValues(list).glom().collect()
		
		# wordcount program
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.groupBy(lambda x: x) \
            		.mapValues(len) 


   11. randomSplit		P: Ratios (ex: [0.6, 0.4])
				Splits the  RDD randomly into multiple RDDs in the given ratios.

		rddList = rdd1.randomSplit([0.5, 0.5])	
		rddList = rdd1.randomSplit([0.5, 0.5], 456)

   12. repartition		P: numPartitions
				Used to reorganize the number of partitions of the output RDD
				Can increase or decrease the number of partitions
				Causes global shuffle. 

		rdd2 = rdd1.repartition(6)   // rdd2 will have 6 partitions

    13. coalesce		P: numPartitions
				Used to only decrease the number of partitions of the output RDD
				Causes partition merging

		rdd2 = rdd1.coalesce(3)    // rdd2 will have 3 partitions

    14. partitionBy		P: numPartitions, Optional: partitioning function (U -> Int)	
				Applied only to Pair RDDs
				Controls which objects go to which partitions based on the value of 
				partition function that takes the key as input. 
				
		rdd6 = rdd5.partitionBy(4)   // hash of the key is used to map to a partition
		rdd6 = rdd5.partitionBy(4, lambda key :  key + 1 )

    15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions & rdd2 has N partitions

	command				number of output partitions
        ------------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide
		

   ..ByKey Transformations
   -----------------------
	-> Are all wide transformations
	-> Are applied only to Pair RDDs
	-> Optional Parameter: numPartitions	
			
    16. sortByKey		P: None, Optional: Ascending (True/False), numPartitions
				Sorts the Pair RDD based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()      # desc based on the key
		rddPairs.sortByKey(True, 4).collect()    # asc sort with 4 output partitions


    17. groupByKey		P: None, Optional: numPartitions
				Groups the elements of the RDD by key. Output RDD will have unique keys and
				grouped values.

				NOTE: Avoid it if possible.

				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]

		rddPairs.groupByKey().mapValues(sum).collect()
		rddPairs.groupByKey(1).mapValues(sum).collect()    // output will have only one partition.


   18. reduceByKey		 P: (U, U) -> U
				 Reduces all the values of each unique key with in each partition and then
				 across partitions by iterativly applying the reduce function.
				
		rddPairs.reduceByKey(lambda a, b: a + b).collect()
		rddPairs.reduceByKey(lambda a, b: a + b, 3).collect()


   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs. 

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions.  

	student_rdd = sc.parallelize([
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

	student_rdd.collect()

	avg_rdd = student_rdd.map(lambda x: (x[0], x[2])) \
            		.aggregateByKey( (0,0),
                             lambda z, v: (z[0] + v, z[1] + 1),
                             lambda a, b: (a[0] + b[0], a[1] + b[1])) \
            		.mapValues(lambda x: x[0]/x[1])

 


   20. Joins 			=> join, leftOuterJoi, rightOuterJoin, fullOuterJoins
				   RDD[(U, V)].join(RDD[(U, W)]) => RDD[(U, (V, W))]
	
		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)	
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup			=> Is used to join RDDs where you may have duplicate keys.
				  -> groupByKey -> fullOuterJoin

	rdd1.cogroup(rdd2)


	rdd1 => [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	-> [(key1, [10, 7]), (key2, [12, 6]), (key3, [6])

	rdd2 => [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
	-> [(key1, [5, 17]), (key2, [4, 7]), (key4, [17])

       (key1, ([10, 7], [5, 17])) (key2, ([12, 6],[4, 7])) (key3, ([6], [])) (key4, ([], [17]))



	
  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce			P: (U, U) -> U
				Reduces the entire RDD into one final value of the same type by iterativly
				applying the reduce function on each partitions first (narrow) and then
				across partitions (wide).

		rdd1:				rdd1.reduce(lambda x, y: x + y)  

		P0: 8, 6, 4, 2, 1, 9, 7 -> f1 -> -21    Stage 2: [-21, -21, -17] -> f1 -> 19
		P1: 5, 3, 1, 8, 7, 4, 3 -> f1 -> -21
		P2: 1, 9, 0, 4, 2, 1, 2 -> f1 -> -17
		
		rdd1.reduce(lambda x, y: x - y)

   5. aggregate		-> Merges all the values of an RDD with a zero-value to produce one final output of
			   type which is different than the type of the elements of the RDD. 
			-> The final output is of the type of zero-value. 

		Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final values of the type
                  of the zero-value.

   6. first	rdd1.first()

   7. take 	rdd1.take(10)

   8. takeOrdered 
		
		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15, lambda x: x%2)

   9. takeSample
	
		rdd1.takeSample(True, 10)       # withReplacement sampling
		rdd1.takeSample(True, 10, 34)   # withReplacement sampling with seed (34)
		rdd1.takeSample(False, 10)	# without replacement sampling

   10. countByValue

   11. countByKey	=> Applied only on PairRDD

   12. foreach		=> Applies a function on all the elements of the RDD
			   Does not return any value.

   13. saveAsSequenceFile


  Use-Case
  --------
   From cars.tsv file find out the average weight of each American make.
   Output required:  make, avgWeight
   Arrange the data in the DESC order of average weight
   Save the output as a single text file. 

      -> Try it yourself
      -> dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv


  
  Spark-submit
  -------------
	
	Is a single command to submit any spark application (Scala, Java, Python, R) to any cluster
	manager (local, spark standalone, yarn, mesos ..)

	$ spark-submit --master yarn 
		--deploy-mode cluster
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--driver-cores 2
		--num-executors 10
		E:\Spark\wordcount.py <command-line-args>

       
      spark-submit --master local[2]  E:\Spark\wordcount.py

      spark-submit --master local[2] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcoutput 1


    Closure
    -------
	=> Closure is those variables and methods which must be visible for an executor to performs its
           computations on the RDD

        => This closure is serialized and a separate copy os sent to every executor.

		// THis code won't work...
		counter = 0

		def isPrime( a ) :
			returns 1 if prime
			else 0

		def f1(n) :
			global counter
			if (isPrime(n) == 1) counter = counter + 1
			return n*2
		
		rdd1 = sc.parallelize( range(1, 4001), 4 )

		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(counter) 

        => Problem: We can not use "local variables" that part of closures to implement global
           counter.

	=> Solution: use 'Accumulators'
	

    Shared Variables
    ----------------

     Two shared variables:

     1. Accumulator

     -> Is a shared variable maintained by driver and is not part of function closure (and hence is
	not a local variable)
     -> All the distributed tasks can add to this accumulator
     -> Accumulators are used to implement counter.

		counter = sc.accumulator(0)

		def isPrime( a ) :
			returns 1 if prime
			else 0

		def f1(n) :
			global counter
			if (isPrime(n) == 1) counter.add(1)
			return n*2
		
		rdd1 = sc.parallelize( range(1, 4001), 4 )

		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(counter.value) 


   2. Broadcast Variables
 
              	lookup = sc.broadcast({1: 'a', 2:'e', 3:'i', 4:'o', 5:'u'}) 
		result = sc.parallelize([2, 1, 3, 4, 5]).map(lambda x: lookup.value[x]) 
		print( result.collect() )
   
  ===================================================
      Spark SQL   (pyspark.sql)
  ===================================================    

   => Spark Structured data processing API

           -> Structured File Formats: Parquet (default), ORC, JSON, CSV (delimited text file)
	   -> Hive 
	   -> JDBC -> RDBMS, NoSQL


   => SparkSession

	-> Starting point of execution
	-> Represents a user-sesssion inside an application

	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()  


  => DataFrame

	-> Data abstraction of Spark SQL

	-> DataFrame is a collection of distributed in-memory partitions

	-> DataFrames is a collection of "Row" objects  (pyspark.sql.Row)

	-> DataFrames components:
		-> data : A collection of partitions of Row objects
		-> schema : StructType objects. Represents the structure of the DF

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		   )
		)


   Working with DataFrames
   ------------------------

	1. Read/Load data from some data-source into a DataFrame

		df1 = spark.read.format("json").load(inputFile)
		df1 = spark.read.json(inputFile)


	2. Apply transformations on the DataFrame using DataFrame API methods or using SQL.

	    DataFrame API approach
            -----------------------

		df2 = df1.select("userid", "name", "age") \
         		.where("age is not null") \
         		.orderBy("age", "name") \
         		.groupBy("age").count() \
         		.limit(4)

            SQL approach
            ------------

		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry =  """select age, count(*) as count
          		from users
          		where age is not null
          		group by age
          		order by age
          		limit 4"""
          
		df3 = spark.sql(qry)


	3. Write/save the DF into a structured destination. 

		df3.write.format("json").save(outputPath)
		df3.write.json(outputPath)


  DataFrame Transformations
  -------------------------
   
   1. select

		df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME")

		df2 = df1.select(
		 	col("DEST_COUNTRY_NAME").alias("destination"),
                 	column("ORIGIN_COUNTRY_NAME").alias("origin"),
                 	expr("count"),
                 	expr("count + 10 as newCount"),
                 	expr("count > 365 as highFrequency"),
                 	expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
		      )


   2. where / filter
	
	df3 = df2.where("count > 300")
	df3 = df2.where( col("count") > 300 )

   3. orderBy / sort

	df3 = df2.orderBy( "count", "destination" )
	df3 = df2.orderBy( desc("count"), asc("destination") )

   4. groupBy    => returns a GroupedData object
		    You have to apply some aggregation to return DataFrame

	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
         .agg( count("count").alias("count"),
              sum("count").alias("sum"),
              max("count").alias("max"),
              avg("count").alias("avg"))

   5. limit
	
	df1.limit(10)

   6. selectExpr

	df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                 "ORIGIN_COUNTRY_NAME as origin",
                 "count",
                 "count + 10 as newCount",
                 "count > 365 as highFrequency",
                 "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

   7. withColumn

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 365")) \
        	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))
     

   8. withColumnRenamed

	df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
         	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 365")) \
        	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        	.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")
     


   Save Modes
   ----------
      -> What to do when writing to an existing directory

	1. errorIfExists
        2. ignore
        3. append
        4. overwrite

	=> df3.write.mode("overwrite").json(outputPath)
	=> df3.write.json(outputPath, mode="overwrite")


   LocalTempViews & GlobalTempViews
   ---------------------------------
	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.


   Working with different file formats
   -----------------------------------
	
    JSON
    -----
	Read
		df1 = spark.read.format("json").load(inputFile)
		df1 = spark.read.json(inputFile)

	Write
		df3.write.format("json").save(outputPath)
		df3.write.json(outputPath)
    Parquet
    --------
	Read
		df1 = spark.read.format("parquet").load(inputFile)
		df1 = spark.read.parquet(inputFile)

	Write
		df3.write.format("parquet").save(outputPath)
		df3.write.parquet(outputPath)
    ORC
    ----
	Read
		df1 = spark.read.format("orc").load(inputFile)
		df1 = spark.read.orc(inputFile)

	Write
		df3.write.format("orc").save(outputPath)
		df3.write.orc(outputPath)
    CSV
    ---
	Read
		df1 = spark.read.format("csv").load(inputFile, header=True, inferSchema=True) 
		df1 = spark.read.csv(inputFile, header=True, inferSchema=True)   // csv with header
		df1 = spark.read.csv(inputFile, header=True, inferSchema=True, sep="|")

	Write
		df2.write.csv(outputPath, mode="overwrite", header=True)
		df2.write.csv(outputPath, mode="overwrite", header=True, sep="|")		


   Creating an RDD from DataFrame
   ------------------------------
	rdd1 = df1.rdd


   Creating a DataFrame from Programmatic Data
   -------------------------------------------
	listUsers = [(1, 'Raju', 45),
             (2, 'Ramesh', 35),
             (3, 'Raghu', 25),
             (4, 'Rajeev', 15),
             (5, 'Ramya', 25),
             (6, 'Radhika', 35)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Creating a DataFrame from RDD
   -----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df2 = spark.createDataFrame(rdd1).toDF("id", "name", "age")


   Creating a DataFrame from RDD with programmatic schema
   ------------------------------------------------------
	mySchema = StructType([
            StructField("id", IntegerType()),
            StructField("name", StringType()),
            StructField("age", IntegerType())])

	df2 = spark.createDataFrame(rdd1, schema=mySchema)


   Creating a DataFrame from structured files with programmatic schema
   -------------------------------------------------------------------
	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType()),
            StructField("DEST_COUNTRY_NAME", StringType()),
            StructField("count", IntegerType())])

	inputFile = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	df1 = spark.read.json(inputFile, schema=mySchema)
	#df1 = spark.read.schema(mySchema).json(inputFile)

