
  Agenda - PySpark
  -----------------
  -> Spark - Basics & Architecture
  -> Spark Core API
	-> RDD Transformations & Actions
	-> Shared Variables
  -> Spark SQL
  -> Machine Learning & Spark MLlib
  -> Introduction to Spark Streaming

  Materials
  ---------
     -> PDF presentations
     -> Code examples
     -> Class notes
    
  Spark
  -----

    -> Is a unified in-memory distributed computing framework.	
    -> Spark is written in 'Scala'

    Cluster => A group of nodes whose cumulative resources can be used to distribute 
               your storage and processing.

    In-Memory Computing => The intermediate results of tasks can be persisted in RAM.

  
    Unified Framework
    -----------------

	=> Spark provides a consistent set of APIs for processing different analytical workloads
	   using the same execution engine.

	-> Batch processsing of unstructured data	: Spark Core API (RDDs)
	-> Batch processsing of structured data		: Spark SQL
        -> Stream processing				: Spark Streaming, Structured Streaming
        -> Predictive analytics	(machine learning)	: Spark MLlib			
        -> Graph parallel computation			: Spark GraphX
 

    -> Spark is a polyglot
	-> You use Scala, Java, Python and R to write Spark application. 

    -> Spark applications can be submitted to muliple cluster manager.
	-> local, Spark Standalone, YARN, Mesos, Kubernetes. 


   Spark Layered Architecture
   ---------------------------   
    Programming Lang. :  Scala, Python, Java, R
    Spark High level  :  Spark SQL, Spark MLlib, Spark Streaming, Spark GraphX
    Spark Low Level   :  Spark Core API (RDDs)
    Cluster Managers  :  Spark Standalone, YARN, Mesos, Kubernetes
    Storage Layer     :  Linux, HDFS, Cloud, Kafka (Messageing systems), NoSQL, RDBMS


    Spark Architecture
    ------------------

     1. Cluster Manager (CM)
	
	-> Applications are submitted to CMs
	-> Schedules the application and lauches the application
	-> Allocates resources (executors) to the application.

     2. Driver 
	-> Master process
	-> manages the user code
	-> analyses the user code and sends tasks to the cluster. 

	Deploy modes
		1. client   -> default. Driver runs on the client machine.
		2. cluster  -> Driver runs on one of the nodes on the cluster.
     
     3. Executors
	-> Execute the tasks sent by the driver
	-> reports the status back to the driver
	-> All tasks does the same function but on different partitions of data. 

     4. SparkContext		
	 -> Starting point of exection that creates a driver process
	 -> Represents an application context
	 -> Is a link between the driver process and the tasks running in the cluster
	 -> Runs inside a driver process.


    Getting started with Spark
    ---------------------------
	1. In you vLab:
 
             -> Click on the CentOS 7 icon on the windows desktop. 
	     -> Login with your userid and password (Readme.txt)
	     -> Open a terminal
	    
	     	1. PySpark shell:   (type 'pyspark' at your prompt)  
			$ pyspark

		2. Working Jupyter Notebook
			$jupyter notebook   
			$jupyter notebook --allow-root	

			-> This will launch Jupyter notebook. 
			-> There is a "PySparkTest.ipynb". Open it to see how to get started. 

	2. Databricks Comminity Edition
		=> URL: https://databricks.com/try-databricks
		-> Signup using the above URL
		-> Complete the signup process by following the link that is sent to your email. 
  		-> Login to Databricks Cloud:
			Link: https://community.cloud.databricks.com/login.html
		-> Go thorugh "Guide: Quickstart tutorial" link & "Data import" link

       3. Setting up PySpark with Spyder (or Jupyter Notebook) on your local machine.
		-> Install "Anaconda Navigator" first
		-> Try "pip install pyspark" 
		        If this does not work: 
			-> Follow the instructions document" shared in the Github.


    RDD (Resilient Distributed Dataset)
    -----------------------------------

     => RDD is the fundamental data abstraction of Spark Core API

     => Is a collection of distributed in-memory partitions.
	  -> Each partition is a collection of objects.

     -> RDDs are immutable

     -> RDDs are lazily evaluated.
	-> Transformations does not cause execution
	-> Action commands trigger execution

     
    How to Create RDDs ?
    -------------------
	
	Three ways:

	1. Create an RDD from external data files

		rdd1 = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

	2. Create RDDs from programmatic data (python collections)

		rdd10 = sc.parallelize( range(1, 101), 3 )

	3. Apply applying transformations on existing RDDs

		rdd2 = rddFile.map(lambda x: x.upper())
		rdd3 = rdd2.filter(lambda x: len(x) > 51)
	

    What can you do with an RDD ?
    -----------------------------

	Only two things:

	1. Transformations
		-> Cause creation of Lineage DAGs (logical plan) of RDDs
		-> Returns an RDD
		-> Does not cause execution.

	2. Actions
		-> Produces some output
		-> Cause the logical plan to be converted into a plan plan and set of tasks
		   to launched on the cluster.


    RDD Lineage DAG (Logical execution plan)
    -----------------------------------------
	-> Is a set of instruction maintained by driver on how to create the RDD.

	-> Is a logical plan, containing all the dependencies all the way from the very first RDD
           that caused creation of this RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
		Lineage of rddFile: rddFile  -> sc.textFile

	rdd2 = rddFile.map(lambda x: x.upper())
		Lineage of rdd2:  rdd2 -> rddFile.map  -> sc.textFile

	rdd3 = rdd2.filter(lambda x: len(x) > 51)
		Lineage of rdd3:  rdd3  -> rdd2.filter -> rddFile.map  -> sc.textFile

	rdd4 = rdd3.flatMap(lambda x: x.split(" "))
		Lineage of rdd4:  rdd4 -> rdd3.flatMap -> rdd2.filter -> rddFile.map -> sc.textFile
    
	rdd4.collect() => This will cause execution..
		Transformations: sc.textFile -> map -> filter -> flatMap -> rdd4


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD

   RDD persistence
   ---------------

	rdd1 = sc.textFile( ... )
	rdd2 = rdd1.t2( .. )
	rdd3 = rdd1.t3( .. )
	rdd4 = rdd3.t4( .. )
	rdd5 = rdd3.t5( .. )
	rdd6 = rdd5.t6( .. )
	rdd6.persist( StorageLevel.MEMORY_ONLY )          --> instruction to spark to persist the rdd6 partitions. 
	rdd7 = rdd6.t7( .. )

	rdd6.collect() 
	
	rdd6 lineage: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	execution: sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) --> list

	rdd7.collect()

	rdd7 lineage: rdd7 -> rdd6.t7
	execution: (rdd6) --> t7 (rdd7) --> list

	
	Storage Levels
        ---------------
	
	1. MEMORY_ONLY 		-> (not applicable to pyspark) 
				   deserialized, in-memory persistence

	2. MEMORY_AND_DISK	-> (not applicable to pyspark) 
	
	3. DISK_ONLY

        4. MEMORY_ONLY_SER	-> (default in pyspark) serialized, in-memory persistence

	5. MEMORY_AND_DISK_SER

	6. MEMORY_ONLY_2        -> 2 replicas on on two different executors.

	7. MEMORY_AND_DISK_2


	Commands:
        ---------	
	  rdd1.persist()     
	  rdd1.persist( StorageLevel.DISK_ONLY )
	  rdd1.cache()  -> in-memory persistence

	  rdd1.unpersist()

     
   RDD Transformations
   -------------------

   1. map			P: U -> V
				Object to object transformation	
				Input RDD: N objects, OutputRDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).collect()

  2. filter			P: U -> Boolean
				Filters the objects that return True for the function to be in the
				output RDD.
				Input RDD: N objects, OutputRDD: <= N objects

	rddFile.filter(lambda x: len(x) > 51).collect()

  3. glom			P: None
				Returns a list object per partition with all the objects of the partition.

		rdd1			rdd2 = rdd1.glom()

		P0: 5,2,7,3,4,8  -> glom -> P0: [5,2,7,3,4,8]
		P1: 7,5,6,8,2,3  -> glom -> P1: [7,5,6,8,2,3]
		P2: 6,2,3,5,1,0  -> glom -> P2: [6,2,3,5,1,0]

		rdd1.count() = 18 (int)	    rdd2.count() = 3 (list)


   4. flatMap			P: U -> Iterable[V]
				Flattens the elements of the iterable produced by the function.
				Input RDD: N objects, OutputRDD: >= N objects

	       rddWords = rddFile.flatMap(lambda x: x.split(" "))

  
   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				Applies the function on the entire partition.

		rdd1			rdd2 = rdd1.mapPartitions( lambda x :  )

		P0: 5,2,7,3,4,8  -> mapPartitions -> P0: 
		P1: 7,5,6,8,2,3  -> mapPartitions -> P1: 
		P2: 6,2,3,5,1,0  -> mapPartitions -> P2: 

		rdd1.mapPartitions(lambda x : [max(x)]).collect()
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x) ).collect()				


   6. mapPartitionsWithIndex	P: (Int, Iterable[U]) -> Iterable[V]
				Same as mapPartitions, but partition-index as an additional function input
				parameter. 
				
	rdd1.mapPartitionsWithIndex(lambda i, x : [(i, max(x))]).collect()
	rdd1.mapPartitionsWithIndex(lambda pid, pdata: map(lambda a: (pid, a*10), pdata) ).glom().collect()


   7. distinct			P: None, Optional: numPartitions
				Returns an RDD with distinct elements of the input RDD.
				Input RDD: N objects, OutputRDD: <= N objects

		rddWords.distinct().collect()

		
   8. sortBy			P: U -> V, Optional: acsending: True/False,  numPartitions
				Objects of the output RDD are sorted based on the funtion output. 

		rdd1.sortBy(lambda x: x%4).glom().collect()
		rdd1.sortBy(lambda x: x%4, False).glom().collect()
		rdd1.sortBy(lambda x: x%4, True, 4).glom().collect()

   Types of RDDs:

	-> Generic RDDs: RDD[U]
	-> Pair RDDs: RDD[(U, V)]

  9. mapValues			P: U -> V
				Applied only to pair RDDs.
				Transforms only the 'value' part of (k, v) parts by applying the function.

	rdd2.mapValues(lambda x: x*10).collect()

  10. groupBy			P: U -> V, Optional: numPartitions
				Returns a pair RDD, where:
					key: each unique value of the function output
					value: A ResultIterable object with elements of the RDD that produced the key.

		rdd1.groupBy(lambda x : x%2).mapValues(list).glom().collect()
		rdd1.groupBy(lambda x : x%2, 2).mapValues(list).glom().collect()
		
		# wordcount program
		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            		.flatMap(lambda x: x.split(" ")) \
            		.groupBy(lambda x: x) \
            		.mapValues(len) 


   11. randomSplit		P: Ratios (ex: [0.6, 0.4])
				Splits the  RDD randomly into multiple RDDs in the given ratios.

		rddList = rdd1.randomSplit([0.5, 0.5])	
		rddList = rdd1.randomSplit([0.5, 0.5], 456)

   12. repartition		P: numPartitions
				Used to reorganize the number of partitions of the output RDD
				Can increase or decrease the number of partitions
				Causes global shuffle. 

		rdd2 = rdd1.repartition(6)   // rdd2 will have 6 partitions

    13. coalesce		P: numPartitions
				Used to only decrease the number of partitions of the output RDD
				Causes partition merging

		rdd2 = rdd1.coalesce(3)    // rdd2 will have 3 partitions

    14. partitionBy		P: numPartitions, Optional: partitioning function (U -> Int)	
				Applied only to Pair RDDs
				Controls which objects go to which partitions based on the value of 
				partition function that takes the key as input. 
				
		rdd6 = rdd5.partitionBy(4)   // hash of the key is used to map to a partition
		rdd6 = rdd5.partitionBy(4, lambda key :  key + 1 )

    15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions & rdd2 has N partitions

	command				number of output partitions
        ------------------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide
		

   ..ByKey Transformations
   -----------------------
	-> Are all wide transformations
	-> Are applied only to Pair RDDs
	-> Optional Parameter: numPartitions	
			
    16. sortByKey		P: None, Optional: Ascending (True/False), numPartitions
				Sorts the Pair RDD based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()      # desc based on the key
		rddPairs.sortByKey(True, 4).collect()    # asc sort with 4 output partitions


    17. groupByKey		P: None, Optional: numPartitions
				Groups the elements of the RDD by key. Output RDD will have unique keys and
				grouped values.

				NOTE: Avoid it if possible.

				RDD[(U, V)].groupByKey() => RDD[(U, ResultIterable[V])]

		rddPairs.groupByKey().mapValues(sum).collect()
		rddPairs.groupByKey(1).mapValues(sum).collect()    // output will have only one partition.


   18. reduceByKey		 P: (U, U) -> U
				 Reduces all the values of each unique key with in each partition and then
				 across partitions by iterativly applying the reduce function.
				

		rddPairs.reduceByKey(lambda a, b: a + b).collect()
		rddPairs.reduceByKey(lambda a, b: a + b, 3).collect()
	
  RDD Actions
  -----------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce			P: (U, U) -> U
				Reduces the entire RDD into one final value of the same type by iterativly
				applying the reduce function on each partitions first (narrow) and then
				across partitions (wide).

		rdd1:				rdd1.reduce( f1 )  

		P0: 8, 6, 4, 2, 1, 9, 7 -> f1 -> -21    Stage 2: [-21, -21, -17] -> f1 -> 19
		P1: 5, 3, 1, 8, 7, 4, 3 -> f1 -> -21
		P2: 1, 9, 0, 4, 2, 1, 2 -> f1 -> -17
		
		rdd1.reduce(lambda x, y: x - y)

		








