
   Agenda  (7 sessions of 4 hours each)
   ------------------------------------

   	- Spark - Basics & architecture
	- Spark Core API (RDD API) - Low Level API
	    - RDD Transformations & Actions	
	- Spark SQL - DataFrames API
	- Machine Learning & Spark MLlib
	- Introduction Spark Streaming

  ---------------------------------------------------

   What is Big Data ?

	-> Any data that is so big and complex that it become hard to store and process
	   using onhand data management systems and traditional application models.

   
   Computing Cluster
   	-> A unified entity containing a lot of nodes whose cumulative resources can be used
	   distribute storage and process.
   
   Hadoop      
       	-> is an opensource framework for storage and processing of big data.
	-> runs on a cluster made of commodity hardware.

	-> Provides two frameworks for big data

	1. HDFS (Hadoop distributed file system)
		=> Distrinuted Storage Solutions
		-> Splits the file into blocks of 128 MB each.
		-> Spread the bloks of the file across many nodes in the cluster	
		-> Each block is replicated in three different nodes for fail-safety.

	2. MapReduce 
		-> Distributed Processing Solutions
		-> Distributes the processing across many machines (Map phase) and then
		   aggregates the results produced by all these mapper instances (reduce phase) 		    
  
   MapReduce is not good at few use-cases:

	-> Not good with lot of small files
	-> Not good with ad-hoc querying or random-access of data
	-> Not good with iterative computations.

	
   What is Spark ?
   ---------------

	-> Spark is a unified in-memory distributed processing framework
	-> Spark is written in Scala
	-> Spark supports multiple languages: Scala, Java, Python & R
	-> Spark applications can run on mulitple cluster managers
		-> local, Spark Standalone, YARN, Mesos, Kubernetes. 
	
	In-memory framework
	--------------------
	   -> The inetermediate partitions can be persisted in memory and further tasks
	      can be launched on these in-memory persisted partitions.

	Unified framework
	------------------

	   -> Spark provides a cosnistenet set of APIs for different analytics workloads
	      running on the same execution engine. 


		Hadoop Ecosystem
                -----------------
		Batch Analytics of unstructured data  	: MapReduce
		Batch Analytics of Structured data	: Hive, Impala, Drill, HBase
		Streaming (real time) Analytics		: Kafka, Storm, Samza
		Predictive Analytics using ML		: Mahout
		Graph parallel computations		: Giraph

		Spark Framework	
		---------------
		Batch Analytics of unstructured data  	: Spark Core API
		Batch Analytics of Structured data	: Spark SQL
		Streaming (real time) Analytics		: Spark Streaming, Structured Streaming
		Predictive Analytics using ML		: Spark MLlib
		Graph parallel computations		: Spark GraphX

	
	Spark Layered Structure
	-----------------------

	Programming Lang  : Scala, Java, Python, R
	Spark High Level  : Spark SQL, Spark Streaming, Sparl MLlib, Spark GraphX
	Spark Low Level	  : Spark Core API (RDDs)
	Cluster Managers  : Spark Standalone, YARN, Mesos, Kebernetes  
	Storage Layer     : HDFS, Linux, RDBMS, NoSQL, Kafka


   Getting started with Spark
   --------------------------
	Pre-requisite. 
		-> Install Anaconda navigator
			Download and install from the following URL
			https://www.anaconda.com/products/individual-d#windows

	1. Install Spark and run PySpark Shell
		https://spark.apache.org/downloads.html

		Download Spark xxx.tgz file and extract it to a suitable folder.

		Setup your environment varibles:
		-> Add SPARK_HOME & HADOOP_HOME env. variables and point them to Spark installation folder. 
		-> Add bin folder of the spark installation folder to the PATH environment var. 
		-> PYTHONPATH -> %SPARK_HOME%/python;%SPARK_HOME%/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH% 
	 
	2. Using an IDE (such as Spyder or PyCharm)		
		-> Follow the steps mentioned in the document shared with you.

	3. Signup to free Databricks community edition account
		https://databricks.com/try-databricks
		-> Spend some time exploring the Quickstart tutorial


    Spark Architecture
    ------------------
  
     1. Cluster manager (CM)
	-> Jobs are submitted to CM
	-> CM schedules the job and allocates resources to the job

     2. Driver Process
	-> Driver process is the first process that gets created.
	-> Driver process manages the user-code and lauches tasks on the cluster.
	-> Driver contains a "SparkContext" object  (or "SparkSession" in the case of Spark SQL).
	
	Deploy-modes:
	
	1. Client Mode (default) -> the driver process runs on the client machine
	2. Cluster Mode		 -> the driver runs on one the node in the cluster

     3. Executor Processes

	-> Different tasks as per the programming logic are sent to be executed on various
	   executors allocated by the Cm to the application
	-> All tasks does the same process (logic) but of different partitions of data.
	-> report the status of the tasks to the driver

      4. SparkContext
	 -> represents an application contextand connection to the cluster.	
	 -> is the first objects that gets created (starting point of Spark core application).
	 -> is the link between driver and different tasks running on the cluster.



    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
		-> Each partition is a collection of objects	

	-> RDDs are immutable
		-> You can not change the content of a partition.

	-> RDD has two components
		1. RDD lineage DAG -> logical plan that describes how to compute the RDD partitions
		2. RDD DAG	   -> in-memory partitions

	-> RDDs are lazily evaluated
		-> Transformations does not cause execution
		-> Only action commands trigger execution

	-> RDDs are resilient
		-> RDDs are resilient to the missing in-memory partitions. RDDs can recreate
		   such missing partition on the fly and continue the tasks on them. 


   How to create RDDs?
   -------------------

	There are three ways:

	1. We can create RDDs from external data

		rdd1 = sc.textFile( <filePath> )

		-> default number of partitions is decided by a config property of SparkContext
		   called "sc.defaultMinPartitions" whose value is 2 if the number of cores 
		   allocated is atleast 2.

		rdd1 = sc.textFile( filePath, 4 )    # 4 partitions


	2. We can create RDD from programmatic data using parallelize

		rdd1 = sc.parallelize([2,3,2,4,5,6,7,8,9,6,6,7])

		-> default number of partitions is decided by a config property of SparkContext
		   called "sc.defaultParallelism" whose value is equal to the number of cores 
		   allocated to the application.


	3. By applying transformations on an existing RDD

		rdd2 = rdd1.map(lambda x: x.upper())


   What are RDD Lineage DAGs
   -------------------------
	-> Maintained by the driver
	-> Is a DAG of dependencies that caused the creation of the RDD all the way from the 
	   first RDD.

	rdd1 = sc.textFile( filePath )
	Lineage DAG : 	rdd1 -> sc.textFile	

	rdd2 = rdd1.map(lambda x: x.upper())
	Lineage DAG : 	rdd2 -> rdd1.map -> sc.textFile	   

	rdd3 = rdd2.filter(lambda a: len(a) > 50) 
	Lineage DAG : 	rdd3 -> rdd2.filter -> rdd1.map -> sc.textFile	 
	
	rdd4 = rdd3.map(lambda x: len(x))
	Lineage DAG : 	rdd4 -> rdd3.map -> rdd2.filter -> rdd1.map -> sc.textFile
		
	rdd3.collect()  => sc.textFile -> map -> filter -> rdd3
	
   What can you do with an RDD ?
   -----------------------------

	Only two things:

	1. Transformations
		-> Does not cause execution
		-> They do not produce any output. They produce only other RDDs
		-> They cause the creation of lineage DAGs

	2. Actions
		-> Trigger execution
		-> Converts the lineage DAg (logical plan) into a physical execution plan

 

  Types of Transformations
  ------------------------

	Two types:

	1. Narrow transformations		
		-> Data shuffling does not happen
		-> The computation of each partition is dependent on only its input partition
		-> partition to partition tranformations
		-> map, filter, flatmap, glom, mapPartitions, ...

	2. Wide transformations
		-> Data shuffling happens
		-> The computation of each partition dependends on multiple input partitions
		-> output RDD may have different number of partitions than input RDD
		-> distinct, sortBy, groupBy, ...ByKey 


  RDD Persistence
  ---------------
      	rdd1 = sc.textFile( file )
	rdd2 = rdd1.t2(..)	
	rdd3 = rdd1.t3(..)	
	rdd4 = rdd3.t4(..)	
	rdd5 = rdd3.t5(..)	
	rdd6 = rdd5.t6(..)	
	rdd7 = rdd6.t7(..)
	rdd7.persist()         ---> instruction to spark to not GC rdd7 automatically. 	
	rdd8 = rdd7.t8(..)

	rdd7.collect()
	
	Lineage rdd7 => rdd7 -> rdd6.t7	-> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6) -> t7 (rdd7) ---> collect()
	
        rdd8.collect()

	Lineage of rdd8 => rdd8 -> rdd7.t8 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		rdd7.t8 -> rdd8 -> collect()

	
	Persistence   -->  in-memory deserialized
			   in-memory serialized
			   on-disk
	
    Commands
    ---------
	rdd.persist()
	rdd.persist( StorageLevel.MEMORY_AND_DISK )
	rdd.cache()
	rdd.unpersist()  // delete the persisted partitions

   Storage Levels
   --------------
	MEMORY_ONLY (default)   -> stores in RAM as deserialized object
				
	MEMORY_AND_DISK		-> stores in RAM if available, or stores on disk.

	DISK_ONLY

	MEMORY_ONLY_SER		-> stores in RAM in serialized format.

	MEMORY_AND_DISK_SER

	MEMORY_ONLY_2

	MEMORY_AND_DISK_2	
      

  Executor Memory Structure
  --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Clusetr Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 


		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only that portion that used by storage beyond its
 	       3 GB limit. 	


	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Transformations
  -------------------
	-> Transformations does not cause execution.
	-> The output of every transformation is 

   1. map  		=> P: U -> V 
			   Element to element transformation
			   input RDD: N elements, output RDD: N elements 

   2. filter		=> P: U -> Boolean
			   Only those elements for which the function returns True will be
			   in the output partition.
			   input RDD: N elements, output RDD: <= N elements 

   3. glom		=> P: None
			   Transform all the elements of each partition into one array object
			   in the output RDD

	    rdd1		rdd2 = rdd1.glom()
	P0: 1,2,4,3,5,7  --> glom  --> P0: [1,2,4,3,5,7]
	P1: 1,2,8,9,4,7  --> glom  --> P1: [1,2,8,9,4,7]
	P2: 8,8,1,1,3,3  --> glom  --> P2: [8,8,1,1,3,3]		
	rdd1.count = 18			rdd2.count = 3

   4. flatMap		=> P: U -> Iterable[V]
			 Flattens all the elements of the iterables produced by the function.
			 input RDD: N elements, output RDD: >= N elements 

   5. distinct		=> P: None, optional: number of output partitions.
			Return distinct elements from the RDD (removes the dups)
			   
		
   6. mapPartitions	=> P: Iterator[U] -> Iterator[V]
			   The function is applied to the entire partition.

		rdd1		rdd2 = rdd1.mapPartitions( lambda x: ... )

	P0: 1,2,4,3,5,7  --> mapPartitions  --> P0:  2,4,8,6,10,14
	P1: 1,2,8,9,4,7  --> mapPartitions  --> P1: 
	P2: 8,8,1,1,3,3  --> mapPartitions  --> P2: 		
	rdd1.count = 18			


   7. mapPartitionsWithIndex	=> p: Int, Iterator[U] -> Iterator[V]
				The function is applied to the entire partition.

   Two types of RDDs:
	1. Generic RDD: RDD[U]
	2. Pair RDD:	RDD[(U, V)]

	
   8. mapValues		=> P: U -> V
			   Applied only to pair RDDs
			   Transforms the 'value' part of the (K,V) pairs by applying the function.

	rddPairs2 = rddWords.map(lambda x: (x, (x, len(x)))  
		-> where x (fn input) represents the 'value' of the (K, V) pairs


   9. sortBy		=> P: U -> V
			  The objects of the RDD are sorted according the value of the function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
	        rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 10).glom().collect()
		rdd1.sortBy(lambda x: x%3, numPartitions = 10).glom().collect()

   10. groupBy		=> P: U -> V
			   Returns a pair RDD, where the 'key' is the unique value of the function output
			   and 'value' is an iterable (ResultIterable) object containing all RDD objects
			   that produced the same key.

			   RDD[U].groupBy( U -> V, [numPartitions] ) => RDD[(V, ResultIterable[U])]
			
	wordcount solution:
	
	rdd1 = sc.textFile(inputFile) \
        	.flatMap(lambda x: x.split(" ")) \
        	.groupBy(lambda x: x) \
        	.mapValues(len) \
        	.sortBy(lambda x: x[1], False) \		
		.saveAsTextFile("E:\\PySpark\\output\\wc")


   11. randomSplit	=> P: Array of ratios
			   returns an array of RDDs approximatly split randomly in the sepecified ratios.
		
	rddArr = rdd1.randomSplit([0.5, 0.5])
	rddArr = rdd1.randomSplit([0.5, 0.5], 4645 )    // here 4645 is a seed.


   12. repartition	=> P: numberOfPartitions
			  Used to increase or decrease the number of partitions of the output RDD
			  This is important in case of 'narrow' transformation.
			  Results in global shuffle
			  Output partitions are approximatly equally sized.

   13. coalesce		=> P: numberOfPartitions
			  Used to only "decrease" the number of partitions
			  Causes partition-merging (no global shuffle)

   14. partitionBy	=> P: numOfPartitions, optional: partitioningfunction
			  Applied ONLY to pair RDD
			  Is used to control partition arrangement. We can programmatically control
			  which elements go to which partition based the key of the (k,v) pairs.


   15. union, intersection, subtract, cartesian
			=> P: RDD[U].union(RDD[U])
	
  	-> Let us say we have rdd1 with M partitions and rdd2 with N partitions

	command				numPartitions
     	-------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow	
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide


   ..ByKey Transformations
	-> Are wide transformations
	-> Applied ONLY to Pair RDD
	-> They operate on the values of each unique key
		
	
   16. sortByKey		P: ascending, optional: numPartitions
				Sorts the elements of the RDD based on the key
				The the elements with the same key will always be in the same partition.

		rddPairs.sortByKey().glom().collect()   	// asc sort
		rddPairs.sortByKey(false).glom().collect()   	// desc sort
		rddPairs.sortByKey(True, 6).glom().collect() 	// asc with 6 partitions
		
   17. groupByKey		P: None, optional: numPartitions
				Groups the elements based on the key.
				Results in global shuffle and is very inefficient.
				Note: Try to avoid if possible..
  
		rdd1 = sc.textFile(inputFile) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False)

   18. reduceByKey		P: U, U -> U	
				reduces all the values of each unique key by iterativly applying the 
				reduce function within each partition first, and then across partitions.

		rdd1 = sc.textFile(inputFile) \
        		.flatMap(lambda x: x.split(" ")) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)

   19. aggregateByKey

   20. join transformations      => join (inner join), leftOuterJoin, rightOuterJoin, fullOuterJoin
			
				    RDD[(U, V)].join(RDD[(U, W)]) => RDD[(U, (V, W))]		

             		--> rdd1.join(rdd2).join(rdd3).join(rdd4)

   21. cogroup			=> Is used group RDDs that have duplicate keys
				   groupByKey on each RDD -> fullOuterJoin

	[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
	-> [(key1, [10, 7]), (key2, [12, 6]), (key3, [6])]

	[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
	-> [(key1, [5, 17]), (key2, [4,7]), (key4, [17])]

    	[(key1, ([10, 7], [5, 17])), (key2, ([12, 6], [4,7])), (key3, ([6], [])), (key4, ([], [17]))]
		

   Recommendations
   ---------------
	1. Size of the partition : around 128 MB  (100MB to 150MB)
	2. Number of partitions can be upto 2 to 3x the available cores.
	3. If your number of partitions is close to 2000, bump it up to more than 2000

	
  Use-Case (RDD API)
  -------------------
	From cars.tsv dataset give me the average weight of all 'American' makes in the descending order
        of average weight and save the output into a directory as in one output file.

	inputFile = "E:\\PySpark\\data\\cars.tsv"

	def seq_fun(zv, el) :
    		return (zv[0] + el, zv[1] + 1)

	def comb_fun(a, b):
    		return (a[0] + b[0], a[1] + b[1])

	rddOut = sc.textFile(inputFile) \
           .map(lambda s: s.split("\t")) \
           .map(lambda arr: (arr[0], arr[6], arr[9])) \
           .filter(lambda t: t[2] == "American") \
           .map(lambda t: (t[0], int(t[1]))) \
           .aggregateByKey( (0,0), seq_fun, comb_fun) \
           .mapValues(lambda x: x[0]/x[1]) \
           .sortBy(lambda x: x[1], False, 1)

 
  RDD Actions
  ------------
	1. collect	  -> returns an array with all the elements of the RDD
	2. count
	3. saveAsTextFile
	   saveAsSequenceFile -> Only pair RDDs can be saved as sequence files.
	4. take		 -> returns an array with the first N the elements of the RDD
	5. takeOrdered

		rdd1.takeOrdered(4)
		rdd1.takeOrdered(4, lambda x: x%2)

	6. takeSample
	7. countByValue
	8. countByKey	 -> (generally) applied on pair RDDs
	9. first
	10. foreach      -> Takes a function and applies it on all the elements
			    Does not return anything.
	11. reduce		p: U, U -> U
				Reduces the entire RDD into one single value of the same type by iterativly
				applying the reduce function.
				Reduces each partition first (narrow), and then reduces across partitions
	12. aggregate		

		rdd1: 
  		P0: 2, 3, 4, 5   	   -> (14, 4)
  		P1: 6, 7, 3, 4		   -> (20, 4)
		P2: 5, 6, 7, 8		   -> (26, 4)
		P3: 9, 5, 6, 4, 6, 7, 8    -> (45, 7)     
	
	  1. zero-value: ""   (final value is always of the type of zero-value)
	  2. sequence-function: merges all the values within each partition to the zero value.
	  3. combine-function: reduces the values produce for each partition by seq-fn to a 
			       final value of the same type

	 rdd1.aggregate( (0, 0), lambda z, v: (z[0]+v, z[1]+1), lambda a, b : ( a[0] + b[0], a[1] + b[1] ) )



   Closure
   -------
	-> A closure constitute all the code (inclusing external variables and functions) that must
	   be visible for an executor to perform its tasks. 
	-> This closure is serialized and a local copy is sent to every executor.


   Shared Variables
   ----------------

	-> Accumulator
	-> Broadcast

      The problem with local variables:

	count = 0     // count of prime numbers

	def isPrime( n ) :
	   return 1 of n if prime number
	   else return 0

	def f1(n):
	   if (isPrime(n) == 1) count = count + 1
	   return n * 10

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

	print(count)   			
  
	=> Because closures are local copies, the local variable that are part of the closures
	can not be used to implement counter.


    Accumulators:
    -------------

	-> Provide a way to implement global counter
	-> Accumulators are maintained by the driver process.
	-> Accumulators are NOT part of function closure (hense they are not local copies)


	count = sc.accumulator(0)     // count of prime numbers

	def isPrime( n ) :
	   return 1 of n if prime number
	   else return 0

	def f1(n):
	   global count
	   if (isPrime(n) == 1) count.add(1)
	   return n * 10

	rdd1 = sc.parallelize( range(1, 4001), 4 )

	rdd2 = rdd1.map( f1 )

        rdd2.collect()

	print(count)   	


    Broadcast variable
    ------------------
	A copy of a broadcast variables are sent to every executor node and all tasks running in that
	node can refer to the same variable. This saves a lot of execution memory.

	   bcDict1 = sc.broadcast({ 1: a, 2: b, 3: c  })  // 100 MB

	   def f1 (key) :
		global bcDict1 
		dict1 = dcDict1.value
		return dict1[key]

	   rdd1 = sc.parallelize([1,2,3,1,2,3,1,2,3,3,3,2,2], 2)
	
	   rdd2 = rdd1.map(f1)      --> rdd2: [a,b,c,a,b,c,a,b,c,c,c,c,b,b] 


   ==========================
     Spark-submit command
   ===========================

     -> Is a single command to submit any spark application (scala, python, java, R) to 
	any cluster manager (YARN, Mesos, Spark Standaline, local etc..)

	spark-submit --master yarn
		     --deploy-mode cluster
		     --executor-memory 10G
		     --driver-memory 5G
		     --executor-core 5
		      --num-executor 100
		       wordcount.py  <commandline-parameters>

	spark-submit --master local[*] E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt wcout
  















